{
    "abstractText": "Recent progress in deep latent variable models has largely been driven by the development of flexible and scalable variational inference methods. Variational training of this type involves maximizing a lower bound on the log-likelihood, using samples from the variational posterior to compute the required gradients. Recently, Burda et al. (2016) have derived a tighter lower bound using a multi-sample importance sampling estimate of the likelihood and showed that optimizing it yields models that use more of their capacity and achieve higher likelihoods. This development showed the importance of such multisample objectives and explained the success of several related approaches. We extend the multi-sample approach to discrete latent variables and analyze the difficulty encountered when estimating the gradients involved. We then develop the first unbiased gradient estimator designed for importance-sampled objectives and evaluate it at training generative and structured output prediction models. The resulting estimator, which is based on low-variance per-sample learning signals, is both simpler and more effective than the NVIL estimator (Mnih & Gregor, 2014) proposed for the single-sample variational objective, and is competitive with the currently used biased estimators.",
    "authors": [
        {
            "affiliations": [],
            "name": "Andriy Mnih"
        },
        {
            "affiliations": [],
            "name": "Danilo J. Rezende"
        }
    ],
    "id": "SP:bd375a2ba90326c2f18737e1137e983e23889286",
    "references": [
        {
            "authors": [
                "Ba",
                "Jimmy",
                "Salakhutdinov",
                "Ruslan R",
                "Grosse",
                "Roger B",
                "Frey",
                "Brendan J"
            ],
            "title": "Learning wake-sleep recurrent attention models",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2015
        },
        {
            "authors": [
                "Burda",
                "Yuri",
                "Grosse",
                "Roger",
                "Salakhutdinov",
                "Ruslan"
            ],
            "title": "Importance weighted autoencoders",
            "year": 2016
        },
        {
            "authors": [
                "Dauphin",
                "Yann N",
                "Grangier",
                "David"
            ],
            "title": "Predicting distributions with linearizing belief networks",
            "year": 2016
        },
        {
            "authors": [
                "Gregor",
                "Karol",
                "Danihelka",
                "Ivo",
                "Graves",
                "Alex",
                "Rezende",
                "Danilo Jimenez",
                "Wierstra",
                "Daan"
            ],
            "title": "DRAW: A recurrent neural network for image generation",
            "venue": "In Proceedings of the 32nd International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Gu",
                "Shixiang",
                "Levine",
                "Sergey",
                "Sutskever",
                "Ilya",
                "Mnih",
                "Andriy"
            ],
            "title": "MuProp: Unbiased backpropagation for stochastic neural networks",
            "year": 2016
        },
        {
            "authors": [
                "Hinton",
                "Geoffrey E",
                "Dayan",
                "Peter",
                "Frey",
                "Brendan J",
                "Neal",
                "Radford M"
            ],
            "title": "The \"wake-sleep\" algorithm for unsupervised neural networks",
            "year": 1995
        },
        {
            "authors": [
                "Jordan",
                "Michael I",
                "Ghahramani",
                "Zoubin",
                "Jaakkola",
                "Tommi S",
                "Saul",
                "Lawrence K"
            ],
            "title": "An introduction to variational methods for graphical models",
            "venue": "Machine Learning,",
            "year": 1999
        },
        {
            "authors": [
                "Kingma",
                "Diederik",
                "Ba",
                "Jimmy"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "ICLR,",
            "year": 2015
        },
        {
            "authors": [
                "Kingma",
                "Diederik P",
                "Welling",
                "Max"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "ICLR,",
            "year": 2014
        },
        {
            "authors": [
                "Mnih",
                "Andriy",
                "Gregor",
                "Karol"
            ],
            "title": "Neural variational inference and learning in belief networks",
            "venue": "In Proceedings of the 31st International Conference on Machine Learning,",
            "year": 2014
        },
        {
            "authors": [
                "Mnih",
                "Volodymyr",
                "Heess",
                "Nicolas",
                "Graves",
                "Alex"
            ],
            "title": "Recurrent models of visual attention",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2014
        },
        {
            "authors": [
                "Neal",
                "Radford M"
            ],
            "title": "Connectionist learning of belief networks",
            "venue": "Artificial intelligence,",
            "year": 1992
        },
        {
            "authors": [
                "Raiko",
                "Tapani",
                "Berglund",
                "Mathias",
                "Alain",
                "Guillaume",
                "Dinh",
                "Laurent"
            ],
            "title": "Techniques for learning binary stochastic feedforward neural networks",
            "year": 2015
        },
        {
            "authors": [
                "Rezende",
                "Danilo Jimenez",
                "Mohamed",
                "Shakir"
            ],
            "title": "Variational inference with normalizing flows",
            "venue": "In Proceedings of the 32nd International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Rezende",
                "Danilo Jimenez",
                "Mohamed",
                "Shakir",
                "Wierstra",
                "Daan"
            ],
            "title": "Stochastic backpropagation and approximate inference in deep generative models",
            "venue": "In Proceedings of the 31st International Conference on Machine Learning,",
            "year": 2014
        },
        {
            "authors": [
                "Salakhutdinov",
                "Ruslan",
                "Murray",
                "Iain"
            ],
            "title": "On the quantitative analysis of Deep Belief Networks",
            "venue": "In Proceedings of the 25th Annual International Conference on Machine Learning",
            "year": 2008
        },
        {
            "authors": [
                "Salimans",
                "Tim",
                "Kingma",
                "Diederik P",
                "Welling",
                "Max"
            ],
            "title": "Markov chain monte carlo and variational inference: Bridging the gap",
            "venue": "In Proceedings of the 32nd International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Sohn",
                "Kihyuk",
                "Lee",
                "Honglak",
                "Yan",
                "Xinchen"
            ],
            "title": "Learning structured output representation using deep conditional generative models",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2015
        },
        {
            "authors": [
                "Tang",
                "Yichuan",
                "Salakhutdinov",
                "Ruslan R"
            ],
            "title": "Learning stochastic feedforward neural networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2013
        },
        {
            "authors": [
                "Titsias",
                "Michalis",
                "L\u00e1zaro-Gredilla",
                "Miguel"
            ],
            "title": "Local expectation gradients for black box variational inference",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2015
        },
        {
            "authors": [
                "Zaremba",
                "Wojciech",
                "Sutskever",
                "Ilya"
            ],
            "title": "Reinforcement learning neural Turing machines",
            "venue": "arXiv preprint arXiv:1505.00521,",
            "year": 2015
        }
    ],
    "sections": [
        {
            "text": "We extend the multi-sample approach to discrete latent variables and analyze the difficulty encountered when estimating the gradients involved. We then develop the first unbiased gradient estimator designed for importance-sampled objectives and evaluate it at training generative and structured output prediction models. The resulting estimator, which is based on low-variance per-sample learning signals, is both simpler and more effective than the NVIL estimator (Mnih & Gregor, 2014) proposed for the single-sample variational objective, and is competitive with the currently used biased estimators."
        },
        {
            "heading": "1. Introduction",
            "text": "Directed latent variable models parameterized using neural networks have recently enjoyed a surge in popularity due to the recent advances in variational inference methods that made it possible to train such models efficiently. These methods (Kingma & Welling, 2014; Rezende et al.,\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\n2014; Mnih & Gregor, 2014) approximate the intractable posterior of the model with a variational posterior parameterized using a neural network and maximize a lower bound on the intractable marginal log-likelihood, estimating the required gradients using samples from the variational posterior. This approach implements an efficient feedforward approximation to the expensive iterative process required by traditional variational inference methods for each data point.\nOne important weakness of variational methods is that training a powerful model using an insufficiently expressive variational posterior can cause the model to use only a small fraction of its capacity. The most direct route to addressing this issue is to develop more expressive but still tractable variational posteriors as was done in (Salimans et al., 2015; Rezende & Mohamed, 2015; Gregor et al., 2015).\nHowever, the crippling effect of an excessively simple posterior on the model can alternatively be seen as a consequence of the form of the lower bound optimized by the variational methods (Burda et al., 2016). As the bound is based on a single-sample estimate of the marginal likelihood of the observation, it heavily penalizes samples that explain the observation poorly and thus produce low estimates of the likelihood. As result, the variational posterior learns to cover only the high-probability areas of the true posterior, which in turn assumes a simpler shape which is easier to approximate by the variational posterior. A simple way to minimize this effect is to average over multiple samples when computing the marginal likelihood estimate. The resulting lower bound on the log-likelihood gets tighter as the number of samples increases (Burda et al., 2016), converging to the true value in the limit of infinitely many samples. We will refer to such objectives derived from likelihood estimates computed by averaging over independent samples as Monte Carlo objectives. When using an objective that averages over multiple samples, the distribution for generating samples no longer explicitly represents the variational posterior and instead is thought of as a proposal distribution due to connections to importance sampling.\nMulti-sample objectives of this type have been used for\nar X\niv :1\n60 2.\n06 72\n5v 2\n[ cs\n.L G\n] 1\nJ un\n2 01\n6\ngenerative modelling (Bornschein & Bengio, 2015; Burda et al., 2016), structured output prediction (Raiko et al., 2015), and models with hard attention (Ba et al., 2015). As a multi-sample objective is a better proxy for the loglikelihood than a single-sample one, models trained using multi-sample objectives are likely to achieve better loglikelihoods. This has been empirically demonstrated in the context of generative models by Burda et al. (2016) and Bornschein & Bengio (2015), who also showed that using more samples in the objective increased the number of latent variables used in the deeper layers.\nUnfortunately, unless all the latent variables in the model are continuous, learning the proposal distribution with a multi-sample objective is difficult as the gradient estimator obtained by differentiating the objective has very high variance. As a result, with the exception of Burda et al. (2016), who used an alternative estimator available for continuous latent variables, none of the above methods update the parameters of the proposal distribution by following the gradient of the multi-sample objective. Thus, updates for the proposal distribution and the model parameters in these methods are not optimizing the same objective function, which can lead to suboptimal performance and even prevent convergence.\nIn this paper we develop a new unbiased gradient estimator for multi-sample objectives that replaces the single learning signal of the naive estimator with much lower variance per-sample learning signals. Unlike the NVIL estimator (Mnih & Gregor, 2014) designed for single-sample variational objectives, our estimator does not require learning any additional parameters for variance reduction. We expect that the availability of an effective unbiased gradient estimator will make it easier to integrate models with discrete latent variables into larger systems that can be trained end-to-end."
        },
        {
            "heading": "2. Multi-sample stochastic lower bounds",
            "text": ""
        },
        {
            "heading": "2.1. Estimating the likelihood",
            "text": "Suppose we would like to fit an intractable latent variable model P (x, h) to data. As the intractability of inference rules out using maximum likelihood estimation, we will proceed by maximizing a lower bound on the loglikelihood. One general way to derive such a lower bound is to start with an unbiased estimator I\u0302 of the marginal likelihood P (x) and then transform it. We will consider estimators of the form I\u0302(h1:K) = 1K \u2211K i=1 f(x, h\ni) where h1, ..., hK are independent samples from some distribution Q(h|x) which can potentially depend on the observation x. Before showing how to transform such an estimator into a bound, let us consider some possible choices for the likelihood estimator.\nPerhaps the simplest estimator of this form can be constructed by sampling hi\u2019s from the prior P (h) and averaging the resulting conditional likelihoods:\nI\u0302(h1:K) = 1\nK \u2211K i=1 P (x|hi) with hi \u223c P (h). (1)\nWhile this estimator is unbiased, it can have very high variance in models where most latent configurations do not explain a given observation well. For such models, the estimator will greatly underestimate the likelihood for most sets of K independent samples and substantially overestimate it for a small number of such sets. This is a consequence of not taking into account the observation we would like the latent variables to explain when sampling them.\nWe can incorporate the information about the observation we are estimating the likelihood for by sampling the latents from a proposal distribution Q(h|x) conditional on the observation x and using importance sampling:\nI\u0302(h1:K) = 1\nK \u2211K i=1 P (x, hi) Q(hi|x) (2)\nwith h1:K \u223c Q(h1:K |x) \u2261 \u220fK i=1Q(h\ni|x). In addition to also being unbiased, the variance of this estimator can be much lower than that of the preceding one because it can assign high probability to the latent configurations with high joint probability with the given observation. In fact, if we were able to use the true posterior as the proposal distribution, the estimator would have zero variance. While this is infeasible for the models we are considering, this fact suggests that making the proposal distribution close to the posterior is a sensible strategy."
        },
        {
            "heading": "2.2. Lower-bounding the log-likelihood",
            "text": "Having chosen an estimator I\u0302 for the likelihood, we can obtain an estimator L\u0302 of a lower bound on the log-likelihood simply by taking the logarithm of I\u0302 . We can justify this by applying Jensen\u2019s inequality:\nEQ(h1:K |x)\n[ log I\u0302(h1:K) ] \u2264 logEQ(h1:K |x) [ I\u0302(h1:K) ] = logP (x),\nwhere the equality follows from the fact that since I\u0302 is unbiased, EQ(h1:K |x)[I\u0302(h1:K)] = P (x). Therefore, we can think of L\u0302(h1:K) = log I\u0302(h1:K) as a stochastic lower bound on the log-likelihood (Burda et al., 2016).\nWe note that this approach is not specific to the to estimators from Section 2.1 and can be used with any unbiased likelihood estimator based on random sampling. Thus it might be possible to obtain better lower bounds by using methods from the importance sampling literature such as control variates and adaptive importance sampling.\nDespite the potential pitfalls described above, estimators involving sampling from the prior have been used successfully for training models for structured output prediction (Tang & Salakhutdinov, 2013; Dauphin & Grangier, 2016) and models with hard attention (Mnih et al., 2014; Zaremba & Sutskever, 2015).\nThe multi-sample (K > 1) version of the above estimator has recently been used for variational training of latent variable models (Burda et al., 2016; Bornschein & Bengio, 2015) as well as models with hard attention (Ba et al., 2015). The single-sample version of the estimator yields the classical variational lower bound (Jordan et al., 1999)\nL(x) = EQ(h|x) [ log P (x, h)\nQ(h|x)\n] , (3)\nwhich is used as the objective in much of the recent work on training generative models (Kingma & Welling, 2014; Rezende et al., 2014; Mnih & Gregor, 2014).\nThe advantage of using a multi-sample stochastic lower bound is that increasing the number of samples K is guaranteed to make the bound tighter (Burda et al., 2016), thus making it a better proxy for the log-likelihood. Intuitively, averaging over the K samples inside the log, removes the burden of every sample having to explain the observation well, which leads to the proposal distribution being considerably less concentrated than the variational posterior, which is its single-sample counterpart. Training models by optimizing a multi-sample objective can be seen as a generalization of variational training that does not explicitly represent the variational posterior."
        },
        {
            "heading": "2.3. Objective",
            "text": "Thus we will be interested in training models by maximizing objectives of the form\nLK(x) =EQ(h1:K |x) [ log 1\nK \u2211K i=1 f(x, hi) ] , (4)\nwhich can be seen as lower bounds on the log-likelihood. This class of objectives is a rich one, including the ones used in variational inference, generative modelling, structured prediction, and hard attention."
        },
        {
            "heading": "2.4. Gradient analysis",
            "text": "In this section we will analyze the gradient of the objective w.r.t. the parameters of the model and the proposal distribution and explain why developing an effective unbiased estimator for the gradient is difficult in general. In the special case of continuous latent variables an alternative approach to gradient estimation based on reparameterization (Kingma & Welling, 2014; Burda et al., 2016) is likely to be preferable to the more general approach we follow in this paper, which is applicable to all types of latent variables.\nAs shown in the supplementary material, differentiating LK(x) w.r.t. the parameters \u03b8 of Q and f gives\n\u2207\u03b8LK(x) =EQ(h1:K |x) [\u2211\nj L\u0302(h1:K)\u2207\u03b8 logQ(hj |x)\n] +\nEQ(h1:K |x) [\u2211 j w\u0303j\u2207\u03b8 log f(x, hj) ] , (5)\nwhere w\u0303j \u2261 f(x,h j)\u2211K\ni=1 f(x,h i)\n.\nAs our objective LK(x) is an expectation of the stochastic lower bound L\u0302(h1:K) w.r.t. to the proposal distribution, it can depend on any given parameter through the proposal distribution, through the value of the stochastic lower bound as a function of a set of K samples, or both. Intuitively, the first and the second terms in Eq. 5 capture the effect of \u03b8 on LK(x) though its effect on the proposal distribution and the value of the stochastic lower bound as a function of a set of samples respectively.\nLet us inspect these two terms, both of which are linear combinations of the gradients corresponding to theK samples. The second term is well-behaved and is easy to estimate because weights {w\u0303j} are non-negative and sum to 1, ensuring that the norm of the linear combination of the gradients is at most as large as the norm of the largest of theK gradients. In mixture modelling terms, we can think of w\u0303j as the responsibility of sample j for the observation x \u2014 a measure of how well sample j explains the observation compared to the other K \u2212 1 samples.\nThe first term however is considerably more problematic for two reasons. First, the gradients for all K samples are multiplied by the same scalar L\u0302(h1:K), which can be thought of as the learning signal for the proposal distribution (Mnih & Gregor, 2014). As a result, the gradient for a sample that explains the observation well is not given any more weight than the gradient for a sample in the same set of K that explains the observation poorly. This means that the first term does not implement credit assignment within each set ofK samples, unlike the second term which achieves that by weighting the gradients using the responsibilities. Thus the learning signal for each sample hi will have high variance, making learning slow.1\nAnother important source of variance when estimating the first term is the magnitude of the learning signal. Unlike the responsibilities used in the second term, which are between 0 and 1, the learning signal can have potentially unbounded magnitude, which means that the norm of the first term can become much larger than the norm of any of the individual sample gradients. This issue can be especially pronounced early in training, when all samples from the proposal Q explain the data poorly, resulting in a very small I\u0302(h1:K) and\n1 Despite not performing credit assignment within sets of K samples, the first term does perform correct credit assignment in expectation over such sets.\nthus a very negative learning signal. Thus unless special measures are taken, the first term in the gradient will overwhelm the second term and make the overall estimate very noisy."
        },
        {
            "heading": "2.5. Gradient estimation",
            "text": "The difficulties described in the previous section affect only the gradient for the parameters the sampling distribution depends on. For all other parameters \u03c8 the first term is identically zero, which leaves only the second, wellbehaved term. As a result, the following naive Monte Carlo estimator based on a single set of K samples works well:\n\u2207\u03c8LK(x) ' \u2211 j w\u0303 j\u2207\u03c8 log f(x, hj), (6)\nwhere hi \u223c Q(h|x). While it is possible to reduce the variance of the estimator by averaging over multiple sets of samples, in this paper we follow the common practice of using a single set and relying on averaging over the training cases in a minibatch to reduce the variance to a reasonable level instead. We will now turn out attention to the more challenging problem of estimating gradients for parameters that affect the proposal distribution."
        },
        {
            "heading": "2.5.1. NAIVE",
            "text": "We will start with the simplest estimator, also based on naive Monte Carlo:\n\u2207\u03b8LK(x) ' \u2211 j L\u0302(h\n1:K)\u2207\u03b8 logQ(hj |x) + \u2211 j w\u0303 j\u2207\u03b8 log f(x, hj), (7)\nwith hi \u223c Q(h|x). This estimator does not attempt to eliminate either of the two sources of variance described in Section 2.4 and we include it here for completeness only."
        },
        {
            "heading": "2.5.2. WITH BASELINES (NVIL)",
            "text": "One simple way to reduce the variance due the large magnitude of the learning signal is to reduce its magnitude by subtracting a quantity, called a baseline, correlated with the learning signal but not dependent on the latent variables. This transformation of the learning signal leaves the gradient estimator unbiased because it amounts to subtracting a term which has the expectation of 0 under the proposal distribution. In our use of baselines, we will follow the Neural Variational Inference and Learning (NVIL, Mnih & Gregor, 2014) method for training generative models, which is based on optimizing the classical variational lower bound (Eq. 3). The main idea behind the NVIL estimator is to reduce the magnitude of the learning signal for the parameters of the variational distribution (which is the single-sample counterpart of our proposal distribution) by subtracting two baselines from it: a constant baseline b and an input-dependent one b(x).\nThe following estimator is a straightforward adaptation of the same idea to multi-sample objectives:\n\u2207\u03b8LK(x) ' \u2211 j(L\u0302(h\n1:K)\u2212 b(x)\u2212 b)\u2207\u03b8 logQ(hj |x) + \u2211 j w\u0303 j\u2207\u03b8 log f(x, hj), (8)\nwith hi \u223c Q(h|x). The constant baseline b tracks the mean of the learning signal, while the input-dependent one is fit to minimize the squared residual of the learning signal L\u0302(h1:K)\u2212 b(x)\u2212 b, with the goal of capturing the effect of the observation on the magnitude of the learning signal. We implement the input dependent baseline using a one-hidden layer neural network.\nWhile introducing baselines can addresses the estimator variance due to the large magnitude of the learning signal, it has no effect on the variance resulting from having the same learning signal for all samples in a set of K."
        },
        {
            "heading": "2.5.3. PER-SAMPLE LEARNING SIGNALS",
            "text": "We can reduce the effect of the second source of variance by defining a different local learning signal for each sample in a way that minimizes its dependence on the other samples in the set. This can be accomplished by using a separate baseline for each sample that depends on the value of all other samples and eliminates much of the variance due to them. We will now show that this approach does not bias the resulting estimator.\nLet h\u2212j denote the set of K \u2212 1 samples obtained by leaving out sample j from the original set. Since the samples in a set are independent, evaluating the expectations with respect to them in any order produces the same result. Thus the contribution of sample j to the first term in Eq. 5 can be expressed as\nEQ(h1:K |x) [ L\u0302(h1:K)\u2207\u03b8 logQ(hj |x) ] =\nEQ(h\u2212j |x) [ EQ(hj |x) [ L\u0302(h1:K)\u2207\u03b8 logQ(hj |x) \u2223\u2223\u2223h\u2212j]] . Since in the inner-most expectation all samples except for hj are conditioned on, adding any function of them to the learning signal for hj has no effect on the value of the expectation. Thus we can define a baseline that depends on h\u2212j in addition to x. We would like this baseline to be as close to L\u0302(h1:K) as possible without using the value of hj .\nInspecting the global learning signal L\u0302(h1:K) (Eq. 4) suggests that we can obtain an effective baseline for the learning signal for sample j by replacing f(x, hj) in it by some quantity close to it but independent of hj . We could, for example, use some mapping f(x) trained to predict f(x, hi) from the observation x. This gives rise to the following local learning signal for sample j:\nL\u0302(hj |h\u2212j) = L\u0302(h1:K)\u2212 log 1 K (\u2211 i 6=j f(x, hi) + f(x) ) .\nFor K = 1, this estimator becomes essentially equivalent to the NVIL estimator, with log f(x) corresponding to the input-dependent baseline b(x).\nWe can avoid having to learn an additional mapping by taking advantage of the fact that we have more than one sample in a set. Since the samples in a set are IID, so are the corresponding values f(x, hi), which means that we can get a reasonable estimate f\u0302(x, h\u2212j) by combining the f(x, hi) values for all the other samples in the set using averaging of some sort. We experimented with using the arithmetic mean (f\u0302(x, h\u2212j) = 1K\u22121 \u2211 i 6=j f(x, h i)) and the geometric\nmean (f\u0302(x, h\u2212j) = exp (\n1 K\u22121 \u2211 i 6=j log f(x, h i) )\n) and found that the geometric mean worked slightly better. The resulting local learning signals can be written as\nL\u0302(hj |h\u2212j) = (9)\nL\u0302(h1:K)\u2212 log 1 K (\u2211 i 6=j f(x, hi) + f\u0302(x, h\u2212j) ) .\nThis approach to variance reduction, unlike the one above or NVIL, does not require learning any additional parameters for performing variance reduction. Moreover, as the total cost of computing the per-sample learning signals in Eq. 9 is of the same order as that of computing of the global learning signal, this approach allows us to implement effective variance reduction in the multi-sample case essentially at no cost. This approach relies on having more than one sample for the same observation, however, and so is not applicable in the single-sample setting.\nThe final estimator has the form \u2207\u03b8LK(x) ' \u2211\nj L\u0302(hj |h\u2212j)\u2207\u03b8 logQ(hj |x) + \u2211\nj w\u0303j\u2207\u03b8 log f(x, hj). (10)\nWe will refer to this estimator as the VIMCO (Variational Inference for Monte Carlo Objectives) estimator. The pseudocode for computing it is provided in the supplementary material. This estimator is a black-box one, in the sense that it can be easily applied to any model for which we can compute the complete log-likelihood logP (x, h) and its parameter gradients exactly. As such, it can be seen as as an alternative to Black Box Variational Inference (Ranganath et al., 2014) and NVIL, specialized for multi-sample objectives."
        },
        {
            "heading": "3. Structured output prediction",
            "text": "Structured output prediction (SOP) is a type of supervised learning with high-dimensional outputs with rich structure such as images or text. The particular emphasis of SOP is on capturing the dependencies between the output variables in addition to capturing their dependence on the inputs.\nHere we will take the approach of viewing SOP as conditional probabilistic modelling with latent variables (Tang & Salakhutdinov, 2013; Sohn et al., 2015).\nTo stay consistent with the terminology for generative models we used so far, we will refer to inputs as contexts and to outputs as observations Thus, given a set of context/observation pairs (c, x), we would like to fit a latent variable model P (x, h|c) to capture the dependencies between the contexts and the observations, as well as those between the observed dimensions. Typically such a model factorizes as P (x, h|c) = P (x|h, c)P (h|c), with both the conditional likelihood and the prior terms being conditional on the context. Thus, this is essentially the same setting as for generative modelling, with the only difference being that every distribution now also conditions on the context c, which makes it straightforward to apply the estimators we presented.\nHowever, historically such models have been trained using samples from the prior P (h|c), with the gradients computed using either importance sampling (Tang & Salakhutdinov, 2013) or heuristic rules for backpropagating through binary units (Raiko et al., 2015). Since using the prior as the proposal distribution does not allow it to use the information about the observation, such methods tend to require a large number of samples to perform well. Though variational training has been applied recently to SOP models with continuous latent variables (Sohn et al., 2015), we are not aware of any work that uses a learned proposal distribution conditional on the observations to train SOP models with multi-sample objectives. We will explore the effectiveness of using this approach in Section 5.2."
        },
        {
            "heading": "4. Related work",
            "text": "Multi-sample objectives: The idea of using a multisample objective for latent variable models was proposed by Raiko et al. (2015), who thought of it not as a lower bound on the log-likelihood but an objective in its own right. They evaluated several gradient estimators at optimizing it for training structured prediction models and showed that a simple biased estimator emulating backpropagation performed best. Tang & Salakhutdinov (2013) proposed an estimator based on importance sampling for an EM-like bound on the log-likelihood using samples from the prior. This is also a biased estimator as it relies on selfnormalized importance sampling to approximate the posterior using a set of weighted samples. Burda et al. (2016) pointed out that the multi-sample objective of Raiko et al. (2015) was a tighter lower bound on the log-likelihood than the single-sample variational lower bound and presented a method for training variational autoencoders by optimizing this multi-sample objective. Their method relies on an unbiased gradient estimator which can be used only for mod-\nels with continuous latent variables.\nReweighted Wake Sleep: Though the Reweighted Wake Sleep algorithm (RWS, Bornschein & Bengio, 2015) for training generative models has been derived from the perspective of approximating the log-likelihood gradients using importance sampling, it is closely related to the bound optimization approach we follow in this paper. Burda et al. (2016) have shown that the RWS gradient estimator for the model parameters is identical to the one given by Eq. 6, which means that the RWS model parameter update aims to maximize the lower bound on the log-likelihood based on the multi-sample importance sampling estimator from Eq. 2. RWS performs two types of updates for the proposal distribution parameters, the first of which, called the wake update, is based on the same weights {w\u0303j} as the model parameter update\n\u2206\u03b8 \u221d \u2211 j w\u0303 j\u2207\u03b8 logQ(hj |x) (11)\nand is motivated as a (biased) estimator of the gradient KL(P (h|x)||Q(h|x)). Its bias decreases with the increasing number of samples, vanishing in the limit of infinitely many samples.\nThe second update, called the sleep update, having the form\n\u2206\u03b8 \u221d \u2207\u03b8 logQ(h|x), (12)\nis based on a sample (x, h) from the model and comes from the original Wake-Sleep algorithm (Hinton et al., 1995). The wake update tends to work better than the sleep update, and using the two updates together works even better (Bornschein & Bengio, 2015). As neither of these updates appears to be related to the lower bound optimized the model parameter update, RWS does not seem to optimize a well-defined objective, a feature it shares with the original Wake-Sleep algorithm. Despite this theoretical weakness RWS works well in practice, outperforming original WakeSleep and NVIL, which are single-sample algorithms, using as few as 5 samples per observation.\nBlack Box Methods: As our approach does not assume anything about the structure of the model or the distribution(s) of its latent variables, it can be seen as a black box method for multi-sample objectives. A number of black box methods have been developed for the classical variational objective, usually based around unbiased gradient estimators for the proposal distribution. Black Box Variational Inference (BBVI Ranganath et al., 2014) and NVIL (Mnih & Gregor, 2014) are two such methods.\nVIMCO shares some similarities with the black box method of the local expectations (LE) of Titsias & L\u00e1zaroGredilla (2015). The LE method provides a relatively low variance unbiased estimator based on local learning signals derived from computing an exact expectation w.r.t. each\nvariable in the model. Both methods work well without baselines and involve considering multiple values for latent variables. Unlike VIMCO, the LE method optimizes a single-sample objective and requires computing exact expectations for each variable, which makes it much more computationally expensive."
        },
        {
            "heading": "5. Results",
            "text": "We evaluate the effectiveness of the proposed approach at training models for generative modelling and structured output prediction. We chose these two tasks because they involve models with hundreds of latent variables, which poses formidable challenges when estimating the gradients for the proposal distributions. In both cases we compare the performance of the VIMCO estimator to that of the NVIL estimator as well as to an effective biased estimator from the literature. We experiment with varying the number of samples in the objective to see how that affects the performance of the resulting models when using different estimators. The details of the training procedure are given in the supplementary material."
        },
        {
            "heading": "5.1. Generative modelling",
            "text": "We start by applying the proposed estimator to training generative models, concentrating on sigmoid belief networks (SBN) (Neal, 1992) which consist of layers of binary latent variables. SBNs have been used to evaluate a number of variational training methods for models with discrete latent variables (Mnih & Gregor, 2014; Bornschein & Bengio, 2015; Gu et al., 2016).\nOur first comparison is on the MNIST dataset of 28 \u00d7 28 images of handwritten digits, using the binarization of Salakhutdinov & Murray (2008) and the standard 50000/10000/10000 split into the training, validation, and test sets. We use an SBN with three hidden layers of 200 binary latent variables (200-200-200-768) as the generative model. The proposal distribution is parameterized as an SBN with the same architecture but going in the opposite direction, from the observation to the deepest hidden layer (768-200-200-200).\nAs our primary goal is here is to see how well the VIMCO estimator performs at optimizing the multisample objective, we train the above model using each of the VIMCO, NVIL, and RWS estimators to optimize the lower bound (Eq. 4) based on 2, 5, 10, and 50 samples (K). To match the computational complexity of the other two estimators, we used only the better-performing wake update for the proposal distribution in RWS. We also trained the model by optimizing the classical variational objective (K = 1) using NVIL to serve as a single-sample baseline. In all cases, the model parameter gradients were estimated using Eq. 6.\nFigure 1 shows the evolution of the training objective on the validation set as training proceeds. From the left plot, which compares the models trained using VIMCO to those trained NVIL, it is apparent that VIMCO is far more effective than NVIL at optimizing the multi-sample objective and benefits much more from using more samples. NVIL performance improves slightly when using a modest number of samples before starting to degrade upon reaching K = 10. The right plot shows the comparison between VIMCO and RWS. The two methods perform similarly, with VIMCO performing better when using 2 samples and RWS learning slightly faster when using more samples.\nHaving selected the best model for each method/number of samples combination based on its validation score, we estimated its negative log-likelihood on the test set using 1000 proposal samples for each data point. The results in Table 1 show that VIMCO and NVIL perform slightly better than RWS for 2 samples. However, as the number of samples increases, VIMCO and RWS performance steadily improves while NVIL performance stays virtually the same until reachingK = 50, when it becomes markedly worse. Overall, RWS and VIMCO perform similarly, though VIMCO seems to have a slight edge over RWS for all numbers of samples we considered.\nWe also investigated the effectiveness of VIMCO and NVIL variance reduction techniques more directly, by monitoring the magnitude of their learning signals during training. While VIMCO and NVIL performed comparably when using the 2-sample objective, VIMCO benefited much more from using more samples. For the 10-sample objective, the average magnitude of the VIMCO learning signal was 3 times lower than that of NVIL. More details are given in the supplementary material."
        },
        {
            "heading": "5.2. Structured output prediction",
            "text": "In the second set of experiments we evaluated the proposed estimator at training structured output prediction models. We chose a task that has been used as a benchmark for evaluating gradient estimators for models with binary latent variables by Raiko et al. (2015) and Gu et al. (2016), which involves predicting the lower half of an MNIST digit from its top half. We trained two SBN models, one with two and one with three layers of 200 binary latent variables between the 392-dimensional (14\u00d7 28) input and output layers. We use the same binarized MNIST dataset for this task as for the generative modelling experiments in Section 5.1.\nWe consider two different kinds of proposal distributions for training the models. In the first case, we follow the standard practice for training structured output prediction models and use the model prior as the proposal distribution. However, as the prior does not have access to the observation information which is available during training, most of the resulting samples are unlikely to explain the observation well, potentially leading to inefficient use of samples and unnecessarily noisy learning signal. Hence, in the second case we learn a separate proposal distribution that takes both the context and the observation halves of the image as input. We parameterize the proposal distribution using an SBN with the same structure as the prior except that the last\nlayer of latent variables in addition to being conditioned on the preceding layer is also conditioned on the observation.\nWe train the models with VIMCO and NVIL using 2, 5, 20, and 50 sample objectives. As in the previous experiment, we also train single-sample baseline models using both types of proposals with NVIL (and K = 1). Figure 2 shows the resulting multi-sample bound values for the three-layer models on the validation set as a function of the number of parameter updates. The left plot, containing the results for models trained by sampling from the prior, shows that model performance improves dramatically as the number of samples is increased. Though NVIL with 1 or 2 samples, performs better than VIMCO with 2 samples, as the number of samples increases their roles reverse, with VIMCO making much faster progress than NVIL for 20 and 50 samples. The fact that increasing the number of samples has such an effect on model performance strongly suggests that samples generated from the prior rarely explain the observation well.\nThe right plot on Figure 2 shows the result of training with a learned proposal distribution. It is clear that using a learned proposal leads to drastic improvement for all method / number of samples combinations. In fact, the worst result obtained using a learned proposal distribution is better than the best result obtained by sampling from the prior. In terms of relative performance, the story here is similar to that from the generative modelling experiment: VIMCO performs better than NVIL and benefits much more from increasing the number of samples. The gap between the methods is considerably smaller here, likely due to the task being easier. Inspecting the conditional digit completions sampled from the models shows that the models trained using a learned proposal distribution capture multimodality inherent in the task very well. We show conditional completions from a three-layer model trained using VIMCO\nwith 20 samples in the supplementary material.\nFinally, to compare to the results of Raiko et al. (2015), we followed their evaluation protocol and estimated the negative log-likelihoods for the trained models using 100 samples. Their best result on this task was 53.8 nats, obtained using a 2-layer SBN trained using a biased estimator emulating backprop to optimize the 20-sample objective. With VIMCO training, the same model achieves 56.5 nats using the prior as the proposal and 46.1 nats with a learned proposal, which is the first sub-50 nat result on this task."
        },
        {
            "heading": "6. Discussion",
            "text": "In this paper we introduced VIMCO, the first unbiased general gradient estimator designed specifically for multisample objectives that generalize the classical variational lower bound. By taking advantage of the structure of the objective function, it implements simple and effective variance reduction at no extra computational cost, eliminating the need for the learned baselines relied on by other general unbiased estimators such as NVIL.\nWe demonstrated the effectiveness of VIMCO by applying it to variational training of generative and structured output prediction models. It consistently outperformed NVIL and was competitive with the currently used biased estimators.\nWhile classical variational methods can perform poorly when using an insufficiently expressive variational posterior, multi-sample objectives provide a graceful way of trading computation for quality of fit simply by increasing the number of samples used inside the objective. Combining such objectives with black box variational inference methods could make the latter substantially more effective. We thus hope that the proposed approach will increase the appeal and applicability of black box variational inference."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "We thank Alex Graves, Guillaume Desjardins, Koray Kavukcuoglu, Volodymyr Mnih, Hugo Larochelle, and M\u00e9lanie Rey for their helpful comments."
        },
        {
            "heading": "A. Algorithm for computing VIMCO gradients",
            "text": "Algorithm 1 provides an outline of our implementation of VIMCO gradient computation for a single training case. This version uses the geometric mean to estimate f(x, hj) from the other K \u2212 1 terms. The computations are performed in the log domain for better numerical stability.\nAlgorithm 1 Compute gradient estimates for the model and proposal distribution parameters for a single observation Require: x , K \u2265 2\nfor i = 1 to K do hi \u223c Q(h|x) l[i] = log f(x, hi) end for {Compute the multi-sample stochastic bound} L\u0302 = LogSumExp(l)\u2212 logK {Precompute the sum of log f} s = Sum(l) {Compute the baseline for each sample} for i = 1 to K do\n{Save the current log f for future use and replace it} {with the average of the other K-1 log f terms} temp = l[i] l[i] = (s\u2212 l[i])/(K \u2212 1) L\u0302\u2212i = LogSumExp(l)\u2212 logK l[i] = temp {Restore the saved value}\nend for w = SoftMax(l) {Compute the importance weights} \u2207\u03b8 = 0,\u2207\u03c8 = 0 {Sum the gradient contributions from the K samples} for i = 1 to K do\n{Proposal distribution gradient contributions} \u2207\u03b8 = \u2207\u03b8 + (L\u0302\u2212 L\u0302\u2212i)\u2207\u03b8 logQ(hi|x) \u2207\u03b8 = \u2207\u03b8 + w[i]\u2207\u03b8 log f(x, hi) {Model gradient contribution} \u2207\u03c8 = \u2207\u03c8 + w[i]\u2207\u03c8 log f(x, hi)\nend for"
        },
        {
            "heading": "B. Details of the experimental protocol",
            "text": "All models were trained using the Adam optimizer (Kingma & Ba, 2015) with minibatches of size 24. The input to the proposal distribution/inference network was centered by subtracting the mean. For each training method/number of samples combination we trained the model several times using different learning rates, saving the model with the best validation score achieved during each training run. The plots and the scores shown in the paper were obtained from the saved model with the highest validation score. For generative training, we considered the learning rates of {3\u00d7 10\u22124, 1\u00d7 10\u22123, 3\u00d7 10\u22123}. For the structured output prediction experiments, the learning rates\nwere {3\u00d710\u22124, 1\u00d710\u22123, 3\u00d710\u22123} for VIMCO and RWS and {1\u00d7 10\u22124, 3\u00d7 10\u22124, 1\u00d7 10\u22123} for NVIL.\nOur NVIL implementation used both constant and inputdependent baselines as well as variance normalization. The input-dependent baseline for NVIL was a neural network with one hidden layer of 100 tanh units. VIMCO used the geometric mean for computing the per-sample learning signals."
        },
        {
            "heading": "C. Effect of variance reduction on the learning signal",
            "text": "As explained in Sections 2.4 and 2.5, the magnitude of the learning signal used for learning the proposal distribution parameters is closely related to the variance of the resulting gradient estimator. Both VIMCO and NVIL aim to reduce the estimator variance by subtracting a baseline from the original learning signal L\u0302(h1:K) in order to reduce its magnitude, while keeping the estimator unbiased. We examined the effectiveness of these two approaches by plotting a smoothed estimate of the magnitude of the resulting learning signal (L\u0302(hj |h\u2212j) for VIMCO and L\u0302(h1:K)\u2212 b(x)\u2212 b for NVIL) as a function of the number of parameter updates when training the SBN on MNIST in Section 5.1. The magnitude of the learning signal was estimated by taking the square root of the mean of the squared signal values for each minibatch. The results for different numbers of samples shown in Figure 3 suggest that while VIMCO and NVIL are equally effective at reducing variance when using a 2-sample objective, VIMCO becomes much more effective than NVIL when using more than 2 samples. For 10 samples, the average magnitude of the learning signal for VIMCO is about 3 times lower than for NVIL, which suggests almost an order of magnitude lower variance of the gradient estimates."
        },
        {
            "heading": "D. Gradient derivation for the multi-sample objective",
            "text": "In this section we will derive the gradient for the multisample objective\nLK(x) =EQ(h1:K |x) [ L\u0302(h1:K) ] =EQ(h1:K |x) [ log I\u0302(h1:K)\n] =EQ(h1:K |x) [ log 1\nK K\u2211 i=1 f(x, hi)\n] .\nWe start by using the product rule:\n\u2207\u03b8LK(x) =\u2207\u03b8EQ(h1:K |x) [ L\u0302(h1:K) ] =\u2207\u03b8\n\u2211 h1:K Q(h1:K |x)L\u0302(h1:K)\n= \u2211 h1:K [ L\u0302(h1:K)\u2207\u03b8Q(h1:K |x)+\nQ(h1:K |x)\u2207\u03b8L\u0302(h1:K) ] . (13)\nUsing the identity \u2207\u03b8g(x) = g(x)\u2207\u03b8 log g(x), we can express the gradient of Q(h1:K |x) as\n\u2207\u03b8Q(h1:K |x) =Q(h1:K |x)\u2207\u03b8 logQ(h1:K |x)\n=Q(h1:K |x)\u2207\u03b8 log K\u220f j=1 Q(hj |x) =Q(h1:K |x) K\u2211 j=1 \u2207\u03b8 logQ(hj |x). (14)\nWe use the chain rule along with the same identity to compute the gradient of L\u0302(h1:K):\n\u2207\u03b8L\u0302(h1:K) =\u2207\u03b8 log 1\nK K\u2211 j=1 f(x, hj)\n= 1\u2211K\ni=1 f(x, h i) K\u2211 j=1 \u2207\u03b8f(x, hj)\n= 1\u2211K\ni=1 f(x, h i) K\u2211 j=1 f(x, hj)\u2207\u03b8 log f(x, hj)\n= K\u2211 j=1 w\u0303j\u2207\u03b8 log f(x, hj) (15)\nwhere w\u0303j \u2261 f(x,h j)\u2211K\ni=1 f(x,h i)\n. Substituting Eq. 14 and Eq. 15\ninto Eq. 13 we obtain\n\u2207\u03b8LK(x) = \u2211 h1:K ( L\u0302(h1:K)Q(h1:K |x) K\u2211 j=1 \u2207\u03b8 logQ(hj |x)+\nQ(h1:K |x) K\u2211 j=1 w\u0303j\u2207\u03b8 log f(x, hj) ) ,\n= \u2211 h1:K Q(h1:K |x)L\u0302(h1:K) K\u2211 j=1 \u2207\u03b8 logQ(hj |x)+\n\u2211 h1:K Q(h1:K |x) K\u2211 j=1 w\u0303j\u2207\u03b8 log f(x, hj),\n=EQ(h1:K |x) \u2211 j L\u0302(h1:K)\u2207\u03b8 logQ(hj |x) + EQ(h1:K |x) \u2211 j w\u0303j\u2207\u03b8 log f(x, hj)\n . (16)"
        },
        {
            "heading": "E. Structured output prediction: digit completions",
            "text": "Figure 4 shows multiple completions for the same set of top digit image halves generated using a three-layer (200- 200-200) SBN trained using VIMCO with the 20-sample objective. The completions were obtained by computing observation probabilities based on a single sample from the prior. The variability of the completions shows how the model captured the multimodality of the data distribution."
        }
    ],
    "title": "Variational Inference for Monte Carlo Objectives",
    "year": 2016
}