{
    "abstractText": "Problems at the intersection of vision and language are of significant importance both as challenging research questions and for the rich set of applications they enable. However, inherent structure in our world and bias in our language tend to be a simpler signal for learning than visual modalities, resulting in models that ignore visual information, leading to an inflated sense of their capability. We propose to counter these language priors for the task of Visual Question Answering (VQA) and make vision (the V in VQA) matter! Specifically, we balance the popular VQA dataset [3] by collecting complementary images such that every question in our balanced dataset is associated with not just a single image, but rather a pair of similar images that result in two different answers to the question. Our dataset is by construction more balanced than the original VQA dataset and has approximately twice the number of image-question pairs. Our complete balanced dataset is publicly available at http://visualqa.org/ as part of the 2nd iteration of the Visual Question Answering Dataset and Challenge (VQA v2.0). We further benchmark a number of state-of-art VQA models on our balanced dataset. All models perform significantly worse on our balanced dataset, suggesting that these models have indeed learned to exploit language priors. This finding provides the first concrete empirical evidence for what seems to be a qualitative sense among practitioners. Finally, our data collection protocol for identifying complementary images enables us to develop a novel interpretable model, which in addition to providing an answer to the given (image, question) pair, also provides a counterexample based explanation. Specifically, it identifies an image that is similar to the original image, but it believes has a different answer to the same question. This can help in building trust for machines among their users. \u2217The first two authors contributed equally. Who is wearing glasses? Where is the child sitting? Is the umbrella upside down? How many children are in the bed? woman man arms fridge",
    "authors": [
        {
            "affiliations": [],
            "name": "Yash Goyal"
        },
        {
            "affiliations": [],
            "name": "Tejas Khot"
        },
        {
            "affiliations": [],
            "name": "Douglas Summers-Stay"
        },
        {
            "affiliations": [],
            "name": "Dhruv Batra"
        },
        {
            "affiliations": [],
            "name": "Devi Parikh"
        }
    ],
    "id": "SP:aaf8aa27defb81509a26ecab207e22ab6df19a96",
    "references": [
        {
            "authors": [
                "A. Agrawal",
                "D. Batra",
                "D. Parikh"
            ],
            "title": "Analyzing the Behavior of Visual Question Answering Models",
            "venue": "EMNLP,",
            "year": 2016
        },
        {
            "authors": [
                "J. Andreas",
                "M. Rohrbach",
                "T. Darrell",
                "D. Klein"
            ],
            "title": "Deep compositional question answering with neural module networks",
            "venue": "CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "S. Antol",
                "A. Agrawal",
                "J. Lu",
                "M. Mitchell",
                "D. Batra",
                "C.L. Zitnick",
                "D. Parikh"
            ],
            "title": "VQA: Visual Question Answering",
            "venue": "ICCV,",
            "year": 2015
        },
        {
            "authors": [
                "X. Chen",
                "C.L. Zitnick"
            ],
            "title": "Mind\u2019s Eye: A Recurrent Visual Representation for Image Caption Generation",
            "venue": "CVPR,",
            "year": 2015
        },
        {
            "authors": [
                "J. Deng",
                "W. Dong",
                "R. Socher",
                "L.-J. Li",
                "K. Li",
                "L. Fei-Fei"
            ],
            "title": "ImageNet: A Large-Scale Hierarchical Image Database",
            "venue": "CVPR,",
            "year": 2009
        },
        {
            "authors": [
                "J. Devlin",
                "S. Gupta",
                "R.B. Girshick",
                "M. Mitchell",
                "C.L. Zitnick"
            ],
            "title": "Exploring nearest neighbor approaches for image captioning",
            "venue": "CoRR, abs/1505.04467,",
            "year": 2015
        },
        {
            "authors": [
                "J. Donahue",
                "L.A. Hendricks",
                "S. Guadarrama",
                "M. Rohrbach",
                "S. Venugopalan",
                "K. Saenko",
                "T. Darrell"
            ],
            "title": "Long-term Recurrent Convolutional Networks for Visual Recognition and Description",
            "venue": "CVPR,",
            "year": 2015
        },
        {
            "authors": [
                "H. Fang",
                "S. Gupta",
                "F.N. Iandola",
                "R. Srivastava",
                "L. Deng",
                "P. Doll\u00e1r",
                "J. Gao",
                "X. He",
                "M. Mitchell",
                "J.C. Platt",
                "C.L. Zitnick",
                "G. Zweig"
            ],
            "title": "From Captions to Visual Concepts and Back",
            "venue": "CVPR,",
            "year": 2015
        },
        {
            "authors": [
                "A. Fukui",
                "D.H. Park",
                "D. Yang",
                "A. Rohrbach",
                "T. Darrell",
                "M. Rohrbach"
            ],
            "title": "Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding",
            "venue": "EMNLP,",
            "year": 2016
        },
        {
            "authors": [
                "H. Gao",
                "J. Mao",
                "J. Zhou",
                "Z. Huang",
                "A. Yuille"
            ],
            "title": "Are you talking to a machine? dataset and methods for multilingual image question answering",
            "venue": "NIPS,",
            "year": 2015
        },
        {
            "authors": [
                "Y. Goyal",
                "A. Mohapatra",
                "D. Parikh",
                "D. Batra"
            ],
            "title": "Towards Transparent AI Systems: Interpreting Visual Question Answering Models",
            "venue": "ICML Workshop on Visualization for Deep Learning,",
            "year": 2016
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "L.A. Hendricks",
                "Z. Akata",
                "M. Rohrbach",
                "J. Donahue",
                "B. Schiele",
                "T. Darrell"
            ],
            "title": "Generating visual explanations",
            "venue": "ECCV,",
            "year": 2016
        },
        {
            "authors": [
                "M. Hodosh",
                "J. Hockenmaier"
            ],
            "title": "Focused evaluation for image description with binary forced-choice tasks",
            "venue": "Workshop on Vision and Language, Annual Meeting of the Association for Computational Linguistics,",
            "year": 2016
        },
        {
            "authors": [
                "I. Ilievski",
                "S. Yan",
                "J. Feng"
            ],
            "title": "A focused dynamic attention model for visual question answering",
            "venue": "CoRR, abs/1604.01485,",
            "year": 2016
        },
        {
            "authors": [
                "A. Jabri",
                "A. Joulin",
                "L. van der Maaten"
            ],
            "title": "Revisiting Visual Question Answering Baselines",
            "venue": "In ECCV,",
            "year": 2016
        },
        {
            "authors": [
                "K. Kafle",
                "C. Kanan"
            ],
            "title": "Answer-type prediction for visual question answering",
            "venue": "CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "K. Kafle",
                "C. Kanan"
            ],
            "title": "Visual Question Answering: Datasets, Algorithms, and Future Challenges",
            "venue": "CoRR, abs/1610.01465,",
            "year": 2016
        },
        {
            "authors": [
                "A. Karpathy",
                "L. Fei-Fei"
            ],
            "title": "Deep Visual-Semantic Alignments for Generating Image Descriptions",
            "venue": "CVPR,",
            "year": 2015
        },
        {
            "authors": [
                "J.-H. Kim",
                "S.-W. Lee",
                "D.-H. Kwak",
                "M.-O. Heo",
                "J. Kim",
                "J.- W. Ha",
                "B.-T. Zhang"
            ],
            "title": "Multimodal Residual Learning for Visual QA",
            "venue": "In NIPS,",
            "year": 2016
        },
        {
            "authors": [
                "R. Kiros",
                "R. Salakhutdinov",
                "R.S. Zemel"
            ],
            "title": "Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models",
            "venue": "TACL,",
            "year": 2015
        },
        {
            "authors": [
                "R. Krishna",
                "Y. Zhu",
                "O. Groth",
                "J. Johnson",
                "K. Hata",
                "J. Kravitz",
                "S. Chen",
                "Y. Kalantidis",
                "L.-J. Li",
                "D.A. Shamma"
            ],
            "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
            "venue": "arXiv preprint arXiv:1602.07332,",
            "year": 2016
        },
        {
            "authors": [
                "T.-Y. Lin",
                "M. Maire",
                "S. Belongie",
                "J. Hays",
                "P. Perona",
                "D. Ramanan",
                "P. Doll\u00e1r",
                "C.L. Zitnick"
            ],
            "title": "Microsoft COCO: Common Objects in Context",
            "venue": "ECCV,",
            "year": 2014
        },
        {
            "authors": [
                "J. Lu",
                "X. Lin",
                "D. Batra",
                "D. Parikh"
            ],
            "title": "Deeper LSTM and normalized CNN Visual Question Answering model",
            "venue": "https://github.com/VT-vision-lab/ VQA_LSTM_CNN,",
            "year": 2015
        },
        {
            "authors": [
                "J. Lu",
                "J. Yang",
                "D. Batra",
                "D. Parikh"
            ],
            "title": "Hierarchical Question-Image Co-Attention for Visual Question Answering",
            "venue": "NIPS,",
            "year": 2016
        },
        {
            "authors": [
                "M. Malinowski",
                "M. Fritz"
            ],
            "title": "A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input",
            "venue": "NIPS,",
            "year": 2014
        },
        {
            "authors": [
                "M. Malinowski",
                "M. Rohrbach",
                "M. Fritz"
            ],
            "title": "Ask your neurons: A neural-based approach to answering questions about images",
            "venue": "ICCV,",
            "year": 2015
        },
        {
            "authors": [
                "J. Mao",
                "W. Xu",
                "Y. Yang",
                "J. Wang",
                "A.L. Yuille"
            ],
            "title": "Explain Images with Multimodal Recurrent Neural Networks",
            "venue": "NIPS,",
            "year": 2014
        },
        {
            "authors": [
                "H. Noh",
                "B. Han"
            ],
            "title": "Training recurrent answering units with joint loss minimization for vqa",
            "venue": "CoRR, abs/1606.03647,",
            "year": 2016
        },
        {
            "authors": [
                "A. Ray",
                "G. Christie",
                "M. Bansal",
                "D. Batra",
                "D. Parikh"
            ],
            "title": "Question Relevance in VQA: Identifying Non-Visual And False-Premise Questions",
            "venue": "EMNLP,",
            "year": 2016
        },
        {
            "authors": [
                "M. Ren",
                "R. Kiros",
                "R. Zemel"
            ],
            "title": "Exploring models and data for image question answering",
            "venue": "NIPS,",
            "year": 2015
        },
        {
            "authors": [
                "M.T. Ribeiro",
                "S. Singh",
                "C. Guestrin"
            ],
            "title": "Why Should I Trust You?\u201d: Explaining the Predictions of Any Classifier",
            "venue": "Knowledge Discovery and Data Mining (KDD),",
            "year": 2016
        },
        {
            "authors": [
                "K. Saito",
                "A. Shin",
                "Y. Ushiku",
                "T. Harada"
            ],
            "title": "Dualnet: Domain-invariant network for visual question answering",
            "venue": "CoRR, abs/1606.06108,",
            "year": 2016
        },
        {
            "authors": [
                "R.R. Selvaraju",
                "A. Das",
                "R. Vedantam",
                "M. Cogswell",
                "D. Parikh",
                "D. Batra"
            ],
            "title": "Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradientbased Localization",
            "venue": "arXiv preprint arXiv:1610.02391,",
            "year": 2016
        },
        {
            "authors": [
                "K.J. Shih",
                "S. Singh",
                "D. Hoiem"
            ],
            "title": "Where to look: Focus regions for visual question answering",
            "venue": "CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "A. Shin",
                "Y. Ushiku",
                "T. Harada"
            ],
            "title": "The Color of the Cat is Gray: 1 Million Full-Sentences Visual Question Answering (FSVQA)",
            "venue": "arXiv preprint arXiv:1609.06657,",
            "year": 2016
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "ICLR,",
            "year": 2015
        },
        {
            "authors": [
                "M. Tapaswi",
                "Y. Zhu",
                "R. Stiefelhagen",
                "A. Torralba",
                "R. Urtasun",
                "S. Fidler"
            ],
            "title": "MovieQA: Understanding Stories in Movies through Question-Answering",
            "venue": "CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "A. Torralba",
                "A. Efros"
            ],
            "title": "Unbiased look at dataset bias",
            "venue": "CVPR,",
            "year": 2011
        },
        {
            "authors": [
                "O. Vinyals",
                "A. Toshev",
                "S. Bengio",
                "D. Erhan"
            ],
            "title": "Show and Tell: A Neural Image Caption Generator",
            "venue": "CVPR,",
            "year": 2015
        },
        {
            "authors": [
                "P. Wang",
                "Q. Wu",
                "C. Shen",
                "A. van den Hengel",
                "A.R. Dick"
            ],
            "title": "Explicit knowledge-based reasoning for visual question answering",
            "venue": "CoRR, abs/1511.02570,",
            "year": 2015
        },
        {
            "authors": [
                "Q. Wu",
                "P. Wang",
                "C. Shen",
                "A. van den Hengel",
                "A.R. Dick"
            ],
            "title": "Ask me anything: Free-form visual question answering based on knowledge from external sources",
            "venue": "In CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "C. Xiong",
                "S. Merity",
                "R. Socher"
            ],
            "title": "Dynamic memory networks for visual and textual question answering",
            "venue": "ICML,",
            "year": 2016
        },
        {
            "authors": [
                "H. Xu",
                "K. Saenko"
            ],
            "title": "Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering",
            "venue": "ECCV,",
            "year": 2016
        },
        {
            "authors": [
                "Z. Yang",
                "X. He",
                "J. Gao",
                "L. Deng",
                "A. Smola"
            ],
            "title": "Stacked Attention Networks for Image Question Answering",
            "venue": "CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "L. Yu",
                "E. Park",
                "A.C. Berg",
                "T.L. Berg"
            ],
            "title": "Visual Madlibs: Fill-in-the-blank Description Generation and Question Answering",
            "venue": "ICCV,",
            "year": 2015
        },
        {
            "authors": [
                "P. Zhang",
                "Y. Goyal",
                "D. Summers-Stay",
                "D. Batra",
                "D. Parikh"
            ],
            "title": "Yin and Yang: Balancing and Answering Binary Visual Questions",
            "venue": "CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "B. Zhou",
                "A. Khosla",
                "A. Lapedriza",
                "A. Oliva",
                "A. Torralba"
            ],
            "title": "Learning Deep Features for Discriminative Localization",
            "venue": "CVPR,",
            "year": 2015
        },
        {
            "authors": [
                "B. Zhou",
                "Y. Tian",
                "S. Sukhbaatar",
                "A. Szlam",
                "R. Fergus"
            ],
            "title": "Simple Baseline for Visual Question Answering",
            "venue": "CoRR, abs/1512.02167,",
            "year": 2015
        }
    ],
    "sections": [
        {
            "text": "We propose to counter these language priors for the task of Visual Question Answering (VQA) and make vision (the V in VQA) matter! Specifically, we balance the popular VQA dataset [3] by collecting complementary images such that every question in our balanced dataset is associated with not just a single image, but rather a pair of similar images that result in two different answers to the question. Our dataset is by construction more balanced than the original VQA dataset and has approximately twice the number of image-question pairs. Our complete balanced dataset is publicly available at http://visualqa.org/ as part of the 2nd iteration of the Visual Question Answering Dataset and Challenge (VQA v2.0).\nWe further benchmark a number of state-of-art VQA models on our balanced dataset. All models perform significantly worse on our balanced dataset, suggesting that these models have indeed learned to exploit language priors. This finding provides the first concrete empirical evidence for what seems to be a qualitative sense among practitioners.\nFinally, our data collection protocol for identifying complementary images enables us to develop a novel interpretable model, which in addition to providing an answer to the given (image, question) pair, also provides a counterexample based explanation. Specifically, it identifies an image that is similar to the original image, but it believes has a different answer to the same question. This can help in building trust for machines among their users.\n\u2217The first two authors contributed equally."
        },
        {
            "heading": "1. Introduction",
            "text": "Language and vision problems such as image captioning [8, 4, 7, 19, 40, 21, 28] and visual question answering (VQA) [3, 26, 27, 10, 31] have gained popularity in recent years as the computer vision research community is progressing beyond \u201cbucketed\u201d recognition and towards solving multi-modal problems.\nThe complex compositional structure of language makes problems at the intersection of vision and language challenging. But recent works [6, 47, 49, 16, 18, 1] have pointed out that language also provides a strong prior that can result in good superficial performance, without the underlying models truly understanding the visual content.\nThis phenomenon has been observed in image captioning [6] as well as visual question answering [47, 49, 16, 18, 1]. For instance, in the VQA [3] dataset, the most common sport answer \u201ctennis\u201d is the correct answer for 41% of the questions starting with \u201cWhat sport is\u201d, and \u201c2\u201d is the correct answer for 39% of the questions starting with \u201cHow many\u201d. Moreover, Zhang et al. [47] points out a particular \u2018visual priming bias\u2019 in the VQA dataset \u2013 specifically, subjects saw an image while asking questions about it. Thus, people only ask the question \u201cIs there a clock tower in the picture?\u201d on images actually containing clock towers. As one particularly perverse example \u2013 for questions\nar X\niv :1\n61 2.\n00 83\n7v 3\n[ cs\n.C V\n] 1\n5 M\nay 2\n01 7\nin the VQA dataset starting with the n-gram \u201cDo you see a . . . \u201d, blindly answering \u201cyes\u201d without reading the rest of the question or looking at the associated image results in a VQA accuracy of 87%!\nThese language priors can give a false impression that machines are making progress towards the goal of understanding images correctly when they are only exploiting language priors to achieve high accuracy. This can hinder progress in pushing state of art in the computer vision aspects of multi-modal AI [39, 47].\nIn this work, we propose to counter these language biases and elevate the role of image understanding in VQA. In order to accomplish this goal, we collect a balanced VQA dataset with significantly reduced language biases. Specifically, we create a balanced VQA dataset in the following way \u2013 given an (image, question, answer) triplet (I,Q,A) from the VQA dataset, we ask a human subject to identify an image I \u2032 that is similar to I but results in the answer to the question Q to become A\u2032 (which is different from A). Examples from our balanced dataset are shown in Fig. 1. More random examples can be seen in Fig. 2 and on the project website1.\nOur hypothesis is that this balanced dataset will force VQA models to focus on visual information. After all, when a question Q has two different answers (A and A\u2032) for two different images (I and I \u2032 respectively), the only way to know the right answer is by looking at the image. Language-only models have simply no basis for differentiating between the two cases \u2013 (Q, I) and (Q, I \u2032), and by construction must get one wrong. We believe that this construction will also prevent language+vision models from achieving high accuracy by exploiting language priors, enabling VQA evaluation protocols to more accurately reflect progress in image understanding.\nOur balanced VQA dataset is also particularly difficult because the picked complementary image I \u2032 is close to the original image I in the semantic (fc7) space of VGGNet [37] features. Therefore, VQA models will need to understand the subtle differences between the two images to predict the answers to both the images correctly.\nNote that simply ensuring that the answer distribution P (A) is uniform across the dataset would not accomplish the goal of alleviating language biases discussed above. This is because language models exploit the correlation between question n-grams and the answers, e.g. questions starting with \u201cIs there a clock\u201d has the answer \u201cyes\u201d 98% of the time, and questions starting with \u201cIs the man standing\u201d has the answer \u201cno\u201d 69% of the time. What we need is not just higher entropy in P (A) across the dataset, but higher entropy in P (A|Q) so that image I must play a role in determining A. This motivates our balancing on a perquestion level.\n1http://visualqa.org/\nOur complete balanced dataset contains approximately 1.1 Million (image, question) pairs \u2013 almost double the size of the VQA [3] dataset \u2013 with approximately 13 Million associated answers on the \u223c200k images from COCO [23]. We believe this balanced VQA dataset is a better dataset to benchmark VQA approaches, and is publicly available for download on the project website.\nFinally, our data collection protocol enables us to develop a counter-example based explanation modality. We propose a novel model that not only answers questions about images, but also \u2018explains\u2019 its answer to an imagequestion pair by providing \u201chard negatives\u201d i.e., examples of images that it believes are similar to the image at hand, but it believes have different answers to the question. Such an explanation modality will allow users of the VQA model to establish greater trust in the model and identify its oncoming failures.\nOur main contributions are as follows: (1) We balance the existing VQA dataset [3] by collecting complementary images such that almost every question in our balanced dataset is associated with not just a single image, but rather a pair of similar images that result in two different answers to the question. The result is a more balanced VQA dataset, which is also approximately twice the size of the original VQA dataset. (2) We evaluate state-of-art VQA models (with publicly available code) on our balanced dataset, and show that models trained on the existing \u2018unbalanced\u2019 VQA dataset perform poorly on our new balanced dataset. This finding confirms our hypothesis that these models have been exploiting language priors in the existing VQA dataset to achieve higher accuracy. (3) Finally, our data collection protocol for identifying complementary scenes enables us to develop a novel interpretable model, which in addition to answering questions about images, also provides a counterexample based explanation \u2013 it retrieves images that it believes are similar to the original image but have different answers to the question. Such explanations can help in building trust for machines among their users."
        },
        {
            "heading": "2. Related Work",
            "text": "Visual Question Answering. A number of recent works have proposed visual question answering datasets [3, 22, 26, 31, 10, 46, 38, 36] and models [9, 25, 2, 43, 24, 27, 47, 45, 44, 41, 35, 20, 29, 15, 42, 33, 17]. Our work builds on top of the VQA dataset from Antol et al. [3], which is one of the most widely used VQA datasets. We reduce the language biases present in this popular dataset, resulting in a dataset that is more balanced and about twice the size of the VQA dataset. We benchmark one \u2018baseline\u2019 VQA model [24], one attention-based VQA model [25], and the winning model from the VQA Real Open Ended Challenge 2016 [9] on our balanced VQA dataset, and compare them to a language-only model.\nData Balancing and Augmentation. At a high level, our work may be viewed as constructing a more rigorous evaluation protocol by collecting \u2018hard negatives\u2019. In that spirit, it is similar to the work of Hodosh et al. [14], who created a binary forced-choice image captioning task, where\na machine must choose to caption an image with one of two similar captions. To compare, Hodosh et al. [14] implemented hand-designed rules to create two similar captions for images, while we create a novel annotation interface to collect two similar images for questions in VQA.\nPerhaps the most relevant to our work is that of Zhang et al. [47], who study this goal of balancing VQA in a fairly restricted setting \u2013 binary (yes/no) questions on abstract scenes made from clipart (part of the VQA abstract scenes dataset [3]). Using clipart allows Zhang et al. to ask human annotators to \u201cchange the clipart scene such that the answer to the question changes\u201d. Unfortunately, such finegrained editing of image content is simply not possible in real images. The novelty of our work over Zhang et al. is the proposed complementary image data collection interface, application to real images, extension to all questions (not just binary ones), benchmarking of state-of-art VQA models on the balanced dataset, and finally the novel VQA model with counter-example based explanations.\nModels with explanation. A number of recent works have proposed mechanisms for generating \u2018explanations\u2019 [13, 34, 48, 11, 32] for the predictions made by deep learning models, which are typically \u2018black-box\u2019 and noninterpretable. [13] generates a natural language explanation (sentence) for image categories. [34, 48, 11, 32] provide \u2018visual explanations\u2019 or spatial maps overlaid on images to highlight the regions that the model focused on while making its predictions. In this work, we introduce a third explanation modality: counter-examples, instances the the model believes are close to but not belonging to the category predicted by the model."
        },
        {
            "heading": "3. Dataset",
            "text": "We build on top of the VQA dataset introduced by Antol et al. [3]. VQA real images dataset contains just over 204K images from COCO [23], 614K free-form natural language questions (3 questions per image), and over 6 million free-form (but concise) answers (10 answers per question). While this dataset has spurred significant progress in VQA domain, as discussed earlier, it has strong language biases.\nOur key idea to counter this language bias is the following \u2013 for every (image, question, answer) triplet (I,Q,A) in the VQA dataset, our goal is to identify an image I \u2032 that\nis similar to I , but results in the answer to the question Q to become A\u2032 (which is different from A). We built an annotation interface (shown in Fig. 3) to collect such complementary images on Amazon Mechanical Turk (AMT). AMT workers are shown 24 nearest-neighbor images of I , the question Q, and the answer A, and asked to pick an image I \u2032 from the list of 24 images for whichQ \u201cmakes sense\u201d and the answer to Q is not A.\nTo capture \u201cquestion makes sense\u201d, we explained to the workers (and conducted qualification tests to make sure that they understood) that any premise assumed in the question must hold true for the image they select. For instance, the question \u201cWhat is the woman doing?\u201d assumes that a woman is present and can be seen in the image. It does not make sense to ask this question on an image without a woman visible in it.\nWe compute the 24 nearest neighbors by first representing each image with the activations from the penultimate (\u2018fc7\u2019) layer of a deep Convolutional Neural Network (CNN) \u2013 in particular VGGNet [37] \u2013 and then using `2- distances to compute neighbors.\nAfter the complementary images are collected, we conduct a second round of data annotation to collect answers on these new images. Specifically, we show the picked image I \u2032 with the question Q to 10 new AMT workers, and collect 10 ground truth answers (similar to [3]). The most common answer among the 10 is the new answer A\u2032.\nThis two-stage data collection process finally results in pairs of complementary images I and I \u2032 that are semantically similar, but have different answers A and A\u2032 respectively to the same question Q. Since I and I \u2032 are semantically similar, a VQA model will have to understand the subtle differences between I and I \u2032 to provide the right answer to both images. Example complementary images are shown in Fig. 1, Fig. 2, and on the project website.\nNote that sometimes it may not be possible to pick one of the 24 neighbors as a complementary image. This is because either (1) the question does not make sense for any of the 24 images (e.g. the question is \u2018what is the woman doing?\u2019 and none of the neighboring images contain a woman), or (2) the question is applicable to some neighboring images, but the answer to the question is still A (same as the original image I). In such cases, our data collection interface allowed AMT workers to select \u201cnot possible\u201d.\nWe analyzed the data annotated with \u201cnot possible\u201d selection by AMT workers and found that this typically happens when (1) the object being talked about in the question is too small in the original image and thus the nearest neighbor images, while globally similar, do not necessarily contain the object resulting in the question not making sense, or (2) when the concept in the question is rare (e.g., when workers are asked to pick an image such that the answer to the question \u201cWhat color is the banana?\u201d is NOT \u201cyellow\u201d).\nIn total, such \u201cnot possible\u201d selections make up 22% of all the questions in the VQA dataset. We believe that a more sophisticated interface that allowed workers to scroll through many more than 24 neighboring images could possibly reduce this fraction. But, (1) it will likely still not be 0 (there may be no image in COCO where the answer to \u201cis the woman flying?\u201d is NOT \u201cno\u201d), and (2) the task would be significantly more cumbersome for workers, making the data collection significantly more expensive.\nWe collected complementary images and the corresponding new answers for all of train, val and test splits of the VQA dataset. AMT workers picked \u201cnot possible\u201d for approximately 135K total questions. In total, we collected approximately 195K complementary images for train, 93K complementary images for val, and 191K complementary images for test set. In addition, we augment the test set with\u223c18K additional (question, image) pairs to provide ad-\nditional means to detect anomalous trends on the test data. Hence, our complete balanced dataset contains more than 443K train, 214K val and 453K test (question, image) pairs. Following original VQA dataset [3], we divide our test set into 4 splits: test-dev, test-standard, test-challenge and testreserve. For more details, please refer to [3]. Our complete balanced dataset is publicly available for download.\nWe use the publicly released VQA evaluation script in our experiments. The evaluation metric uses 10 groundtruth answers for each question to compute VQA accuracies. As described above, we collected 10 answers for every complementary image and its corresponding question to be consistent with the VQA dataset [3]. Note that while unlikely, it is possible that the majority vote of the 10 new answers may not match the intended answer of the person picking the image either due to inter-human disagreement, or if the worker selecting the complementary image simply\nmade a mistake. We find this to be the case \u2013 i.e., A to be the same as A\u2032 \u2013 for about 9% of our questions.\nFig. 4 compares the distribution of answers per questiontype in our new balanced VQA dataset with the original (unbalanced) VQA dataset [3]. We notice several interesting trends. First, binary questions (e.g. \u201cis the\u201d, \u201cis this\u201d, \u201cis there\u201d, \u201care\u201d, \u201cdoes\u201d) have a significantly more balanced distribution over \u201cyes\u201d and \u201cno\u201d answers in our balanced dataset compared to unbalanced VQA dataset. \u201cbaseball\u201d is now slightly more popular than \u201ctennis\u201d under \u201cwhat sport\u201d, and more importantly, overall \u201cbaseball\u201d and \u201ctennis\u201d dominate less in the answer distribution. Several other sports like \u201cfrisbee\u201d, \u201cskiing\u201d, \u201csoccer\u201d, \u201cskateboarding\u201d, \u201csnowboard\u201d and \u201csurfing\u201d are more visible in the answer distribution in the balanced dataset, suggesting that it contains heavier tails. Similar trends can be seen across the board with colors, animals, numbers, etc. Quantitatively, we find that the entropy of answer distributions averaged across various question types (weighted by frequency of question types) increases by 56% after balancing, confirming the heavier tails in the answer distribution.\nAs the statistics show, while our balanced dataset is not perfectly balanced, it is significantly more balanced than the original VQA dataset. The resultant impact of this balancing on performance of state-of-the-art VQA models is discussed in the next section."
        },
        {
            "heading": "4. Benchmarking Existing VQA Models",
            "text": "Our first approach to training a VQA model that emphasizes the visual information over language-priors-alone is to re-train the existing state-of-art VQA models (with code publicly available [24, 25, 9]) on our new balanced VQA dataset. Our hypothesis is that simply training a model to answer questions correctly on our balanced dataset will already encourage the model to focus more on the visual signal, since the language signal alone has been impoverished. We experiment with the following models:\nDeeper LSTM Question + norm Image (d-LSTM+nI) [24]: This was the VQA model introduced in [3] together with the dataset. It uses a CNN embedding of the image, a Long-Short Term Memory (LSTM) embedding of the question, combines these two embeddings via a point-wise multiplication, followed by a multi-layer perceptron classifier to predict a probability distribution over 1000 most frequent answers in the training dataset.\nHierarchical Co-attention (HieCoAtt) [25]: This is a recent attention-based VQA model that \u2018co-attends\u2019 to both the image and the question to predict an answer. Specifically, it models the question (and consequently the image via the co-attention mechanism) in a hierarchical fashion: at the word-level, phrase-level and entire question-level. These levels are combined recursively to produce a distribution over the 1000 most frequent answers.\nMultimodal Compact Bilinear Pooling (MCB) [9]: This is the winning entry on the real images track of the VQA Challenge 2016. This model uses a multimodal compact bilinear pooling mechanism to attend over image features and combine the attended image features with language features. These combined features are then passed through a fully-connected layer to predict a probability distribution over the 3000 most frequent answers. It should be noted that MCB uses image features from a more powerful CNN architecture ResNet [12] while the previous two models use image features from VGGNet [37].\nBaselines: To put the accuracies of these models in perspective, we compare to the following baselines: Prior: Predicting the most common answer in the training set, for all test questions. The most common answer is \u201cyes\u201d in both the unbalanced and balanced sets. Language-only: This language-only baseline has a similar architecture as Deeper LSTM Question + norm Image [24] except that it only accepts the question as input and does not utilize any visual information. Comparing VQA models to languageonly ablations quantifies to what extent VQA models have succeeded in leveraging the image to answer the questions.\nThe results are shown in Table 1. For fair comparison of accuracies with original (unbalanced) dataset, we create a balanced train set which is of similar size as original dataset (referred to as Bhalf in table). For benchmarking, we also report results using the full balanced train set.\nWe see that the current state-of-art VQA models trained on the original (unbalanced) VQA dataset perform significantly worse when evaluated on our balanced dataset, compared to evaluating on the original unbalanced VQA dataset (i.e., comparing UU to UB in the table). This finding confirms our hypothesis that existing models have learned severe language biases present in the dataset, resulting in a reduced ability to answer questions correctly when the same question has different answers on different images. When these models are trained on our balanced dataset, their performance improves (compare UB to BhalfB in the table). Further, when models are trained on complete balanced\ndataset (\u223ctwice the size of original dataset), the accuracy improves by 2-3% (compare BhalfB to BB). This increase in accuracy suggests that current VQA models are data starved, and would benefit from even larger VQA datasets.\nAs the absolute numbers in the table suggest, there is significant room for improvement in building visual understanding models that can extract detailed information from images and leverage this information to answer free-form natural language questions about images accurately. As expected from the construction of this balanced dataset, the question-only approach performs significantly worse on the balanced dataset compared to the unbalanced dataset, again confirming the language-bias in the original VQA dataset, and its successful alleviation (though not elimination) in our proposed balanced dataset.\nNote that in addition to the lack of language bias, visual reasoning is also challenging on the balanced dataset since there are pairs of images very similar to each other in image representations learned by CNNs, but with different answers to the same question. To be successful, VQA models need to understand the subtle differences in these images.\nThe paired construction of our dataset allows us to analyze the performance of VQA models in unique ways. Given the prediction of a VQA model, we can count the number of questions where both complementary images (I ,I \u2032) received correct answer predictions for the corresponding question Q, or both received identical (correct or incorrect) answer predictions, or both received different answer predictions. For the HieCoAtt [25] model, when trained on the unbalanced dataset, 13.5% of the pairs were answered correctly, 59.9% of the pairs had identical predictions, and 40.1% of the pairs had different predictions. In comparison, when trained on balanced dataset, the same model answered 17.7% of the pairs correctly, a 4.2% increase in performance! Moreover, it predicts identical answers for 10.5% fewer pairs (49.4%). This shows that by training on balanced dataset, this VQA model has learned to tell the difference between two otherwise similar images. However, significant room for improvement remains. The VQA model still can not tell the difference between two images that have a noticeable difference \u2013 a difference enough to result in the two images having different ground truth answers for the same question asked by humans.\nTo benchmark models on VQA v2.0 dataset, we also train these models on VQA v2.0 train+val and report results on VQA v2.0 test-standard in Table 2. Papers reporting results on VQA v2.0 dataset are suggested to report teststandard accuracies and compare their methods\u2019 accuracies with accuracies reported in Table 2.\nAnalysis of Accuracies for Different Answer Types: We further analyze the accuracy breakdown over answer types for Multimodal Compact Bilinear Pooling (MCB) [9] and Hierarchical Co-attention (HieCoAtt) [25] models.\nThe results are shown in Table 3. First, we immediately notice that the accuracy for the answer-type \u201cyes/no\u201d drops significantly from UU to UB (\u223c10.8% for MCB and \u223c12.4% for HieCoAtt). This suggests that these VQA models are really exploiting language biases for \u201cyes/no\u201d type questions, which leads to high accuracy on unbalanced val set because the unbalanced val set also contains these biases. But performance drops significantly when tested on the balanced val set which has significantly reduced biases.\nSecond, we note that for both the state-of-art VQA models, the largest source of improvement from UB to BhalfB is the \u201cyes/no\u201d answer-type (\u223c4.5% for MCB and \u223c3% for HieCoAtt) and the \u201cnumber\u201d answer-type (\u223c3% for MCB and \u223c2% for HieCoAtt).\nThis trend is particularly interesting since the \u201cyes/no\u201d and \u201cnumber\u201d answer-types are the ones where existing approaches have shown minimal improvements. For instance, in the results announced at the VQA Real Open Ended Challenge 20162, the accuracy gap between the top-4 approaches is a mere 0.15% in \u201cyes/no\u201d answer-type category (and a gap of 3.48% among the top-10 approaches). Similarly, \u201cnumber\u201d answer-type accuracies only vary by\n2http://visualqa.org/challenge.html\n1.51% and 2.64% respectively. The primary differences between current generation of state-of-art approaches seem to come from the \u201cother\u201d answer-type where accuracies vary by 7.03% and 10.58% among the top-4 and top-10 entries.\nThis finding suggests that language priors present in the unbalanced VQA dataset (particularly in the \u201cyes/no\u201d and \u201cnumber\u201d answer-type questions) lead to similar accuracies for all state-of-art VQA models, rendering vastly different models virtually indistinguishable from each other (in terms of their accuracies for these answer-types). Benchmarking these different VQA models on our balanced dataset (with reduced language priors) may finally allow us to distinguish between \u2018good\u2019 models (ones that encode the \u2018right\u2019 inductive biases for this task, such as attention-based or compositional models) from others that are simply high-capacity models tuning themselves to the biases in the dataset."
        },
        {
            "heading": "5. Counter-example Explanations",
            "text": "We propose a new explanation modality: counterexamples. We propose a model that when asked a question about an image, not only provides an answer, but also provides example images that are similar to the input image but the model believes have different answers to the input question. This would instill trust in the user that the model does in fact \u2018understand\u2019 the concept being asked about. For instance, for a question \u201cWhat color is the fire-hydrant?\u201d a VQA model may be perceived as more trustworthy if in addition to saying \u201cred\u201d, it also adds \u201cunlike this\u201d and shows an example image containing a fire-hydrant that is not red.3"
        },
        {
            "heading": "5.1. Model",
            "text": "Concretely, at test time, our \u201cnegative explanation\u201d or \u201ccounter-example explanation\u201d model functions in two steps. In the first step, similar to a conventional VQA model, it takes in an (image, question) pair (Q, I) as input and predicts an answer Apred. In the second step, it uses this predicted answer Apred along with the question Q to retrieve an image that is similar to I but has a different answer than Apred to the question Q. To ensure similarity, the model picks one ofK nearest neighbor images of I , INN = {I1, I2, ..., IK} as the counter-example.\nHow may we find these \u201cnegative explanations\u201d? One way of picking the counter-example from INN is to follow the classical \u201chard negative mining\u201d strategy popular in computer vision. Specifically, simply pick the image that has the lowest P (Apred|Q, Ii) where i \u2208 1, 2, ...,K. We compare to this strong baseline. While this ensures that P (Apred|Q, Ii) is low for Ii, it does not ensure that the Q \u201cmakes sense\u201d for Ii. Thus, when trying to find a negative explanation for \u201cQ: What is the woman doing? A: Playing\n3It could easily also convey what color it thinks the fire-hydrant is in the counter-example. We will explore this in future work.\ntennis\u201d, this \u201chard negative mining\u201d strategy might pick an image without a woman in it, which would make for a confusing and non-meaningful explanation to show to a user, if the goal is to convince them that the model has understood the question. One could add a component of question relevance [30] to identify better counter-examples.\nInstead, we take advantage of our balanced data collection mechanism to directly train for identifying a good counter-example. Note that the I \u2032 picked by humans is a good counter-example, by definition. Q is relevant to I \u2032 (since workers were asked to ensure it was), I \u2032 has a different answerA\u2032 thanA (the original answer), and I \u2032 is similar to I . Thus, we have supervised training data where I \u2032 is a counter-example from INN (K = 24) for question Q and answer A. We train a model that learns to provide negative or counter-example explanations from this supervised data.\nTo summarize, during test time, our model does two things: first it answers the question (similar to a conventional VQA model), and second, it explains its answer via a counter-example. For the first step, it is given as input an image I and a question Q, and it outputs a predicted answer Apred. For the second (explaining) step, it is given as input the question Q, an answer to be explained A4, and a set INN from which the model has to identify the counterexample. At training time, the model is given image I , the question Q, and the corresponding ground-truth answer A to learn to answer questions. It is also given Q, A, I \u2032 (human-picked), INN (I \u2032 \u2208 INN ) to learn to explain. Our model architecture contains two heads on top of a shared base \u2018trunk\u2019 \u2013 one head for answering the question and the other head for providing an explanation. Specifically, our model consists of three major components:\n1. Shared base: The first component of our model is learning representations of images and questions. It is a 2-channel network that takes in an image CNN embedding as input in one branch, question LSTM embedding as input in another branch, and combines the two embeddings by a point-wise multiplication. This gives us a joint QI embedding, similar to the model in [24]. The second and third components \u2013 the answering model and the explaining model \u2013 take in this jointQI embedding as input, and therefore can be considered as two heads over this first shared component. A total of 25 images \u2013 the original image I and 24 candidate images {I1, I2, ..., I24} are passed through this shared component of the network.\n2. Answering head: The second component is learning to answer questions. Similar to [24], it consists of a fullyconnected layer fed into a softmax that predicts the prob-\n4In practice, this answer to be explained would be the answer predicted by the first step Apred. However, we only have access to negative explanation annotations from humans for the ground-truth answer A to the question. Providing A to the explanation module also helps in evaluating the two steps of answering and explaining separately.\nability distribution over answers given the QI embedding. Only the QI embedding corresponding to the original image I is passed through this component and result in a crossentropy loss.\n3. Explaining head: The third component is learning to explain an answer A via a counter-example image. It is a 2-channel network which linearly transforms the joint QI embedding (output from the first component) and the answer to be explained A (provided as input)5 into a common embedding space. It computes an inner product of these 2 embeddings resulting in a scalar number for each image in INN (also provided as input, from which a counter-example is to be picked). TheseK inner-product values forK candidate images are then passed through a fully connected layer to generateK scores S(Ii), where i \u2208 {1, 2, ...,K}. TheK candidate images {I1, I2, ..., IK} are then sorted according to these scores S(Ii) as being most to least likely of being good counter-examples or negative explanations. This component is trained with pairwise hinge ranking losses that encourage S(I \u2032) \u2212 S(Ii) > M \u2212 , Ii \u2208 {I1, I2, ..., IK} \\ {I \u2032}, i.e. the score of the human picked image I \u2032 is encouraged to be higher than all other candidate images by a desired margin ofM (a hyperparameter) and a slack of . This is of course the classical \u2018constraint form\u2019 of the pairwise hinge ranking loss, and we minimize the standard expression max ( 0,M \u2212 ( S(I \u2032) \u2212 S(Ii) )) . The combined loss\nfunction for the shared component is\nL = \u2212 logP (A|I,Q) + \u03bb \u2211 i max ( 0,M \u2212 ( S(I \u2032)\u2212 S(Ii) )) (1)\nwhere, the first term is the cross-entropy loss (for training the answering module) on (I,Q), the second term is the sum of pairwise hinge losses that encourage the explaining model to give high score to image I \u2032 (picked by humans) than other Iis in INN , and \u03bb is the trade-off weight parameter between the two losses."
        },
        {
            "heading": "5.2. Results",
            "text": "Fig. 5 shows qualitative examples of negative explanations produced by our model. We see the original image I , the question asked Q, the answer Apred predicted by the\n5Note that in theory, one could provide Apred as input during training instead of A. After all, this matches the expected use case scenario at test time. However, this alternate setup (where Apred is provided as input instead of A) leads to a peculiar and unnatural explanation training goal \u2013 specifically, the explanation head will still be learning to explain A since that is the answer for which we collected negative explanation human annotations. It is simply unnatural to build that model that answers a question with Apred but learn to explain a different answer A! Note that this is an interesting scenario where the current push towards \u201cend-to-end\u201d training for everything breaks down.\nVQA head in our model, and top three negative explanations produced by the explanation head. We see that most of these explanations are sensible and reasonable \u2013 the images are similar to I but with answers that are different from those predicted for I .\nFor quantitative evaluation, we compare our model with a number of baselines: Random: Sorting the candidate images in INN randomly. That is, a random image from INN is picked as the most likely counter-example. Distance: Sorting the candidate images in increasing order of their distance from the original image I . That is, the image from INN most similar to I is picked as the most likely counterexample. VQA Model: Using a VQA model\u2019s probability for the predicted answer to sort the candidate images in ascending order of P (A|Q, Ii). That is, the image from INN least likely to have A as the answer to Q is picked as the most likely counter-example.\nNote that while I \u2032 \u2013 the image picked by humans \u2013 is a good counter-example, it is not necessarily the unique (or even the \u201cbest\u201d) counter-example. Humans were simply asked to pick any image where Q makes sense and the answer is not A. There was no natural criteria to convey to humans to pick the \u201cbest\u201d one \u2013 it is not clear what \u201cbest\u201d would mean in the first place. To provide robustness to this potential ambiguity in the counter-example chosen by humans, in a manner similar to the ImageNet [5] top-5 evaluation metric, we evaluate our approach using the Recall@5 metric. It measures how often the human picked I \u2032 is among the top-5 in the sorted list of Iis in INN our model produces.\nIn Table 4, we can see that our explanation model significantly outperforms the random baseline, as well as the VQA [3] model. Interestingly, the strongest baseline is Distance. While our approach outperforms it, it is clear that identify-\ning an image that is a counter-example to I from among I\u2019s nearest neighbors is a challenging task. Again, this suggests that visual understanding models that can extract meaningful details from images still remain elusive."
        },
        {
            "heading": "6. Conclusion",
            "text": "To summarize, in this paper we address the strong language priors for the task of Visual Question Answering and elevate the role of image understanding required to be successful on this task. We develop a novel data-collection interface to \u2018balance\u2019 the popular VQA dataset [3] by collecting \u2018complementary\u2019 images. For every question in the dataset, we have two complementary images that look similar, but have different answers to the question.\nThis effort results in a dataset that is not only more balanced than the original VQA dataset by construction, but also is about twice the size. We find both qualitatively and quantitatively that the \u2018tails\u2019 of the answer distribution are heavier in this balanced dataset, which reduces the strong language priors that may be exploited by models. Our complete balanced dataset is publicly available at http://visualqa.org/ as part of the 2nd iteration of the Visual Question Answering Dataset and Challenge (VQA v2.0).\nWe benchmark a number of (near) state-of-art VQA models on our balanced dataset and find that testing them on this balanced dataset results in a significant drop in performance, confirming our hypothesis that these models had indeed exploited language biases.\nFinally, our framework around complementary images enables us to develop a novel explainable model \u2013 when asked a question about an image, our model not only returns an answer, but also produces a list of similar images that it considers \u2018counter-examples\u2019, i.e. where the answer is not the same as the predicted response. Producing such explanations may enable a user to build a better mental model of what the system considers a response to mean, and ultimately build trust.\nAcknowledgements. We thank Anitha Kannan and Aishwarya Agrawal for helpful discussions. This work was funded in part by NSF CAREER awards to DP and DB, an ONR YIP award to DP, ONR Grant N00014-14-1-0679 to DB, a Sloan Fellowship to DP, ARO YIP awards to DB and DP, an Allen Distinguished Investigator award to DP from the Paul G. Allen Family Foundation, ICTAS Junior Faculty awards to DB and DP, Google Faculty Research Awards to DP and DB, Amazon Academic Research Awards to DP and DB, AWS in Education Research grant to DB, and NVIDIA GPU donations to DB. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the U.S. Government, or any sponsor."
        }
    ],
    "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
    "year": 2017
}