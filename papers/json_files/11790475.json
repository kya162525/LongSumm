{
    "abstractText": "We propose to meta-learn causal structures based on how fast a learner adapts to new distributions arising from sparse distributional changes, e.g. due to interventions, actions of agents and other sources of non-stationarities. We show that under this assumption, the correct causal structural choices lead to faster adaptation to modified distributions because the changes are concentrated in one or just a few mechanisms when the learned knowledge is modularized appropriately. This leads to sparse expected gradients and a lower effective number of degrees of freedom needing to be relearned while adapting to the change. It motivates using the speed of adaptation to a modified distribution as a meta-learning objective. We demonstrate how this can be used to determine the cause-effect relationship between two observed variables. The distributional changes do not need to correspond to standard interventions (clamping a variable), and the learner has no direct knowledge of these interventions. We show that causal structures can be parameterized via continuous variables and learned end-to-end. We then explore how these ideas could be used to also learn an encoder that would map low-level observed variables to unobserved causal variables leading to faster adaptation out-of-distribution, learning a representation space where one can satisfy the assumptions of independent mechanisms and of small and sparse changes in these mechanisms due to actions and non-stationarities.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yoshua Bengio"
        },
        {
            "affiliations": [],
            "name": "Tristan Deleu"
        },
        {
            "affiliations": [],
            "name": "Nasim Rahaman"
        },
        {
            "affiliations": [],
            "name": "Nan Rosemary Ke"
        },
        {
            "affiliations": [],
            "name": "S\u00e9bastien Lachapelle"
        },
        {
            "affiliations": [],
            "name": "Olexa Bilaniuk"
        },
        {
            "affiliations": [],
            "name": "Anirudh Goyal"
        },
        {
            "affiliations": [],
            "name": "Christopher Pal"
        }
    ],
    "id": "SP:492ba3ad3f0cb85f0636bc275fecd7e7960709da",
    "references": [
        {
            "authors": [
                "Ferran Alet",
                "Tom\u00e1s Lozano-P\u00e9rez",
                "Leslie P Kaelbling"
            ],
            "title": "Modular meta-learning",
            "venue": "arXiv preprint arXiv:1806.10166,",
            "year": 2018
        },
        {
            "authors": [
                "Elias Bareinboim",
                "Judea Pearl"
            ],
            "title": "Causal inference and the data-fusion problem",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2016
        },
        {
            "authors": [
                "Yoshua Bengio"
            ],
            "title": "The consciousness prior",
            "venue": "arXiv preprint arXiv:1709.08568,",
            "year": 2017
        },
        {
            "authors": [
                "Yoshua Bengio",
                "Aaron Courville",
                "Pascal Vincent"
            ],
            "title": "Representation learning: A review and new perspectives",
            "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence (PAMI),",
            "year": 2013
        },
        {
            "authors": [
                "Christopher M Bishop"
            ],
            "title": "Mixture density networks",
            "venue": "Technical report, Citeseer,",
            "year": 1994
        },
        {
            "authors": [
                "Ishita Dasgupta",
                "Jane Wang",
                "Silvia Chiappa",
                "Jovana Mitrovic",
                "Pedro Ortega",
                "David Raposo",
                "Edward Hughes",
                "Peter Battaglia",
                "Matthew Botvinick",
                "Zeb Kurth-Nelson"
            ],
            "title": "Causal Reasoning from Meta-reinforcement Learning",
            "year": 1901
        },
        {
            "authors": [
                "Andrzej Ehrenfeucht",
                "David Haussler",
                "Michael Kearns",
                "Leslie Valiant"
            ],
            "title": "A general lower bound on the number of examples needed for learning",
            "venue": "Information and Computation,",
            "year": 1989
        },
        {
            "authors": [
                "Chelsea Finn",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
            "venue": "International Conference on Machine Learning (ICML),",
            "year": 2017
        },
        {
            "authors": [
                "Nir Friedman",
                "Moises Goldszmidt"
            ],
            "title": "Learning bayesian networks with local structure. In Learning in graphical models, pages 421\u2013459",
            "year": 1998
        },
        {
            "authors": [
                "Ian J. Goodfellow",
                "Yoshua Bengio",
                "Aaron Courville"
            ],
            "title": "Deep Learning. MIT Press, 2016",
            "venue": "URL http: //deeplearningbook.org",
            "year": 2016
        },
        {
            "authors": [
                "David Heckerman",
                "Dan Geiger",
                "David M Chickering"
            ],
            "title": "Learning bayesian networks: The combination of knowledge and statistical data",
            "venue": "Machine learning,",
            "year": 1995
        },
        {
            "authors": [
                "Aapo Hyv\u00e4rinen",
                "Juha Karhunen",
                "Erkki Oja"
            ],
            "title": "Independent Component Analysis",
            "venue": "ISBN 047140540X",
            "year": 2001
        },
        {
            "authors": [
                "Aapo Hyv\u00e4rinen",
                "Hiroaki Sasaki",
                "Richard E. Turner"
            ],
            "title": "Nonlinear ica using auxiliary variables and generalized contrastive learning",
            "year": 2018
        },
        {
            "authors": [
                "Fredrik Johansson",
                "Uri Shalit",
                "David Sontag"
            ],
            "title": "Learning representations for counterfactual inference",
            "venue": "In International Conference on Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Daphne Koller",
                "Nir Friedman"
            ],
            "title": "Probabilistic Graphical Models: Principles and Techniques",
            "venue": "MIT press,",
            "year": 2009
        },
        {
            "authors": [
                "Wai Lam",
                "Fahiem Bacchus"
            ],
            "title": "Using causal information and local measures to learn bayesian networks",
            "venue": "In Proceedings of the Ninth international conference on Uncertainty in artificial intelligence,",
            "year": 1993
        },
        {
            "authors": [
                "Francesco Locatello",
                "Stefan Bauer",
                "Mario Lucic",
                "Sylvain Gelly",
                "Bernhard Sch\u00f6lkopf",
                "Olivier Bachem"
            ],
            "title": "Challenging common assumptions in the unsupervised learning of disentangled",
            "venue": "representations. CoRR,",
            "year": 2018
        },
        {
            "authors": [
                "Sara Magliacane",
                "Thijs van Ommen",
                "Tom Claassen",
                "Stephan Bongers",
                "Philip Versteeg",
                "Joris M Mooij"
            ],
            "title": "Domain adaptation by using causal inference to predict invariant conditional distributions",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Alex Nichol",
                "Joshua Achiam",
                "John Schulman"
            ],
            "title": "On First-Order Meta-Learning Algorithms",
            "year": 2018
        },
        {
            "authors": [
                "Giambattista Parascandolo",
                "Niki Kilbertus",
                "Mateo Rojas-Carulla",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Learning independent causal mechanisms",
            "venue": "arXiv preprint arXiv:1712.00961,",
            "year": 2017
        },
        {
            "authors": [
                "Judea Pearl"
            ],
            "title": "Causal diagrams for empirical research",
            "venue": "Biometrika, 82(4):669\u2013688,",
            "year": 1995
        },
        {
            "authors": [
                "Jonas Peters",
                "Dominik Janzing",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Elements of causal inference: foundations and learning algorithms",
            "venue": "MIT press,",
            "year": 2017
        },
        {
            "authors": [
                "Mateo Rojas-Carulla",
                "Bernhard Sch\u00f6lkopf",
                "Richard Turner",
                "Jonas Peters"
            ],
            "title": "Invariant models for causal transfer learning",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2018
        },
        {
            "authors": [
                "Mark W. Schmidt",
                "Alexandru Niculescu-Mizil",
                "Kevin P. Murphy"
            ],
            "title": "Learning graphical model structure using l1-regularization paths",
            "venue": "In AAAI,",
            "year": 2007
        },
        {
            "authors": [
                "Shai Shalev-Shwartz",
                "Shai Ben-David"
            ],
            "title": "Understanding Machine Learning - from Theory to Algorithms",
            "year": 2014
        },
        {
            "authors": [
                "Uri Shalit",
                "Fredrik D Johansson",
                "David Sontag"
            ],
            "title": "Estimating individual treatment effect: generalization bounds and algorithms",
            "venue": "In Proceedings of the 34th International Conference on Machine Learning-Volume",
            "year": 2017
        },
        {
            "authors": [
                "V.N. Vapnik",
                "A.Y. Chervonenkis"
            ],
            "title": "On the uniform convergence of relative frequencies of events to their probabilities",
            "venue": "Theory of Probability and its Applications,",
            "year": 1971
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Current machine learning methods seem weak when they are required to generalize beyond the training distribution, which is what is often needed in practice. It is not enough to obtain good generalization on a test set sampled from the same distribution as the training data, we would also like what has been learned in one setting to generalize well in other related distributions. These distributions may involve the same concepts that were seen previously by the learner, with the changes typically arising because of actions of agents. More generally, we would like what has been learned previously to form a rich base from which very fast adaptation to a new but related distribution can take place, i.e., obtain good transfer. Some new concept may have to be learned but because most of the other relevant concepts have already been captured by the learner (as well as how they can be composed), learning can be very fast on the transfer distribution.\nShort of any assumption, it is impossible to have a successful transfer to an unrelated distribution. In this paper we focus on the assumption that the changes are sparse when the knowledge is represented in an appropriately modularized way, with only one or a few of the modules having changed. This is especially relevant when the distributional change is due to actions by one or more agents, such as the interventions discussed in the causality literature (Pearl, 2009; Peters et al., 2017), where a single causal variable is clamped to a particular value. In general, it is difficult for agents to influence many underlying causal variables at a time, and although this paper is not about agent learning as such, this is a property of the world that we propose to exploit here, to help discovering these variables and how they are causally related to each other.\nar X\niv :1\n90 1.\n10 91\n2v 2\n[ cs\n.L G\n] 4\nF eb\nTo motivate the need for inferring causal structure, consider that interventions may be actually performed or may be imagined. In order to properly plan in a way that takes into account interventions, one needs to imagine a possible change to the joint distribution of the variables of interest due to an intervention, even one that has never been observed before. This goes beyond good transfer learning and requires causal learning and causal reasoning. For this purpose, it is not sufficient to learn the joint distribution of the observed variables. One also should learn enough about the underlying high-level variables and their causal relations to be able to properly infer the effect of an intervention. For example, A=Raining causes B=Open Umbrella (and not vice-versa). Changing the marginal probability of Raining (say because the weather changed) does not change the mechanism that relates A and B (captured by P (B|A)), but will have an impact on the marginal P (B). Conversely, an agent\u2019s intervention on B (Open umbrella) will have no effect on the marginal distribution of A (Raining). That asymmetry is generally not visible from the (A,B) training pairs alone, until a change of distribution occurs, e.g. due to an intervention. This motivates the setup of this paper, where one learns from a set of distributions arising from not necessarily known interventions, not simply to capture a joint distribution but to discover the some underlying causal structure.\nMachine learning methods are often exploiting some form of assumption about the data distribution (or else, the no free lunch theorem tells us that we cannot have any confidence in generalization). In this paper, we are considering not just assumptions on the data distribution but also on how it changes (e.g., when going from a training distribution to a transfer distribution, possibly resulting from some agent\u2019s actions). We propose to rely on the assumption that, when the knowledge about the distribution is appropriately represented, these changes would be small. This arises because of an underlying assumption (but more difficult to verify directly) that only one or few of the ground truth mechanisms have been changed, due to some generalized form of intervention leading to the modified distribution.\nHow can we exploit this assumption? As we explain theoretically and verify experimentally here, if we have the right knowledge representation, then we should get fast adaptation to the transfer distribution when starting from a model that is well trained on the training distribution. This arises because of our assumption that the ground truth data generative process is obtained as the composition of independent mechanisms and that, very few ground truth mechanisms and parameters need to change when going from the training distribution to the transfer distribution. A model capturing a corresponding factorization of knowledge would thus require just a few updates, a few examples, for this adaptation to the transfer distribution. As shown below, the expected gradient on the unchanged parameters would be near 0 (if the model was already well trained on the training distribution), so the effective search space during adaptation to the transfer distribution would be greatly reduced, which tends to produce fast adaptation, as found experimentally.\nThus, based on the assumption of small change in the right knowledge representation space, we can define a meta-learning objective that measures the speed of adaptation, i.e., a form of regret, in order to optimize the way in which knowledge should be represented, factorized and structured. This is the core idea presented in this paper. Note that a stronger signal can be obtained when there are more non-stationarities, i.e., many changes in distribution, just like in meta-learning we get better results with more meta-examples.\nIn this way, we can take what is normally considered a nuisance in machine learning (changes in distribution due to non-stationarity, uncontrolled interventions, etc.) and turn that into a training signal to find a good way to factorize knowledge into components and mechanisms that match the assumption of small change. Thus, we end up optimizing in an end-to-end way the very thing we care about at the end, i.e. fast transfer and robustness to distributional changes. If the data was really generated from the composition of independent causal mechanisms (Peters et al., 2017), then there exists a good factorization of knowledge that mimics that structure. If in addition, at each time step, agents in the real world tend to only be able to change one or very few high-level variables (or the associated mechanisms producing them), then our assumption of small change (in the right representation) should be generally valid. Also, in addition to obtaining fast transfer, we may be able to recover a good approximation of the true causal decomposition into independent mechanisms (to the extent that the observations and interventions can reveal those mechanisms).\nIn this paper, we begin exploring the above ideas with specific experiments on synthetically generated data in order to validate them and demonstrate the existence of simple algorithms to exploit them. However it is clear to us that much more work will be needed to evaluate the proposed approach in a diversity of settings and with different specific parametrizations, training objectives, environments, etc. We begin with what are maybe the simplest possible settings and evaluate whether the above approach can be used to\nlearn the direction of causality. We then study the crucial question of obtaining a training signal about how to transform raw observed data into a representation space where the latent variables can be modeled by a sparse causal graph with sparse distributional changes and show results that confirm that the correct encoder leads to a better value of our expected regret meta-learning objective."
        },
        {
            "heading": "2. Which is Cause and Which is Effect?",
            "text": "To anchor ideas and show an example of application of the above-proposed meta-objective for knowledge decomposition, we consider in this section the problem of determining if variable A causes variable B or vice-versa. The learner observes training samples (a, b) from a pair of related distributions, which by convention we call the training distribution and the transfer distribution. Note that based only on samples from a single (training) distribution, in general both the A \u2192 B model (A causes B) and the B \u2192 A model (vice-versa, see Equation (1) below) tend to perform as well in terms of ordinary generalization (to a test set sampled from the training distribution), see also a theoretical argument and simulation results in Appendix A. To highlight the power of the proposed meta-learning objective, we consider the situation where lots of examples are available for the training distribution but very few for the transfer distribution. In fact, as we will argue below, the training signal that will allow us to infer the correct causal direction will be stronger if we have access to many short transfer adaptation episodes. Short episodes are most informative because after having seen a lot of data from the transfer distribution, it will not matter much whether A causes B or vice-versa (when there is enough training data compared to the number of free parameters, both models converge towards an optimal estimation of the joint). However, in order to generalize quickly from very few examples of the transfer distribution, it does matter to have made the correct choice of the causal direction. Let us now justify this in more detail below and then demonstrate this by simulations."
        },
        {
            "heading": "2.1 Learning a Causal Graph with two Discrete Variables",
            "text": "Let both A and B be discrete variables each taking N possible values and consider the following two parametrizations (the A\u2192 B model and the B \u2192 A model) to estimate their joint distribution:\nPA\u2192B(A,B) = PA\u2192B(A)PA\u2192B(B | A) PB\u2192A(A,B) = PB\u2192A(B)PB\u2192A(A | B) (1)\nEach of these two graphical models (denoted A\u2192 B and B \u2192 A) decomposes the joint into two separately parametrized modules, each corresponding to a different causal mechanism associated with the probability of a variable given its parents in the graph. This amounts to four modules: PA\u2192B(A), PA\u2192B(B | A), PB\u2192A(B) and PB\u2192A(A | B). We will train both models independently. Since we assume in this section that the pairs (A,B) are completely observed, we can use a simple maximum likelihood estimator to independently train all four modules (the log-likelihood of the joint decomposes into separate objective functions, one for each conditional, in a directed graphical model with fully observed variables). In the discrete case with tabular parametrization, the maximum likelihood estimator can be computed analytically, and corresponds to the appropriately normalized relative frequencies. Let \u03b8 denote the parameters of all these models, split into sub-vectors for each module, e.g., \u03b8A|B for the N\n2 conditional probabilities for each possible value of B and each possible value of A. In our experiments, we parametrized these probabilities via softmax of unnormalized quantities."
        },
        {
            "heading": "2.1.1 The Advantage of the Correct Causal Model",
            "text": "First, let us consider simply the likelihood of the training data only (i.e., no change of distribution) for the different causal models considered. Both models have O(N2) parameters, and maximum likelihood estimation leads to indistinguishable test set performance (where the test set is sampled from the training distribution). See Appendix A for a demonstration that both models would have the same likelihood, and associated experimental results. These results are not surprising in light of the existing literature on non-identifiability of causality from observations (Pearl, 2009; Peters et al., 2017), but they highlight the importance of using changes in distribution to provide a signal about the causal structure.\nNow instead let us compare the performance of our two hypotheses (A\u2192 B vs B \u2192 A) in terms of how fast the two models adapt on a transfer distribution after having been trained on the training distribution. We will assume simple stochastic gradient descent on the parameters for this adaptation but other procedures could be used, of course. Without loss of generality, let A\u2192 B be the correct causal model. To make the case stronger, let us consider that the change between the two distributions amounts to a random change in the parameters of the true P (A) for the cause A (because this will have an impact on the effect B, which can be picked up and reveal the causal direction). We do not assume that the learner knows what intervention was performed, unlike in more common approaches to causal discovery and controlled experiments. We only assume that some change happened and we try to exploit that to reveal structural causal information."
        },
        {
            "heading": "2.2 Experiments on Adaptation to the transfer distribution",
            "text": "We present experiments comparing the learning curve of the correct causal model on the transfer distribution vs the learning curve of the incorrect model. The adaptation with only a few gradient steps on data coming from a different, but related, transfer distribution is critical in getting a signal that can be leveraged by our meta-learning algorithm. To show the effect of this adaptation, and motivate our use of only a small amount of data from the transfer distribution, we experimented with a model on discrete random variables taking N = 10 possible values.\nIn this experiment, we fixed the underlying causal model to be A \u2192 B, and trained the modules for each marginal and conditional distributions with maximum likelihood on a large amount of data from some training distribution, as explained in Appendix A. See also Appendix G.1 and Table G.1 for details on the definitions of these modules.\nWe then adapt all the modules on data coming from a transfer distribution, corresponding on an intervention on the random variable A (i.e., the marginal P (A) of the ground truth model is modified, while leaving P (B | A) fixed). We used RMSprop for the adaptation, with the same learning rate. For assessing reproducibility and statistical robustness, the experiment was repeated over 100 different training distributions, and over 100 transfer distributions for each training distributions, leading to 10 000 experiments overall. The procedure to acquire different training/transfer distributions is detailed in Appendix G.1.\nIn Figure 1, we report the log-likelihoods of both models, evaluated on a large test set of 10 000 from the transfer distribution. We can see that as the number of examples from the transfer distribution (equal to the number of adaptation steps) increases, the two models eventually reach the same log-likelihood, reflecting\nour observation from Appendix A. However the causal model A\u2192 B adapts faster than the other model B \u2192 A, with the most informative part of the trajectory (where the difference is the largest) is within the first 10 to 20 examples."
        },
        {
            "heading": "2.2.1 Parameter Counting Argument",
            "text": "A simple parameter counting arguments helps us understand what we are observing in Figure 1. First, consider the expected gradient on the parameters of the different modules, during the adaptation phase to the transfer distribution, which we designate as adaptation episode, and corresponds to learning from a meta-example.\nProposition 1 The expected gradient over the transfer distribution of the regret (accumulated negative log-likelihood during the adaptation episode) with respect to the module parameters is zero for the parameters of the modules that (a) were correctly learned in the training phase, and (b) have the correct set of causal parents, corresponding to the ground truth causal graph, if (c) the corresponding ground truth conditional distributions did not change from the training distribution to the transfer distribution.\nThe proof is given in Appendix B. The basic justification for this proposition is that for the modules that were correctly learned in the training distribution and whose ground truth conditional distribution did not change with the transfer distribution, the parameters already are at a maximum of the log-likelihood over the transfer distribution, so the expected gradient is zero.\nAs a consequence, the effective number of parameters that need to be adapted, when one has the correct causal graph structure, is reduced to those of the mechanisms that actually changed from the training to the transfer distribution. Since sample complexity - the number of training examples necessary to learn a model - grows approximately linearly (Ehrenfeucht et al., 1989) with VC-dimension (Vapnik and Chervonenkis, 1971), and since VC-dimension grows approximately linearly in the number of parameters in linear models and neural networks Shalev-Shwartz and Ben-David (2014), the learning curve on the transfer distribution will tend to improve faster for the model with the correct causal structure, for which fewer parameters need to be changed. Interestingly, we do not need to have the whole causal graph correctly specified before getting benefits from this phenomenon. If we only have part of the causal graph correctly specified and we change our causal hypothesis to include one more correctly specified mechanism, then we will obtain a gain in terms of the adaptation sample complexity (which shows up when the change in distribution does not touch that mechanism). This nice property also shows up in Proposition 4 (Appendix F), showing a decoupling of the meta-objective across the independent mechanisms.\nLet us consider the special case we have been studying up to now. We have four modules, two of which (PA\u2192B(A) and PB\u2192A(B)) are marginal discrete distributions over N values, which require each N \u2212 1 free parameters. The other two modules are conditional probability tables that have N rows each with N \u2212 1 free parameters, i.e., a total of N(N \u2212 1) free parameters. If A\u2192 B is the correct model and the transfer distribution only changed the true P (A) (the cause), and if P (B | A) had been correctly estimated on the training distribution, then for the correct model only N \u22121 parameters need to be re-estimated. On the other hand, because of Bayes\u2019 rule, under the incorrect model (B \u2192 A), a change in P (A) leads to new parameters for both P (B) and P (A | B), i.e., all N(N \u2212 1) + (N \u2212 1) = N2 \u2212 1 parameters must be re-estimated. In this case we see that sample complexity may be O(N2) for the incorrect model while it would be O(N) for the correct model (assuming linear relationship between sample complexity and number of free parameters). Of course, if the change in distribution had been over P (B | A) instead of P (A), the advantage would not have been as great. This would motivate information gathering actions generally resulting in a very sparse change in the mechanisms."
        },
        {
            "heading": "2.3 Smooth parameterization of the causal structure",
            "text": "In the more general case with many more than two hypotheses for the structure of the causal graph, there will be an exponentially large set of possible causal structures explaining the data and we won\u2019t be able to enumerate all of them (and pick the best one after observing episodes of adaptation). However, we can parameterize our belief about an exponentially large set of hypotheses by keeping track of the probability for each directed edge of the graph to be present, i.e., specify for each variable B whether some variable A is a\ndirect causal parent of B (for all pairs (A,B) in the graph). We will develop such a smooth parametrization further in Appendix F, but it hinges on gradually changing our belief in the individual binary decisions associated with each edge of the causal graph, so we can jointly do gradient descent on all these beliefs at the same time.\nIn this section, we study the simplest possible version of this idea, representing that edge belief via a structural parameter \u03b3 with sigmoid(\u03b3) = sigmoid(\u03b3), our believed probability that A\u2192 B is the correct choice. For that single pair of variables scenario, let us consider two explanations for the data (as in the above sections, for models A\u2192 B and B \u2192 A), one with probability p(A\u2192 B) = sigmoid(\u03b3) and the other with probability p(B \u2192 A) = 1\u2212 sigmoid(\u03b3). We can write down our transfer objective as a log-likelihood over the mixture of these two models. Note this is different from the usual mixture models, which assume separately for each example that it was sampled from one component or another with some probability. Here, we assume that all of the observed data was sampled from one component or the other. The transfer data regret (negative log-likelihood accumulated along the online adaptation trajectory) under that mixture is therefore as follows:\nR = \u2212 log [sigmoid(\u03b3)LA\u2192B + (1\u2212 sigmoid(\u03b3))LB\u2192A] (2)\nwhere LA\u2192B and LB\u2192A are the online likelihoods of both models respectively on the transfer data. They are defined as\nLA\u2192B = T\u220f t=1 PA\u2192B(at, bt ; \u03b8t)\nLB\u2192A = T\u220f t=1 PB\u2192A(at, bt ; \u03b8t),\nwhere {(at, bt)}t is the set of transfer examples for a given episode and \u03b8t aggregates all the modules\u2019 parameters as of time step t (since the parameters could be updated after each observation of an example (at, bt) from the transfer distribution). Pmodel(a, b; \u03b8) is the likelihood of example (a, b) under some model that has parameters \u03b8.\nThe quantity of interest here is \u2202R\u2202\u03b3 , which is our training signal for updating \u03b3. In the experiments below, after each episode involving T transfer examples we update \u03b3 by doing one step of gradient descent, to reduce the transfer negative log-likelihood or regret R. What we are proposing is a meta-learning framework in which the inner training loop updates the module parameters (separately) as examples are seen (from either distribution being currently observed), while the outer loop updates the structural parameters (here it is only the scalar \u03b3) with respect to the transfer negative log-likelihood.\nThe gradient of the transfer log-likelihood with respect to the structural parameter \u03b3 is pushing sigmoid(\u03b3) towards the posterior probability that the correct model is A\u2192 B and (1\u2212 sigmoid(\u03b3)) towards the posterior probability that the correct model is B \u2192 A:\nProposition 2 The gradient of the negative log-likelihood of the transfer data in Equation (2) wrt. the structural parameter \u2202R\u2202\u03b3 is given by\n\u2202R \u2202\u03b3 = \u03c3(\u03b3)\u2212 P (A\u2192 B | D2), (3)\nwhere D2 is the transfer data, and P (A \u2192 B | D2) is the posterior probability of the hypothesis A \u2192 B (when the alternative is B \u2192 A). Furthermore, this can be equivalently written as\n\u2202R \u2202\u03b3 = \u03c3(\u03b3)\u2212 \u03c3(\u03b3 + \u2206), (4)\nwhere \u2206 = logLA\u2192B \u2212 logLB\u2192A is the difference between the log-likelihoods of the two hypotheses on the transfer data D2.\nThe proof is given in Appendix D. Note how this posterior probability is basically measuring which hypothesis is better explaining the episode transfer data D2 overall along the adaptation trajectory. D2 is a metaexample for updating the structural parameters like \u03b3. Larger \u2206 of one hypothesis over the other leads to moving meta-parameters faster towards the favoured hypothesis. This difference in online accumulated log-likelihoods \u2206 also relates to log-likelihood scores in score-based methods for structure learning of graphical models (Koller and Friedman, 2009)1.\nTo find where SGD converges, note that the actual posterior depends on the prior sigmoid(\u03b3) and thus keeps changing after each gradient step. We are really doing SGD on the expected value of R over transfer sets D2. Equating the gradient of this expected value to zero to look for the stationary convergence point, we thus see sigmoid(\u03b3) on both sides of the equation, and we obtain convergence when the new value of sigmoid(\u03b3) is consistent with the old value, as clarified in this proposition.\nProposition 3 Stochastic gradient descent (with appropriately decreasing learning rate) on ED2 [R] with steps from \u2202R\u2202\u03b3 converges towards sigmoid(\u03b3) = 1 if ED2 [logLA\u2192B ] > ED2 [logLB\u2192A], or \u03c3(\u03b3) = 0 otherwise.\nThe proof is given in Section E of the Appendix, and shows that optimizing \u03b3 will end up picking the correct hypothesis, i.e., the one that has the smallest regret (or fastest convergence), measured as the accumulated log-likelihood as adaptation proceeds on the transfer distributions sampled from the distribution D2, which we can think of like a distribution over tasks, in meta-learning. This analogy with meta-learning also appears in our gradient-based adaptation procedure, which is linked to existing methods like the first-order approximation of MAML (Finn et al., 2017), and its related algorithms (Nichol et al., 2018). Algorithm 1 (Appendix C) illustrates the general pseudo-code for the proposed meta-learning framework."
        },
        {
            "heading": "2.3.1 Experimental Results",
            "text": "To illustrate the convergence result from Proposition 3, we experiment with learning the structural parameter \u03b3 in a bivariate model, with discrete random variables, each taking N = 10 and N = 100 possible values. In this experiment, we assume that the underlying causal model (unknown to the algorithm) is fixed to A\u2192 B, so that we want the structural parameter to eventually converge to \u03c3(\u03b3) = 1. The details of the experimental setup can be found in Appendix G.1.\n1. One can see logLA\u2192B as a score attributed to graph A \u2192 B, analogously for logLB\u2192A. The gradient is then pushing toward the graph with the highest score.\nIn Figure 2, we show the evolution of \u03c3(\u03b3) (which is the model\u2019s belief of A\u2192 B being the correct causal model) as the number of episodes increases. Starting from an equal belief for both A\u2192 B and B \u2192 A to occur (\u03c3(\u03b3) = 0.5), the structural parameter converges to \u03c3(\u03b3) = 1 within 500 episodes.\nThis observation is consistent across a range of domains, including models with multimodal or multivariate continuous variables, and different parametrizations of the models. In Appendix G.2, we present results for two discrete variables but using MLPs to parametrize the conditional distributions, and where there are more causal hypotheses: we consider one binary choice for each directed edge in the graph, to decide whether one variable is a direct causal parent or not. Figure 3 shows that the correct causal graph is quickly recovered. To estimate the gradient, we use a generalization of the regret loss (introduced above, Equation (2)) and its gradient, described in Appendix F.\nIn Appendix G.3, we consider the case of continuous scalar multimodal variables. The ground truth joint distribution is obtained by making the effect B a non-linear function f(A) of cause A, where f is a randomly generated spline. Figure G.1 shows an example of a resulting joint distribution. We model the conditionals with mixture density networks (Bishop, 1994) and the marginals by a Gaussian mixture. We obtain results that are similar to the discrete case, with the correct causal interpretation being recovered quickly, as illustrated in Figure G.2.\nWe also show in Appendix G.4 results on models with two continuous random variables, each being distributed as a multivariate Gaussian, with N = 10 dimensions. Similar to the experiment with discrete random variables, the same argument about parameter counting mentioned in Section 2.2.1 holds here. Again, we obtain results consistent with the previous examples, where the structural parameter \u03b3 converges to 1, effectively recovering the correct causal model A\u2192 B."
        },
        {
            "heading": "3. Representation Learning",
            "text": "So far, we have assumed that the system has unrestricted access to the true underlying causal variables, A and B. However in many realistic scenarios for learning agents, the observations available to the learner might not be instances of the true causal variables but sensory-level data instead, like pixels and sounds. If this is the case, our working assumption \u2013 that the correct causal graph will be sparsely connected, made of independent components, and affected sparsely by distributional shifts \u2013 can not be expected to hold true in general in the space of observed variables. To tackle this, we propose to follow the deep learning objective of disentangling the underlying causal variables (Bengio et al., 2013), and learn a representation in which these properties hold. In the simplest form of this setting, the learner must map its raw observations to a hidden\nrepresentation space H via an encoder E . The encoder is trained such that the hidden space H helps to optimize the meta-transfer objective described above, i.e., we consider the encoder, along with \u03b3, as part of the set of structural or meta-parameters to be optimized with respect to the meta-transfer objective.\nTo study this simplified setting, we consider that our raw observations (X,Y ) originate from the true causal variables (A,B) via the action of a ground truth decoder D (or generator network) that the learner is not aware of but is implicitly trying to invert, as illustrated in Figure 4. The variables A, B, X and Y are assumed to be scalars, and we first consider D be a rotation matrix such that:[\nX Y\n] = R(\u03b8D) [ A B ] (5)\nThe encoder is set to another rotation matrix, one that maps the observations X,Y to the hidden representation U, V as follows: [\nU V\n] = R(\u03b8E) [ X Y ] (6)\nThe causal modules are now to be trained on the variables U and V in the same way as detailed in Section 2, as if they were observed directly. Indeed, if the encoder is valid one would obtain either (U, V ) = (A,B) or (U, V ) = (B,A) up to a negative sign, but we say in that case and without loss of generality that (U, V ) recovered (A,B), corresponding to the solution \u03b8E = \u2212\u03b8D. In this case, the model U \u2192 V is causal and should therefore have an advantage over the anticausal model V \u2192 U , as far as adaptation speed on the transfer distribution is concerned. However, if the encoder is not valid, one would obtain superpositions of the form:\nU = cos(\u03b8)A\u2212 sin(\u03b8)B (7) V = sin(\u03b8)A+ cos(\u03b8)B (8)\nwhere \u03b8 = \u03b8E + \u03b8D. In the extremum where \u03b8 = \u03c0 4 , it is clear that the model U \u2192 V will not have an advantage over the model V \u2192 U in terms of regret on the transfer distribution. However, the question we are interested in is whether it is possible to learn the encoder \u03b8E . We verify this experimentally using Algorithm 1, but where the meta-parameters are now both \u03b3 (choosing between cause and effect which is which) and the parameters of the encoder (here the angle of a rotation matrix). The details of that experiment are provided in Appendix H, which illustrates \u2013 see Figure 5 \u2013 how the proposed objective can disentangle (here in a very simple setting) the ground truth variables (up to permutation)."
        },
        {
            "heading": "4. Related Work",
            "text": "Although this paper focuses on the causal graph, the proposed objective is motivated by the more general question of discovering the underlying causal variables (and their dependencies) that explain the environment of a learner and make it possible for that learner to plan appropriately. The discovery of underlying explanatory variables has come under different names, in particular the notion of disentangling underlying variables (Bengio et al., 2013). As stated already by Bengio et al. (2013) and clearly demonstrated by Locatello et al. (2018), assumptions, priors or biases are necessary to identify the underlying explanatory\n\u2212\u03c04 . For the former, we obtain \u03b8 = 0 corresponding to the correct causal graph U \u2192 V ; and for the latter, \u03b8 = \u03c02 , corresponding to the correct causal graph V \u2192 U . The encoder parameter \u03b8E is trained jointly with the structural parameter, and we find that it converges to one of the two valid solutions. Further details and the corresponding evolution of the structural parameter can be found in Appendix H.\nvariables. The latter paper (Locatello et al., 2018) also reviews and evaluates recent work on disentangling, and discusses different metrics that have been proposed. An extreme view of disentangling is that the explanatory variables should be marginally independent, and many deep generative models (Goodfellow et al., 2016) and independent component analysis models (Hyva\u0308rinen et al., 2001; Hyva\u0308rinen et al., 2018) are built on this assumption. However, the kinds of high-level variables that we manipulate with natural language are not marginally independent: they are related to each other through statements that are usually expressed in sentences (e.g., a classical symbolic AI fact or rule), involving only a few concepts at a time. This kind of assumption has been proposed to help discover such linguistically relevant high-level representations from raw observations, such as the consciousness prior (Bengio, 2017), with the idea that humans focus at any particular time on just a few concepts that are present to our consciousness. The work presented here could provide an interesting meta-learning objective to help learn such encoders as well as figure out how the resulting variables are related to each other. In that case, one should distinguish two important assumptions: the first one is that the causal graph is sparse (has few edges, as in the consciousness prior (Bengio, 2017) and in some methods to learn Bayes net structure, e.g. (Schmidt et al., 2007)); and the second one is that it changes sparsely due to interventions (which is the focus of this work).\nApproaches for Bayesian network structure learning based on discrete search over model structures and simulated annealing are reviewed in Heckerman et al. (1995). There, it has been common to use Minimum Description Length (MDL) principles to score and search over models Lam and Bacchus (1993); Friedman and Goldszmidt (1998), or the Bayesian Information Criterion (BIC) to search for models with high relative posterior probability Heckerman et al. (1995). Prior work such as Heckerman et al. (1995) has also relied upon purely observational data, without the possibility of interventions and therefore focused on learning likelihood or hypothesis equivalence classes for network structures. Since then, numerous methods have also been devised to infer the causal direction from purely observational data (Peters et al., 2017), based on specific, generally parametric assumptions, on the underlying causal graph. Pearl\u2019s seminal work on do-calculus Pearl (1995, 2009); Bareinboim and Pearl (2016) lays a foundation for expressing the impact of interventions on probabilistic graphical models \u2013 we use it in our work. In contrast, here we are proposing a meta-learning objective function for learning causal structure, not requiring any specific constraints on causal graph structure, only on the sparsity of the changes in distribution in the correct causal graph parametrization.\nOur work is also related to other recent advances in causation, domain adaptation and transfer learning. Magliacane et al. (2018) have sought to identify a subset of features that lead to the best predictions for a variable of interest in a source domain such that the conditional distribution of the variable of interest given these features is the same in the target domain. Johansson et al. (2016) examine counterfactual inference and formulate it as a domain adaptation problem. Shalit et al. (2017) propose a technique called counterfactual regression for estimating individual treatment effects from observational data. Rojas-Carulla et al. (2018) propose a method to find an optimal subset that makes the target independent from the selection variables. To do so, they make the assumption that if the conditional distribution of the target given some subset is invariant across different source domains, then this conditional distribution must also be the same in the target domain. Parascandolo et al. (2017) propose an algorithm to recover a set of independent causal mechanisms by establishing competition between mechanisms, hence driving specialization. Alet et al. (2018) proposed a meta learing algorithm to recover a set of specialized modules, but did not establish any connections to causal mechanisms. More recently, Dasgupta et al. (2019) adopted a meta-learning approach to draw causal inferences from purely observational data."
        },
        {
            "heading": "5. Conclusion and Future Work",
            "text": "We have established in very simple bivariate settings that the rate at which a learner adapts to sparse changes in the distribution of observed data can be exploited to select or optimize causal structure and disentangle the causal variables. This relies on the assumption that with the correct causal structure, those distributional changes are localized and sparse. We have demonstrated these ideas through theoretical results as well as experimental validation. See https://github.com/authors-1901-10912/ A-Meta-Transfer-Objective-For-Learning-To-Disentangle-Causal-Mechanisms for source code of the experiments.\nThis work is only a first step in the direction of optimizing causal structure based on the speed of adaptation to modified distributions. On the experimental side, many settings other than those studied here should be considered, with different kinds of parametrizations, richer and larger causal graphs, different kinds of optimization procedures, etc. Also, much more needs to be done in exploring how the proposed ideas can be used to learn good representations in which the causal variables are disentangled, since we have only experimented at this point with the simplest possible encoder with a single degree of freedom. Scaling up these ideas would permit their application towards improving the way in which learning agents deal with non-stationarities, and thus improving sample complexity and robustness of learning agents."
        },
        {
            "heading": "Acknowledgements",
            "text": "We would like to acknowledge support for this project from NSERC, CIFAR and Canada Research Chairs, as well as the feedback from Re\u0301mi Le Priol, Isabelle Lacroix, Alexandre Piche\u0301, and Akram Erraqabi. AG would like to thank Sergey Levine, Chelsea Finn, Michael Chang, Abhishek Gupta for useful discussions."
        },
        {
            "heading": "Appendix A. Results on Non-Identifiability of Causal Structure",
            "text": "We show here that the maximum likelihood estimation of both models specified in Equation (1) yields the same estimated distribution over A and B, i.e., the joint likelihood on the training distribution is not sufficient to distinguish the A\u2192 B and B \u2192 A causal models, in the non-parametric case (no assumption at all on the family of distributions). Let\n\u03b8i = PA\u2192B(A = i) \u03b8j|i = PA\u2192B(B = j | A = i) \u03b7j = PB\u2192A(B = j) \u03b7i|j = PB\u2192A(A = i | B = j).\nWe now state the maximum likelihood estimators for each models:\n\u03b8\u0302i = ni/n \u03b8\u0302j|i = nij/ni\n\u03b7\u0302j = nj/n \u03b7\u0302i|j = nij/nj (9)\nwhere n is the total number of observations, ni the number of times we observed A = i, nj the number of times we observed B = j and nij the number of times we observed A = i and B = j jointly. We can now compute the likelihood for each model:\nP\u0302A\u2192B(A,B) = \u03b8\u0302i\u03b8\u0302j|i = nij/n\nP\u0302B\u2192A(A,B) = \u03b7\u0302j \u03b7\u0302i|j = nij/n (10)\nwhich is what we intended to show. To illustrate this result, we also experiment with learning the modules for both models A \u2192 B and B \u2192 A with SGD. In Figure A.1, we show the difference in log-likelihoods between these two models, evaluated on training and test data sampled from the same distribution, during training. We can see that while the model A\u2192 B fits the data faster than the other model (corresponding to a positive difference in Figure A.1), both models achieve the same log-likelihoods on both models at convergence. This shows that the two models are indistinguishable based on data sampled from the same distribution, even on test data."
        },
        {
            "heading": "Appendix B. Proof of the Zero-Gradient Proposition",
            "text": "Let us restate more formally and prove Proposition 1.\nProposition 1 Consider conditional probability modules P\u03b8i(Vi|pa(i, V,Bi)) where Bij = 1 indicates that Vj is among the parents pa(i, V,Bi) of Vi in a directed acyclic causal graph. Consider ground truth training distribution P1 and transfer distribution P2 over these variables, and ground truth causal structure B. The joint log-likelihood L(V ) for a sample V with respect to the module parameters \u03b8 decomposed into module parameters \u03b8i is L(V ) = \u2211 i logP\u03b8i(Vi|pa(i, V,Bi)). If (a) a model has the correct causal structure B, and (b) it been trained perfectly on P1, leading to estimated parameters \u03b8, and (c) the ground truth P1 and P2 only differ from each other only for some P (Vi|pa(i, V,Bi)) for i \u2208 C, then EV\u223cP2 [ \u2202L(V ) \u2202\u03b8i ] = 0 for i /\u2208 C.\nProof Let V\u2212i be the subset of V excluding Vi. We can simplify the expected gradient as follows.\nEV\u223cP2 [ \u2202L(V ) \u2202\u03b8i ] =\u2211\nV P2(V ) \u2211 k \u2202 \u2202\u03b8i logP\u03b8k(Vk|pa(k, V,Bk))\n= 1i\u2208C \u2211 V\u2212i P2(V\u2212i) \u2211 Vi P2(Vi|pa(i, V,Bi))\n\u2202\n\u2202\u03b8i logP\u03b8i(Vi|pa(i, V,Bi)) + 1i/\u2208C \u2211 V\u2212i P2(V\u2212i) \u2211 Vi P1(Vi|pa(i, V,Bi))\n\u2202\n\u2202\u03b8i logP\u03b8i(Vi|pa(i, V,Bi))\n(11)\nwhere the second equality is obtained because \u03b8i does not influence module k 6= i, and P2 is the same P1 for conditionals with i /\u2208 C (assumption (c)). Now for the special case of i /\u2208 C, we obtain\nEV\u223cP2 [ \u2202L(V ) \u2202\u03b8i ] = \u2211 V\u2212i P2(V\u2212i) \u2211 Vi P1(Vi|pa(i, V,Bi))\n\u2202\n\u2202\u03b8i logP\u03b8i(Vi|pa(i, V,Bi)) = \u2211 V\u2212i P2(V\u2212i) \u2211 Vi P\u03b8i(Vi|pa(i, V,Bi))\n\u2202\n\u2202\u03b8i logP\u03b8i(Vi|pa(i, V,Bi))\n= 0 (12)\nwhere the second equality arises from assumption (b), and the last line from zeroing the inner sum via the general identity \u2211\nv\np\u03b8(v) \u2202\n\u2202\u03b8 log p\u03b8(v) =\n\u2202\n\u2202\u03b8 \u2211 v p\u03b8(v) = \u22021 \u2202\u03b8 = 0."
        },
        {
            "heading": "Appendix C. Pseudo-Code",
            "text": "Draw initial meta-parameters of learner Draw a training set from training distr. Set causal structure to include all edges Initialize learner parameters for this model Pre-train the learner\u2019s parameters on the training set Repeat J times\nDraw a transfer distr. Draw causal structure(s) according to meta-parameters Repeat T times\nSample minibatch from transfer distribution Accumulate online log-likelihood of minibatch Update the model parameters accordingly\nCompute the meta-parameters gradient estimator Update the meta-parameters by SGD Optionally reset parameters to pre-training value\nAlgorithm 1: Meta-Transfer Learning of Causal Structure"
        },
        {
            "heading": "Appendix D. Proof of the Structural Parameter Gradient Proposition",
            "text": "Let us restate more formally and prove Proposition 2.\nProposition 2 The gradient of the negative log-likelihood regret of the transfer data\nR = \u2212 log [sigmoid(\u03b3)LA\u2192B + (1\u2212 sigmoid(\u03b3))LB\u2192A]\nwith respect to the structural parameter \u03b3 (where \u03c3(\u03b3) = P (A\u2192 B)) is given by\n\u2202R \u2202\u03b3 = \u03c3(\u03b3)\u2212 P (A\u2192 B | D2), (13)\nwhere D2 is the transfer data, and P (A \u2192 B | D2) is the posterior probability of the hypothesis A \u2192 B (when the alternative is B \u2192 A), defined by applying Bayes rule to P (D2 | A \u2192 B) = \u220fT t=1 P (at, bt|A \u2192 B, \u03b8t) = LA\u2192B. Furthermore, this can be equivalently written as\n\u2202R \u2202\u03b3 = \u03c3(\u03b3)\u2212 \u03c3(\u03b3 + \u2206), (14)\nwhere \u2206 = logLA\u2192B\u2212 logLB\u2192A is the difference between the log-likelihoods of the two hypotheses on transfer data D2.\nProof First note that, using Bayes rule,\nP (A\u2192 B | D2) = P (D2 | A\u2192 B)P (A\u2192 B) P (D2 | A\u2192 B)P (A\u2192 B) + P (D2 | B \u2192 A)P (B \u2192 A)\n= LA\u2192B\u03c3(\u03b3)\nLA\u2192B\u03c3(\u03b3) + LB\u2192A(1\u2212 \u03c3(\u03b3))\n= \u03c3(\u03b3)LA\u2192B\nM , (15)\nwhere M = \u03c3(\u03b3)LA\u2192B + (1\u2212 \u03c3(\u03b3))LB\u2192A is the online likelihood of the transfer data under the mixture, so that the regret is R = \u2212 logM . For the second line above, note that\nP (D2|A\u2192 B) = T\u220f t=1 P (at, bt|A\u2192 B, {(as, bs)}t\u22121s=1)\n= T\u220f t=1 P (at, bt|A\u2192 B, \u03b8t) = LA\u2192B (16)\nwhere \u03b8t encapsulates the information about {(as, bs)}t\u22121s=1) (through some adaptation procedure). Since we only consider the two hypotheses A\u2192 B and B \u2192 A, we also have P (B \u2192 A | D2) = (1\u2212\u03c3(\u03b3))LB\u2192AM = 1\u2212 P (A\u2192 B | D2). Then\n\u2202R \u2202\u03b3 = \u2212\u03c3(\u03b3)(1\u2212 \u03c3(\u03b3))LA\u2192B \u2212 \u03c3(\u03b3)(1\u2212 \u03c3(\u03b3))LB\u2192A M\n= \u03c3(\u03b3)P (B \u2192 A | D2) \u2212 (1\u2212 \u03c3(\u03b3))P (A\u2192 B | D2) = \u03c3(\u03b3) + \u03c3(\u03b3)P (A\u2192 B | D2) \u2212 P (A\u2192 B | D2)\u2212 \u03c3(\u03b3)P (A\u2192 B | D2) = \u03c3(\u03b3)\u2212 P (A\u2192 B | D2) (17)\nwhich concludes the first part of the proof. Moreover, in order to prove the equivalent formulation in Equation (14), it is sufficient to prove that P (A \u2192 B | D2) = \u03c3(\u03b3 + \u2206). Using the logit function \u03c3\u22121(z) = log z1\u2212z , and the expression in Equation (15), we have\n\u03c3\u22121(P (A\u2192 B | D2)) = log \u03c3(\u03b3)LA\u2192B\nM \u2212 \u03c3(\u03b3)LA\u2192B\n= log \u03c3(\u03b3)LA\u2192B\n(1\u2212 \u03c3(\u03b3))LB\u2192A\n= log \u03c3(\u03b3)\n1\u2212 \u03c3(\u03b3)\ufe38 \ufe37\ufe37 \ufe38 = \u03b3 +\nlogLA\u2192B \u2212 logLB\u2192A\ufe38 \ufe37\ufe37 \ufe38 = \u2206\n= \u03b3 + \u2206 (18)"
        },
        {
            "heading": "Appendix E. Proof of the Proposition on the Convergence Point of Gradient",
            "text": "Descent on the Structural Parameter\nWe use the same notation as in the above proof and statement.\nProposition 3 Stochastic gradient descent (with appropriately decreasing learning rate) on ED2 [R], with R = \u2212 log [sigmoid(\u03b3)LA\u2192B + (1\u2212 sigmoid(\u03b3))LB\u2192A] and with steps following \u2202R\u2202\u03b3 converges towards sigmoid(\u03b3) = 1 if ED2 [logLA\u2192B ] > ED2 [logLB\u2192A], or \u03c3(\u03b3) = 0 otherwise.\nProof We are going to consider the fixed point of gradient descent when the gradient is zero, since we already know that SGD converges with an appropriately decreasing learning rate. Let us introduce some notation to simplify the algebra: p = sigmoid(\u03b3), M = pLA\u2192B + (1\u2212 p)LB\u2192A, so R = logM , and define\nP1 = pLA\u2192B M = P (A\u2192 B | D2), and P2 = (1\u2212p)LB\u2192A M = 1\u2212 P1. Framing the stationary point in terms of p rather than \u03b3 gives us the inequality constraints \u2212p \u2264 0 and p\u2212 1 \u2264 0 and no equality constraint. Applying the KKT conditions with constraint functions \u2212p and p\u2212 1 gives us\nED2 [ \u2202R \u2202p ] = \u2212\u00b51 + \u00b52\n\u00b5i \u2265 0 \u00b51p = 0\n\u00b52(p\u2212 1) = 0 (19)\nWe already see from the last two equations that if p \u2208 (0, 1) (i.e. excluding 0 and 1), we must have \u00b51 = \u00b52 = 0, i.e., E[ \u2202R \u2202p ] = 0 (with drop the D2 subscript on E when it is clear from context). Let us study that case first and show that it leads to an inconsistent set of equations (thus forcing the solution to be either p = 0 or p = 1). Let us rewrite the gradient to highlight p in it:\n\u2202R \u2202p =P (A\u2192 B | D2)\u2212 p\n= pLA\u2192B\npLA\u2192B + (1\u2212 p)LB\u2192A \u2212 p\n= pLA\u2192B \u2212 p(pLA\u2192B + (1\u2212 p)LB\u2192A)\nM\n= p(1\u2212 p)LA\u2192B \u2212 p(1\u2212 p)LB\u2192A\nM\n=p(1\u2212 p)LA\u2192B \u2212 LB\u2192A M\n(20)\nThe KKT conditions with the above two inequality constraints for 0 \u2264 p \u2264 1 give\nE [ \u2202R \u2202p ] = \u00b52 \u2212 \u00b51. (21)\nIf we consider the solutions p \u2208 (0, 1) (i.e., \u00b51 = \u00b52 = 0) we now show that we get a contradiction. First note that to satisfy the above equation with \u00b51 = \u00b52 = 0 means that either p = 0 or p = 1 (which is inconsistent with the assumption that p \u2208 (0, 1)) or that E [LA\u2192B\u2212LB\u2192A M ] = 0. Let us consider that equation, and since p 6= 0 and p 6= 1 we can either multiply by p or by 1\u2212 p on both sides. Assuming p 6= 0 and multiplying by p gives\n0 = E\n[ p(LA\u2192B \u2212 LB\u2192A)\nM\n] = E [ P1 \u2212\npLB\u2192A M ] = E [ P1 \u2212\nLB\u2192A \u2212 LB\u2192A \u2212 pLB\u2192A M ] = E [ P1 +\nLB\u2192A M\n\u2212 P2 ]\n= E [ P1 +\nLB\u2192A M\n\u2212 (1\u2212 P1) ] = E [ LB\u2192A M \u2212 1 ] . (22)\nFor this equation to be satisfied, we need LB\u2192A = M all the time, since LB\u2192A \u2264M by construction. This would however correspond to p = 0. Similarly, assuming p 6= 1 we can multiply the stationarity equation by 1\u2212 p and get\n0 = E\n[ (1\u2212 p)(LA\u2192B \u2212 LB\u2192A)\nM ] = E [ (1\u2212 p)LA\u2192B\nM \u2212 P2 ] = E [ LA\u2192B M \u2212 P1 \u2212 (1\u2212 P1) ] = E [ LA\u2192B M \u2212 1 ]\n(23)\nAgain, this can only be 0 if LA\u2192B = M all the time, i.e., p = 1. We conclude that the solutions p \u2208 (0, 1) are not possible because they would lead to inconsistent conclusions, which leaves only p = 0 or p = 1. When p = 0 we have E[R] = E[logLA\u2192B ], and when p = 1 we have E[R] = E[logLB\u2192A]. Thus the minimum will be achieved at p = 1 when ED2 [logLA\u2192B ] > ED2 [logLB\u2192A], or p = 0 otherwise."
        },
        {
            "heading": "Appendix F. More Than Two Causal Hypotheses",
            "text": "In this section, we consider one approach to generalize to more than two causal structures. We consider m variables, corresponding to O(2m 2\n) possible causal graphs, since each variable Vj could be (or not) a direct cause of any variable Vi, leading to m\n2 binary decisions. Note that a causal graph can in principle have cycles (if time is not measured with sufficient precision), although having a directed acyclic graph allows a much simpler sampling procedure (ancestral sampling). In our experiments the ground truth graph will always be directed, to make sampling easier and faster, but the learning procedure will not directly assume that. Motivated by the mechanism independence assumption, we propose a heuristic to learn the causal graph in which we independently parametrize the binary probability pij that Vj is a parent (direct cause) of Vi. As was the case for Section 2, we parametrize this Binomial distribution via binary edges Bij that specify the graph structure:\nBij \u223c Bernoulli(pij), P (B) = \u220f ij P (Bij). (24)\nwhere pij = sigmoid(\u03b3ij). Let us define the parents of Vi, given B, as the set of Vj \u2019s such that Bij = 1:\npa(i, V,Bi) = {Vj | Bij = 1, j 6= i} (25)\nwhere Bi is the bit vector with elements Bij (and Bii = 0 is ignored). Similarly, we could parametrize the causal graph with a structural causal model where some of the inputs (from variable j) of each function (for variable i) can be ignored with some probability pij :\nVi = fi(\u03b8i, Bi, V,Ni) (26)\nwhere Ni is an independent noise source to generate Vi and fi parametrizes the generator (as in a GAN), while not being allowed to use variable Vj unless Bij = 1 (and of course not being allowed to use Vi). We can consider that fi is a kind of neural network similar to the denoising auto-encoders or with dropout on the input, where Bi is a binary mask vector that prevents fi from using some of the Vj \u2019s (for which Bij = 0).\nThe conditional likelihood PBi(Vi = vti | pa(i, vt, Bi)) measures how well the model that uses the incoming edges Bi for node i performs for example vt. We build a multiplicative (or exponentiated) form of regret by multiplying these likelihoods as \u03b8t changes during an adaptation episode, for node i:\nLBi = \u220f t PBi(Vi = vti | pa(i, vt, Bi)). (27)\nThe overall exponentiated regret for the given graph structure B is LB = \u220f i LBi . Similarly to the bivariate case, we want to consider a mixture over all the possible graph structures, but where each component must explain the whole adaptation sequence, thus we define as a loss for the generalized multi-variable case\nR = \u2212 logEB [LB ] (28)\nNote the expectation over the 2m 2\npossible values of B, which is intractable. However, we can still get an efficient stochastic gradient estimator, which can be computed separately for each node of the graph (with samples arising only out of Bi, the incoming edges into Vi):\nProposition 4 The overall regret (Equation (28)) rewrites R = \u2212 \u2211 i log \u2211 Bi P (Bi)LBi (29)\nand if we are willing to consider multiple samples of B in parallel, a biased but asymptotically unbiased (as the number K of these samples B(k) increases to infinity) estimator of the gradient of the overall regret with respect to meta-parameters can be defined:\ngij =\n\u2211 k(\u03c3(\u03b3ij)\u2212B (k) ij )L\n(k) Bi\u2211\nk L (k) Bi\n(30)\nwhere the (k) index indicates the values obtained for the k-th draw of B. Proof Recall that LB = \u220f i LBi so we can rewrite the regress loss as follows:\nR = \u2212 logEB [LB ] = \u2212 log \u2211 B P (B)LB\n= \u2212 log \u2211 B1 \u2211 B2 . . . \u2211 BM \u220f i P (Bi)LBi\n= \u2212 log \u220f i (\u2211 Bi P (Bi)LBi ) = \u2212\n\u2211 i log \u2211 Bi P (Bi)LBi (31)\nSo the regret gradient on meta-parameters \u03b3i of node i is\n\u2202R \u2202\u03b3i\n= \u2212 \u2211 Bi P (Bi)LBi \u2202 logP (Bi) \u2202\u03b3i\u2211\nBi P (Bi)LBi\n= \u2212 EBi [LBi \u2202 logP (Bi) \u2202\u03b3i ]\nEBi [LBi ] (32)\nNote that with the sigmoidal parametrization of P (Bij),\nlogP (Bij) = Bij log sigmoid(\u03b3ij) + (1\u2212Bij) log(1\u2212 sigmoid(\u03b3ij))\nas in the cross-entropy loss. Its gradient can similarly be simplified to\n\u2202 logP (Bij)\n\u2202\u03b3ij = Bij sigmoid(\u03b3ij) sigmoid(\u03b3ij)(1\u2212 sigmoid(\u03b3ij))\n\u2212 (1\u2212Bij) (1\u2212 sigmoid(\u03b3ij)) sigmoid(\u03b3ij)(1\u2212 sigmoid(\u03b3ij)))\n= Bij \u2212 sigmoid(\u03b3ij) (33)\nA biased but asymptotically unbiased estimator of \u2202R\u2202\u03b3ij is thus obtained by sampling K graphs (over which the means below are run):\ngij = \u2211 k (\u03c3(\u03b3ij)\u2212B(k)ij ) L B (k) i\u2211 k\u2032 LB(k\u2032)i (34)\nwhere index (k) indicates the k-th draw of B, and we obtain a weighted sum of the individual binomial gradients weighted by the relative regret of each draw B (k) i of Bi, leading to Equation (30).\nThis decomposition is good news because the loss is a sum of independent terms, one per node i, depending only of Bi and and similarly gij only depends on Bi rather than the full graph structure. We use the estimator from Equation (30) in the general pseudo-code for meta-transfer learning of causal structure displayed in Algorithm 1."
        },
        {
            "heading": "Appendix G. Results on Learning which is Cause and which is Effect",
            "text": "In order to assess the performance of our meta-learning algorithm, we applied it on generated data from three different domains: discrete random variables, multimodal continuous random variables and multivariate gaussian-distributed variables. In this section, we describe the setups for all three experiments, along with additional results to complement the results described in the main text. Note that in all these experiments, we fix the structure of the ground-truth to be A\u2192 B, and only perform interventions on the cause A.\nG.1 Discrete variables and Two Causal Hypotheses\nWe consider a bivariate model, where both random variables are sampled from a categorical distribution. The underlying ground-truth model can be described as\nA \u223c Categorical(\u03c0A) B | A = a \u223c Categorical(\u03c0B|a), (35)\nwith \u03c0A is a probability vector of size N , and \u03c0B|a is a probability vector of size N , which depends on the value of the variable A. In our experiment, each random variable can take one of N = 10 values. Since we are working with only two variables, the only two possible models are:\n\u2022 Model A\u2192 B: P (A,B) = P (A)P (B | A)\n\u2022 Model B \u2192 A: P (A,B) = P (B)P (A | B) We build 4 different modules, corresponding to the model of each possible marginal and conditional distribution. These modules\u2019 definition and their corresponding parameters are shown in Table G.1.\nDistribution Module Parameters Dimension\nModel A\u2192 B P (A) P (xA = i ; \u03b8A) = [softmax(\u03b8A)]i \u03b8A N P (B | A) P (xB = j | xA = i ; \u03b8B|A) = [softmax(\u03b8B|A(i))]j \u03b8B|A N2 Model B \u2192 A P (B) P (xB = j ; \u03b8B) = [softmax(\u03b8B)]j \u03b8B N P (A | B) P (xA = i | xB = j ; \u03b8A|B) = [softmax(\u03b8A|B(j))]i \u03b8A|B N2\nTable G.1: Description of the 2 models, with the parametrization of each module, for a bivariate model with discrete random variables. Model A\u2192 B and Model B \u2192 A both have the same number of parameters N2 +N .\nIn order to get a set of initial parameters, we first train all 4 modules on a training distribution. This training distribution corresponds to a fixed choice of \u03c0 (1) A and \u03c0B|a (for all N possible values of a). Note that the superscript in \u03c0 (1) A emphasizes the fact that this defines the distribution prior to intervention, with the mechanism P (B | A) being unchanged by the intervention. These probability vectors are sampled randomly from a uniform Dirichlet distribution\n\u03c0 (1) A \u223c Dirichlet(1N )\n\u03c0B|a \u223c Dirichlet(1N ) \u2200a \u2208 [1, N ]. (36)\nGiven this initial training distribution, we can sample a large dataset of training examples {(ai, bi)}ni=1 from the ground-truth model, using ancestral sampling.\na \u223c Categorical(\u03c0(1)A ) b \u223c Categorical(\u03c0B|a). (37)\nUsing this large dataset from the training distribution, we can train all 4 modules using gradient descent, or any other advanced first-order optimizer, like RMSprop. The parameters \u03b8A, \u03b8B|A, \u03b8B & \u03b8A|B of the different modules found after this initial training will be used as the initial parameters for the adaptation on a new transfer distribution.\nSimilar to the way we defined the training distribution, we can define a transfer distribution as a soft intervention on the random variable A. In this experiment, this accounts for changing the distribution of A, that is with a new probability vector \u03c0 (2) A , also sampled randomly from a uniform Dirichlet distribution\n\u03c0 (2) A \u223c Dirichlet(1N ) (38)\nTo perform adaptation on the transfer distribution, we also sample a smaller dataset of transfer examples D2 = {(ai, bi}mi=1, with m n the size of the training set. In our experiment, we used m = 20 transfer examples. We also used ancestral sampling on this new transfer distribution to acquire samples, similar to Equation (37) (with \u03c0 (2) A instead of \u03c0 (1) A ).\nStarting from the parameters estimated after the initial training on the training distribution, we perform a few steps of adaptation on the modules parameters \u03b8A, \u03b8B|A, \u03b8B & \u03b8A|B using T steps of gradient descent based on the transfer dataset D2. The value of the likelihoods for both models is recorded as well, and computed as\nLA\u2192B = T\u220f t=1 P (at | \u03b8(t)A )P (bt | at ; \u03b8 (t) B|A)\nLB\u2192A = T\u220f t=1 P (bt | \u03b8(t)B )P (at | bt ; \u03b8 (t) A|B), (39)\nwhere (at,bt) represents a mini-batch of examples from D2, and the superscript t on the parameters highlights the fact that these likelihoods are computed after t steps of adaptation. This product over t ensures that we monitor the progress of adaptation along the whole trajectory. In this experiment, we used T = 2 steps of gradient descent on mini-batch of size 10 for the adaptation.\nFinally, in order to update the structural parameter \u03b3, we can use Proposition 2 to compute the gradient of the loss L with respect to \u03b3:\nR(\u03b3) = \u2212 log[\u03c3(\u03b3)LA\u2192B + (1\u2212 \u03c3(\u03b3))LB\u2192A] (40) \u2202R \u2202\u03b3 = \u03c3(\u03b3 + \u2206)\u2212 \u03c3(\u03b3), (41)\nwhere \u2206 = logLA\u2192B \u2212 logLB\u2192A. The update of \u03b3 can be one step of gradient descent, or using any first-order optimizer like RMSprop. We perform multiple interventions over the course of meta-training by sampling multiple transfer distributions, and following the same steps of adaptation and update of the structural parameter \u03b3.\nIn Figure 2, we report the evolution of the structural parameter \u03b3 (or rather, \u03c3(\u03b3)) as a function of the number of meta-training steps or, similarly, the number of different interventions made on the causal model. The model\u2019s belief P (A \u2192 B) = \u03c3(\u03b3) indeed converges to 1, proving that the algorithm was capable of recovering the correct causal direction A\u2192 B.\nG.2 Discrete Variables with MLP Parametrization\nWe consider a bivariate model similar to the ones defined above, where each random variable is sampled from a categorical distribution. Instead of expressing probabilities in a tabular form, we train M = 2 simple feed-foward neural networks (MLP), one per conditional variable. MLP i is the independent mechanism of causal variable i that determines the conditional probability of the N discrete choices for variable i, given its parents.\nEach MLP receives M concatenated N -dimensional one-hot vectors, masked appropriately according to the chosen causal structure B, i.e., with the j-th input of the i-th MLP being multiplied by Bij . Each\ndirected edge presence or absence is thus indicated by Bij , with Bij = 1 if variable j is a direct causal parent of variable i. The MLP maps the MN input units through one hidden layer that contains H = 4M hidden units and a ReLU non-linearity, and then maps the H hidden units to N output units and a softmax representing a predicted categorical distribution.\nThe causal structure belief is specified by an M \u00d7M matrix \u03b3, with \u03c3(\u03b3ij) the estimated probability that variable i is directly caused by variable j. The causal structure Bij is drawn from Ber(\u03b3ij), as per Algorithm 1. We generalize the estimator introduced for the 2-hypotheses case as per Appendix F, i.e., we use the gradient estimator in Equation 30.\nTo evaluate the correctness of the structure being learnt, we measure the cross entropy between the ground-truth SCM and the learned SCM. In Figure 3 we show this cross-entropy over different episodes of training for bivariate discrete distributions with either 10 categories or 100 categories. Both models are first pretrained for 100 examples with fully connected edges before starting training on the transfer distributions.\nG.3 Continuous Multimodal Variables\nConsider a family of joint distributions P\u00b5(A,B) over the causal variables A and B sampled from the structural causal model (SCM):\nA \u223c P\u00b5(A) = N (\u00b5, \u03c32 = 4) B := f(A) +NB NB \u223c N (\u00b5 = 0, \u03c32 = 1) (42)\nwhere f is a randomly generated spline and NB is sampled i.i.d from the unit-normal distribution. To obtain the spline, we sample the K points {xk}Kk=1 uniformly spaced from the interval [\u2212RA, RA], and another K points {yk}Kk=1 uniform randomly from the interval [\u2212RB , RB ]. This yields K pairs {(xk, yk)}Kk=1, which make the knots of a second-order spline. We set K = 8, RA = RB = 8 for our experiments. In Figure G.1, we plot samples from one such SCM for the training distribution (\u00b5 = 0) and two transfer distributions (\u00b5 = \u00b14).\n10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 A\n15\n10\n5\n0\n5\n10\nB\nTransfer ( = 4) Transfer ( = + 4) Training\nFigure G.1: Train (red) and transfer (green and blue) samples from an SCM generated with the procedure described in Equation (42). The green data-points are sampled from P\u00b5=(\u22124)(A,B), whereas the blue data-points are samples from P\u00b5=(+4)(A,B) and the red data points (training set) are from P\u00b5=0(A,B).\nThe conditionals P (B | A ; \u03b8B|A) and P (A | B ; \u03b8A|B) are parameterized as 2-layer Mixture Density Networks Bishop (1994) with 32 hidden units and 10 components. The marginals P (A | \u03b8A) and P (B | \u03b8B) are parameterized as Gaussian Mixture Models, also with 10 components. The training now follows as described below.\nSimilar to Appendix G.1, we first pre-train the modules corresponding to the conditionals and marginals on the training distribution. To that end, we select P\u00b5=0(A,B) as the training distribution, sample a\n(large) training dataset {(ai, bi)}ni=1 from it using ancestral sampling, and solve the following two problems independently until convergence:\nmax \u03b8A,\u03b8B|A n\u2211 i=1 logP (ai | \u03b8A)P (bi | ai; \u03b8B|A) (43)\nmax \u03b8B ,\u03b8A|B n\u2211 i=1 logP (bi | \u03b8B)P (ai | bi; \u03b8A|B) (44)\nThe adaptation performance of A\u2192 B and B \u2192 A models can now be evaluated on transfer distributions. For a \u00b5 sampled uniformly in [\u22124, 4], we select P\u00b5(A\u2032, B\u2032) as the transfer distribution, and denote with (A\u2032, B\u2032) samples from it. Both models are fine-tuned on P\u00b5(A\n\u2032, B\u2032) for T = 10 iterations (see Algorithm 1), and the area under the corresponding negative-log-likelihood curves becomes the regret:\nRA\u2192B = \u2212 T\u2211 t=1 logP (B\u2032|A\u2032; \u03b8(t)A\u2192B)P (A \u2032|\u03b8(T )A\u2192B) (45)\nand likewise for RB\u2192A. In these experiments, the modules corresponding to the marginals (ie. GMM) are learned offline via Expectation Maximization, and we denote with P (A\u2032|\u03b8(T )A\u2192B) the trained model. These can now be used to define the following meta-objective for the structural meta-parameter \u03b3:\nR(\u03b3) = log[\u03c3(\u03b3)eRA\u2192B + (1\u2212 \u03c3(\u03b3))eRB\u2192A ] (46)\nThe structural regretR(\u03b3) is now minimized with respect to \u03b3 for 200 iterations (updates of \u03b3). Figure G.2 shows the evolution of \u03c3(\u03b3) as training progresses. This is expected, given that we expect the causal model to perform better on the transfer distributions, i.e. we expect RA\u2192B < RB\u2192A in expection. Consequently, assigning a larger weight to RA\u2192B optimizes the objective.\n0 100 200 300 400 Number of episodes\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n( )\nN = 10\nFigure G.2: Evolution of the sigmoid of structural meta-parameter \u03c3(\u03b3) with training iterations. It is indeed expected to increase if A\u2192 B is the true causal graph (see Equation (46)).\nG.4 Linear Gaussian Model\nIn this experiment, the two variables we consider are vectors (i.e. A \u2208 Rd and B \u2208 Rd). The ground truth causal model is given by\nA \u223c N (\u00b5A,\u03a3A) B := \u03b21A+ \u03b20 +NB NB \u223c N (0,\u03a3B) (47)\nwhere \u00b5A \u2208 Rd, \u03b20 \u2208 Rd and \u03b21 \u2208 Rd\u00d7d. \u03a3A and \u03a3B are d\u00d7 d covariance matrices2. In our experiments, d = 100. Once again, we want to identify the correct causal direction between A and B. To do so, we consider two models: A\u2192 B and B \u2192 A. We parameterize both models symmetrically:\nPA\u2192B(A) = N (A; \u00b5\u0302A, \u03a3\u0302A) PA\u2192B(B | A = a) = N (B; W\u03021a+ W\u03020, \u03a3\u0302A\u2192B)\nPB\u2192A(B) = N (B; \u00b5\u0302B , \u03a3\u0302B) PB\u2192A(A | B = b) = N (B; V\u03021b+ V\u03020, \u03a3\u0302B\u2192A) (48)\nNote that each covariance matrix is parameterized using the Cholesky decomposition. Unlike previous experiments, we are not conducting any pre-training on actual data. Instead, we fix the parameters of both models to their exact values according to the ground truth parameters introduced in Equation 47. For model A\u2192 B, this can be done trivially. For the second model, we can compute its exact parameters analytically. Once the exact parameters are set, both models are equivalent in the sense that PA\u2192B(A,B) = PB\u2192A(A,B) \u2200A,B.\nEach meta-learning episode starts by initializing the parameters of both models to the values identified during the pre-training. Afterward, a transfer distribution is sampled (i.e. \u00b5A \u223c N (0, I)). Then, both models are trained on samples from this distribution, for 10 iterations only. During this adaptation, the log-likelihoods of both models are accumulated in order to compute LA\u2192B and LB\u2192A. At this stage, we compute the meta objective estimate R = \u2212 log [sigmoid(\u03b3)LA\u2192B + (1\u2212 sigmoid(\u03b3))LB\u2192A], compute its gradient w.r.t. \u03b3 and update \u03b3.\nFigure G.3 shows that, after 200 episodes, \u03c3(\u03b3) converges to 1, indicating the success of the method on this particular task.\nFigure G.3: Convergence of the causal belief (to the correct answer) as a function of the number of metalearning episodes, for the linear Gaussian experiments."
        },
        {
            "heading": "Appendix H. Results on Learning the Correct Encoder",
            "text": "The causal variables (A,B) are sampled from the distribution described in Eqn 42, and are mapped to observations (X,Y ) \u223c P\u00b5(X,Y ) via a hidden (and a priori unknown) decoder D = R(\u03b8D), where R is a rotation matrix. The observations are then mapped to the hidden state (U, V ) \u223c P\u00b5(U, V ) via the encoder E = R(\u03b8E). The computational graph is depicted in Figure 4.\n2. Ground truth parameters \u00b5A, \u03b21 and \u03b20 are sampled from a Gaussian distribution, while \u03a3A and \u03a3B are sampled from an inverse Wishart distribution.\nAnalogous to Equation 46 in Appendix G.3, we now define the regret over the variables (U, V ) instead of (A,B):\nR(\u03b3, \u03b8E) = log[\u03c3(\u03b3)eRU\u2192V + (1\u2212 \u03c3(\u03b3))eRV\u2192U ] (49)\nwhere the dependence on \u03b8E is implicit in (U, V ). In every meta-training iteration, the U \u2192 V and V \u2192 U models are trained on the training distribution P\u00b5=0(U, V ) for T\n\u2032 = 20 iterations. Subsequently, the regrets RU\u2192V and RV\u2192U are obtained by a process identical to that described in Equation 45 of Appendix G.3 (albeit with variables (U, V ) and T = 5). Finally, the gradients of R(\u03b3, \u03b8E) are evaluated and the metaparameters \u03b3 and \u03b8E are updated. This process is repeated for 1000 meta-iterations, and Figure 5 shows the evolution of \u03b8E as training progresses (where \u03b8D has been set to \u2212\u03c04 ). Further, Figure H.1 shows the corresponding evolution of the structural parameter \u03b3.\n0 200 400 600 800 1000 Iterations\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nSt ru\nct ur\nal P\nar am\net er\nFigure H.1: Evolution of the structural parameter \u03b3 as training progresses with the encoder. The corresponding evolution of the encoder parameter \u03b8E is shown in Figure 5. Observe that the system converges to \u03b8 = 0, implying that the correct causal direction is U \u2192 V and the parameter \u03b3 should increase with meta-training iterations."
        }
    ],
    "title": "A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms",
    "year": 2019
}