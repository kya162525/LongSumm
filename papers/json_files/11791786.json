{
    "abstractText": "The ability to recognize and predict temporal sequences of sensory inputs is vital for survival in natural environments. Based on many known properties of cortical neurons, hierarchical temporal memory (HTM) sequence memory is recently proposed as a theoretical framework for sequence learning in the cortex. In this paper, we analyze properties of HTM sequence memory and apply it to sequence learning and prediction problems with streaming data. We show the model is able to continuously learn a large number of variable-order temporal sequences using an unsupervised Hebbian-like learning rule. The sparse temporal codes formed by the model can robustly handle branching temporal sequences by maintaining multiple predictions until there is sufficient disambiguating evidence. We compare the HTM sequence memory with other sequence learning algorithms, including statistical methods: autoregressive integrated moving average (ARIMA), feedforward neural networks: online sequential extreme learning machine (ELM), and recurrent neural networks: long short-term memory (LSTM) and echo-state networks (ESN), on sequence prediction problems with both artificial and real-world data. The HTM model achieves comparable accuracy to other state-of-the-art algorithms. The model also exhibits properties that are critical for sequence learning, including continuous online learning, the ability to handle multiple predictions and branching sequences with high order statistics, robustness to sensor noise and fault tolerance, and good performance without task-specific hyperparameters tuning. Therefore the HTM sequence memory not only advances our understanding of how the brain may solve the sequence learning problem, but is also applicable to a wide range of real-world problems such as discrete and continuous sequence prediction, anomaly detection, and sequence classification.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuwei Cui"
        },
        {
            "affiliations": [],
            "name": "Subutai Ahmad"
        },
        {
            "affiliations": [],
            "name": "Jeff Hawkins"
        }
    ],
    "id": "SP:505a5f90ba73c76011d5e07fe2486050f9bb14b7",
    "references": [
        {
            "authors": [
                "M Abeles"
            ],
            "title": "Local cortical circuits: an electrophysiological study",
            "year": 1982
        },
        {
            "authors": [
                "S Ahmad",
                "J Hawkins"
            ],
            "title": "How do neurons operate on sparse distributed representations? A mathematical theory of sparsity, neurons and active dendrites. :arXiv:1601.00720 [q \u2013 bio.NC",
            "year": 2016
        },
        {
            "authors": [
                "SD Antic",
                "WL Zhou",
                "AR Moore",
                "SM Short",
                "KD Ikonomu"
            ],
            "title": "The decade of the dendritic NMDA spike",
            "venue": "J Neurosci Res",
            "year": 2010
        },
        {
            "authors": [
                "S Ben Taieb",
                "G Bontempi",
                "AF Atiya",
                "Sorjamaa"
            ],
            "title": "A (2012) A review and comparison of strategies for multi-step ahead time series forecasting based on the NN5 forecasting competition",
            "venue": "Expert Syst Appl 39:7067\u20137083",
            "year": 2012
        },
        {
            "authors": [
                "C Bishop"
            ],
            "title": "Pattern recognition and machine learning",
            "year": 2006
        },
        {
            "authors": [
                "J Brea",
                "W Senn",
                "J-P Pfister"
            ],
            "title": "Matching recall and storage in sequence learning with spiking neural networks",
            "venue": "J Neurosci",
            "year": 2013
        },
        {
            "authors": [
                "J Bridle"
            ],
            "title": "Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. In: Neurocomputing \u2212 Algorithms, Architectures and Applications., pp 227\u2013236",
            "year": 1989
        },
        {
            "authors": [
                "M Brosch",
                "CE Schreiner"
            ],
            "title": "Sequence sensitivity of neurons in cat primary auditory cortex",
            "venue": "Cereb Cortex",
            "year": 2000
        },
        {
            "authors": [
                "SF Crone",
                "M Hibon",
                "K Nikolopoulos"
            ],
            "title": "Advances in forecasting with neural networks? Empirical evidence from the NN3 competition on time series prediction",
            "venue": "Int J Forecast",
            "year": 2011
        },
        {
            "authors": [
                "TG Dietterich"
            ],
            "title": "Machine learning for sequential data: a review",
            "venue": "Proceedings of the Joint IAPR International Workshop on Structural, Syntactic, and Statistical Pattern Recognition,",
            "year": 2002
        },
        {
            "authors": [
                "P Domingos",
                "G Hulten"
            ],
            "title": "Mining high-speed data streams. In: Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining - KDD \u201900, pp 71\u201380",
            "year": 2000
        },
        {
            "authors": [
                "J Durbin",
                "SJ Koopman"
            ],
            "title": "Time series analysis by state space methods, 2 edition",
            "year": 2012
        },
        {
            "authors": [
                "S Fine",
                "Y Singer",
                "N Tishby"
            ],
            "title": "The hierarchical hidden markov model: analysis and applications",
            "venue": "Mach Learn 32:41\u201362",
            "year": 1998
        },
        {
            "authors": [
                "P F\u00f6ldi\u00e1k"
            ],
            "title": "The handbook of brain theory and neural networks. In, Second Edi. (Arbib M, ed), pp 1064\u20131068",
            "year": 2002
        },
        {
            "authors": [
                "J Gama"
            ],
            "title": "Knowledge discovery from data streams",
            "venue": "Boca Raton, Florida: Chapman and Hall/CRC",
            "year": 2010
        },
        {
            "authors": [
                "JP Gavornik",
                "MF Bear"
            ],
            "title": "Learned spatiotemporal sequence recognition and prediction in primary visual cortex",
            "venue": "Nat Neurosci",
            "year": 2014
        },
        {
            "authors": [
                "A Graves"
            ],
            "title": "Supervised sequence labelling with recurrent neural networks",
            "year": 2012
        },
        {
            "authors": [
                "J Hawkins",
                "S Ahmad"
            ],
            "title": "Why neurons have thousands of synapses, a theory of sequence memory in neocortex",
            "venue": "Front Neural Circuits",
            "year": 2016
        },
        {
            "authors": [
                "J Hawkins",
                "S Ahmad",
                "D Dubinsky"
            ],
            "title": "Cortical learning algorithm and hierarchical temporal memory. Numenta Whitepaper:1\u201368 Available at: http://numenta.org/resources/HTM_CorticalLearningAlgorithms.pdf",
            "year": 2011
        },
        {
            "authors": [
                "D Hebb"
            ],
            "title": "The organization of behavior: a neuropsychological theory",
            "venue": "Sci Educ",
            "year": 1949
        },
        {
            "authors": [
                "M Henaff",
                "A Szlam",
                "Y Lecun"
            ],
            "title": "Orthogonal RNNs and long-memory tasks. :arXiv:1602.06662 [cs.NE",
            "year": 2016
        },
        {
            "authors": [
                "G Hinton",
                "N Srivastava",
                "A Krizhevsky",
                "I Sutskever",
                "R Salakhutdinov"
            ],
            "title": "Improving neural networks by preventing coadaptation of feature detectors",
            "year": 2012
        },
        {
            "authors": [
                "S Hochreiter",
                "J Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural Comput",
            "year": 1997
        },
        {
            "authors": [
                "G-B Huang",
                "DH Wang",
                "Y Lan"
            ],
            "title": "Extreme learning machines: a survey",
            "venue": "Int J Mach Learn Cybern",
            "year": 2011
        },
        {
            "authors": [
                "Huang G-B",
                "Zhu Q-Y",
                "Siew C-K"
            ],
            "title": "Extreme learning machine: theory and applications",
            "venue": "Neurocomputing",
            "year": 2006
        },
        {
            "authors": [
                "RJ Hyndman",
                "G Athanasopoulos"
            ],
            "title": "Forecasting: principles and practice. OTexts",
            "year": 2013
        },
        {
            "authors": [
                "RJ Hyndman",
                "Y Khandakar"
            ],
            "title": "Automatic time series forecasting: The forecast package for R",
            "venue": "J Stat Softw",
            "year": 2008
        },
        {
            "authors": [
                "C Igel",
                "M H\u00fcsken"
            ],
            "title": "Empirical evaluation of the improved Rprop learning algorithms. Neurocomputing 50:105\u2013123",
            "year": 2003
        },
        {
            "authors": [
                "H Jaeger"
            ],
            "title": "Tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the \u201cecho state network",
            "year": 2002
        },
        {
            "authors": [
                "H Jaeger",
                "H Haas"
            ],
            "title": "Harnessing nonlinearity: predicting chaotic systems and saving energy in wireless communication",
            "venue": "Science",
            "year": 2004
        },
        {
            "authors": [
                "P Kanerva"
            ],
            "title": "Sparse Distributed Memory",
            "year": 1988
        },
        {
            "authors": [
                "A Lavin",
                "S Ahmad"
            ],
            "title": "Evaluating real-time anomaly detection algorithms - the numenta anomaly benchmark",
            "venue": "In: 14th International Conference on Machine Learning and Applications (IEEE ICMLA)",
            "year": 2015
        },
        {
            "authors": [
                "M Lee",
                "K Hwang",
                "W Sung"
            ],
            "title": "Fault tolerance analysis of digital feed-forward deep neural networks",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2014
        },
        {
            "authors": [
                "Liang N-Y",
                "Huang G-B",
                "Saratchandran P",
                "Sundararajan N"
            ],
            "title": "A fast and accurate online sequential learning algorithm for feedforward networks",
            "venue": "IEEE Trans Neural Netw",
            "year": 2006
        },
        {
            "authors": [
                "ZC Lipton",
                "J Berkowitz",
                "C Elkan"
            ],
            "title": "A critical review of recurrent neural networks for sequence learning",
            "year": 2015
        },
        {
            "authors": [
                "E Lughofer",
                "P Angelov"
            ],
            "title": "Handling drifts and shifts in on-line data streams with evolving fuzzy systems",
            "venue": "Appl Soft Comput",
            "year": 2011
        },
        {
            "authors": [
                "G Major",
                "ME Larkum",
                "J Schiller"
            ],
            "title": "Active properties of neocortical pyramidal neuron dendrites",
            "venue": "Annu Rev Neurosci",
            "year": 2013
        },
        {
            "authors": [
                "V Massey P",
                "ZI Bashir"
            ],
            "title": "Long-term depression: multiple forms and implications for brain function",
            "venue": "Trends Neurosci",
            "year": 2007
        },
        {
            "authors": [
                "MD Mauk",
                "V Buonomano D"
            ],
            "title": "The neural basis of temporal processing",
            "venue": "Annu Rev Neurosci",
            "year": 2004
        },
        {
            "authors": [
                "JM McFarland",
                "Y Cui",
                "DA Butts"
            ],
            "title": "Inferring nonlinear neuronal computation based on physiologically plausible inputs",
            "venue": "PLoS Comput Biol 9:e1003143",
            "year": 2013
        },
        {
            "authors": [
                "T Mikolov",
                "K Chen",
                "G Corrado",
                "J Dean"
            ],
            "title": "Efficient Estimation of Word Representations in Vector Space. :arXiv:1301.3781",
            "year": 2013
        },
        {
            "authors": [
                "J Mnatzaganian",
                "E Fokou\u00e9",
                "D Kudithipudi"
            ],
            "title": "2016) A mathematical formalization of hierarchical temporal memory\u2019s spatial pooler. :arXiv:1601.06116 [stat.ML",
            "year": 2016
        },
        {
            "authors": [
                "L Moreira-Matias",
                "J Gama",
                "M Ferreira",
                "J Mendes-Moreira",
                "L Damas"
            ],
            "title": "Predicting taxi\u2013passenger demand using streaming data",
            "venue": "IEEE Trans Intell Transp Syst 14:1393\u20131402",
            "year": 2013
        },
        {
            "authors": [
                "VB Mountcastle"
            ],
            "title": "The columnar organization of the neocortex. Brain 120 ( Pt 4:701\u2013722",
            "year": 1997
        },
        {
            "authors": [
                "D Nikoli\u0107",
                "S H\u00e4usler",
                "W Singer",
                "W Maass"
            ],
            "title": "Distributed fading memory for stimulus properties in the primary visual cortex",
            "venue": "PLoS Biol 7:e1000260",
            "year": 2009
        },
        {
            "authors": [
                "BA Olshausen",
                "DJ Field"
            ],
            "title": "Sparse coding of sensory inputs",
            "venue": "Curr Opin Neurobiol",
            "year": 2004
        },
        {
            "authors": [
                "A Polsky",
                "BW Mel",
                "J Schiller"
            ],
            "title": "Computational subunits in thin dendrites of pyramidal cells",
            "venue": "Nat Neurosci",
            "year": 2004
        },
        {
            "authors": [
                "F Ponulak"
            ],
            "title": "Kasi\u0144ski A (2010) Supervised learning in spiking neural networks with ReSuMe: sequence learning, classification, and spike shifting",
            "venue": "Neural Comput",
            "year": 2010
        },
        {
            "authors": [
                "S Purdy"
            ],
            "title": "Encoding Data for HTM Systems. arXiv:1602.05925",
            "year": 2016
        },
        {
            "authors": [
                "L Rabiner",
                "B Juang"
            ],
            "title": "An introduction to hidden Markov models",
            "venue": "IEEE ASSP Mag",
            "year": 1986
        },
        {
            "authors": [
                "RP Rao",
                "TJ Sejnowski"
            ],
            "title": "Predictive learning of temporal sequences in recurrent neocortical circuits",
            "venue": "Novartis Found Symp 239:208\u2013229;",
            "year": 2001
        },
        {
            "authors": [
                "N Sadato",
                "A Pascual-Leone",
                "J Grafman",
                "V Iba\u00f1ez",
                "MP Deiber",
                "G Dold",
                "M Hallett"
            ],
            "title": "Activation of the primary visual cortex by Braille reading in blind subjects",
            "venue": "Nature",
            "year": 1996
        },
        {
            "authors": [
                "M Sayed-Mouchaweh",
                "E Lughofer"
            ],
            "title": "Learning in non-stationary environments: methods and applications",
            "year": 2012
        },
        {
            "authors": [
                "J Schmidhuber"
            ],
            "title": "Simple algorithmic theory of subjective beauty, novelty, surprise, interestingness, attention, curiosity, creativity, art, science, music, jokes",
            "venue": "J SICE 48:21\u201332",
            "year": 2009
        },
        {
            "authors": [
                "J Schmidhuber"
            ],
            "title": "Deep learning in neural networks: An overview",
            "venue": "Neural Networks",
            "year": 2014
        },
        {
            "authors": [
                "T Sejnowski",
                "C Rosenberg"
            ],
            "title": "Parallel networks that Learn to pronounce English text",
            "venue": "J Complex Syst 1:145\u2013168",
            "year": 1987
        },
        {
            "authors": [
                "J Sharma",
                "A Angelucci",
                "M Sur"
            ],
            "title": "Induction of visual orientation modules in auditory cortex",
            "venue": "Nature",
            "year": 2000
        },
        {
            "authors": [
                "SL Smith",
                "IT Smith",
                "T Branco",
                "M H\u00e4usser"
            ],
            "title": "Dendritic spikes enhance stimulus selectivity in cortical neurons in vivo",
            "venue": "Nature",
            "year": 2013
        },
        {
            "authors": [
                "N Spruston"
            ],
            "title": "Pyramidal neurons: dendritic structure and synaptic integration",
            "venue": "Nat Rev Neurosci",
            "year": 2008
        },
        {
            "authors": [
                "EB Tchernev",
                "RG Mulvaney",
                "DS Phatak"
            ],
            "title": "Investigating the fault tolerance of neural networks",
            "venue": "Neural Comput",
            "year": 2005
        },
        {
            "authors": [
                "AH Tran",
                "SN Yanushkevich",
                "SE Lyshevski",
                "VP Shmerko"
            ],
            "title": "Design of neuromorphic logic networks and fault-tolerant computing",
            "venue": "11th IEEE International Conference on Nanotechnology,",
            "year": 2011
        },
        {
            "authors": [
                "A Waibel",
                "T Hanazawa",
                "G Hinton",
                "K Shikano",
                "KJ Lang"
            ],
            "title": "Phoneme recognition using time-delay neural networks",
            "venue": "IEEE Trans Acoust",
            "year": 1989
        },
        {
            "authors": [
                "X Wang",
                "M Han"
            ],
            "title": "Online sequential extreme learning machine with kernels for nonstationary time series prediction",
            "venue": "Neurocomputing",
            "year": 2014
        },
        {
            "authors": [
                "RJ Williams",
                "J Peng"
            ],
            "title": "An efficient gradient-based algorithm for on-line training of recurrent network trajectories",
            "venue": "Neural Comput",
            "year": 1990
        },
        {
            "authors": [
                "RJ Williams",
                "D Zipser"
            ],
            "title": "A learning algorithm for continually running fully recurrent neural networks",
            "venue": "Neural Comput",
            "year": 1989
        },
        {
            "authors": [
                "S Xu",
                "W Jiang",
                "M-M Poo",
                "Y Dan"
            ],
            "title": "Activity recall in a visual cortical ensemble",
            "venue": "Nat Neurosci 15:449\u2013455,",
            "year": 2012
        },
        {
            "authors": [
                "K Zito",
                "K Svoboda"
            ],
            "title": "Activity-dependent synaptogenesis in the adult Mammalian cortex",
            "venue": "Neuron",
            "year": 2002
        }
    ],
    "sections": [
        {
            "text": ""
        },
        {
            "heading": "1. Introduction",
            "text": "In natural environments, the cortex continuously processes streams of sensory information and builds a rich spatiotemporal model of the world. The ability to recognize and predict ordered temporal sequences is critical to almost every function of the brain, including speech recognition, active tactile perception, and natural vision. Neuroimaging studies have demonstrated that multiple cortical regions are involved in temporal sequence processing (Clegg et al., 1998; Mauk and Buonomano, 2004). Recent neurophysiology studies have shown that even neurons in primary visual cortex can learn to recognize and predict spatiotemporal sequences (Xu et al., 2012; Gavornik and Bear, 2014) and that neurons in primary visual and auditory cortex exhibit sequence sensitivity (Brosch and Schreiner, 2000; Nikoli\u0107 et al., 2009). These studies suggest that sequence learning is an important problem that is solved by many cortical regions.\nMachine learning researchers have also extensively studied sequence learning independently of neuroscience. Statistical models, such as hidden-markov models (HMM)(Rabiner and Juang, 1986; Fine et al., 1998) and autoregressive integrated\nmoving average (ARIMA)(Durbin and Koopman, 2012) have been developed for temporal pattern recognition and timeseries prediction respectively. A variety of neural network models have been proposed to model sequential data. Feedforward networks, such as time delay neural networks (TDNN), have been used to model sequential data by adding a set of delays to the input (Waibel et al., 1989). Recurrent neural networks can model sequence structure with recurrent lateral connections and process the data sequentially one record at a time. For example, long short-term memory (LSTM) has the ability to selectively pass information across time and can model very long term dependencies using gating mechanisms (Hochreiter and Schmidhuber, 1997) and gives impressive performance on a wide variety of real-world problems (Lipton et al., 2015). Echo state network (ESN) uses a randomly connected recurrent network as a dynamics reservoir and model a sequence as trainable linear combination of these response signals (Jaeger and Haas, 2004).\nCan machine learning algorithms gain any insight from cortical algorithms? The current state-of-the-art statistical and machine-learning algorithms achieve impressive prediction accuracy on benchmark problems. However, most time-series prediction benchmarks do not focus on model performance in dynamic, non-stationary scenarios. Benchmarks typically have separate training and testing datasets, where the underlying assumption is that the test data share similar statistics as the training data (Crone et al., 2011; Ben Taieb et al., 2012). In contrast, sequence learning in the brain has to occur continuously to deal with the noisy, constantly changing streams of sensory inputs. Notably, with the increasing availability of streaming data, there is also an increasing demand for online sequence algorithms that can handle complex, noisy data streams. Therefore, reverseengineering the computational principles used in the brain could offer additional insights into the sequence learning problem that lies at the heart of many machine learning applications.\nThe exact neural mechanisms underlying sequence memory in the brain remain unknown but biologically plausible models based on spiking neurons have been studied. For example, (Rao and Sejnowski, 2001) showed that spike-time-dependent plasticity rules can lead to predictive sequence learning in recurrent neocortical circuits. Spiking recurrent network models have been shown to recognize and recall precisely timed sequences of inputs using supervised learning rules (Ponulak and Kasi\u0144ski, 2010; Brea et al., 2013). These studies demonstrate that certain limited types of sequence learning can be solved with biologically plausible mechanisms. However, only a few practical sequence learning applications use spiking network models as these models only recognize relatively simple and limited types of sequences. These models also do not match performance of non-biological statistical and machine learning approaches on real-world problems.\nIn this paper we present a comparative study of HTM sequence memory, a detailed model of sequence learning in the cortex (Hawkins and Ahmad, 2016). The HTM neuron model incorporates many recently discovered properties of pyramidal cells and active dendrites (Antic et al., 2010; Major et al., 2013). Complex sequences are represented using sparse distributed temporal codes (Kanerva, 1988; Ahmad and Hawkins, 2016) and the network is trained using an online unsupervised Hebbian-style learning rule. The algorithms have been applied to many practical problems, including discrete and continuous sequence prediction, anomaly detection (Lavin and Ahmad, 2015), and sequence recognition and classification.\nWe compare HTM sequence memory with four popular statistical and machine learning techniques, including ARIMA, a statistical method for time-series forecasting (Durbin and Koopman, 2012); extreme learning machine (ELM), a feedforward network with sequential online learning (Huang et al., 2006; Liang et al., 2006) and two recurrent networks LSTM and ESN. We show that HTM sequence memory achieves comparable prediction accuracy to these other techniques. In addition it exhibits a set of features that is desirable for real-world sequence learning from streaming data. We demonstrate that HTM networks learns complex high-order sequences from data streams, rapidly adapts to changing statistics in the data, naturally handles multiple predictions and branching sequences and exhibits high tolerance to system faults.\nThe paper is organized as follows. In section 2, we discuss a list of desired properties of sequence learning algorithms for real-time streaming data analysis. In section 3, we introduce the HTM temporal memory model. In sections 4 and 5, we apply the HTM temporal memory and other sequence learning algorithms to discrete artificial data and continuous real world data respectively. Discussion and conclusions are given in section 6."
        },
        {
            "heading": "2. Criteria for a good sequence learning algorithm",
            "text": "With the increasing availability of streaming data, there is an increasing demand for online sequence learning algorithms. Here, a data stream is an ordered sequence of data records that must be processed in real-time using limited computing and storage capabilities. In the field of data stream mining, the goal is to extract knowledge from continuous data streams such as computer network traffic, sensor data, financial transactions, etc. (Domingos and Hulten, 2000; Gaber et al., 2005; Gama, 2010), which often have changing statistics (non-stationary) (Sayed-Mouchaweh and Lughofer, 2012). Real-world sequence learning from such complex, noisy data streams requires many other properties in addition to prediction accuracy. This stands in contrast to many machine learning algorithms, which are developed to optimize performance on static datasets, and lack the flexibility to solve real-time streaming data analysis tasks. In contrast to these algorithms, the cortex solves the sequence learning problem in a drastically different way. Rather than achieving optimal performance for a specific problem (e.g., through gradient-based optimization), the cortex learns continuously from noisy sensory input streams and quickly\nadapts to the changing statistics of the data. When information is insufficient or ambiguous, the cortex can make multiple plausible predictions given the available sensory information.\nReal-time sequence learning from data streams presents unique challenges for machine learning algorithms. In addition to prediction accuracy, below we list a set of criteria that applies to both biological systems and real-world streaming applications."
        },
        {
            "heading": "1) Continuous learning",
            "text": "Continuous data streams often have changing statistics. As a result, the algorithm needs to continuously learn from the data streams and rapidly adapt to changes. This property is important for processing continuous real-time sensory streams, but has not been well studied in machine learning. For real-time data stream analysis, it is much valuable if the algorithm can recognize and learn new patterns rapidly.\nMachine learning algorithms can be classified into batch or online learning algorithms. Both types of algorithms can be adopted for continuous learning applications. To apply a batch-learning algorithm to continuous data stream analysis, one needs to keep a buffered dataset of past data records. The model is retrained at regular intervals as the statistics of the data can change over time. The batch-training paradigm potentially requires significant computing and storage resources, particularly in situations where the data velocity is high. In contrast, online sequential algorithms can learn sequences in a single-pass and do not require a buffered dataset."
        },
        {
            "heading": "2) High-order predictions",
            "text": "Real-world sequences contain contextual dependencies that span multiple time steps, i.e. the ability to make high-order predictions. The term \u201corder\u201d refers to Markov order, specifically the minimum number of previous time steps the algorithm needs to consider in order to make accurate predictions. An ideal algorithm should learn the order automatically and efficiently."
        },
        {
            "heading": "3) Multiple simultaneous predictions",
            "text": "For a given temporal context, there could be multiple possible future outcomes. With real-world data, it is often insufficient to only consider the single best prediction when information is ambiguous. A good sequence learning algorithm should be able to make multiple predictions simultaneously and evaluate the likelihood of each prediction online. This requires the algorithm to output a distribution of possible future outcomes. This property is present in HMMs (Rabiner and Juang, 1986) and generative recurrent neural network models (Hochreiter and Schmidhuber, 1997), but not in other approaches like ARIMA, which are limited to maximum likelihood prediction."
        },
        {
            "heading": "4) Noise robustness and fault tolerance",
            "text": "Real world sequence learning deals with noisy data sources where sensor noise, data transmission errors and inherent device limitations frequently result in inaccurate or missing data. A good sequence learning algorithm should exhibit robustness to noise in the inputs.\nThe algorithm should also be able to learn properly in the event of system faults, such as loss of synapses and neurons in a neural network. The property of fault tolerance and robustness to failure is present in the brain. This property is important for the development of next-generation neuromorphic processors (Tran et al., 2011). Noise robustness and fault tolerance ensures flexibility and wide applicability of the algorithm to a wide variety of problems."
        },
        {
            "heading": "5) No hyperparameter tuning",
            "text": "Learning in the cortex is extremely robust for a wide range of problems. In contrast, most machine-learning algorithms require optimizing a set of hyperparameters for each task. It typically involves searching through a manually specified subset of the hyperparameter space, guided by performance metrics on a cross-validation dataset. Hyperparameter tuning presents a major challenge for applications that require a high degree of automation, like data stream mining. An ideal algorithm should have acceptable performance on a wide range of problems without any task-specific hyperparameter tuning.\nMany of the existing machine learning techniques demonstrate these properties to various degrees. A truly flexible and powerful system for streaming analytics would meet all of them. In the rest of the paper, we will compare HTM sequence memory with other common sequence learning algorithms (ARIMA, ELM, ESN and LSTM) on various tasks using the above criteria."
        },
        {
            "heading": "3. HTM sequence memory",
            "text": "In this section we describe the computational details of HTM sequence memory. We first describe our neuron model. We then describe the representation of high order sequences, followed by a formal description of our learning rules. We point out some of the relevant neuroscience experimental evidence in our description, but a detailed mapping to the biology can be found in (Hawkins and Ahmad, 2016)."
        },
        {
            "heading": "3.1. HTM neuron model",
            "text": "The HTM neuron (Fig. 1B) implements non-linear synaptic integration inspired by recent neuroscience findings regarding the function of cortical neurons and dendrites (Spruston, 2008; Major et al., 2013). Each neuron in the network contains two separate zones: a proximal zone containing a single dendritic segment and a distal zone containing a set of independent dendritic segments. Each segment maintains a set of synapses. The source of the synapses is different depending on the zone (Fig. 1B). Proximal synapses represent feedforward inputs into the layer whereas distal synapses represent lateral connections within a layer and feedback connections from a higher region. In this paper, we only consider a single region and ignore feedback connections.\nEach distal dendritic segment contains a set of lateral synaptic connections from other neurons within the layer. A segment becomes active if the number of simultaneously active connections exceeds a threshold. An active segment does not cause the cell to fire but instead causes the cell to enter a depolarized state, which we call the \u201cpredictive state\u201d. In this way each segment detects a particular temporal context and makes predictions based on that context. Each neuron can be in one of three internal states: an active state, a predictive state, or a non-active state. The output of the neuron is always binary: it is either active or not.\nThe above neuron model is inspired by a large number of recent experimental findings that suggest neurons do not perform a simple weighted sum of their inputs and fire based on that sum (Polsky et al., 2004; Smith et al., 2013), as in most neural network models (McFarland et al., 2013; Schmidhuber, 2014; LeCun et al., 2015). Instead, dendritic branches are active processing elements. The activation of several synapses within close spatial and temporal proximity on a dendritic branch can initiate a local NMDA spike, which then cause a significant and sustained depolarization of the cell body (Antic et al., 2010; Major et al., 2013)."
        },
        {
            "heading": "3.2. Two separate sparse representations",
            "text": "The HTM network consists of a layer of HTM neurons organized into a set of columns (Fig. 1A). The network represents high-order sequences using a composition of two separate sparse representations. At any time, both the current feedforward input and the previous sequence context are simultaneously represented using sparse distributed representations.\nThe first representation is at the column level. We assume that all neurons within a column detect identical feed-forward input patterns on their proximal dendrites (Mountcastle, 1997; Buxhoeveden, 2002). Through an inter-columnar inhibition mechanism, each input element is encoded as a sparse distributed activation of columns at any point in time. At any time, the top 2% columns that receive most active feedforward inputs are activated. The second representation is at the level of individual cells within these columns. At any given point a subset of cells in the active columns will represent information regarding the learned temporal context of the current pattern. These cells in turn lead to predictions of the upcoming input through lateral projections to other cells within the same network. The predictive state of a cell controls inhibition within a column. If a column contains predicted cells and later receives sufficient feed-forward input, these cells become active and inhibit others within that column. If there were no cells in the predicted state, all cells within the column become active.\nTo illustrate the intuition behind these representations, consider two abstract sequences A-B-C-D and X-B-C-Y (Fig. 1C-D). In this example remembering that the sequence started with A or X is required to make the correct prediction following \u201cC\u201d. The current inputs are represented by the subset of columns that contains active cells (black, Fig. 1CD). This set of active columns does not depend on temporal context, just on the current input. After learning, different cells in this subset of columns will be active depending on predications based on the past context (B\u2019 vs. B\u2019\u2019, C\u2019 vs. C\u2019\u2019, Fig. 1D). These cells then lead to predictions of the element following C (D or Y) based on the set of cells containing lateral connections to columns representing C.\nThis dual representation paradigm leads to a number of interesting properties. First, the use of sparse representations allows the model to make multiple predictions simultaneously. For example, if we present input \u201cB\u201d to the network without any context, all cells in columns representing the \u201cB\u201d input will fire, which leads to a prediction of both C\u2019\nand C\u2019\u2019. Second, because information is stored by coactivation of multiple cells in a distributed manner, the model is naturally robust to both noise in the input and system faults such as loss of neurons and synapses. A detailed discussion on this topic can be found in (Hawkins and Ahmad, 2016)."
        },
        {
            "heading": "3.3. HTM activation and learning rules",
            "text": "The previous sections provided an intuitive description of network behavior. In this section we describe the formal activation and learning rules for the HTM network. Consider a network with N columns and M neurons per column; we denote the activation state at time step t with an \ud835\udc40\u00d7\ud835\udc41 binary matrix \ud835\udc00!, where \ud835\udc4e!\"! is the activation state of the \ud835\udc56\u2019th cell in the \ud835\udc57\u2019th column. Similarly, an \ud835\udc40\u00d7\ud835\udc41 binary matrix \ud835\udeb7! denotes cells in a predictive state at time \ud835\udc61, where \ud835\udf0b!\"! is the predictive state of the \ud835\udc56\u2019th cell in the \ud835\udc57\u2019th column. We model each synapse with a scalar permanence value, and consider a synapse connected if its permanence value is above a\nconnection threshold. We use an \ud835\udc40\u00d7\ud835\udc41 matrix \ud835\udc03!\"! to denote the permanence of \ud835\udc51\u2019th segment of the \ud835\udc56\u2019th cell in the \ud835\udc57\u2019th column. The synaptic permanence matrix is bounded between 0 and 1. We use a binary matrix \ud835\udc03!\"! to denote only the connected synapses. The network can be initialized such that each segment contains a set of potential synapses (i.e. with non-zero permanence value) to a randomly chosen subset of cells in the layer. To speed up simulation, instead of explicitly initializing a complete set of synapses across every segment and every cell, we greedily create segments at run time (see Appendix).\nThe predictive state of the neuron is handled as follows: if a dendritic segment receives enough input, it becomes active and subsequently depolarizes the cell body without causing an immediate spike. Mathematically, the predictive state at time step t is calculated as follows:\n\ud835\udf0b!\"! =  \u00a0\n1 \u00a0if \u00a0\u2203! \ud835\udc03!\" ! \u2218 \ud835\udc00!\n! > \ud835\udf03 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\n0 \u00a0otherwise \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\n(1)\nThreshold \ud835\udf03 represents the segment activation threshold and \u2218 represents element-wise multiplication. Since the distal synapses receive inputs from previously active cells in the same layer, it contains contextual information about future inputs (Fig. 1B).\nAt any time, an inter-columnar inhibitory process select a sparse set of columns that best match the current feed forward input pattern. We calculate the number of active proximal synapses for each column, and activate the top 2% of the columns that receive the most synaptic inputs. We denote this set as \ud835\udc16!. The proximal synapses were initialized such that each column is randomly connected to 50% of the inputs. Since we focused sequence learning in this paper, the proximal synapses were fixed during learning. In principle, the proximal synapses can also adapt continuously during learning according to a spatial competitive learning rule (Hawkins et al., 2011; Mnatzaganian et al., 2016).\nNeurons in the predictive state (i.e. depolarized) will have competitive advantage over other neurons receiving the same feed-forward inputs. Specifically, a depolarized cell fires faster than other non-depolarized cells if it subsequently receives sufficient feed-forward input. By firing faster, it prevents neighboring cells in the same column from activating with intra-column inhibition. The active state for each cell is calculated as follows:\n\ud835\udc4e!\"! =  \u00a0\n1 \u00a0if \u00a0\ud835\udc57 \u2208\ud835\udc16! \u00a0and \u00a0\ud835\udf0b!\" !!! = 1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\n1 \u00a0if \u00a0\ud835\udc57 \u2208\ud835\udc16! \u00a0and \u00a0 \ud835\udf0b!\" !!!\n!\n= 0 \u00a0 \u00a0\n0 \u00a0otherwise \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\n(2)\nThe first conditional expression of Eq. 2 represents a cell in a winning column becoming active if it was in a predictive state during the preceding time step. If none of the cells in a winning column are in a predictive state, all cells in that column become active, as in the second conditional of Eq. 2.\nThe lateral connections in the sequence memory model are learned using a Hebbian-like rule. Specifically, if a cell is depolarized and subsequently becomes active, we reinforce the dendritic segment that caused the depolarization. If no cell\nin an active column is predicted, we select the cell with the most activated segment and reinforce that segment. Reinforcement of a dendritic segment involves decreasing permanence values of inactive synapses by a small value \ud835\udc5d! and increasing the permanence for active synapses by a larger value \ud835\udc5d!:\n\u2206\ud835\udc03!\"! = \ud835\udc5d!\ud835\udc03!\"! \u2218 \ud835\udc00!!! \u2212 \ud835\udc5d!\ud835\udc03!\"! \u2218 (\ud835\udfcf \u2212 \ud835\udc00!!!) (3)\n\ud835\udc03!\"! denotes a binary matrix containing only the positive entries in \ud835\udc03!\"! . We also apply a very small decay to active segments of cells that did not become active, mimicking the effect of long-term depression (Massey and Bashir, 2007):\n\u2206\ud835\udc03!\"! = \ud835\udc5d!!\ud835\udc03!\"!  \u00a0where \u00a0\ud835\udc4e!\"! = 0 \u00a0and \u00a0 \ud835\udc03!\"! \u2218 \ud835\udc00!!! ! > \ud835\udf03 \u00a0 (4)\nwhere \ud835\udc5d!! \u226a \ud835\udc5d!.\nThe learning rule is inspired by neuroscience studies of activity-dependent synaptogenesis (Zito and Svoboda, 2002), which showed that the adult cortex generates new synapses in response to sensory activity rapidly. The mathematical formula we chose captured this Hebbian synaptogenesis learning rule. We did not derive the rule by implementing gradient descent on a cost function. There could be other mathematical formulations that give similar or better results.\nA complete set of parameters and further implementation details can be found in the appendix. These parameters were set based on properties of sparse distributed representations (Ahmad and Hawkins, 2016). Notably, we used the same set of parameters for all the different types of sequence learning tasks in this paper."
        },
        {
            "heading": "3.4. SDR encoder and classifier",
            "text": "The HTM sequence memory operates with sparse distributed representations (SDRs) internally. To apply HTM to realworld sequence learning problems, we need to first convert the original data to SDRs using an encoder. We have created a variety of encoders to deal with different data types (Purdy, 2016). In this paper, we used a random SDR encoder for categorical data, and used scalar and date-time encoders for the taxi passenger prediction experiment. To decode prediction values from the output SDRs of HTM, we considered two classifiers: a simple classifier based on SDR overlaps and a maximum-likelihood classifier. For the single-step discrete sequence prediction task, we computed the overlap of the predicted cells with the SDRs of all observed elements and selected the one with the highest overlap. For the continuous scalar value prediction task, we divided the whole range of scalar value into 22 disjoint buckets, and used a single layer feedforward classification network. Given a large array of cell activation pattern \ud835\udc31, the classification network computes a probability distribution over all possible classes using a softmax activation function (Bridle, 1989). There are as many output units as the number of possible classes. The jth output unit receives a weighted summation of all the inputs,\n\ud835\udc4e! = \ud835\udc64!\"\ud835\udc65!\n!\n!!!\n(5)\n\ud835\udc64!\" is the connection weight from the ith input neuron to the jth output neuron. The estimated class probability is given by the activation level of the output units,\n\ud835\udc66! = \ud835\udc43 \ud835\udc36! \ud835\udc31 = \ud835\udc86\ud835\udc82\ud835\udc8c\n\ud835\udc86\ud835\udc82\ud835\udc8c\ud835\udc72\ud835\udc8a!\ud835\udfcf\n(6)\nUsing a maximum likelihood optimization, we derived the learning rule for the weight matrix w. \ud835\udee5\ud835\udc64!\" = \ud835\udf06 \ud835\udc66! \u2212 \ud835\udc67! \ud835\udc65! (7)\n\ud835\udc67! is the observed (target) distribution and \ud835\udf06 is the learning rate. Note that since x is highly sparse, we only need to update a very small fraction of the weight matrix at any time. Therefore the learning algorithm for the classifier is fast despite the high dimensionality of the weight matrix."
        },
        {
            "heading": "4. High-order sequence prediction with artificial data",
            "text": "We conducted experiments to test whether the HTM sequence memory model, online sequential extreme learning machine (OS-ELM) and the LSTM network are able to learn highorder sequences in an online manner, recover after modification to the sequences, and make multiple predictions simultaneously. LSTM represents the state-of-the-art recurrent neural network model for sequence learning tasks (Hochreiter and Schmidhuber, 1997; Graves, 2012). OS-ELM is a feedforward neural network model that is widely used for time-series predictions (Huang et al., 2011; Wang and Han, 2014)."
        },
        {
            "heading": "4.1. Continuous online learning from streaming data",
            "text": "We created a discrete high-order temporal sequence dataset. Sequences are designed such that any learning algorithm would have to maintain context of at least the first two elements of each sequence in order to correctly predict the last element of the sequence (Figure 3). We used the sequence dataset in a continuous streaming scenario (Fig. 3C). At the beginning of a trial, we randomly chose a sequence from the dataset and sequentially presented each of its elements. At the end of each sequence, we presented a single noise element to the model. The noise element is randomly chosen from a large set of 50,000 noise symbols (not used among the set of sequences). This is a difficult learning problem, since sequences are embedded in random noise; the start and end points are not marked. The set of noise symbols is large so the algorithm cannot learn every possible noise transition. We\ntested the algorithms for predicting the last element of each sequence continuously as the algorithm observed a stream of sequences, and reported the percentage of correct predictions over time.\nWe encoded each symbol in the sequence as a random SDR for HTM sequence memory, with 40 randomly chosen active bits in a vector of 2048 bits. This SDR representation matches the internal representation used in HTM, which has 2048 columns with 40 active at any time (see Implementation Details). We initially tried to use the same SDR encoding for ELM and LSTM. This high dimensional representation does not work well for ELM and LSTM due to the large number of parameters required. Instead, we used a random real-valued dense distributed representation for ELM and LSTM. Each symbol is encoded as a 25-dimensional vector with each dimension\u2019s value randomly chosen from [-1, 1]1. We chose this encoding format because it both gives better accuracy and has large representational capacity, which is required for streaming data analysis. Similar dense distributed representations are commonly used for LSTM in natural language processing applications (Mikolov et al., 2013).\nSince the sequences are presented in a streaming fashion and predictions are required continuously, this task represents a continuous online learning problem. The HTM sequence memory is naturally suitable for online learning as it learns from every new data point and does not require the data stream to be broken up into predefined chunks. ELM also has a well-established online sequential learning model (Liang et al., 2006). Online sequential algorithm, such as real-time recurrent learning (RTRL) has been proposed for LSTM in the past, (Williams and Zipser, 1989; Hochreiter and Schmidhuber, 1997). However, most LSTM applications used batch learning due to the high computational cost of RTRL (Jaeger, 2002). We use two variants of LSTM networks for this task. First, we retrained an LSTM network at regular intervals on a buffered dataset of the previous time steps using a variant of the resilient backpropagation algorithm until convergence (Igel and H\u00fcsken, 2003). The experiments include several LSTM models with varying buffer sizes. Second, we trained a LSTM network with online truncated back-propagation through time (BPTT) (Williams and Peng, 1990). At each time point, we calculated the gradient using BPTT over the last 100 elements and adjusted the parameters along the gradient by a small amount.\nWe tested sequences with either single or multiple possible endings (Fig. 3A-B). To quantify model performance, we classified the state of the model before presenting the last element of each sequence to retrieve the top K predictions, where K = 1 for the single prediction case and K = 2 or 4 for the multiple predictions case. We considered the prediction correct if the actual last element was among the top K predictions of the model. Since these are online learning tasks, there are no separate training and test phases. Instead, we continuously report the prediction accuracy of the end of each sequence before the model has seen it.\n1 We manually tuned the number of dimensions and found\nthat 25 dimensions gave the best performance on our tasks.\nIn the single prediction experiment (Fig. 4, left of the black dashed line), each sequence in the dataset has only one possible ending given its high-order context (Fig. 3A). The HTM sequence memory quickly achieves perfect prediction accuracy on this task (Fig. 4, red). Given a large enough learning window, LSTM also learns to predict the high-order sequences (Fig. 4, green). Despite comparable model performance, TM and LSTM use the data in different ways: LSTM requires many passes over the learning window each time it is retrained to perform gradient-descent optimization, whereas HTM only needs to see each element once (one-pass learning). LSTM also takes longer than HTM to achieve perfect accuracy; we speculate that since LSTM optimizes over all transitions in the data stream, including the random ones between sequences, it is initially overfitting on the training data. Online LSTM and ELM are also trained in an online, sequential fashion similar to HTM. But both algorithms require keeping a short history buffer of the past elements. ELM learned the sequences slower than HTM, and never achieved perfect performance (Fig. 4, blue). Online LSTM has the best performance initially, but does not achieve perfect performance in the end. HTM and LSTM are the only algorithms to achieve perfect prediction accuracy on this task."
        },
        {
            "heading": "4.2. Adaptation to changes in the data stream",
            "text": "Once the models have achieved stable performance, we altered the dataset by swapping the last elements of pairs of high-order sequences (Fig. 4, black dashed line). This forces the model to forget the old sequences and subsequently learn the new ones. HTM sequence memory and online LSTM quickly recover from the modification. In contrast, it takes a long time for batch LSTM and ELM to recover from the modification as its buffered dataset contains contradictory information before and after the modification. Although using a smaller learning window can speed up the recovery (Fig. 4, blue; purple), it also causes worse prediction performance due to limited number of training samples.\nA summary of the model performance on the high-order sequence prediction task is shown in Fig. 5. In general, there is a tradeoff between prediction accuracy and flexibility. For batch learning algorithms, a shorter learning window is required for fast adaptation to changes in the data, but a longer learning window is required to perfectly learn highorder sequences (Fig. 5, green vs. yellow). Although online LSTM and ELM do not require batch learning, it does require the user to specify the maximal lag, which limits the maximum sequence order it can learn. The HTM sequence memory model dynamically learns high-order sequences without requiring a learning window or a maximum sequence length. It achieved the best final prediction accuracy with a small number of data samples. After the modification to the sequences, HTM\u2019s recovery is much faster than ELM and LSTM trained with batch learning, demonstrating its ability to adapt quickly to changes in data streams."
        },
        {
            "heading": "4.3. Simultaneous multiple predictions",
            "text": "In the experiment with multiple predictions (Fig. 3), each sequence in the dataset has 2 or 4 possible endings, given its high-order context. The HTM sequence memory model rapidly achieves perfect prediction accuracy for both the 2- predictions and the 4-predictions cases. While only these two cases are shown, in reality HTM is able to make many multiple predictions correctly if the dataset requires it. Given a large learning window, LSTM is able to achieve good prediction accuracy for the 2-predictions case, but when the number of predictions is increased to 4 or greater, it is not able to make accurate predictions.\nHTM sequence memory is able to simultaneously make multiple predictions due to its use of SDRs. Because there is little overlap between two random SDRs, it is possible to predict a union of many SDRs and classify a particular SDR as being a member of the union with low chance of a false positive (Ahmad and Hawkins, 2016). On the other hand, the real-valued dense distributed encoding used in LSTM is not suitable for multiple predictions, because the average of multiple dense representations in the encoding space is not necessarily close to any of the component encodings, especially when the number of predictions being made is large. The problem can be solved by using local one-hot representations to code target inputs, but such representations have very limited capacity and do not work well when the number of possible inputs is large or unknown upfront. This suggests that modifying LSTMs to use SDRs might enable better performance on this task."
        },
        {
            "heading": "4.4. Learning long term dependencies from high-order sequences",
            "text": "For feedforward networks like ELM, the number of time lags that can be included in the input layer significantly limits the maximum sequence order a network can learn. The conventional recurrent neural networks cannot handle sequences with long term dependencies because error signals \u201cflowing backwards in time\u201d tend to either blow up or vanish with the classical back-propagation through time (BPTT) algorithm. LSTM is capable of learning very long-term dependencies using gating mechanisms (Henaff et al., 2016). Here we tested whether HTM sequence memory can learn long-term dependencies by varying the Markov order of the\nsequences, which is determined by the length of shared subsequences (Fig. 3A).\nWe examined the prediction accuracy over training while HTM sequence memory learns variable order sequences. The model is able to achieve perfect prediction performance up to 100-order sequences (Fig. 7A). The number of sequences that are required to achieve perfect prediction performance increases linearly as a function of the order of sequences (Fig. 7B). Note that the model quickly achieves 50% accuracy much faster because it requires only first-order knowledge, yet it requires high-order knowledge to make perfect prediction (Fig. 3A)."
        },
        {
            "heading": "4.5. Disruption of high-order context with temporal noise",
            "text": "In the previous experiments noise was presented between sequences. In this experiment, we tested the effect of noise within sequences. At run time, we randomly replaced either the second, third or fourth element in each sequence with a random symbol. Such temporal noise could disrupt the highorder sequence context and make it much harder to predict the sequence endings. We considered two scenarios: (1) temporal noise throughout training; (2) noise introduced only after the models achieved perfect performance.\nThe performance of HTM and LSTM are shown in Fig. 8. If temporal noise is present throughout training, neither HTM nor LSTM can make perfect predictions (Fig. 8A). LSTM has slightly better performance than HTM in this scenario, presumably because the gating mechanisms in LSTM can maintain some of the high-order sequence context. HTM behaves like a first order model and has an accuracy of about 0.5. This experiment demonstrates the sensitivity of the HTM model to temporal noise.\nIf we inject temporal noise after the models achieved perfect performance on the noise-free sequences, the performance of both models drop rapidly (Fig. 8B). The performance of HTM drops to 0.5 (performance of the first-order model), whereas LSTM has worse performance. This result demonstrates that if the high-order sequence context is disrupted, HTM would robustly behave as a low-order model, whereas the performance of LSTM is dependent on the training history."
        },
        {
            "heading": "4.6. Robustness of the network to damage",
            "text": "We \u00a0tested \u00a0the \u00a0robustness \u00a0of \u00a0ELM, \u00a0LSTM \u00a0and \u00a0HTM \u00a0network \u00a0 with \u00a0respective \u00a0to \u00a0removal \u00a0of \u00a0neurons. \u00a0This \u00a0fault \u00a0tolerance \u00a0 property \u00a0 is \u00a0 important \u00a0 for \u00a0 hardware \u00a0 implementations \u00a0 of \u00a0 neural \u00a0network \u00a0models. \u00a0After \u00a0 the \u00a0models \u00a0achieved \u00a0stable \u00a0 performance \u00a0 on \u00a0 the \u00a0 high-\u2010order \u00a0 sequence \u00a0 prediction \u00a0 task \u00a0 (at \u00a0the \u00a0black \u00a0dashed \u00a0line, \u00a0Fig. \u00a02), \u00a0we \u00a0eliminated \u00a0a \u00a0fraction \u00a0 of \u00a0the \u00a0cells \u00a0and \u00a0their \u00a0associated \u00a0synaptic \u00a0connections \u00a0from \u00a0 the \u00a0network. \u00a0We \u00a0then \u00a0measured \u00a0the \u00a0prediction \u00a0accuracy \u00a0of \u00a0 both \u00a0networks \u00a0on \u00a0the \u00a0same \u00a0data \u00a0streams \u00a0for \u00a0an \u00a0additional \u00a0 5000 \u00a0steps \u00a0without \u00a0further \u00a0learning. \u00a0There \u00a0is \u00a0no \u00a0impact \u00a0on \u00a0 HTM \u00a0sequence \u00a0memory \u00a0model \u00a0performance \u00a0at \u00a0up \u00a0to \u00a030% \u00a0 cell \u00a0 death, \u00a0 whereas \u00a0 performance \u00a0 of \u00a0 ELM \u00a0 and \u00a0 LSTM \u00a0 network \u00a0declined \u00a0rapidly \u00a0with \u00a0small \u00a0fraction \u00a0of \u00a0cell \u00a0death \u00a0 (Fig. \u00a09). \u00a0 \u00a0\nFault \u00a0 tolerance \u00a0 of \u00a0 traditional \u00a0 artificial \u00a0 neural \u00a0 networks \u00a0 depends \u00a0 on \u00a0 many \u00a0 factors, \u00a0 such \u00a0 as \u00a0 the \u00a0 network \u00a0 size \u00a0 and \u00a0 training \u00a0methods \u00a0(Lee \u00a0et \u00a0al., \u00a02014). \u00a0The \u00a0experiments \u00a0here \u00a0 applied \u00a0 commonly \u00a0 used \u00a0 training \u00a0 methods \u00a0 for \u00a0 ELM \u00a0 and \u00a0 LSTM \u00a0(see \u00a0Appendix). \u00a0It \u00a0is \u00a0possible \u00a0that \u00a0the \u00a0fault \u00a0tolerance \u00a0 of \u00a0 LSTM \u00a0 or \u00a0 any \u00a0 other \u00a0 artificial \u00a0 neural \u00a0 network \u00a0 may \u00a0 be \u00a0 improved \u00a0by \u00a0 introducing \u00a0redundancy \u00a0(replicating \u00a0trained \u00a0 network) \u00a0 (Tchernev \u00a0 et \u00a0 al., \u00a0 2005) \u00a0 or \u00a0 by \u00a0 special \u00a0 training \u00a0 method \u00a0such \u00a0as \u00a0dropout \u00a0(Hinton \u00a0et \u00a0al., \u00a02012). \u00a0In \u00a0contrast, \u00a0 the \u00a0 fault \u00a0 tolerance \u00a0 of \u00a0 HTM \u00a0 is \u00a0 naturally \u00a0 derived \u00a0 from \u00a0 properties \u00a0 of \u00a0 sparse \u00a0 distributed \u00a0 representations \u00a0 (Ahmad \u00a0\nand \u00a0 Hawkins, \u00a0 2016), \u00a0 in \u00a0 analogy \u00a0 to \u00a0 biological \u00a0 neural \u00a0 networks. \u00a0"
        },
        {
            "heading": "5. Prediction of taxi passenger demand",
            "text": "In order to compare the performance of HTM sequence memory with other sequence learning techniques in realworld scenarios, we consider the problem of predicting taxi passenger demand. Specifically, we aggregated the passenger counts in New York City taxi rides at 30-minute intervals using a public data stream provided by the New York City Transportation Authority2. This leads to sequences exhibiting rich patterns at different time scales (Fig. 10A). The task is to predict the taxi passenger demand 5 steps (2.5 hours) in advance. This problem is an example of a large class of sequence learning problems that require rapid processing of streaming data to deliver information for real-time decision making (Moreira-Matias et al., 2013).\nWe applied HTM sequence memory and other sequence prediction algorithms to this problem. The ARIMA model is a widely used statistical approach for time series analysis (Hyndman and Athanasopoulos, 2013). As before, we converted ARIMA and LSTM to an online learning algorithm by re-training the model on every week of data with a buffered dataset of the previous 1000, 3000, or 6000 samples, and by re-training ARIMA at every time step on a buffered dataset of 6000 samples. ELM and ESN were adapted at every time step using sequential online learning methods. The parameters of the ESN, ELM and LSTM network were extensively hand-tuned to provide the best possible accuracy on this dataset. The ARIMA model was optimized using R\u2019s \u201cauto ARIMA\u201d package (Hyndman and Khandakar, 2008). The HTM model did not undergo any parameter tuning \u2013 it uses the same parameters that were used for the previous artificial sequence task.\nWe used two error metrics to evaluate model performance: mean absolute percentage error (MAPE), and negative loglikelihood. The MAPE metrics focus on the single best point estimation, while negative log-likelihood evaluates the models\u2019 predicted probability distributions of future inputs (see Appendix for details). We found that the HTM sequence memory had comparable performance to LSTM on both error metrics. Both techniques had much lower error than ELM, ESN and ARIMA (Fig. 10B). Note that HTM sequence memory achieves this performance with a single-pass training paradigm, whereas LSTM require multiple-passes on a buffered dataset.\n2\nhttp://www.nyc.gov/html/tlc/html/about/trip_record_data.sh tml\nWe then tested how fast different sequence learning algorithms can adapt to changes in the data. We artificially modified the data by decreasing weekday morning traffic (7am-11am) by 20% and increasing weekday night traffic (9pm-11pm) by 20% starting from April 1st. These changes in the data caused an immediate increase in prediction error for both HTM and LSTM (Fig. 11A). The prediction error of HTM sequence memory quickly dropped back in about two weeks, whereas the LSTM prediction error stayed high for a much longer period of time. As a result, HTM sequence memory had better prediction accuracy than LSTM and other models after the data modification (Fig. 11B-C)."
        },
        {
            "heading": "6. Discussion and conclusions",
            "text": "In this paper we have applied HTM sequence memory, a recently developed neural network model, to real-time sequence learning problems with time-varying input streams. The sequence memory model is derived from computational principles of cortical pyramidal neurons (Hawkins and Ahmad, 2016). We discussed model performance on both artificially generated and real-world datasets. The model satisfies a set of properties that are important for online sequence learning from noisy data streams with continuously changing statistics, a problem the cortex has to solve in natural environments. These properties govern the overall flexibility of an algorithm and its ability to be used in an automated fashion. Although HTM is still at a very early stage compared to other traditional neural network models, it satisfies these properties and shows promising results on realtime sequence learning problems."
        },
        {
            "heading": "6.1. Continuous learning with streaming data",
            "text": "Most supervised sequence learning algorithms use a batchtraining paradigm, where a cost function, such as prediction error, is minimized on a batch training dataset (Dietterich, 2002; Bishop, 2006). Although we can train these algorithms continuously using a sliding window (Sejnowski and Rosenberg, 1987), this batch-training paradigm is not a good match for time-series prediction on continuous streaming data. A small window may not contain enough training samples for learning complex sequences, while a large window introduces a limit on how fast the algorithm can adapt to changing statistics in the data. In either case a buffer must be maintained and the algorithm must make multiple passes for every retraining step. It may be possible to use a smooth forgetting mechanism instead of hard retraining (Williams and Zipser, 1989; Lughofer and Angelov, 2011), but this requires the user to tune parameters governing the forgetting speed to achieve good performance.\nIn contrast HTM sequence memory adopts a continuous learning paradigm. The model does not need to store a batch of data as the \u201ctraining dataset\u201d. Instead, it learns from each data point using unsupervised Hebbian-like associative learning mechanisms (Hebb, 1949). As a result the model rapidly adapts to changing statistics in the data."
        },
        {
            "heading": "6.2. Using sparse distributed representations for sequence learning",
            "text": "A key difference between HTM sequence memory and previous biologically inspired sequence learning models (Abeles, 1982; Rao and Sejnowski, 2001; Ponulak and Kasi\u0144ski, 2010; Brea et al., 2013) is the use of sparse distributed representations (SDRs). In the cortex, information is primarily represented by strong activation of a small set of neurons at any time, known as sparse coding (F\u00f6ldi\u00e1k, 2002; Olshausen and Field, 2004). HTM sequence memory uses SDRs to represent temporal sequences. Based on mathematical properties of SDRs (Kanerva, 1988; Ahmad and Hawkins, 2016), each neuron in the HTM sequence memory model can robustly learn and classify a large number of patterns under noisy conditions (Hawkins and Ahmad, 2016).\nA rich distributed neural representation for temporal sequences emerges from computation in HTM sequence memory. Although we focus on sequence prediction in this paper, this representation is valuable for a number of tasks, such as anomaly detection (Lavin and Ahmad, 2015) and sequence classification.\nThe use of a flexible coding scheme is particularly important for online streaming data analysis, where the number of unique symbols is often not known upfront. It is desirable to be able to change the range of the coding scheme at run-time without affecting the previous learning. This requires the algorithm to use a flexible coding scheme that can represent a large number of unique symbols or a wide range of data. The SDRs used in HTM have a very large coding capacity and allow simultaneous representations of multiple predictions with minimal collisions. These properties make SDR an ideal coding format for the next generation of neural network models."
        },
        {
            "heading": "6.3. Robustness and generalization",
            "text": "An intelligent learning algorithm should be able to automatically deal with a large variety of problems without parameter tuning, yet most machine learning algorithms require a task-specific parameter search when applied to a novel problem. Learning in the cortex does not require an external tuning mechanism, and the same cortical region can be used for different functional purposes if the sensory input changes (Sadato et al., 1996; Sharma et al., 2000). Using computational principles derived from the cortex, we show that HTM sequence memory achieves performance comparable to LSTM networks on very different problems using the same set of parameters. These parameters were chosen according to known properties of real cortical neurons (Hawkins and Ahmad, 2016) and basic properties of sparse distributed representations (Ahmad and Hawkins, 2016)."
        },
        {
            "heading": "6.4. Limitations of HTM and future directions",
            "text": "We have identified a few limitations of HTM. First, as a strict one-pass algorithm with access to only the current input, it may take longer for HTM to learn sequences with very longterm dependencies (Fig. 7) than algorithms that have access to a longer history buffer. Learning of sequences with long-term dependencies can be sped up if we maintain a history buffer and run HTM on it multiple times. Indeed, it has been argued that an intelligent agent should store the entire raw history of sensory inputs and motor actions during interaction with the world (Schmidhuber, 2009). Although it may be computationally challenging to store the entire history, doing so may improve performance given the same amount of sensory experience.\nSecond, although HTM is robust to spatial noise due to the use of sparse distributed representations, the current HTM sequence memory model is sensitive to temporal noise. It can lose high-order sequence context if elements in the sequence are replaced by a random symbol (Fig. 8). In contrast, the gating mechanisms of LSTM networks appear to be more robust to temporal noise. The noise robustness of HTM can be improved by using a hierarchy of sequence memory models\nthat operate on different time scales. A sequence memory model that operates over longer time scales would be less sensitive to temporal noise. A lower region in the hierarchy may inherit the robustness to temporal noise through feedback connections to a higher region.\nThird, the HTM model as discussed does not perform as well as LSTM on grammar learning tasks. We found that on the Reber grammar task (Hochreiter and Schmidhuber, 1997), HTM achieves an accuracy of 98.4% and ELM an accuracy of 86.7% (online training after observing 500 sequences), whereas LSTM achieves an accuracy of 100%. HTM can approximately learn artificial grammars by memorizing example sentences. This strategy could require more training samples to fully learn recursive grammars with arbitrary sequence lengths. In contrast, LSTM learn grammars much faster using the gating mechanisms.\nFinally, we have only tested HTM on low-dimensional categorical or scalar data streams in this paper. It remains to be determined whether HTM can handle high-dimensional data such as speech and video streams. The high capacity of the sparse distributed representations in HTM should be able to represent high-dimensional data. However, it is more challenging to learn sequence structure in high-dimensional space, as the raw data could be much less repeatable. It may require additional pre-processing, such as dimensionality reduction and feature extractions, before HTM can learn meaningful sequences with high-dimensional data. It would be an interesting future direction to explore how to combine HTM with other machine learning methods, such as deep networks, to solve high-dimensional sequence learning problems."
        },
        {
            "heading": "7. Appendix",
            "text": ""
        },
        {
            "heading": "7.1. HTM sequence model implementation details",
            "text": "In our software implementation, we made a few simplifying assumptions to speed up simulation for large networks. We did not explicitly initialize a complete set of synapses across every segment and every cell. Instead, we greedily created segments on the least used cells in an unpredicted column and initialized potential synapses on that segment by sampling from previously active cells. This happened only when there is no match to any existing segment. The initial synaptic permanence for newly created synapses is set as 0.21(Table 1), which is below the connection threshold (0.5).\nThe HTM sequence model operates with sparse distributed representations (SDRs). Specialized encoders are required to encode real-world data into SDRs. For the artificial datasets with categorical elements, we simply encoded each symbol in the sequence as a random SDR, with 40 randomly chosen active bits in a vector of 2048 bits.\nFor the NYC taxi dataset, three pieces of information were fed into the HTM model: raw passenger count, the time of day, and the day of week (LSTM received the same information as input). We used NuPIC\u2019s standard scalar encoder to convert each piece of information into an SDR. The encoder converts a scalar value into a large binary vector with a small number of ON bits clustered within a sliding window, where the center position of the window corresponds to the data value. We subsequently combined three SDRs via a competitive sparse spatial pooling process, which also resulted in 40 active bits in a vector of 2048 bits as in the artificial dataset. The spatial pooling process is described in detail here (Hawkins et al., 2011).\nThe HTM sequence memory model used an identical set of model parameters for all the experiments described in the paper. A complete list of model parameters is shown below. The full source code for the implementation is available on Github at https://github.com/numenta/nupic"
        },
        {
            "heading": "7.2. Implementation details of other sequence learning algorithms",
            "text": "ELM We used the online sequential learning algorithm for ELM (Liang et al., 2006). The network used 50 hidden neurons and a time lag of 100 for the taxi data and 200 hidden neurons and a time lag of 10 for the artificial dataset.\nESN We used the Matlab toolbox for Echo State Network developed by Jaeger\u2019s group (http://reservoircomputing.org/node/129). The ESN network 100 internal units, a spectral radius of 0.1, a teacher scaling of 0.01 and a learning rate of 0.1 for the ESN model. The parameters were hand tuned to achieve the best performance. We used the online learning mode and adapted the weight at every time step.\nLSTM We used the PyBrain implementation of LSTM (Schaul et al., 2010). For the artificial sequence learning task, the network contains 25 input units, 20 internal LSTM neurons and 25 output units. For the NYC taxi task, the network contains 3 input units, 20 LSTM cells, 1 output units for calculation of the MAPE metric and 22 output units for calculation of the sequence likelihood metric. The LSTM cells have forget gates but not peephole connections. The output units have a biased term. The maximum\ntime lag is the same as the buffer size for the batch-learning LSTMs. We used two training paradigms. For the batch-learning paradigm, the network were retrained every 1000 iterations with a popular version of the resilient backpropagation method (Igel and H\u00fcsken, 2003). For the online-learning paradigm, we calculated the gradient at every time step using truncated backpropgation through time over the last 100 elements (Williams and Peng, 1990), and adjusted the parameters along the gradient with a learning rate of 0.01."
        },
        {
            "heading": "7.3. Evaluation of model performance in the continuous sequence learning task",
            "text": "Two error metrics were used to evaluate the prediction accuracy of the model. First, we considered mean absolute percentage error (MAPE) metric, an error metric that is less sensitive to outliers than root mean squared error.\nMAPE = |\ud835\udc66! \u2212 \ud835\udc66!|!!!! |\ud835\udc66!|!!!!\n(1)\nIn Eq. 1, \ud835\udc66! is the observed data at time t, \ud835\udc66! is the model prediction for the data observed at time t, and N is the length of the dataset.\nA good prediction algorithm should output a probability distribution of future elements of the sequence. However, MAPE only consider the single best prediction from the model, and thus do not incorporate other possible predictions from the model. We used negative log-likelihood as a complementary error metric to address this problem. The sequence probability can be decomposed into:\n\ud835\udc5d \ud835\udc66!, \ud835\udc66!,\u2026 , \ud835\udc66! = \ud835\udc5d \ud835\udc66! \ud835\udc5d \ud835\udc66!|\ud835\udc66! \ud835\udc5d \ud835\udc66!|\ud835\udc66!, \ud835\udc66! \ud835\udc5d \ud835\udc66!|\ud835\udc66!,\u2026 , \ud835\udc66!!! (2) The conditional probability distribution is modeled by HTM or LSTM based on network state at the previous time step.\n\ud835\udc5d \ud835\udc66!|\ud835\udc66!,\u2026 , \ud835\udc66!!! = \ud835\udc43(\ud835\udc66!|network \u00a0state!!!)\n(3)\nThe negative log-likelihood of the sequence is then given by:\n\ud835\udc41\ud835\udc3f\ud835\udc3f = 1 \ud835\udc41\nlog\ud835\udc43(\ud835\udc66!|model) !\n!!!\n(4)\n \u00a0"
        }
    ],
    "title": "Continuous online sequence learning with an unsupervised neural network model",
    "year": 2016
}