{
    "abstractText": "36. J. B. Tenenbaum, Adv. Neural Info. Proc. Syst. 10, 682 (1998). 37. T. Martinetz, K. Schulten, Neural Netw. 7, 507 (1994). 38. V. Kumar, A. Grama, A. Gupta, G. Karypis, Introduction to Parallel Computing: Design and Analysis of Algorithms (Benjamin/Cummings, Redwood City, CA, 1994), pp. 257\u2013297. 39. D. Beymer, T. Poggio, Science 272, 1905 (1996). 40. Available at www.research.att.com/;yann/ocr/mnist. 41. P. Y. Simard, Y. LeCun, J. Denker, Adv. Neural Info. Proc. Syst. 5, 50 (1993). 42. In order to evaluate the fits of PCA, MDS, and Isomap on comparable grounds, we use the residual variance 1 \u2013 R(D\u0302M , DY). DY is the matrix of Euclidean distances in the low-dimensional embedding recovered by each algorithm. D\u0302M is each algorithm\u2019s best estimate of the intrinsic manifold distances: for Isomap, this is the graph distance matrix DG; for PCA and MDS, it is the Euclidean input-space distance matrix DX (except with the handwritten \u201c2\u201ds, where MDS uses the tangent distance). R is the standard linear correlation coefficient, taken over all entries of D\u0302M and DY. 43. In each sequence shown, the three intermediate images are those closest to the points 1/4, 1/2, and 3/4 of the way between the given endpoints. We can also synthesize an explicit mapping from input space X to the low-dimensional embedding Y, or vice versa, using the coordinates of corresponding points {xi , yi} in both spaces provided by Isomap together with standard supervised learning techniques (39). 44. Supported by the Mitsubishi Electric Research Laboratories, the Schlumberger Foundation, the NSF (DBS-9021648), and the DARPA Human ID program. We thank Y. LeCun for making available the MNIST database and S. Roweis and L. Saul for sharing related unpublished work. For many helpful discussions, we thank G. Carlsson, H. Farid, W. Freeman, T. Griffiths, R. Lehrer, S. Mahajan, D. Reich, W. Richards, J. M. Tenenbaum, Y. Weiss, and especially M. Bernstein.",
    "authors": [
        {
            "affiliations": [],
            "name": "Sam T. Roweis"
        },
        {
            "affiliations": [],
            "name": "Lawrence K. Saul"
        }
    ],
    "id": "SP:54470bb2b776047f38a75df86b39a4cc5c591b9f",
    "references": [
        {
            "authors": [
                "W K wj"
            ],
            "title": "Fits: The constrained weights that best reconstruct each data point from its neighbors can be computed in closed form. Consider a particular data point W x with neighbors W hj and sum-to-one reconstruction weights wj",
            "venue": "SCIENCE VOL",
            "year": 2000
        },
        {
            "authors": [
                "Manifold"
            ],
            "title": "Data points in Fig",
            "venue": "1B",
            "year": 2000
        },
        {
            "authors": [
                "T. Kohonen"
            ],
            "title": "Self-Organization and Associative Memory",
            "venue": "(Springer-Verlag, Berlin,",
            "year": 1988
        },
        {
            "authors": [
                "R. Tarjan"
            ],
            "title": "Data Structures and Network Algorithms, CBMS 44 (Society for Industrial and Applied",
            "year": 1983
        },
        {
            "authors": [
                "I.T. Jolliffe"
            ],
            "title": "Principal Component Analysis (SpringerVerlag",
            "venue": "New York,",
            "year": 1989
        },
        {
            "authors": [
                "M.G. Hinton"
            ],
            "title": "Revow for sharing their unpublished work (at the University of Toronto) on segmentation and pose estimation that motivated us to \u201cthink globally, fit locally\u201d; J. Tenenbaum (Stanford University) for many stimulating discussions about his work (4) and for sharing his code for the Isomap algorithm",
            "venue": "National Science Foundation, and the National Sciences and Engineering Research Council of Canada",
            "year": 2000
        }
    ],
    "sections": [
        {
            "text": "35. R. N. Shepard, Psychon. Bull. Rev. 1, 2 (1994). 36. J. B. Tenenbaum, Adv. Neural Info. Proc. Syst. 10, 682\n(1998). 37. T. Martinetz, K. Schulten, Neural Netw. 7, 507 (1994). 38. V. Kumar, A. Grama, A. Gupta, G. Karypis, Introduc-\ntion to Parallel Computing: Design and Analysis of Algorithms (Benjamin/Cummings, Redwood City, CA, 1994), pp. 257\u2013297. 39. D. Beymer, T. Poggio, Science 272, 1905 (1996). 40. Available at www.research.att.com/;yann/ocr/mnist. 41. P. Y. Simard, Y. LeCun, J. Denker, Adv. Neural Info.\nProc. Syst. 5, 50 (1993). 42. In order to evaluate the fits of PCA, MDS, and Isomap\non comparable grounds, we use the residual variance\n1 \u2013 R2(D\u0302M , DY). DY is the matrix of Euclidean distances in the low-dimensional embedding recovered by each algorithm. D\u0302M is each algorithm\u2019s best estimate of the intrinsic manifold distances: for Isomap, this is the graph distance matrix DG; for PCA and MDS, it is the Euclidean input-space distance matrix DX (except with the handwritten \u201c2\u201ds, where MDS uses the tangent distance). R is the standard linear correlation coefficient, taken over all entries of D\u0302M and DY. 43. In each sequence shown, the three intermediate images are those closest to the points 1/4, 1/2, and 3/4 of the way between the given endpoints. We can also synthesize an explicit mapping from input space X to the low-dimensional embedding Y, or vice versa, us-\ning the coordinates of corresponding points {xi , yi} in both spaces provided by Isomap together with standard supervised learning techniques (39).\n44. Supported by the Mitsubishi Electric Research Laboratories, the Schlumberger Foundation, the NSF (DBS-9021648), and the DARPA Human ID program. We thank Y. LeCun for making available the MNIST database and S. Roweis and L. Saul for sharing related unpublished work. For many helpful discussions, we thank G. Carlsson, H. Farid, W. Freeman, T. Griffiths, R. Lehrer, S. Mahajan, D. Reich, W. Richards, J. M. Tenenbaum, Y. Weiss, and especially M. Bernstein.\n10 August 2000; accepted 21 November 2000\nNonlinear Dimensionality Reduction by\nLocally Linear Embedding Sam T. Roweis1 and Lawrence K. Saul2\nMany areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text.\nHow do we judge similarity? Our mental representations of the world are formed by processing large numbers of sensory inputs\u2014including, for example, the pixel intensities of images, the power spectra of sounds, and the joint angles of articulated bodies. While complex stimuli of this form can be represented by points in a high-dimensional vector space, they typically have a much more compact description. Coherent structure in the world leads to strong correlations between inputs (such as between neighboring pixels in images), generating observations that lie on or close to a smooth low-dimensional manifold. To compare and classify such observations\u2014in effect, to reason about the world\u2014depends crucially on modeling the nonlinear geometry of these low-dimensional manifolds.\nScientists interested in exploratory analysis or visualization of multivariate data (1) face a similar problem in dimensionality reduction. The problem, as illustrated in Fig. 1, involves mapping high-dimensional inputs into a lowdimensional \u201cdescription\u201d space with as many\ncoordinates as observed modes of variability. Previous approaches to this problem, based on multidimensional scaling (MDS) (2), have computed embeddings that attempt to preserve pairwise distances [or generalized disparities (3)] between data points; these distances are measured along straight lines or, in more sophisticated usages of MDS such as Isomap (4),\nalong shortest paths confined to the manifold of observed inputs. Here, we take a different approach, called locally linear embedding (LLE), that eliminates the need to estimate pairwise distances between widely separated data points. Unlike previous methods, LLE recovers global nonlinear structure from locally linear fits.\nThe LLE algorithm, summarized in Fig. 2, is based on simple geometric intuitions. Suppose the data consist of N real-valued vectors WXi, each of dimensionality D, sampled from some underlying manifold. Provided there is sufficient data (such that the manifold is well-sampled), we expect each data point and its neighbors to lie on or close to a locally linear patch of the manifold. We characterize the local geometry of these patches by linear coefficients that reconstruct each data point from its neighbors. Reconstruction errors are measured by the cost function\n\u03b5~W ! 5 O i U WXi2SjWij WXjU 2\n(1)\nwhich adds up the squared distances between all the data points and their reconstructions. The weights Wij summarize the contribution of the jth data point to the ith reconstruction. To compute the weights Wij, we minimize the cost\n1Gatsby Computational Neuroscience Unit, University College London, 17 Queen Square, London WC1N 3AR, UK. 2AT&T Lab\u2014Research, 180 Park Avenue, Florham Park, NJ 07932, USA.\nE-mail: roweis@gatsby.ucl.ac.uk (S.T.R.); lsaul@research. att.com (L.K.S.)\nwww.sciencemag.org SCIENCE VOL 290 22 DECEMBER 2000 2323\nfunction subject to two constraints: first, that each data point WXi is reconstructed only from its neighbors (5), enforcing Wij 5 0 if WXj does not belong to the set of neighbors of WXi; second, that the rows of the weight matrix sum to one: SjWij 5 1. The optimal weights Wij subject to these constraints (6) are found by solving a least-squares problem (7).\nThe constrained weights that minimize these reconstruction errors obey an important symmetry: for any particular data point, they are invariant to rotations, rescalings, and translations of that data point and its neighbors. By symmetry, it follows that the reconstruction weights characterize intrinsic geometric properties of each neighborhood, as opposed to properties that depend on a particular frame of reference (8). Note that the invariance to translations is specifically enforced by the sum-to-one constraint on the rows of the weight matrix.\nSuppose the data lie on or near a smooth nonlinear manifold of lower dimensionality d ,, D. To a good approximation then, there exists a linear mapping\u2014consisting of a translation, rotation, and rescaling\u2014that maps the high-dimensional coordinates of each neighborhood to global internal coordinates on the manifold. By design, the reconstruction weights Wij reflect intrinsic geometric properties of the data that are invariant to exactly such transformations. We therefore expect their characterization of local geometry in the original data space to be equally valid for local patches on the manifold. In particular, the same weights Wij that reconstruct the ith data point in D dimensions should also reconstruct its embedded manifold coordinates in d dimensions.\nLLE constructs a neighborhood-preserving mapping based on the above idea. In the final step of the algorithm, each high-dimensional observation WXi is mapped to a low-dimensional vector WYi representing global internal coordinates on the manifold. This is done by choosing d-dimensional coordinates WYi to minimize the embedding cost function\nF~Y ! 5 O i U WYi 2 SjWij WYjU 2\n(2)\nThis cost function, like the previous one, is based on locally linear reconstruction errors, but here we fix the weights Wij while optimizing the coordinates WYi. The embedding cost in Eq. 2 defines a quadratic form in the vectors WYi. Subject to constraints that make the problem well-posed, it can be minimized by solving a sparse N 3 N eigenvalue problem (9), whose bottom d nonzero eigenvectors provide an ordered set of orthogonal coordinates centered on the origin.\nImplementation of the algorithm is straightforward. In our experiments, data points were reconstructed from their K nearest neighbors, as measured by Euclidean distance or normalized dot products. For such implementations of LLE, the algorithm has only one free parameter: the number of neighbors, K. Once neighbors are chosen, the optimal weights Wij and coordinates WYi are\nFig. 2. Steps of locally linear embedding: (1) Assign neighbors to each data point WXi (for example by using the K nearest neighbors). (2) Compute the weights Wij that best linearly reconstruct WXi from its neighbors, solving the constrained least-squares problem in Eq. 1. (3) Compute the low-dimensional embedding vectors WYi best reconstructed by Wij, minimizing Eq. 2 by finding the smallest eigenmodes of the sparse symmetric matrix in Eq. 3. Although the weights Wij and vectors Yi are computed by methods in linear algebra, the constraint that points are only reconstructed from neighbors can result in highly nonlinear embeddings.\nFig. 3. Images of faces (11) mapped into the embedding space described by the first two coordinates of LLE. Representative faces are shown next to circled points in different parts of the space. The bottom images correspond to points along the top-right path (linked by solid line), illustrating one particular mode of variability in pose and expression.\n22 DECEMBER 2000 VOL 290 SCIENCE www.sciencemag.org2324\ncomputed by standard methods in linear algebra. The algorithm involves a single pass through the three steps in Fig. 2 and finds global minima of the reconstruction and embedding costs in Eqs. 1 and 2.\nIn addition to the example in Fig. 1, for which the true manifold structure was known (10), we also applied LLE to images of faces (11) and vectors of word-document counts (12). Two-dimensional embeddings of faces and words are shown in Figs. 3 and 4. Note how the coordinates of these embedding spaces are related to meaningful attributes, such as the pose and expression of human faces and the semantic associations of words.\nMany popular learning algorithms for nonlinear dimensionality reduction do not share the favorable properties of LLE. Iterative hill-climbing methods for autoencoder neural networks (13, 14), self-organizing maps (15), and latent variable models (16) do not have the same guarantees of global optimality or convergence; they also tend to involve many more free parameters, such as learning rates, convergence criteria, and ar-\nchitectural specifications. Finally, whereas other nonlinear methods rely on deterministic annealing schemes (17) to avoid local minima, the optimizations of LLE are especially tractable.\nLLE scales well with the intrinsic manifold dimensionality, d, and does not require a discretized gridding of the embedding space. As more dimensions are added to the embedding space, the existing ones do not change, so that LLE does not have to be rerun to compute higher dimensional embeddings. Unlike methods such as principal curves and surfaces (18) or additive component models (19), LLE is not limited in practice to manifolds of extremely low dimensionality or codimensionality. Also, the intrinsic value of d can itself be estimated by analyzing a reciprocal cost function, in which reconstruction weights derived from the embedding vectors WYi are applied to the data points\nWXi. LLE illustrates a general principle of mani-\nfold learning, elucidated by Martinetz and Schulten (20) and Tenenbaum (4), that overlapping local neighborhoods\u2014collectively an-\nalyzed\u2014can provide information about global geometry. Many virtues of LLE are shared by Tenenbaum\u2019s algorithm, Isomap, which has been successfully applied to similar problems in nonlinear dimensionality reduction. Isomap\u2019s embeddings, however, are optimized to preserve geodesic distances between general pairs of data points, which can only be estimated by computing shortest paths through large sublattices of data. LLE takes a different approach, analyzing local symmetries, linear coefficients, and reconstruction errors instead of global constraints, pairwise distances, and stress functions. It thus avoids the need to solve large dynamic programming problems, and it also tends to accumulate very sparse matrices, whose structure can be exploited for savings in time and space.\nLLE is likely to be even more useful in combination with other methods in data analysis and statistical learning. For example, a parametric mapping between the observation and embedding spaces could be learned by supervised neural networks (21) whose target values are generated by LLE. LLE can also be generalized to harder settings, such as the case of disjoint data manifolds (22), and specialized to simpler ones, such as the case of time-ordered observations (23).\nPerhaps the greatest potential lies in applying LLE to diverse problems beyond those considered here. Given the broad appeal of traditional methods, such as PCA and MDS, the algorithm should find widespread use in many areas of science."
        },
        {
            "heading": "References and Notes",
            "text": "1. M. L. Littman, D. F. Swayne, N. Dean, A. Buja, in\nComputing Science and Statistics: Proceedings of the 24th Symposium on the Interface, H. J. N. Newton, Ed. (Interface Foundation of North America, Fairfax Station, VA, 1992), pp. 208\u2013217. 2. T. Cox, M. Cox, Multidimensional Scaling (Chapman & Hall, London, 1994). 3. Y. Takane, F. W. Young, Psychometrika 42, 7 (1977). 4. J. Tenenbaum, in Advances in Neural Information\nProcessing 10, M. Jordan, M. Kearns, S. Solla, Eds. (MIT Press, Cambridge, MA, 1998), pp. 682\u2013688. 5. The set of neighbors for each data point can be assigned in a variety of ways: by choosing the K nearest neighbors in Euclidean distance, by considering all data points within a ball of fixed radius, or by using prior knowledge. Note that for fixed number of neighbors, the maximum number of embedding dimensions LLE can be expected to recover is strictly less than the number of neighbors. 6. For certain applications, one might also constrain the weights to be positive, thus requiring the reconstruction of each data point to lie within the convex hull of its neighbors. 7. Fits: The constrained weights that best reconstruct each data point from its neighbors can be computed in closed form. Consider a particular data point Wx with neighbors Whj and sum-to-one reconstruction weights wj. The reconstruction error  Wx \u2013 Sj51K wj Whj2 is minimized in three steps. First, evaluate inner products between neighbors to compute the neighborhood correlation matrix, Cjk 5 Whj z Whk, and its matrix inverse, C 21. Second, compute the Lagrange multiplier, l 5 a/b, that enforces the sum-to-one constraint, where a 5 1 2 SjkC jk\n21( Wx z Whk) and b 5 SjkCjk\n21. Third, compute the reconstruction weights: wj 5 SkC jk 21( Wx z Whk 1 l). If the correlation matrix C is\nFig. 4. Arranging words in a continuous semantic space. Each word was initially represented by a high-dimensional vector that counted the number of times it appeared in different encyclopedia articles. LLE was applied to these word-document count vectors (12), resulting in an embedding location for each word. Shown are words from two different bounded regions (A) and (B) of the embedding space discovered by LLE. Each panel shows a twodimensional projection onto the third and fourth coordinates of LLE; in these two dimensions, the regions (A) and (B) are highly overlapped. The inset in (A) shows a three-dimensional projection onto the third, fourth, and fifth coordinates, revealing an extra dimension along which regions (A) and (B) are more separated. Words that lie in the intersection of both regions are capitalized. Note how LLE colocates words with similar contexts in this continuous semantic space.\nwww.sciencemag.org SCIENCE VOL 290 22 DECEMBER 2000 2325\nnearly singular, it can be conditioned (before inversion) by adding a small multiple of the identity matrix. This amounts to penalizing large weights that exploit correlations beyond some level of precision in the data sampling process. 8. Indeed, LLE does not require the original data to be described in a single coordinate system, only that each data point be located in relation to its neighbors. 9. The embedding vectors WYi are found by minimizing the cost function F(Y ) 5 Si WYi \u2013 SjWij WYj2 over WYi with fixed weights Wij. This optimization is performed subject to constraints that make the problem well posed. It is clear that the coordinates WYi can be translated by a constant displacement without affecting the cost, F(Y ). We remove this degree of freedom by requiring the coordinates to be centered on the origin: Si WYi 5 W0. Also, to avoid degenerate solutions, we constrain the embedding vectors to have unit covariance, with outer products that satisfy 1\nN Si WYi R WYi 5 I, where I is the d 3\nd identity matrix. Now the cost defines a quadratic form, F(Y ) 5 Sij Mij( WYiz WYj), involving inner products of the embedding vectors and the symmetric N 3 N matrix\nM ij 5 d ij 2 W ij 2 W ji 1 S k WkiWkj (3)\nwhere dij is 1 if i 5 j and 0 otherwise. The optimal embedding, up to a global rotation of the embedding space, is found by computing the bottom d 1 1 eigenvectors of this matrix (24). The bottom eigenvector of this matrix, which we discard, is the unit vector with all equal components; it represents a free translation mode of eigenvalue zero. (Discarding it enforces the constraint that the embeddings have zero mean.) The remaining d eigenvectors form the d embedding coordinates found by LLE. Note that the matrix M can be stored and manipulated as the sparse matrix (I 2 W )T(I 2 W ), giving substantial computational savings for large values of N. Moreover, its bottom d 1 1 eigenvectors (those corresponding to its smallest d 1 1 eigenvalues) can be found efficiently without performing a full matrix diagonalization (25).\n10. Manifold: Data points in Fig. 1B (N 5 2000) were sampled from the manifold (D 5 3) shown in Fig. 1A. Nearest neighbors (K 5 20) were determined by Euclidean distance. This particular manifold was introduced by Tenenbaum (4), who showed that its global structure could be learned by the Isomap algorithm. 11. Faces: Multiple photographs (N 5 2000) of the same face were digitized as 20 3 28 grayscale images. Each image was treated by LLE as a data vector with D 5 560 elements corresponding to raw pixel intensities. Nearest neighbors (K 5 12) were determined by Euclidean distance in pixel space. 12. Words: Word-document counts were tabulated for N 5 5000 words from D 5 31,000 articles in Grolier\u2019s Encyclopedia (26). Nearest neighbors (K 5 20) were determined by dot products between count vectors normalized to unit length. 13. D. DeMers, G. W. Cottrell, in Advances in Neural Information Processing Systems 5, D. Hanson, J. Cowan, L. Giles, Eds. (Kaufmann, San Mateo, CA, 1993), pp. 580\u2013587. 14. M. Kramer, AIChE J. 37, 233 (1991). 15. T. Kohonen, Self-Organization and Associative Mem-\nory (Springer-Verlag, Berlin, 1988). 16. C. Bishop, M. Svensen, C. Williams, Neural Comput.\n10, 215 (1998). 17. H. Klock, J. Buhmann, Pattern Recognition 33, 651\n(1999). 18. T. J. Hastie, W. Stuetzle, J. Am. Stat. Assoc. 84, 502\n(1989). 19. D. J. Donnell, A. Buja, W. Stuetzle, Ann. Stat. 22, 1635\n(1994). 20. T. Martinetz, K. Schulten, Neural Networks 7, 507\n(1994). 21. D. Beymer, T. Poggio, Science 272, 1905 (1996). 22. Although in all the examples considered here, the\ndata had a single connected component, it is possible to formulate LLE for data that lies on several disjoint manifolds, possibly of different underlying dimensionality. Suppose we form a graph by connecting each data point to its neighbors. The number of connected components (27) can be detected by ex-\namining powers of its adjacency matrix. Different connected components of the data are essentially decoupled in the eigenvector problem for LLE. Thus, they are best interpreted as lying on distinct manifolds, and are best analyzed separately by LLE. 23. If neighbors correspond to nearby observations in time, then the reconstruction weights can be computed online (as the data itself is being collected) and the embedding can be found by diagonalizing a sparse banded matrix. 24. R. A. Horn, C. R. Johnson, Matrix Analysis (Cambridge Univ. Press, Cambridge, 1990). 25. Z. Bai, J. Demmel, J. Dongarra, A. Ruhe, H. van der Vorst, Eds., Templates for the Solution of Algebraic Eigenvalue Problems: A Practical Guide (Society for Industrial and Applied Mathematics, Philadelphia, PA, 2000). 26. D. D. Lee, H. S. Seung, Nature 401, 788 (1999). 27. R. Tarjan, Data Structures and Network Algorithms,\nCBMS 44 (Society for Industrial and Applied Mathematics, Philadelphia, PA, 1983). 28. I. T. Jolliffe, Principal Component Analysis (SpringerVerlag, New York, 1989). 29. N. Kambhatla, T. K. Leen, Neural Comput. 9, 1493 (1997). 30. We thank G. Hinton and M. Revow for sharing their unpublished work (at the University of Toronto) on segmentation and pose estimation that motivated us to \u201cthink globally, fit locally\u201d; J. Tenenbaum (Stanford University) for many stimulating discussions about his work (4) and for sharing his code for the Isomap algorithm; D. D. Lee (Bell Labs) and B. Frey (University of Waterloo) for making available word and face data from previous work (26); and C. Brody, A. Buja, P. Dayan, Z. Ghahramani, G. Hinton, T. Jaakkola, D. Lee, F. Pereira, and M. Sahani for helpful comments. S.T.R. acknowledges the support of the Gatsby Charitable Foundation, the U.S. National Science Foundation, and the National Sciences and Engineering Research Council of Canada.\n7 August 2000; accepted 17 November 2000\n22 DECEMBER 2000 VOL 290 SCIENCE www.sciencemag.org2326"
        }
    ],
    "title": "Nonlinear Dimensionality Reduction by Locally Linear Embedding"
}