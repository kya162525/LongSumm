{
    "abstractText": "This work is dedicated to eliminating the overhead required for guaranteeing the storage order in the modern IO stack. The existing block device adopts a prohibitively expensive approach in ensuring the storage order among write requests: interleaving the write requests with Transfer-and-Flush. Exploiting the cache barrier command for Flash storage, we overhaul the IO scheduler, the dispatch module, and the filesystem so that these layers are orchestrated to preserve the ordering condition imposed by the application with which the associated data blocks are made durable. The key ingredients of Barrier-Enabled IO stack are Epoch-based IO scheduling, Order-Preserving Dispatch, and Dual-Mode Journaling. Barrier-enabled IO stack can control the storage order without Transfer-and-Flush overhead. We implement the barrier-enabled IO stack in server as well as in mobile platforms. SQLite performance increases by 270% and 75%, in server and in smartphone, respectively. In a server storage, BarrierFS brings as much as by 43\u00d7 and by 73\u00d7 performance gain in MySQL and SQLite, respectively, against EXT4 via relaxing the durability of a transaction.",
    "authors": [
        {
            "affiliations": [],
            "name": "Youjip Won"
        },
        {
            "affiliations": [],
            "name": "Jaemin Jung"
        },
        {
            "affiliations": [],
            "name": "Gyeongyeol Choi"
        },
        {
            "affiliations": [],
            "name": "Joontaek Oh"
        },
        {
            "affiliations": [],
            "name": "Seongbae Son"
        },
        {
            "affiliations": [],
            "name": "Jooyoung Hwang"
        },
        {
            "affiliations": [],
            "name": "Sangyeun Cho"
        }
    ],
    "id": "SP:8eed4eb91a0fd39ba90afea248664c5ae728ffc4",
    "references": [
        {
            "authors": [
                "J. AXBOE"
            ],
            "title": "Linux block IO present and future",
            "venue": "In Proc. of Ottawa Linux Symposium (Ottawa,",
            "year": 2004
        },
        {
            "authors": [
                "CHANG",
                "Y.-M",
                "KUO Y.-H",
                "LI T.-W",
                "Y.- C",
                "LI",
                "H.-P"
            ],
            "title": "Achieving SLC Performance with MLC Flash Memory",
            "venue": "In Proc. of DAC",
            "year": 2015
        },
        {
            "authors": [
                "F. CHEN",
                "R. LEE",
                "X. ZHANG"
            ],
            "title": "Essential roles of exploiting internal parallelism of flash memory based solid state drives in high-speed data processing",
            "venue": "In Proc. of IEEE HPCA 2011 (San Antonio,",
            "year": 2011
        },
        {
            "authors": [
                "Q. CHEN",
                "L. LIANG",
                "Y. XIA",
                "H. CHEN",
                "KIM"
            ],
            "title": "Mitigating sync amplification for copy-on-write virtual disk",
            "venue": "In Proc. of USENIX FAST 2016 (Santa Clara, CA,",
            "year": 2016
        },
        {
            "authors": [
                "V. CHIDAMBARAM"
            ],
            "title": "Orderless and Eventually Durable File Systems",
            "venue": "PhD thesis, UNIVIRSITY OF WISCONSIN\u2013MADISON,",
            "year": 2015
        },
        {
            "authors": [
                "V. CHIDAMBARAM",
                "T.S. PILLAI",
                "A.C. ARPACI-DUSSEAU",
                "R.H. ARPACI-DUSSEAU"
            ],
            "title": "Optimistic Crash Consistency",
            "venue": "In Proc. of ACM SOSP",
            "year": 2013
        },
        {
            "authors": [
                "V. CHIDAMBARAM",
                "T. SHARMA",
                "A.C. ARPACI-DUSSEAU",
                "R.H. ARPACI-DUSSEAU"
            ],
            "title": "Consistency Without Ordering",
            "venue": "In Proc. of USENIX FAST",
            "year": 2012
        },
        {
            "authors": [
                "Y.S. CHO",
                "I.H. PARK",
                "S.Y. YOON",
                "N.H. LEE",
                "S.H. JOO",
                "SONG",
                "K.-W",
                "K. CHOI",
                "HAN",
                "J.-M",
                "K.H. KYUNG",
                "JUN",
                "Y.-H"
            ],
            "title": "Adaptive multi-pulse program scheme based on tunneling speed classification for next generation multi-bit/cell NAND flash",
            "venue": "IEEE Journal of Solid-State Circuits(JSSC) 48,",
            "year": 2013
        },
        {
            "authors": [
                "J. CIPAR",
                "G. GANGER",
                "K. KEETON",
                "C.B. MORREY III",
                "C.A. SOULES",
                "A. VEITCH"
            ],
            "title": "LazyBase: trading freshness for performance in a scalable database",
            "venue": "In Proc. of ACM EuroSys 2012 (Bern,",
            "year": 2012
        },
        {
            "authors": [
                "D. COBB",
                "A. HUFFMAN"
            ],
            "title": "NVM express and the PCI express SSD Revolution",
            "venue": "In Proc. of Intel Developer Forum (San Francisco, CA,",
            "year": 2012
        },
        {
            "authors": [
                "J. CONDIT",
                "E.B. NIGHTINGALE",
                "C. FROST",
                "E. IPEK",
                "B. LEE",
                "D. BURGER",
                "D. COETZEE"
            ],
            "title": "Better I/O through byte-addressable, persistent memory",
            "venue": "In Proc. of ACM SOSP 2009 (Big Sky, MT,",
            "year": 2009
        },
        {
            "authors": [
                "J. CORBET"
            ],
            "title": "Barriers and journaling filesystems",
            "venue": "http: //lwn.net/Articles/283161/,",
            "year": 2010
        },
        {
            "authors": [
                "J. CORBET"
            ],
            "title": "The end of block barriers",
            "venue": "https://lwn. net/Articles/400541/,",
            "year": 2010
        },
        {
            "authors": [
                "CUI H",
                "CIPAR J",
                "HO Q",
                "KIM J. K",
                "LEE S",
                "MAR A. KU",
                "WEI J",
                "DAI W",
                "GANGER G. R",
                "GIBBONS",
                "B P"
            ],
            "title": "Exploiting bounded staleness to speed up big data analytics",
            "venue": "In Proc. of USENIX ATC 2014 (Philadelihia, PA,",
            "year": 2014
        },
        {
            "authors": [
                "F. DABEK",
                "M.F. KAASHOEK",
                "D. KARGER",
                "R. MORRIS",
                "I. STOICA"
            ],
            "title": "Wide-area Cooperative Storage with CFS",
            "venue": "In Proc. of ACM SOSP",
            "year": 2001
        },
        {
            "authors": [
                "B. DEES"
            ],
            "title": "Native command queuing-advanced performance in desktop storage",
            "venue": "IEEE Potentials Magazine 24,",
            "year": 2005
        },
        {
            "authors": [
                "C. FROST",
                "M. MAMMARELLA",
                "E. KOHLER",
                "A. DE LOS REYES",
                "S. HOVSEPIAN",
                "A. MATSUOKA",
                "L. ZHANG"
            ],
            "title": "Generalized File System Dependencies",
            "venue": "In Proc. of ACM SOSP",
            "year": 2007
        },
        {
            "authors": [
                "GIM J",
                "WON"
            ],
            "title": "Extract and infer quickly: Obtaining sector geometry of modern hard disk drives",
            "venue": "ACM Transactions on Storage (TOS) 6,",
            "year": 2010
        },
        {
            "authors": [
                "L.M. GRUPP",
                "J.D. DAVIS",
                "S. SWANSON"
            ],
            "title": "The bleak future of nand flash memory",
            "venue": "In Proc.of USENIX FAST",
            "year": 2012
        },
        {
            "authors": [
                "J. GUO",
                "J. YANG",
                "Y. ZHANG",
                "Y. CHEN"
            ],
            "title": "Low cost power failure protection for mlc nand flash storage systems with pram/dram hybrid buffer",
            "venue": "In Proc. of DATE 2013 (Alpexpo Grenoble, France,",
            "year": 2013
        },
        {
            "authors": [
                "M. HELM",
                "PARK",
                "J.-K",
                "A. GHALAM",
                "J. GUO",
                "C. WAN HA",
                "C. HU",
                "H. KIM",
                "K. KAVALIPURAPU",
                "E. LEE",
                "A MOHAMMADZADEH"
            ],
            "title": "A 128Gb MLC NAND-Flash device using 16nm planar cell",
            "venue": "In Proc. of IEEE ISSCC",
            "year": 2014
        },
        {
            "authors": [
                "JEONG S",
                "LEE K",
                "LEE S",
                "SON S",
                "WON"
            ],
            "title": "I/O Stack Optimization for Smartphones",
            "venue": "In Proc. of USENIX ATC 2013 (San Jose, CA,",
            "year": 2013
        },
        {
            "authors": [
                "KANG J",
                "ZHANG B",
                "WO T",
                "YU W",
                "DU L",
                "MA S",
                "HUAI J"
            ],
            "title": "SpanFS: A Scalable File System on Fast Storage Devices",
            "venue": "In Proc. of USENIX ATC",
            "year": 2015
        },
        {
            "authors": [
                "KANG",
                "LEE W.-H",
                "S.-W",
                "B. MOON",
                "OH",
                "G.-H",
                "MIN"
            ],
            "title": "X-FTL: Transactional FTL for SQLite Databases",
            "venue": "In Proc. of ACM SIGMOD 2013 (New York, NY,",
            "year": 2013
        },
        {
            "authors": [
                "R. KESAVAN",
                "R. SINGH",
                "T. GRUSECKI",
                "Y. PATEL"
            ],
            "title": "Algorithms and data structures for efficient free space reclamation in wafl",
            "venue": "In Proc. of USENIX FAST 2017 (Santa Clara,",
            "year": 2017
        },
        {
            "authors": [
                "KIM",
                "H.-J",
                "J.-S"
            ],
            "title": "Tuning the ext4 filesystem performance for android-based smartphones",
            "venue": "ICFCE",
            "year": 2011
        },
        {
            "authors": [
                "KIM Y"
            ],
            "title": "An empirical study of redundant array of independent solid-state drives (RAIS)",
            "venue": "Springer Cluster Computing 18,",
            "year": 2015
        },
        {
            "authors": [
                "LEE C",
                "SIM D",
                "HWANG J",
                "CHO"
            ],
            "title": "F2FS: A New File System for Flash Storage",
            "venue": "In Proc. of USENIX FAST",
            "year": 2015
        },
        {
            "authors": [
                "LEE S",
                "LEE",
                "J.-Y",
                "PARK",
                "I.-H",
                "PARK J",
                "YUN",
                "W. KIM S",
                "M.-S. LEE",
                "J.-H. KIM",
                "M. LEE",
                "K. KIM"
            ],
            "title": "A 128Gb 2b/cell NAND flash memory in 14nm technology with tPROG=640us and 800MB/s I/O rate",
            "venue": "In Proc. of IEEE ISSCC",
            "year": 2016
        },
        {
            "authors": [
                "LEE W",
                "LEE K",
                "SON H",
                "KIM",
                "W.-H. NAM",
                "WON"
            ],
            "title": "WALDIO: eliminating the filesystem journaling in resolving the journaling of journal anomaly",
            "venue": "In Proc. of USENIX ATC",
            "year": 2015
        },
        {
            "authors": [
                "L. LU",
                "Y. ZHANG",
                "T. DO",
                "S. AL-KISWANY",
                "A.C. ARPACI-DUSSEAU",
                "R.H. ARPACI-DUSSEAU"
            ],
            "title": "Physical Disentanglement in a Container-Based File System",
            "venue": "In Proc. of USENIX OSDI",
            "year": 2014
        },
        {
            "authors": [
                "LU Y",
                "SHU J",
                "GUO J",
                "LI S",
                "MUTLU O"
            ],
            "title": "Lighttx: A lightweight transactional design in flash-based ssds to support flexible transactions",
            "venue": "In Proc. of IEEE ICCD",
            "year": 2013
        },
        {
            "authors": [
                "A. MARTINEZ",
                "V. CHIDAMBARAM"
            ],
            "title": "Crashmonkey: A framework to automatically test file-system crash consistency",
            "venue": "In 9th USENIX Workshop on Hot Topics in Storage and File Systems (HotStorage",
            "year": 2017
        },
        {
            "authors": [
                "A. MATHUR",
                "M. CAO",
                "S. BHATTACHARYA",
                "A. DILGER",
                "A. TOMAS",
                "L. VIVIER"
            ],
            "title": "The new ext4 filesystem: current status and future plans",
            "venue": "In Proc. of Linux symposium",
            "year": 2007
        },
        {
            "authors": [
                "M.K. MCKUSICK",
                "GANGER",
                "G. R"
            ],
            "title": "Soft Updates: A Technique for Eliminating Most Synchronous Writes in the Fast Filesystem",
            "venue": "In Proc. of USENIX ATC",
            "year": 1999
        },
        {
            "authors": [
                "MIN C",
                "KANG",
                "W.-H. KIM",
                "T. LEE",
                "S.-W",
                "EOM",
                "I. Y"
            ],
            "title": "Lightweight application-level crash consistency on transactional flash storage",
            "venue": "In Proc. of USENIX ATC",
            "year": 2015
        },
        {
            "authors": [
                "MIN C",
                "KASHYAP S",
                "MAASS S",
                "KIM"
            ],
            "title": "Understanding Manycore Scalability of File Systems",
            "venue": "In Proc. of USENIX ATC 2016 (Denver, CO,",
            "year": 2016
        },
        {
            "authors": [
                "MIN C",
                "KASHYAP S",
                "MAASS S",
                "KIM"
            ],
            "title": "Understanding manycore scalability of file systems",
            "venue": "In Proc.of USENIX ATC 2016 (Denver, CO,",
            "year": 2016
        },
        {
            "authors": [
                "C. MOHAN",
                "D. HADERLE",
                "B. LINDSAY",
                "H. PIRAHESH",
                "P. SCHWARZ"
            ],
            "title": "ARIES: a transaction recovery method supporting fine-granularity locking and partial rollbacks using write-ahead logging",
            "venue": "ACM Transactions on Database Systems(TODS) 17,",
            "year": 1992
        },
        {
            "authors": [
                "D. NARAYANAN",
                "A. DONNELLY",
                "A. ROWSTRON"
            ],
            "title": "Write Off-loading: Practical Power Management for Enterprise Storage",
            "venue": "ACM Transactions on Storage(TOS) 4,",
            "year": 2008
        },
        {
            "authors": [
                "E.B. NIGHTINGALE",
                "K. VEERARAGHAVAN",
                "P.M. CHEN",
                "J. FLINN"
            ],
            "title": "Rethink the Sync",
            "venue": "In Proc. of USENIX OSDI",
            "year": 2006
        },
        {
            "authors": [
                "M. OKUN",
                "A. BARAK"
            ],
            "title": "Atomic writes for data integrity and consistency in shared storage devices for clusters",
            "venue": "In Proc. of ICA3PP",
            "year": 2002
        },
        {
            "authors": [
                "OU J",
                "SHU J",
                "LU"
            ],
            "title": "A high performance file system for non-volatile main memory",
            "venue": "In Proc. of ACM EuroSys 2016 (London, UK,",
            "year": 2016
        },
        {
            "authors": [
                "X. OUYANG",
                "D. NELLANS",
                "R. WIPFEL",
                "D. FLYNN",
                "D.K. PANDA"
            ],
            "title": "Beyond block I/O: Rethinking traditional storage primitives",
            "venue": "In Proc. of IEEE HPCA 2011 (San Antonio,",
            "year": 2011
        },
        {
            "authors": [
                "S. PALANCA",
                "S.A. FISCHER",
                "S. MAIYURAN",
                "S. QAWAMI"
            ],
            "title": "Mfence and lfence micro-architectural implementation method and system, July 5 2016",
            "venue": "US Patent 9,383,998",
            "year": 2016
        },
        {
            "authors": [
                "S. PARK",
                "T. KELLY",
                "K. SHEN"
            ],
            "title": "Failure-atomic Msync(): A Simple and Efficient Mechanism for Preserving the Integrity of Durable Data",
            "venue": "In Proc. of ACM EuroSys 2013 (Prague, Czech Republic,",
            "year": 2013
        },
        {
            "authors": [
                "T.S. PILLAI",
                "R. ALAGAPPAN",
                "L. LU",
                "V. CHI- DAMBARAM",
                "A.C. ARPACI-DUSSEAU",
                "R.H. ARPACI-DUSSEAU"
            ],
            "title": "Application crash consistency and performance with ccfs",
            "venue": "In Proc.of USENIX FAST",
            "year": 2017
        },
        {
            "authors": [
                "V. PRABHAKARAN",
                "T.L. RODEHEFFER",
                "L. ZHOU"
            ],
            "title": "Transactional flash",
            "venue": "In Proc. of USENIX OSDI 2008 (Berkeley, CA, USA,",
            "year": 2008
        },
        {
            "authors": [
                "D. PUROHITH",
                "J. MOHAN",
                "V. CHIDAMBARAM"
            ],
            "title": "The dangers and complexities of sqlite benchmarking",
            "venue": "In Proceedings of the 8th Asia-Pacific Workshop on Systems (New York, NY,",
            "year": 2017
        },
        {
            "authors": [
                "REV H"
            ],
            "title": "SCSI Commands Reference Manual. http://www.seagate.com/files/staticfiles/ support/docs/manual/Interface%20manuals/ 100293068h.pdf",
            "venue": "Jul 2014. Seagate",
            "year": 2014
        },
        {
            "authors": [
                "O. RODEH",
                "J. BACIK",
                "C. MASON"
            ],
            "title": "Btrfs: The linux b-tree filesystem",
            "venue": "ACM Transactions on Storage (TOS) 9,",
            "year": 2013
        },
        {
            "authors": [
                "M. ROSENBLUM",
                "J.K. OUSTERHOUT"
            ],
            "title": "The design and implementation of a log-structured file system",
            "venue": "ACM Transactions on Computer Systems (TOCS) 10,",
            "year": 1992
        },
        {
            "authors": [
                "P. SEHGAL",
                "V. TARASOV",
                "E. ZADOK"
            ],
            "title": "Evaluating Performance and Energy in File System Server Workloads",
            "venue": "In Proc. of USENIX FAST",
            "year": 2010
        },
        {
            "authors": [
                "M.I. SELTZER",
                "G.R. GANGER",
                "M.K. MCKUSICK",
                "K.A. SMITH",
                "C.A. SOULES",
                "C.A. STEIN"
            ],
            "title": "Journaling Versus Soft Updates: Asynchronous Meta-data Protection in File Systems",
            "venue": "In Proc. of USENIX ATC",
            "year": 2000
        },
        {
            "authors": [
                "A. SWEENEY",
                "D. DOUCETTE",
                "W. HU",
                "C. ANDERSON",
                "M. NISHIMOTO",
                "G. PECK"
            ],
            "title": "Scalability in the xfs file system",
            "venue": "In Proc. of USENIX ATC",
            "year": 1996
        },
        {
            "authors": [
                "T. TS\u2019O"
            ],
            "title": "Using Cache barrier in liue of REQ FLUSH",
            "venue": "http://www.spinics.net/lists/linux-ext4/ msg49018.html,",
            "year": 2015
        },
        {
            "authors": [
                "S.C. TWEEDIE"
            ],
            "title": "Journaling the linux ext2fs filesystem",
            "venue": "In Proc.of The Fourth Annual Linux Expo (Durham, NC,",
            "year": 1998
        },
        {
            "authors": [
                "R. VERMA",
                "A.A. MENDEZ",
                "S. PARK",
                "S. MANNAR- SWAMY",
                "T. KELLY",
                "C. MORREY"
            ],
            "title": "Failure- Atomic Updates of Application Data in a Linux File System",
            "venue": "In Proc. of USENIX FAST",
            "year": 2015
        },
        {
            "authors": [
                "Y. WANG",
                "M. KAPRITSOS",
                "Z. REN",
                "P. MAHAJAN",
                "J. KIRUBANANDAM",
                "L. ALVISI",
                "M. DAHLIN"
            ],
            "title": "Robustness in the salus scalable block store",
            "venue": "In Proceedings of the 10th USENIX Conference on Networked Systems Design and Implementation (Berkeley, CA,",
            "year": 2013
        },
        {
            "authors": [
                "Z. WEISS",
                "S. SUBRAMANIAN",
                "S. SUNDARARAMAN",
                "N. TALAGALA",
                "A. ARPACI-DUSSEAU",
                "R. ARPACI- DUSSEAU"
            ],
            "title": "ANViL: Advanced Virtualization for Modern Non-Volatile Memory Devices",
            "venue": "In Proc. of USENIX FAST",
            "year": 2015
        },
        {
            "authors": [
                "A. WILSON"
            ],
            "title": "The new and improved FileBench",
            "venue": "In Proc. of USENIX FAST",
            "year": 2008
        },
        {
            "authors": [
                "Q. XU",
                "H. SIYAMWALA",
                "M. GHOSH",
                "T. SURI",
                "M. AWASTHI",
                "Z. GUZ",
                "A. SHAYESTEH",
                "V. BAL- AKRISHNAN"
            ],
            "title": "Performance Analysis of NVMe SSDs and Their Implication on Real World Databases",
            "venue": "In Proc. of ACM SYSTOR",
            "year": 2015
        },
        {
            "authors": [
                "S.Y. PARK",
                "E. SEO",
                "J.Y. SHIN",
                "S. MAENG",
                "LEE"
            ],
            "title": "Exploiting Internal Parallelism of Flash-based SSDs",
            "venue": "IEEE Computer Architecture Letters(CAL)",
            "year": 2010
        },
        {
            "authors": [
                "C. ZHANG",
                "Y. WANG",
                "T. WANG",
                "R. CHEN",
                "D. LIU",
                "Z. SHAO"
            ],
            "title": "Deterministic crash recovery for NAND flash based storage systems",
            "venue": "In Proc. of ACM/EDAC/IEEE DAC 2014 (San Francisco, CA,",
            "year": 2014
        }
    ],
    "sections": [
        {
            "text": ""
        },
        {
            "heading": "1 Motivation",
            "text": "The modern Linux IO stack is a collection of the arbitration layers; the IO scheduler, the command queue manager, and the writeback cache manager shuffle the incoming requests at their own disposal before passing them to the next layers. Despite the compound uncertainties from the multiple layers of arbitration, it is essential for the software writers to enforce a certain order in which the data blocks are reflected to the storage surface, storage order, in many cases such as in guaranteeing the durability and the atomicity of a database transaction [47, 26, 35], in filesystem journaling [67, 41, 65, 4], in soft-update [42, 63], or in copy-onwrite or log-structure filesystems [61, 35, 60, 31]. Enforcing a storage order is achieved by an extremely expensive approach: dispatching the following request only\n\u2217This work was done while the author was a graduate student at Hanyang University.\nafter the data block associated with the preceding request is completely transferred to the storage device and is made durable. We call this mechanism a Transfer-andFlush. For decades, interleaving the write requests with a Transfer-and-Flush has been the fundamental principle to guarantee the storage order in a set of requests [24, 16].\nWe observe a phenomenal increase in the performance and the capacity of the Flash storage. The performance increase owes much to the concurrency and the parallelism in the Flash storage, e.g. the multi-channel/way controller [73, 6], the large size storage cache [48], and the deep command queue [19, 27, 72]. A state of the art NVMe SSD reportedly exhibits up to 750 KIOPS random read performance [72]. It is nearly 4,000\u00d7 of a HDD\u2019s performance. The capacity increase is due to the adoption of the finer manufacturing process (sub-10 nm) [25, 36], and the multi-bits per cell (MLC, TLC, and QLC) [5, 11]. Meanwhile, the time to program a Flash cell has barely improved, and is even deteriorating in some cases [22].\nThe Transfer-and-Flush based order-preserving mechanism conflicts with the parallelism and the concurrency in the Flash storage. It disables the parallelism and the concurrency feature of the Flash storage and exposes the raw Flash cell programming latency to the host. The\nUSENIX Association 16th USENIX Conference on File and Storage Technologies 211\noverhead of the Transfer-and-Flush mechanism will become more significant as the Flash storage employs a higher degree of parallelism and the denser Flash device. Fig. 1 illustrates an important trend. We measure the sustained throughput of orderless random write (plain buffered write) and the ordered random write in EXT4 filesystem. In ordered random write, each write request is followed by fdatasync(). X-axis denotes the throughput of orderless write which corresponds to the rate at which the storage device services the write requests at its full throttle. This usually matches the vendor published performance of the storage device. The number next to each point denotes the sustained throughput of the ordered write. The Y-axis denotes the ratio between the two. In a single channel mobile storage for smartphone (SSD A), the performance of ordered write is 20% of that of unordered write (1351 IOPS vs. 7000 IOPS). In a thirty-two channel Flash array (SSD G), this ratio decreases to 1% (2296 IOPS vs. 230K IOPS). In SSD with supercap (SSD E), the ordered write performance is 25% of that of the unordered write. The Flash storage uses supercap to hide the flush latency from the host. Even in a Flash storage with supercap, the overhead of Transferand-Flush is significant.\nMany researchers have attempted to address the overhead of storage order guarantee. The techniques deployed in the production platforms include non-volatile writeback cache at the Flash storage [23], no-barrier mount option at the EXT4 filesystem [15], and transactional checksum [56, 32, 64]. Efforts such as transactional filesystem [50, 18, 54, 35, 68] and transactional block device [30, 74, 43, 70, 52] save the application from the overhead of enforcing the storage order associated with filesystem journaling. A school of work address more fundamental aspects in controlling the storage order, such as separating the ordering guarantee from durability guarantee [9], providing a programming model to define the ordering dependency among the set of writes [20], and persisting a data block only when the result needs to be externally visible [49]. Despite their elegance, these works rely on Transfer-and-Flush when it is required to enforce the storage order. OptFS [9] relies on Transfer-and-Flush in enforcing the order between the journal commit and the associated checkpoint. Featherstitch [20] relies on Transfer-and-Flush to implement the ordering dependency between the patchgroups.\nIn this work, we revisit the issue of eliminating the Transfer-and-Flush overhead in the modern IO stack. We develop a Barrier-Enabled IO stack, in which the filesystem can issue the following request before the preceding request is serviced and yet the IO stack can enforce the storage order between them. The barrier-enabled IO stack consists of the cache barrier-aware storage device, the order-preserving block device layer, and the bar-\nrier enabled filesystem. For cache barrier-aware storage device, we exploit the \u201ccache barrier\u201d command [28]. The barrier-enabled IO stack is built upon the foundation that the host can control a certain partial order in which the cache contents are flushed. The \u201ccache barrier\u201d command precisely serves this purpose. For the order-preserving block device layer, the command dispatch mechanism and the IO scheduler are overhauled so that the block device layer ensures that the IO requests from the filesystem are serviced preserving a certain partial order. For the barrier-enabled filesystem, we define new interfaces, fbarrier() and fdatabarrier(), to separate the ordering guarantee from the durability guarantee. They are similar to fsync() and fdatasync(), respectively, except that they return without waiting for the associated blocks to become durable. We modify EXT4 for the order-preserving block device layer. We develop dual-mode journaling for the order-preserving block device. Based upon the dual-mode journaling, we newly implement fbarrier() and fdatabarrier() and rewrite fsync().\nBarrier-enabled IO stack removes the flush overhead as well as the transfer overhead in enforcing the storage order. While large body of the works have focused on eliminating the flush overhead, few works have addressed the overhead of DMA transfer to enforce the storage order. The benefits of the barrier-enabled IO stack include the followings;\n\u2022 The application can control the storage order virtually without any overheads, including the flush overhead, DMA transfer overhead, and context switch.\n\u2022 The latency of a journal commit decreases significantly. The journaling module can enforce the storage order between the journal logs and the journal commit block without interleaving them with flush or with DMA transfer.\n\u2022 Throughput of the filesystem journaling improves significantly. The dual-mode journaling commits multiple transactions concurrently and yet can guarantee the durability of the individual journal commit.\nBy eliminating the Transfer-and-Flush overhead, the barrier-enabled IO stack successfully exploits the concurrency and the parallelism in modern Flash storage."
        },
        {
            "heading": "2 Background",
            "text": ""
        },
        {
            "heading": "2.1 Orders in IO stack",
            "text": "A write request travels a complicated route until the data blocks reach the storage surface. The filesystem puts the request to the IO scheduler queue. The block device driver removes one or more requests from the queue and constructs a command. It probes the device and dispatches the command if the device is available.\n212 16th USENIX Conference on File and Storage Technologies USENIX Association\nThe device is available if the command queue is not full. The storage controller inserts the incoming command at the command queue. The storage controller removes the command from the command queue and services it ( i.e. transfers the associated data block between the host and the storage). When the transfer finishes, the device signals the host. The contents of the writeback cache are committed to the storage surface either periodically or by an explicit request from the host.\nWe define four types of orders in the IO stack; Issue Order, I , Dispatch Order, D , Transfer Order, X , and Persist Order, P . The issue order I = {i1, i2, . . . , in} is a set of write requests issued by the file system. The subscript denotes the order in which the requests enter the IO scheduler. The dispatch order D = {d1,d2, . . . ,dn} denotes a set of the write requests dispatched to the storage device. The subscript denotes the order in which the requests leave the IO scheduler. The transfer order, X = {x1,x2, . . . ,xn}, is the set of transfer completions. The persist order, P = {p1, p2, . . . , pn}, is a set of operations that make the data blocks in the writeback cache durable. We say that a partial order is preserved if the relative position of the requests against a designated request, barrier, are preserved between two different types of orders. We use the notation \u2018=\u2019 to denote that a partial order is preserved. The partial orders between the different types of orders may not coincide due to the following reasons.\n\u2022 I 6= D . The IO scheduler reorders and coalesces the IO requests subject to the scheduling principle, e.g. CFQ, DEADLINE, etc. When there is no scheduling mechanism, e.g. NO-OP scheduler [3] or NVMe [13] interface, the dispatch order may be equal to the issue order.\n\u2022 D 6= X . The storage controller can freely schedule the commands in its command queue. In addition, the commands can be serviced out-of-order due to the errors, the time-outs, and the retry.\n\u2022 X 6= P . The writeback cache of the storage is not FIFO. In Flash storage, persist order is governed not by the order in which the data blocks are made durable but by the order in which the associated mapping table entries are updated. The two may not coincide.\nDue to all these uncertainties, the modern IO stack is said to be orderless [8]."
        },
        {
            "heading": "2.2 Transfer-and-Flush",
            "text": "Enforcing a storage order corresponds to preserving a partial order between the order in which the filesystem issues the requests, I , and the order in which the associated data blocks are made durable, P . It is equivalent to collectively enforcing the partial orders between the pair of the orders in the adjacent layers in Fig. 2. It can be formally represented as in Eq. 1.\n(I = P)\u2261 (I = D)\u2227 (D = X )\u2227 (X = P) (1)\nThe modern IO stack has evolved under the assumption that the host cannot control the persist order, i.e. X 6= P . This is due to the physical characteristics of the rotating media. For rotating media such as HDDs, a persist order is governed by disk scheduling algorithm. The disk scheduling is entirely left to the storage controller due to its complicated sector geometry which is hidden from outside [21]. When the host blindly enforces a certain persist order, it may experience anomalous delay in IO service. Due to this constraint of X 6= P , Eq. 1 is unsatisfiable. The constraint that the host cannot control the persist order is a fundamental limitation in modern IO stack design.\nThe block device layer adopts the indirect and the expensive approach to control the storage order in spite of the constraint X 6= P . First, after dispatching the write command to the storage device, the caller postpones dispatching the following command until the preceding command is serviced, i.e. until the associated DMA transfer completes. We refer to this mechanism as Wait-on-Transfer. Wait-on-Transfer mechanism ensures that the commands are serviced in order and to satisfy D = X . Wait-on-Transfer is expensive; it blocks the caller and interleaves the requests with DMA transfer. Second, when the preceding command is serviced, the caller issues the flush command and waits for its completion. The caller issues the following command only after the flush command returns. This is to ensure that the associated data blocks are persisted in order and to satisfy X = P . We refer to this mechanism as Waiton-Flush. The modern block device layer uses Wait-onTransfer and Wait-on-Flush in pair when it needs to enforce the storage order between the write requests. We call this mechanism as Transfer-and-Flush.\nThe cost of Transfer-and-Flush is prohibitive. It neutralizes the internal parallelism of the Flash storage controller, stalls the command queue, and exposes the caller to DMA transfer and raw cell programming delays.\n2.3 Analysis: fsync() in EXT4 We examine how the EXT4 filesystem controls the storage order in an fsync(). In Ordered journaling mode (default), the data blocks are persisted before the journal\nUSENIX Association 16th USENIX Conference on File and Storage Technologies 213\ntransaction. Fig. 3 illustrates the behavior of an fsync(). The filesystem issues the write requests for a set of dirty pages, D. D may consist of the data blocks from different files. After issuing the write requests, the application thread blocks waiting for the completion of the DMA transfer. When the DMA transfer completes, the application thread resumes and triggers the JBD thread to commit the journal transaction. After triggering the JBD thread, the application thread sleeps again. When the JBD thread makes journal transaction durable, the fsync() returns. It should be emphasized that the application thread triggers the JBD thread only after D is transferred. Otherwise, the storage controller may service the write request for D and the write requests for journal commit in an out-of-order manner, and the storage controller may persist the journal transaction prematurely (before D is transferred).\nA journal transaction is usually committed using two write requests: one for writing the coalesced chunk of the journal descriptor block and the log blocks and the other for writing the commit block. In the rest of the paper, we will use JD and JC to denote the coalesced chunk of the journal descriptor and the log blocks, and the commit block, respectively. In committing a journal transaction, JBD needs to enforce the storage orders in two relations: within a transaction and between the transactions. Within a transaction, JBD needs to ensure that JD is made durable ahead of JC. Between the journal transactions, JBD has to ensure that journal transactions are made durable in order. When either of the two conditions are violated, the file system may recover incorrectly in case of unexpected failure [67, 9]. For the storage order within a transaction, JBD interleaves the write request for JD and the write request for JC with Transfer-andFlush. To control the storage order between the transactions, JBD thread waits for JC to become durable before it starts committing the following transaction. JBD uses Transfer-and-Flush mechanism in enforcing both intratransaction and inter-transaction storage order.\nIn earlier days of Linux, the block device layer explicitly issued a flush command in committing a jour-\nnal transaction [15]. In this approach, the flush command blocks not only the caller but also the other requests in the same dispatch queue. Since Linux 2.6.37, the filesystem (JBD) implicitly issues a flush command [16]. In writing JC, JBD tags the write request with REQ FLUSH and REQ FUA. Most storage controllers have evolved to support these two flags; with these two flags, the storage controller flushes the writeback cache before servicing the command and in servicing the command it directly persists JC to storage surface bypassing the writeback cache. In this approach, only the JBD thread blocks and the other threads that share the same dispatch queue can proceed. Our effort can be thought as a continuation to this evolution of the IO stack. We mitigate the Transferand-Flush overhead by making the storage device more capable: supporting a barrier command and by redesigning the host side IO stack accordingly."
        },
        {
            "heading": "3 Order-Preserving Block Device Layer",
            "text": ""
        },
        {
            "heading": "3.1 Design",
            "text": "The order-preserving block device layer consists of the newly defined barrier write command, order-preserving dispatch module, and Epoch-based IO scheduler. We overhaul the IO scheduler, the dispatch module, and the write command so that they can preserve the partial order between the different types of orders, I = D , D = X , and X = P , respectively. Order-preserving dispatch module eliminates the Wait-on-Transfer overhead and the newly defined barrier write command eliminates the wait-on-flush overhead. They collectively together preserve the partial order between the issue order I and the persist order P without Transfer-and-Flush.\nThe order-preserving block device layer categorizes the write requests into two categories, orderless write and order-preserving write. The order-preserving requests are the ones that are subject to the storage ordering constraint. Orderless request is the one which is irrelevant to the ordering dependency and which can be scheduled freely. We distinguish the two to avoid imposing unnecessary ordering constraint in scheduling the requests. The details are to come shortly. We refer to a set of the order-preserving requests that can\n214 16th USENIX Conference on File and Storage Technologies USENIX Association\nbe reordered with each other as an epoch [14]. We define a special type of order-preserving write as a barrier write. A barrier write is used to delimit an epoch. We introduce two new attributes REQ ORDERED and REQ BARRIER for the bio object and the request object to represent an order-preserving write and a barrier write. REQ ORDERED attribute is used to specify the order-preserving write. Barrier write request has both REQ ORDERED and REQ BARRIER attributes. The orderpreserving block device layer handles the request differently based upon its category. Fig. 4 illustrates the organization of Barrier-Enabled IO stack.\n3.2 Barrier Write, the Command The \u201ccache barrier,\u201d or \u201cbarrier\u201d for short, command is defined in the standard command set for mobile Flash storage [28]. With barrier command, the host can control the persist order without explicitly invoking the cache flush. When the storage controller receives the barrier command, the controller guarantees that the data blocks transferred before the barrier command becomes durable after the ones that follow the barrier command do. A few eMMC products in the market support cache barrier command [1, 2]. The barrier command can satisfy the condition X = P in Eq. 1 which has been unsatisfiable for several decades due to the mechanical characteristics of the rotating media. The naive way of using barrier is to replace the existing flush operation [66]. This simple replacement still leaves the caller under the Waiton-Transfer overhead to enforce the storage order.\nImplementing a barrier as a separate command occupies one entry in the command queue and costs the host the latency of dispatching a command. To avoid this overhead, we define a barrier as a command flag. We designate one unused bit in the SCSI command for a barrier flag. We set the barrier flag of the write command to make itself a barrier write. When the storage controller receives a barrier write command, it services the barrier write command as if the barrier command has arrived immediately following the write command.\nWith reasonable complexity, the Flash storage can be made to support a barrier write command [30, 57, 39]. When the Flash storage has Power Loss Protection (PLP) feature, e.g. a supercapacitor, the writeback cache contents are guaranteed to be durable. The storage controller can flush the writeback cache fully utilizing its parallelism and yet can guarantee the persist order. In Flash storage with PLP, we expect that the performance overhead of the barrier write is insignificant.\nFor the devices without PLP, the barrier write command can be supported in three ways; in-order writeback, transactional writeback, or in-order recovery. In in-order writeback, the storage controller flushes the data blocks in epoch granularity. The amount of data blocks in an\nepoch may not be large enough to fully utilize the parallelism of the Flash storage. The in-order writeback style of the barrier write implementation can bring the performance degradation in cache flush. In transactional writeback, the storage controller flushes the writeback cache contents as a single unit [57, 39]. Since all epochs in the writeback cache are flushed together, the persist order imposed by the barrier command is satisfied. The transactional writeback can be implemented without any performance overhead if the controller exploits the spare area of the Flash page to represent a set of pages in a transaction [57]. The in-order recovery method relies on a crash recovery routine to control the persist order. When multiple controller cores concurrently write the data blocks to multiple channels, one may have to use sophisticated crash recovery protocol such as ARIES [46] to recover the storage to consistent state. If the entire Flash storage is treated as a single log device, we can use simple crash recovery algorithm used in LFS [61]. Since the persist order is enforced by the crash recovery logic, the storage controller can flush the writeback cache at the full throttle as if there is no ordering dependency. The controller is saved from performance penalty at the cost of complexity in the recovery routine.\nIn this work, we modify the firmware of the UFS storage device to support the barrier write command. We use a simple LFS style in-order recovery scheme. The modified firmware is loaded at the commercial UFS product of the Galaxy S6 smartphone1. The modified firmware treats the entire storage device as a single log structured device. It maintains an active segment in memory. FTL appends incoming data blocks to the active segment in the order in which they are transferred. When an active segment becomes full, the controller stripes the active segment across the multiple Flash chips in log-structured manner. In crash recovery, the UFS controller locates the beginning of the most recently flushed segment. It scans the pages in the segment from the beginning till it encounters the page that has not been programmed successfully. The storage controller discards the rest of the pages including the incomplete one.\nDeveloping a barrier-enabled SSD controller is an engineering exercise. It is governed by a number of design choices and should be addressed in a separate context. In this work, we demonstrate that the performance benefit achieved by the barrier command well deserves its complexity if the host side IO stack can properly exploit it."
        },
        {
            "heading": "3.3 Order-Preserving Dispatch",
            "text": "Order-preserving dispatch is a fundamental innovation in this work. In order-preserving dispatch, the block device\n1Some of the authors are firmware engineers at Samsung Electronics and have an access to the FTL firmware of Flash storage products.\nUSENIX Association 16th USENIX Conference on File and Storage Technologies 215\nlayer dispatches the following command immediately after it dispatches the preceding one (Fig. 5) and yet the host can ensure that the two commands are serviced in order. We refer to this mechanism as Wait-on-Dispatch. The order-preserving dispatch is to satisfy the condition D = X in Eq. 1 without Wait-on-Transfer overhead.\nThe dispatch module constructs a command from the requests. The dispatch module constructs the barrier write command when it encounters the barrier write request, i.e. the write request with REQ ORDERED and REQ BARRIER flags. For the other requests, it constructs the commands as it used to do in the legacy block device.\nImplementing an order-preserving dispatch is rather simple; the block device driver sets the priority of a barrier write command as ordered. Then, the SCSI compliant storage device services the command satisfying the ordering constraint. The following is the reason. SCSI standard defines three command priority levels: head of the queue, ordered, and simple [59]. With each, the storage controller puts the incoming command at the head of the command queue, at the tail of the command queue or at an arbitrary position determined at its disposal, respectively. The default priority is simple. The command with simple priority cannot be inserted in front of the existing ordered or head of the queue command. Exploiting the command priority of existing SCSI interface, the order-preserving dispatch module ensures that the barrier write is serviced only after the existing requests in the command queue are serviced and before any of the commands that follow the barrier write are serviced.\nThe device can temporarily be unavailable or the caller can be switched out involuntarily after dispatching a write request. The order-preserving dispatch module uses the same error handling routine of the existing block device driver; the kernel daemon inherits the task and retries the failed request after a certain time interval, e.g. 3 msec for SCSI devices [59]. The ordered priority command has rarely been used in the existing block device implementations. This is because when the host cannot control the persist order, enforcing a transfer order with ordered priority command barely carries any mean-\ning from the perspective of ensuring the storage order. In the emergence of the barrier write, the ordered priority plays an essential role in making the entire IO stack an order-preserving one.\nThe importance of order-preserving dispatch cannot be emphasized further. With order-preserving dispatch, the host can control the transfer order without releasing the CPU and without stalling the command queue. IO latency can become more predictable since there exists less chance that the CPU scheduler interferes with the caller\u2019s execution. \u2206WoT and \u2206WoD in Fig. 5 illustrate the delays between the consecutive requests in Wait-onTransfer and Wait-on-Dispatch, respectively. In Wait-onDispatch, the host issues the next request Wi+1(WoD) immediately after it issues Wi. In Wait-on-Transfer, the host issues the next request Wi+1(WoT ) only after Wi is serviced. \u2206WoD is an order of magnitude smaller than \u2206WoT ."
        },
        {
            "heading": "3.4 Epoch-Based IO scheduling",
            "text": "Epoch-based IO scheduling is designed to preserve the partial order between the issue order and the dispatch order. It satisfies the condition I = D . It is designed with three principles; (i) it preserves the partial order between the epochs, (ii) the requests within an epoch can be freely scheduled with each other, and (iii) an orderless request can be scheduled across the epochs.\nWhen an IO request enters the scheduler queue, the IO scheduler determines if it is a barrier write. If the request is a barrier write, the IO scheduler removes the barrier flag from the request and inserts it into the queue. Otherwise, the scheduler inserts it to the queue as is. When the scheduler inserts a barrier write to the queue, it stops accepting more requests. Since the scheduler blocks the queue after it inserts the barrier write, all orderpreserving requests in the queue belong to the same epoch. The requests in the queue can be freely re-ordered and merged with each other. The IO scheduler uses the existing scheduling discipline, e.g. CFQ. The merged request will be order-preserving if one of the components is order-preserving request. The IO scheduler designates the last order-preserving request that leaves the queue as a new barrier write. This mechanism is called EpochBased Barrier Reassignment. When there are not any order-preserving requests in the queue, the IO scheduler starts accepting the IO requests again. When the IO scheduler unblocks the queue, there can be one or more orderless requests in the queue. These orderless requests are scheduled with the requests in the following epoch. Differentiating orderless requests from the orderpreserving ones, we avoid imposing unnecessary ordering constraint on the irrelevant requests.\nFig. 6 illustrates an example. The circle and the rectangle that enclose the write request denote the order-preserving flag and barrier flag, respectively. An\n216 16th USENIX Conference on File and Storage Technologies USENIX Association\nfdatasync() creates three write requests: w1,w2, and w4. The barrier-enabled filesystem, which will be detailed shortly, marks the write requests as ordering preserving ones. The last request, w4, is designated as a barrier write and an epoch, {w1,w2,w4}, is established. A pdflush creates three write requests w3,w5, and w6. They are all orderless writes. The requests from the two threads are fed to the IO scheduler as w1,w2,w3,w5,wbarrier4 ,w6. When the barrier write, w4, enters the queue, the scheduler stops accepting the new request. Thus, w6 cannot enter the queue. The IO scheduler reorders the requests in the queue and dispatches them as w2,w3,w4,w5,wbarrier1 order. The IO scheduler relocates the barrier flag from w4 to w1. The epoch is preserved after IO scheduling.\nThe order-preserving block device layer now satisfies all three conditions, I = D ,D = X and X = P in Eq. 1 with an Epoch-based IO scheduling, an orderpreserving dispatch and a barrier write, respectively. The order-preserving block device layer successfully eliminates the Transfer-and-Flush overhead in controlling the storage order and can control the storage order with only Wait-on-Dispatch overhead."
        },
        {
            "heading": "4 Barrier-Enabled Filesystem",
            "text": ""
        },
        {
            "heading": "4.1 Programming Model",
            "text": "The barrier-enabled IO stack offers four synchronization primitives: fsync(), fdatasync(), fbarrier(), and fdatabarrier(). We propose two new filesystem interfaces, fbarrier() and fdatabarrier(), to separately support ordering guarantee. fbarrier() and fdatabarrier() synchronize the same set of blocks with fsync() and fdatasync(), respectively, but they return without ensuring that the associated blocks become durable. fbarrier() bears the same semantics as osync() in OptFS [9] in that it writes the data blocks and the journal transactions in order but returns without ensuring that they become durable. fdatabarrier() synchronizes the modified blocks, but not the journal transaction. Unlike fdatasync(), fdatabarrier() returns without persisting the associated blocks. fdatabarrier() is a generic storage barrier. By interleaving the write() calls with fdatabarrier(), the application ensures that the data\nblocks associated with the write requests that precede fdatabarrier() are made durable ahead of the data blocks associated with the write requests that follow fdatabarrier(). It plays the same role as mfence for memory barrier [53]. Refer to the following codelet. Using fdatabarrier(), the application ensures that the \u201dworld\u201d is made durable only after \u201dHello\u201d does.\nwrite(fileA, \"Hello\") ;\nfdatabarrier(fileA) ;\nwrite(fileA, \"World\") ;\nThe order-preserving block device layer is filesystem agnostic. In our work, we modify EXT4 for barrier enabled IO stack."
        },
        {
            "heading": "4.2 Dual Mode Journaling",
            "text": "Committing a journal transaction essentially consists of two saparate tasks: (i) dispatching the write commands for JD and JC and (ii) making JD and JC durable. Exploiting the order-preserving nature of the underlying block device, we physically separate the control plane activity (dispatching the write requests) and the data plane activity (persisting the associated data blocks and journal transaction) of a journal commit operation. Further, we allocate the separate threads to each task so that the two activities can proceed in parallel with minimum dependency. The two threads are called as commit thread and flush thread, respectively. We refer to this mechanism as Dual Mode Journaling. Dual Mode Journaling mechanism can support two journaling modes, durability guarantee mode and ordering guarantee mode, in versatile manner.\nThe commit thread is responsible for dispatching the write requests for JD and JC. The commit thread writes\nUSENIX Association 16th USENIX Conference on File and Storage Technologies 217\neach of the two with a barrier write so that JD and JC are persisted in order. The commit thread dispatches the write requests without any delay in between (Fig. 7(b)). In EXT4, JBD thread interleaves the write request for JC and JD with Transfer-and-Flush (Fig. 7(a)). After dispatching the write request for JC, the commit thread inserts the journal transaction to the committing transaction list and hands over the control to the flush thread.\nThe flush thread is responsible for (i) issuing the flush command, (ii) handling error and retry and (iii) removing the transaction from the committing transaction list. The behavior of the flush thread varies subject to the durability requirement of the journal commit. If the journal commit is triggered by fbarrier(), the flush thread returns after removing the transaction from the committing transaction list. It returns without issuing the flush command. If the journal commit is triggered by fsync(), the flush thread involves more steps. It issues a flush command and waiting for the completion. When the flush completes, it removes the the associated transaction from the committing transaction list and returns. BarrierFS supports all journal modes in EXT4; WRITEBACK, ORDERED and DATA.\nThe dual thread organization of BarrierFS journaling bears profound implications in filesystem design. First, the separate support for the ordering guarantee and the durability guarantee naturally becomes an integral part of the filesystem. Ordering guarantee involves only the control plane activity. Durability guarantee requires the control plane activity as well as data plane activity. BarrierFS partitions the journal commit activity into two independent components, control plane and data plane and dedicates separate threads to each. This modular design enables the filesystem primitives to adaptively adjust the activity of the data plane thread with respect to the durability requirement of the journal commit operation; fsync() vs. fbarrier(). Second, the filesystem journaling becomes concurrent activity. Thanks to the dual thread design, there can be multiple committing transactions in flight. In most journaling filesystems that we are aware of, the filesystem journaling is a serial activity; the journaling thread commits the following transaction only after the preceding transaction becomes durable. In dual thread design, the commit thread can commit a new journal transaction without waiting for the preceding committing transaction to become durable. The flush thread asynchronously notifies the application thread about the completion of the journal commit."
        },
        {
            "heading": "4.3 Synchronization Primitives",
            "text": "In fbarrier() and fsync(), BarrierFS writes D, JD, and JC in a piplelined manner without any delays in between (Fig. 7(b)). BarrierFS writes D with one or more order-preserving writes whereas it writes JD and\nJC with the barrier writes. In this manner, BarrierFS forms two epochs {D,JD} and {JC} in an fsync() or in an fbarrier() and ensures the storage order between these two epochs. fbarrier() returns when the filesystem dispatches the write request for JC. fsync() returns after it ensures that JC is made durable. Order-preserving block device satisfies prefix constraint [69]. When JC becomes durable, the order-preserving block device guarantees that all blocks associated with preceding epochs have been made durable. An application may repeatedly call fbarrier() committing multiple transactions simultaneously. By writing JC with a barrier write, BarrierFS ensures that these committing transactions become durable in order. The latency of an fsync() reduces significantly in BarrierFS. It reduces the number of flush operations from two in EXT4 to one and eliminates the Wait-on-Transfer overheads (Fig. 7).\nIn fdatabarrier() and fdatasync(), BarrierFS writes D with a barrier write. If there are more than one write requests in writing D, only the last one is set as a barrier write and the others are set as the orderpreserving writes. An fdatasync() returns after the data blocks, D, become durable. An fdatabarrier() returns immediately after dispatching the write requests for D. fdatabarrier() is the crux of the barrierenabled IO stack. With fdatabarrier(), the application can control the storage order virtually without any overheads: without waiting for the flush, without waiting for DMA completion, and even without the context switch. fdatabarrier() is a very light-weight storage barrier.\nAn fdatabarrier() (or fdatasync()) may not find any dirty pages to synchronize upon its execution. In this case, BarrierFS explicitly triggers the journal commit. It forces BarrierFS to issue the barrier writes for JD and JC. Through this mechanism, fdatabarrier() or fdatasync() can delimit an epoch as desired by the application even in the absence of any dirty pages."
        },
        {
            "heading": "4.4 Handling Page Conflicts",
            "text": "A buffer page may have been held by the committing transaction when an application tries to insert it to the running transaction. We refer to this situation as page conflict. Blindly inserting a conflict page into the running transaction yields its removal from the committing transaction before it becomes durable. The EXT4 filesystem checks for the page conflict when it inserts a buffer page to the running transaction [67]. If the filesystem finds a conflict, the thread delegates the insertion to the JBD thread and blocks. When the committing transaction becomes durable, the JBD thread identifies the conflict pages in the committed transaction and inserts them to the running transaction. In EXT4, there can be at most one committing transaction. The running transaction is\n218 16th USENIX Conference on File and Storage Technologies USENIX Association\nguaranteed to be free from page conflict when the JBD thread has made it durable and finishes inserting the conflict pages to the running transaction.\nIn BarrierFS, there can be more than one committing transactions. The conflict pages may be associated with different committing transactions. We refer to this situation as multi-transaction page conflict. As in EXT4, BarrierFS inserts the conflict pages to the running transaction when it makes a committing transaction durable. However, to commit a running transaction, BarrierFS has to scan all buffer pages in the committing transactions for page conflicts and ensure that it is free from any page conflicts. When there exists large number of committing transactions, the scanning overhead to check for the page conflict can be prohibitive in BarrierFS.\nTo reduce this overhead, we propose the conflict-page list for a running transaction. The conflict-page list represents the set of conflict pages associated with a running transaction. The filesystem inserts the buffer page to the conflict-page list when it finds that the buffer page that it needs to insert to the running transaction is subject to the page conflict. When the filesystem has made a committing transaction durable, it removes the conflict pages from the conflict-page list in addition to inserting them to the running transaction. A running transaction can only be committed when the conflict-page list is empty.\n4.5 Concurrency in Journaling\nWe examine the degree of concurrency in journal commit operation under different storage order guarantee mechanisms: BarrierFS, EXT4 with no-barrier option (EXT4 (no flush)), EXT4 with supercap SSD (EXT4 (quick flush)), and plain EXT4 (EXT4 (full flush)). With no-barrier mount option, the JBD thread omits the flush command in committing a journal transaction. With this option, the EXT4 guarantees neither durability nor ordering in journal commit operation since the storage controller may make the data blocks durable out-of-\norder. We examine this configuration to illustrate the filesystem journaling behavior when the flush command is removed in the journal commit operation.\nIn Fig. 8, each horizontal line segment represents a journal commit activity. It consists of the solid line segment and the dashed line segment. The end of the horizontal line segment denotes the time when the transaction reaches the disk surface. The end of the solid line segment represents the time when the journal commit returns. If they do not coincide, it means that the journal commit finishes before the transaction reaches the disk surface. In EXT4 (full flush), EXT4 (quick flush), and EXT4 (no flush), the filesystem commits a new transaction only after preceding journal commit finishes. The journal commit is a serial activity. In EXT4 (full flush), the journal commit finishes only after all associated blocks are made durable. In EXT4 (quick flush), the journal commit finishes more quickly than in EXT4 (full flush) since the SSD returns the flush command without persisting the data blocks. In EXT4 (no flush), the journal commit finishes more quickly than EXT (quick flush) since it does not issue the flush command. In journaling throughput, BarrierFS prevails the remainders by far since the interval between the consecutive journal commits is as small as the dispatch latency, tD.\nThe concurrencies in journaling in EXT4 (no flush) and in EXT4 (quick flush) have their price. EXT4 (quick flush) requires the additional hardware component, supercap, in the SSD. EXT4 (quick flush) guarantees neither durability or ordering in the journal commit. BarrierFS commits multiple transactions concurrently and yet can guarantee the durability of the individual journal commit without the assistance of additional hardware.\nThe barrier enabled IO stack does not require any major changes in the existing in-memory or on-disk structure of the IO stack. The only new data structure we introduce is the \u201cconflict-page-list\u201d for a running transaction. Barrier enabled IO stack consists of approximately 3K LOC changes in the IO stack of the Linux kernel ."
        },
        {
            "heading": "4.6 Comparison with OptFS",
            "text": "As the closest approach of our sort, OptFS deserves an elaboration. OptFS and barrier-enabled IO stack differ mainly in three aspects; the target storage media, the technology domain, and the programming model. First, OptFS is not designed for the Flash storage but the barrier-enabled IO stack is. OptFS is designed to reduce the disk seek overhead in a filesystem journaling; via committing multiple transactions together (delayed commit) and via making the disk access sequential (selective data mode journaling). Second, OptFS is the filesystem technique while the barrier enabled IO stack deals with the entire IO stack; the storage device, the block device layer and the filesystem. OptFS is built upon the\nUSENIX Association 16th USENIX Conference on File and Storage Technologies 219\nlegacy block device layer. It suffers from the same overhead as the existing filesystems do. OptFS uses Wait-onTransfer to control the transfer order between D and JD. OptFS relies on Transfer-and-Flush to control the storage order between the journal commit and the associated checkpoint in osync(). Barrier-enabled IO stack eliminates the overhead of Wait-on-Transfer and Transferand-Flush in controlling the storage order. Third, OptFS focuses on revising the filesystem journaling model. BarrierFS is not limited to revising the filesystem journaling model but also exports generic storage barrier with which the application can group a set of writes into an epoch."
        },
        {
            "heading": "5 Applications",
            "text": "To date, fdatasync() has been the sole resort to enforce the storage order between the write requests. The virtual disk managers for VM disk image, e.g., qcow2, use fdatasync() to enforce the storage order among the writes to the VM disk image [7]. SQLite uses fdatasync() to control the storage order between the undo-log and the journal header and between the updated database node and the commit block [37]. In a single insert transaction, SQLite calls fdatasync() four times, three of which are to control the storage order. In these cases, fdatabarrier() can be used in place of fdatasync(). In some modern applications, e.g. mail server [62] or OLTP, fsync() accounts for the dominant fraction of IO. In TPC-C workload, 90% of IOs are created by fsync() [51]. With improved fsync() of BarrierFS, the performance of the application can increase significantly. Some applications prefer to trade the durability and the freshness of the result for the performance and scalability of the operation [12, 17]. One can replace all fsync() and fdatasync() with ordering guarantee counterparts, fbarrier() and fdatabarrier(), respectively, in these applications."
        },
        {
            "heading": "6 Experiment",
            "text": "We implement a barrier-enabled IO stack on three different platforms, enterprise server (12 cores, Linux 3.10.61), PC server (4 cores, Linux 3.10.61) and smartphone (Galaxy S6, Android 5.0.2, Linux 3.10). We test three storage devices: 843TN (SATA 3.0, QD2=32, 8 channels, supercap), 850PRO (SATA 3.0, QD=32, 8 channels), and mobile storage (UFS 2.0, QD=16, single channel). We compare the BarrierFS against EXT4 and OptFS [9]. We refer to each of these as supercap-SSD, plain-SSD, and UFS, respectively. We implement barrier write command in UFS device. In plain-SSD and supercap SSD, we assume that the performance overhead of barrier write is 5% and none, repsectively.\n2QD: queue depth"
        },
        {
            "heading": "6.1 Order-Preserving Block Layer",
            "text": "We examine the performance of 4 KByte random write with different ways of enforcing the storage order: P (orderless write [i.e. plain buffered write]), B (barrier write), X (Wait-on-Transfer) and XnF (Transfer-andFlush). Fig. 9 illustrates the result.\nThe overhead of Transfer-and-Flush is severe. With Transfer-and-Flush, the IO performances of the ordered write are 0.5% and 10% of orderless write in plain-SSD and UFS, respectively. In supercap SSD, the performance overhead is less significant, but is still considerable; the performance of the ordered write is 35% of the orderless write in UFS. The overhead of DMA transfer is significant. When we interleave the write requests with DMA transfer, the IO performance is less than 40% of the orderless write in each of the three storage devices.\nThe overhead of barrier write is negligible. When using a barrier write, the ordered write exhibits 90% performance of the orderless write in plain-SSD and super-cap SSD. For UFS, it exhibits 80% performance of the orderless write. The barrier write drives the queue to its maximum in all three Flash storages. The storage performance is closely related to the command queue utilization [33]. In Wait-on-Transfer, the queue depth never goes beyond one (Fig. 10(a) and Fig. 10(c)). In barrier write, the queue depth grows near to its maximum in all storage devices (Fig. 10(b) and Fig. 10(d)).\n220 16th USENIX Conference on File and Storage Technologies USENIX Association"
        },
        {
            "heading": "6.2 Filesystem Journaling",
            "text": "We examine the latency, the number of context switches and the queue depth in filesystem journaling in EXT4 and BarrierFS. We use Mobibench [26]. For latency, we perform 4 KByte allocating write() followed by fsync(). With this, an fsync() always finds the updated metadata to journal and the fsync() latency properly represents the time to commit a journal transaction. For context switch and queue depth, we use 4 KByte non-allocating random write followed by different synchronization primitives.\nLatency: In plain-SSD and supercap-SSD, the average fsync() latency decreases by 40% when we use BarrierFS against when we use EXT4 (Table 2). In UFS, the fsync() latency decreases by 60% in BarrierFS compared against EXT4. UFS experiences more significant reduction in fsync() latency than the other SSDs do.\nBarrierFS makes the fsync() latency less variable. In supercap-SSD and UFS, the fsync() latencies at the 99.99th percentile are 30\u00d7 of the average fsync() latency (Table 2). In BarrierFS, the tail latencies at 99.99th percentile decrease by 50%, 20%, and 70% in UFS, plain-SSD, and supercap-SSD, respectively, against EXT4.\nContext Switches: We examine the number of application level context switches in different journaling modes (Fig. 11). In EXT4, fsync() wakes up the caller twice: after D is transferred and after the journal transaction is made durable(EXT4-DR). This applies to all three storages. In BarrierFS, the number of context switches in an fsync() varies subject to the storage device. In UFS and supercap SSD, fsync() of BarrierFS wakes\nup the caller twice, as in the case of fsync() of EXT4. However, the reasons are entirely different. In UFS and supercap-SSD, the intervals between the successive write requests are much smaller than the timer interrupt interval due to small flush latency. A write() request rarely finds the updated metadata and an fsync() often resorts to an fdatasync(). fdatasync() wakes up the caller (the application thread) twice in BarrierFS: after transferring D and after flush completes. In plain SSD, fsync() of BarrierFS wakes up the caller once: after the transaction is made durable. The plain-SSD uses TLC Flash. The interval between the successive write()s is longer than the timer interrupt interval. The application thread blocks after triggering the journal commit and and wakes up after the journal commit operation completes.\nBFS-OD manifests the benefits of BarrierFS. The fbarrier() rarely finds updated metadata since it returns quickly and as a result, most fbarrier() calls are serviced as fdatabarrier(). fdatabarrier() does not block the caller and therefore does not accompany any involuntary context switch.\nCommand Queue Depth: In BarrierFS, the host dispatches the write requests for D, JD, and JC in tandem. Ideally, there can be as many as three commands in the queue. We observe only up to two commands in the queue in servicing an fsync() (Fig. 12(a)). This is due to the context switch between the application thread and the commit thread. Writing D and writing JD are 160 \u00b5sec apart, but it takes 70\u00b5sec to service the write request for D. In fbarrier(), BarrierFS successfully drives the command queue to its full capacity (Fig. 12(b)).\nThroughput and Scalability: The filesystem journaling is a main obstacle against building an manycore scalable system [44]. We examine the throughput of filesystem journaling in EXT4 and BarrierFS with a varying number of CPU cores in a 12 core machine. We use modified DWSL workload in fxmark [45]; each thread performs a 4-Kbyte allocating write followed by fsync(). Each thread operates on its own file. BarrierFS exhibits much more scalable behavior than EXT4 (Fig. 13). In plain-SSD, BarrierFS exhibits 2\u00d7 performance against EXT4 in all numbers of cores (Fig. 13(a)). In supercap-\nUSENIX Association 16th USENIX Conference on File and Storage Technologies 221\nSSD, the performance saturates with six cores in both EXT4 and BarrierFS. BarrierFS exhibits 1.3\u00d7 journaling throughput against EXT4 (Fig. 13(b))."
        },
        {
            "heading": "6.3 Server Workload",
            "text": "We run two workloads: varmail [71] and OLTPinsert [34]. OLTP-insert workload uses MySQL DBMS [47]. varmail is a metadata-intensive workload. It is known for the heavy fsync() traffic. There are total four combinations of the workload and the SSD (plainSSD and supercap-SSD) pair. For each combination, we examine the benchmark performances for durability guarantee and ordering guarantee, respectively. For durability guarantee, we leave the application intact and use two filesystems, the EXT4 and the BarrierFS (EXT4-DR and BFS-DR). The objective of this experiment is to examine the efficiency of fsync() implementations in EXT4 and BarrierFS, respectively. For ordering guarantee, we test three filesystems, OptFS, EXT4 and BarrierFS. In OptFS and BarrierFS, we use osync() and fdatabarrier() in place of fsync(), respectively. In EXT4, we use nobarrier mount option. This experiment examines the benefit of Wait-on-Dispatch. Fig. 14 illustrates the result.\nLet us examine the performances of varmail workload. In plain-SSD, BFS-DR brings 60% performance gain against EXT4-DR in varmail workload. In supercap-SSD, BFS-DR brings 10% performance gain against EXT4-DR. The experimental result of supercapSSD case clearly shows the importance of eliminating the Wait-on-Transfer overhead in controlling the storage order. The benefit of BarrierFS manifests itself when we relax the durability guarantee. In ordering guarantee, BarrierFS achieves 80% performance gain against EXT4-OD. Compared to the baseline, EXT4-DR, BarrierFS achieves 36\u00d7 performance (1.0 vs. 35.6 IOPS) when we enforce only ordering guarantee with BarrierFS (BFS-OD) in plain SSD .\nIn MySQL, BFS-OD prevails EXT4-OD by 12%. Compared to the baseline, EXT4-DR, BarrierFS achieves 43\u00d7 performance (1.3 vs. 56.0 IOPS) when we enforce only ordering guarantee with BarrierFS (BFS-OD) in plain SSD."
        },
        {
            "heading": "6.4 Mobile Workload: SQLite",
            "text": "We examine the performances of the libarary based embedded DBMS, SQLite, under the durability guarantee and the ordering guarantee, respectively. We examine two journal modes, PERSIST and WAL. We use \u2019Full Sync\u2019 and the WAL file size is set to 1,000 pages, both of which are default settings [58]. In a single insert transaction, SQLite calls fdatasync() four times. Three of them are to control the storage order and the last one is for making the result of a transaction durable.\nFor durability guarantee mode, We replace the first three fdatasync()\u2019s with fdatabarrier()\u2019s and leave the last one. In mobile storage, BarrierFS achieves 75% performance improvement against EXT4 in default PERSIST journal mode under durability guarantee (Fig. 15). In ordering guarantee, we replace all four fdatasync()\u2019s with fdatabarrier()\u2019s. In UFS, SQLite exhibits 2.8\u00d7 performance gain in BFSOD against EXT4-DR. The benefit of eliminating the Transfer-and-Flush becomes more dramatic as the storage controller employs higher degree of parallelism. In plain-SSD, SQLite exhibits 73\u00d7 performance gain in BFS-OD against EXT4-DR (73 vs. 5300 ins/sec).\nNotes on OptFS: OptFS does not perform well in our experiment (Fig. 14 and Fig. 15), unlike that in [9]. We find two reasons. First, the benefit of delayed checkpoint and selective data mode journaling becomes marginal in Flash storage. Second, in Flash storage (i.e. the storage with short IO latency) the delayed checkpoint and the selective data mode journaling negatively interact with each other and bring substantial increase in the memory pressure. The increased memory pressure severely impacts the performance of osync(). The osync() scans all dirty pages for the checkpoint at its beginning. Selective data mode journaling inserts the updated data blocks to the journal transaction. Delayed checkpoint prohibits the data blocks in the journal transaction from being checkpointed until the associated ADN arrives. As a result, osync() checkpoints only a small fraction of dirty pages each time it is called. The dirty pages in the journal transactions are scanned multiple times before they are checkpointed. The osync() shows particularly poor performance in OLTP workload (Fig. 14), where most\n222 16th USENIX Conference on File and Storage Technologies USENIX Association\nupdates are subject to data mode journaling."
        },
        {
            "heading": "6.5 Crash Consistency",
            "text": "We test if the BarrierFS recovers correctly against the unexpected system failure. We use CrashMonkey for the test [40]. We modify CrashMonkey to understand the barrier write so that the CrashMonkey can properly delimit an epoch when it encounters a barrier write. We run two workloads; rename root to sub and create delete. For durability guarantee (fsync()), BarrierFS passes all 1,000 test cases as EXT4 does in both workloads. For ordering guarantee (fsync() in EXT4-OD and fbarrier() in BarrierFS), BarrierFS passes all 1,000 test cases whereas EXT4-OD fails in some cases. This is not surprising since EXT4 with nobarrier option guarantees neither the transfer orders nor the persist orders in committing the filesystem journal transaction."
        },
        {
            "heading": "7 Related Work",
            "text": "Featherstitch [20] proposes a programming model to specify the set of requests that can be scheduled together, patchgroup, and the ordering dependency between them, pg depend(). While xsyncfs [49] mitigates the overhead of fsync(), it needs to maintain complex causal dependencies among buffered updates. NoFS (no order file system) [10] introduces \u201cbackpointer\u201d to eliminate the Transfer-and-Flush based ordering in the file system. It does not support transaction.\nA few works proposed to use multiple running transactions or multiple committing transactions to circumvent the Transfer-and-Flush overhead in filesystem journaling [38, 29, 55]. IceFS [38] allocates separate running transaction for each container. SpanFS [29] splits a jour-\nnal region into multiple partitions and allocates committing transactions for each partition. CCFS [55] allocates separate running transactions for individual threads. In these systems, each journaling session still relies on the Transfer-and-Flush mechanism.\nA number of file systems provide a multi-block atomic write feature [18, 35, 54, 68] to relieve applications from the overhead of logging and journaling. These file systems internally use the Transfer-and-Flush mechanism to enforce the storage order in writing the data blocks and the associated metadata blocks. Exploiting the orderpreserving block device layer, these filesystems can use Wait-on-Dispatch mechanism to enforce the storage order between the data blocks and the metadata blocks and can be saved from the Transfer-and-Flush overhead."
        },
        {
            "heading": "8 Conclusion",
            "text": "The Flash storage provides the cache barrier command to allow the host to control the persist order. HDD cannot provide this feature. It is time for designing the new IO stack for the Flash storage that is free from the unnecessary constraint inherited from the old legacy that the host cannot control the persist order. We built a barrierenabled IO stack based upon the foundation that the host can control the persist order. In the barrier-enabled IO stack, the host can dispense with Transfer-and-Flush overhead in controlling the storage order and can successfully saturate the underlying Flash storage. We like to conclude this work with two key observations. First, the \u201ccache barrier\u201d command is a necessity rather than a luxury. It should be supported in all Flash storage products ranging from the mobile storage to the highperformance Flash storage with supercap. Second, the block device layer should be designed to eliminate the DMA transfer overhead in controlling the storage order. As the Flash storage becomes quicker, the relative cost of tardy \u201cWait-on-Transfer\u201d will become more substantial. To saturate the Flash storage, the host should be able to control the transfer order without interleaving the requests with DMA transfer.\nWe hope that this work provides a useful foundation in designing a new IO stack for the Flash storage3."
        },
        {
            "heading": "9 Acknowledgement",
            "text": "We would like to thank our shepherd Vijay Chidambaram and the anonymous reviewers for their valuable feedback. We also would like to thank Jayashree Mohan for her help in CrashMonkey. This work is funded by Basic Research Lab Program (NRF, No. 2017R1A4A1015498), the BK21 plus (NRF), ICT R&D program (IITP, R7117-16-0232) and Samsung Elec.\n3The source code for barrier enabled IO stack is available at https: //github.com/ESOS-Lab/barrieriostack.\nUSENIX Association 16th USENIX Conference on File and Storage Technologies 223"
        }
    ],
    "title": "Barrier-Enabled IO Stack for Flash Storage",
    "year": 2018
}