{
    "abstractText": "This paper establishes the existence of observable footprints that reveal the \u201ccausal dispositions\u201d of the object categories appearing in collections of images. We achieve this goal in two steps. First, we take a learning approach to observational causal discovery, and build a classifier that achieves state-of-the-art performance on finding the causal direction between pairs of random variables, given samples from their joint distribution. Second, we use our causal direction classifier to effectively distinguish between features of objects and features of their contexts in collections of static images. Our experiments demonstrate the existence of a relation between the direction of causality and the difference between objects and their contexts, and by the same token, the existence of observable signals that reveal the causal dispositions of objects.",
    "authors": [
        {
            "affiliations": [],
            "name": "David Lopez-Paz"
        },
        {
            "affiliations": [],
            "name": "Robert Nishihara"
        },
        {
            "affiliations": [],
            "name": "Soumith Chintala"
        },
        {
            "affiliations": [],
            "name": "Bernhard Sch\u00f6lkopf"
        }
    ],
    "id": "SP:3726931ad0b0f93067e71b0238e4bbe05351092d",
    "references": [
        {
            "authors": [
                "K. Chalupka",
                "F. Eberhardt",
                "P. Perona"
            ],
            "title": "Estimating causal direction and confounding of two discrete variables",
            "venue": "arXiv",
            "year": 2016
        },
        {
            "authors": [
                "K. Chalupka",
                "P. Perona",
                "F. Eberhardt"
            ],
            "title": "Visual causal feature learning",
            "venue": "UAI",
            "year": 2015
        },
        {
            "authors": [
                "P. Daniu\u0161is",
                "D. Janzing",
                "J. Mooij",
                "J. Zscheischler",
                "B. Steudel",
                "K. Zhang",
                "B. Sch\u00f6lkopf"
            ],
            "title": "Inferring deterministic causal relations",
            "venue": "UAI",
            "year": 2010
        },
        {
            "authors": [
                "M. Everingham",
                "L. Van Gool",
                "C.K.I. Williams",
                "J. Winn"
            ],
            "title": "and A",
            "venue": "Zisserman. The PASCAL Visual Object Classes Challenge 2012 ",
            "year": 2012
        },
        {
            "authors": [
                "A. Fire",
                "S. Zhu"
            ],
            "title": "Using causal induction in humans to learn and infer causality from video",
            "venue": "Annual Meeting of the Cognitive Science Society",
            "year": 2013
        },
        {
            "authors": [
                "A. Fire",
                "S. Zhu"
            ],
            "title": "Learning perceptual causality from video",
            "venue": "TIST",
            "year": 2016
        },
        {
            "authors": [
                "S. Gross"
            ],
            "title": "ResNet training in Torch, 2016",
            "year": 2016
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "CVPR",
            "year": 2016
        },
        {
            "authors": [
                "G. Hinton",
                "N. Srivastava"
            ],
            "title": "and K",
            "venue": "Swersky. Lecture 6a: Overview of mini-batch gradient descent",
            "year": 2014
        },
        {
            "authors": [
                "P.O. Hoyer",
                "D. Janzing",
                "J.M. Mooij",
                "J. Peters",
                "B. Sch\u00f6lkopf"
            ],
            "title": "Nonlinear causal discovery with additive noise models",
            "venue": "NIPS",
            "year": 2009
        },
        {
            "authors": [
                "S. Ioffe",
                "C. Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "ICML",
            "year": 2015
        },
        {
            "authors": [
                "D. Janzing",
                "B. Sch\u00f6lkopf"
            ],
            "title": "Causal inference using the algorithmic Markov condition",
            "venue": "IEEE Transactions on Information Theory",
            "year": 2010
        },
        {
            "authors": [
                "K. Lebeda",
                "S. Hadfield",
                "R. Bowden"
            ],
            "title": "Exploring causal relationships in visual object tracking",
            "venue": "ICCV",
            "year": 2015
        },
        {
            "authors": [
                "Y. LeCun",
                "B. Boser",
                "J.S. Denker",
                "D. Henderson",
                "R. Howard",
                "W. Hubbard",
                "L. Jackel"
            ],
            "title": "Backpropagation applied to handwritten zip code recognition",
            "venue": "Neural Computation",
            "year": 1989
        },
        {
            "authors": [
                "J. Lemeire",
                "E. Dirkx"
            ],
            "title": "Causal models as minimal descriptions of multivariate systems",
            "year": 2006
        },
        {
            "authors": [
                "T. Lin",
                "M. Maire",
                "S.J. Belongie",
                "J. Hays",
                "P. Perona",
                "D. Ramanan",
                "P. Doll\u00e1r",
                "C.L. Zitnick"
            ],
            "title": "Microsoft COCO: common objects in context",
            "venue": "Proceedings of 13th European Conference on Computer Vision ",
            "year": 2014
        },
        {
            "authors": [
                "D. Lopez-Paz",
                "K. Muandet",
                "B. Recht"
            ],
            "title": "The randomized causation coefficient",
            "venue": "JMLR",
            "year": 2015
        },
        {
            "authors": [
                "D. Lopez-Paz",
                "K. Muandet",
                "B. Sch\u00f6lkopf",
                "I.O. Tolstikhin"
            ],
            "title": "Towards a learning theory of cause-effect inference",
            "venue": "Proceedings of the 32nd International Conference on Machine Learning ",
            "year": 2015
        },
        {
            "authors": [
                "J. Mooij",
                "J. Peters",
                "D. Janzing",
                "J. Zscheischler",
                "B. Sch\u00f6lkopf"
            ],
            "title": "Distinguishing cause from effect using observational data: methods and benchmarks",
            "venue": "JMLR",
            "year": 2016
        },
        {
            "authors": [
                "S. Mumford",
                "R.L. Anjum"
            ],
            "title": "Getting Causes from Powers",
            "venue": "Oxford University Press",
            "year": 2011
        },
        {
            "authors": [
                "M. Oquab",
                "L. Bottou",
                "I. Laptev",
                "J. Sivic"
            ],
            "title": "Is object localization for free? - weakly-supervised learning with convolutional neural networks",
            "venue": "CVPR",
            "year": 2015
        },
        {
            "authors": [
                "J. Pearl"
            ],
            "title": "Causality: Models",
            "venue": "Reasoning, and Inference. Cambridge University Press",
            "year": 2000
        },
        {
            "authors": [
                "J. Peters",
                "J. Mooij",
                "D. Janzing",
                "B. Sch\u00f6lkopf"
            ],
            "title": "Causal discovery with continuous additive noise models",
            "venue": "JMLR",
            "year": 2014
        },
        {
            "authors": [
                "L.C. Pickup",
                "Z. Pan",
                "D. Wei",
                "Y. Shih",
                "C. Zhang",
                "A. Zisserman",
                "B. Sch\u00f6lkopf",
                "W.T. Freeman"
            ],
            "title": "Seeing the arrow of time",
            "venue": "CVPR",
            "year": 2014
        },
        {
            "authors": [
                "D.B. Rubin"
            ],
            "title": "Which ifs have causal answers",
            "venue": "Discussion of Holland\u2019s \u201cStatistics and Causal Inference\u201d. Journal of the Americal Statistical Association",
            "year": 1986
        },
        {
            "authors": [
                "B. Sch\u00f6lkopf",
                "D. Janzing",
                "J. Peters",
                "E. Sgouritsa",
                "K. Zhang",
                "J.M. Mooij"
            ],
            "title": "On causal and anticausal learning",
            "venue": "Proceedings of the 29th International Conference on Machine Learning ",
            "year": 2012
        },
        {
            "authors": [
                "A. Smola",
                "A. Gretton",
                "L. Song",
                "B. Sch\u00f6lkopf"
            ],
            "title": "A Hilbert space embedding for distributions",
            "venue": "Proceedings ALT",
            "year": 2007
        },
        {
            "authors": [
                "N. Srivastava",
                "G. Hinton",
                "A. Krizhevsky",
                "I. Sutskever",
                "R. Salakhutdinov"
            ],
            "title": "Dropout: A simple way to prevent neural networks from overfitting",
            "venue": "JMLR",
            "year": 2014
        },
        {
            "authors": [
                "M. Steyvers",
                "J.B. Tenenbaum",
                "E.J. Wagenmakers",
                "B. Blum"
            ],
            "title": "Inferring causal networks from observations and interventions",
            "venue": "Cognitive science",
            "year": 2003
        },
        {
            "authors": [
                "M. Zeiler",
                "R. Fergus"
            ],
            "title": "Visualizing and understanding convolutional networks",
            "venue": "arXiv",
            "year": 2013
        },
        {
            "authors": [
                "B. Zhou",
                "A. Khosla",
                "A. Lapedriza",
                "A. Oliva",
                "A. Torralba"
            ],
            "title": "Object detectors emerge in deep scene CNNs",
            "venue": "ICLR",
            "year": 2015
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Imagine an image representing a bridge over a river. On top of the bridge, a car is speeding through the right lane. Modern computer vision algorithms excel at answering questions about the observable properties of the scene, such as such as \u201cIs there a car in this image?\u201d. This is achieved by leveraging correlations between pixels and image features across large datasets of images. However, a more nuanced understanding of images arguably requires the ability to reason about how the scene depicted in the image would change in response to interventions. Since the list of possible interventions is long and complex, we can, as a first step, reason about the intervention of removing an object.\nTo this end, consider the two counterfactual questions \u201cWhat would the scene look like if we were to remove the car?\u201d and \u201cWhat would the scene look like if we were to remove the bridge?\u201d On the one hand, the first intervention seems rather benign. After removing the car, we could argue\nthat the rest of the scene depicted in the image (the river, the bridge) would remain invariant. On the other hand, the second intervention seems more severe. If the bridge had been removed from the scene, it would, in general, make little sense to observe the car floating weightlessly above the river. Thus, we understand that the presence of the bridge has an effect on the presence of the car. Reasoning about these and similar counterfactuals allows to begin asking \u201cWhy is there a car in this image?\u201d This question is of course poorly\nar X\niv :1\n60 5.\n08 17\n9v 2\n[ st\nat .M\nL ]\n3 1\nO ct\ndefined, but the answer is linked to the causal relationship between the bridge and the car. In our example, the presence of the bridge causes the presence of the car, in the sense that if the bridge were not there, then the car would not be either (needless to say, it is not the only cause for the car). Such interventional semantics of what is meant by causation align with current approaches in the literature [25, 22]."
        },
        {
            "heading": "1.1. Causal dispositions",
            "text": "We have so far discussed causal relations between two objects present in a single image, representing a particular scene. In order to deploy statistical techniques, we must work with a large collection of images representing a variety of scenes. Similar objects may have different causal relationships in different scenes. For instance, an image may show a car passing under the bridge, instead of over the bridge.\nThe dispositional semantics of causation [20] provide a way to address this difficulty. In this framework, causal relations are established when objects exercise some of their causal dispositions, which are sometimes informally called the powers of objects. For instance a bridge has the power to provide support for a car, and a car has the power to cross a bridge. Although the objects present in a scene do not necessarily exercise all of their powers, the foundation of the dispositional theory of causation is that all causal relationships are manifestations of the powers of objects.1\nSince the list of potential causal dispositions is as long and complex as the list of possible interventions, we again restrict our attention to interventions that affect the presence of certain objects in the scene. In particular we can count the number C(A,B) of images in which the causal dispositions of objects of categories A and B are exercised in a manner that the objects of category B would disappear if one were to remove objects of category A. We then say that the objects of category A cause the presence of objects of category B when C(A,B) is (sufficiently) greater than the converse C(B,A). This definition induces a network of asymmetric causal relationships between object categories that represents, on average, how real-world scenes would be modified when one were to make certain objects disappear.\nThe fundamental question addressed in this paper is to determine whether such an asymmetric causal relationship can be inferred from statistics observed in image datasets.\nHypothesis 1. Image datasets carry an observable statistical signal revealing the asymmetric relationship between object categories that results from their causal dispositions.\nTo our knowledge, no prior work has established or even considered the existence of such a signal. If such a signal were found, it would imply that it is in principle possible for\n1Causal dispositions are more primitive concepts than the causal graphs of Pearl\u2019s approach [22]. Therefore, in our case, causal dispositions are responsible for the shape of causal graphs.\nstatistical computer vision algorithms to reason about the causal structure of the world. This is not small feat, given that it is being debated in statistics until this day whether one can at all infer causality from purely statistical information, without performing interventions. The focus of this contribution is to establish the existence of such causal signals using a newly proposed method. We do not, in contrast, make any engineering contribution advancing the state-of-the-art in standard computer vision tasks using these signals \u2014 this is beyond the scope of the present paper."
        },
        {
            "heading": "1.2. Object features and context features",
            "text": "Since image datasets do not provide labels describing the causal dispositions of objects, we cannot resort to supervised learning techniques to find the causal signal put forward by Hypothesis 1. Instead, we take an indirect approach described below.\nThe features computed by the final layers of a convolutional neural network (CNN) [14, 21, 8] often indicate the presence of a well localized object-like feature in the scene depicted by the image under study.2 Various techniques have been developed to investigate where these object-like features appear in the scene and what they look like in the image [32, 31]. We can therefore examine large collections of images representing different objects of interest such as cats, dogs, trains, buses, cars, and people. The locations of these objects in the images are given to us in the form of bounding boxes. For each object of interest, we can distinguish between object features and context features. By definition, the object features are those that are mostly activated inside the bounding box of the object of interest, and the context features are those that are mostly activated outside the bounding box of the object of interest. Independently and in parallel, we also distinguish between causal features and anticausal features [27]. Causal features are those that cause the presence3 of the object in the scene, whereas anticausal features are those caused by the presence of the object in the scene.\nHaving made a distinction between object and context features, our indirect approach relies on a second hypothesis:\nHypothesis 2. There exists an observable statistical dependence between object features and anticausal features. The statistical dependence between context features and causal features is nonexistent or much weaker.\nWe expect Hypothesis 2 to be true, because many of the features caused by the presence of an object of interest are in fact parts of the object itself, and hence are likely to be contained inside its bounding box. For instance, the presence of a car often causes the presence of wheels. In contrast, the context of an object of interest may either cause or be\n2The word feature in this work describes a property of the scene whose presence is flagged by feature activations computed by the CNN.\n3In the sense defined in Section 1.1.\ncaused by the presence of the object. For instance, asphaltlike features cause the presence of a car, but the car\u2019s shadow is caused by the presence of the car. Importantly, empirical support in favour of Hypothesis 2 translates into support in favour of Hypothesis 1."
        },
        {
            "heading": "1.3. Our contribution",
            "text": "Our plan is to use a large collection of images to provide empirical evidence in favour of Hypothesis 2. In order to do so, we must effectively determine, for each object category, which features are causal or anti-causal. In this manner we would support Hypothesis 2, and consequently, Hypothesis 1.\nOur exposition is organized as follows. After a discussing related literature, Section 2 introduces the basics of causal inference from observational data. Section 3 proposes a new algorithm, the Neural Causation Coefficient (NCC), able to learn causation from a corpus of labeled data. NCC is shown to outperform the previous state-of-the-art in causeeffect inference. Section 4 makes use of NCC to distinguish between causal and anticausal features in collections of images. As hypothesized, we show a consistent relationship between anticausal features and object features. Finally, Section 5 closes our exposition by offering some conclusions and directions for future research."
        },
        {
            "heading": "1.4. Related work",
            "text": "The experiments described in this paper depend crucially on the properties of the features computed by the convolutional layers of a CNN [14]. Zeiler et al. [31] show that the final convolutional layers can often be interpreted as objectlike features. Work on weak supervision [21, 32] suggests that such features can be accurately localized.\nWe also build on the growing literature discussing the discovery of causal relationships from observational data [10, 19, 17, 1]. In particular, the Neural Causation Coefficient (Section 3) is related to [17] but offers superior performance, and is learned end-to-end from data. The notion of causal and anticausal features was inspired by [27]. We believe that our work is the first observational causal discovery technique that targets the causal dispositions of objects.\nCausation in computer vision has been the object of at least four recent works. Pickup et al. [24] use observational causal discovery techniques to determine the direction of time in video playback. Lebeda et al. [13] use transfer entropy to study the causal relationship between object and camera motions in video data. Fire and Zhu [5, 6] use video data annotated with object status and actions to infer perceptual causality. The work of Chalupka et al. [2] is closer to our work because it addresses causation issues in images. However, their work deploys interventional experiments to target causal relationships in the labelling process, that is, which pixel manipulations can result in different labels, whereas\nwe target causal relationships in scenes from a purely observational perspective. This critical difference leads to very different conceptual and technological challenges."
        },
        {
            "heading": "2. Observational causal discovery",
            "text": "Randomized experiments are the gold standard for causal inference [22]. Like a child may drop a toy to probe the nature of gravity, these experiments rely on interacting with the world to reveal causal relations between variables of interest. When such experiments are expensive, unethical, or impossible to conduct, we must discern cause from effect using observational data only, and without the ability to intervene [30]. This is the domain of observational causal discovery.\nIn the absence of any assumptions, the determination of causal relations between random variables given samples from their joint distribution is fundamentally impossible [22, 23]. However, it may still be possible to determine a plausible causal structure in practice. For joint distributions that occur in the real world, the different causal interpretations may not be equally likely. That is, the causal direction between typical variables of interest may leave a detectable signature in their joint distribution. We shall exploit this insight to build a classifier for determining the cause-effect relation between two random variables from samples of their joint distribution.\nIn its simplest form, observational causal discovery [23, 19, 18] considers the observational sample\nS = {(xj , yj)}mj=1 \u223c Pm(X,Y ), (1)\nand aims to infer whether X \u2192 Y or Y \u2192 X . In particular, S is assumed to be drawn from one of two models: from a causal model where X \u2192 Y , or from an anticausal model where X \u2190 Y . Figure 2 exemplifies a family of such models, the Additive Noise Model (ANM) [10], where the effect variable Y is a nonlinear function f of the cause variable X , plus some independent random noise E.\nIf we make no assumptions about the distributions Pf , Pc, and Pe appearing in Figure 2, the problem of observational causal discovery is nonidentifiable [23]. To address this issue, we assume that whenever X \u2192 Y , the cause,\nnoise, and mechanism distributions are \u201cindependent\u201d. This should be interpreted as an informal statement that includes two types of independences. One is the independence between the cause and the mechanism (ICM) [15, 27], which is formalized not as an independence between the input variable x and the mechanism f , but as an independence between the data source (that is, the distribution P (X)) and the mechanism P (Y |X) mapping cause to effect. This can be formalized either probabilistically [3] or in terms of algorithmic complexity [12]. The ICM is one incarnation of uniformitarianism: processes f in nature are fixed and agnostic to the distributions Pc of their causal inputs. The second independence is between the cause and the noise. This is a standard assumption in structural equation modeling, and it can be related to causal sufficiency. Essentially, if this assumption is violated, our causal model is too small and we should include additional variables [22]. In lay terms, believing these assumptions amounts to not believing in spurious correlations.\nFor most choices of (Pc, Pe, Pf ), the ICM will be violated in the anticausal direction X \u2190 Y . This violation will often leave an observable statistical footprint, rendering cause and effect distinguishable from observational data alone [23]. But, what exactly are these causal footprints, and how can we develop statistical tests to find them?"
        },
        {
            "heading": "2.1. Examples of observable causal footprints",
            "text": "Let us illustrate two types of observable causal footprints. First, consider a linear additive noise model Y \u2190 f(X)+ E, where the cause X and the noise E are two independent uniform random variables with bounded range, and the mechanism f is a linear function (Figure 3a). Crucially, it is impossible to construct a linear additive noise model X \u2190 f\u0303(Y )+E\u0303 where the new cause Y and the new noise E\u0303 are two independent random variables (except in degenerate cases). This is illustrated in Figure 3b, where the variance of the new noise variable E\u0303 varies (as depicted in red bars) across different locations of the new cause variable Y . Therefore, the ICM assumption is satisfied for the correct causal direction X \u2192 Y but violated for the wrong causal direction Y \u2192 X . This asymmetry makes cause distinguishable from\neffect [10]. Here, the relevant footprint is the independence between X and E.\nSecond, consider a new observational sample where X \u2192 Y , Y = f(X), and f is a monotone function. The causal relationship X \u2192 Y is deterministic, so the noisebased footprints from the previous paragraphs are rendered useless. Let us assume that P (X) is a uniform distribution. Then, the probability density function of the effect Y increases whenever the derivative f \u2032 decreases, as depicted by Figure 3c. Loosely speaking, the shape of the effect distribution P (Y ) is thus not independent of the mechanism f . In this example, ICM is satisfied under the correct causal direction X \u2192 Y , but violated under the wrong causal direction Y \u2192 X [3]. Again, this asymmetry renders the cause distinguishable from the effect [3]. Here, the relevant footprint is a form of independence between the density of X and f \u2032.\nIt may be possible to continue in this manner, considering more classes of models and adding new footprints to detect causation in each case. However, engineering and maintaining a catalog of causal footprints is a tedious task, and any such catalog will most likely be incomplete. The next section thus proposes to use neural networks to learn causal footprints directly from data."
        },
        {
            "heading": "3. The neural causation coefficient",
            "text": "To learn causal footprints from data, we follow [18] and pose cause-effect inference as a binary classification task. Our input patterns Si are effectively scatterplots similar to those shown in Figures 3a and 3b. That is, each data point is a bag of samples (xij , yij) \u2208 R2 drawn iid from a distribution P (Xi, Yi). The class label li indicates the causal direction between Xi and Yi.\nD = {(Si, li)}ni=1, Si = {(xij , yij)}mij=1 \u223c Pmi(Xi, Yi),\nli = { 0 if Xi \u2192 Yi 1 if Xi \u2190 Yi . (2)\nUsing data of this form, we will train a neural network to classify samples from probability distributions as causal or anticausal. Since the input patterns Si are not fixed-dimensional vectors, but bags of points, we borrow inspiration from the literature on kernel mean embedding classifiers [28] and construct a feedforward neural network of the form\nNCC({(xij , yij)}mij=1) = \u03c8  1 mi mi\u2211 j=1 \u03c6(xij , yij)  . In the previous, \u03c6 is a feature map, and the average over all \u03c6(xij , yij) is the mean embedding of the empirical distribution 1mi \u2211mi i=1 \u03b4(xij ,yij). The function \u03c8 is a binary classifier that takes a fixed-length mean embedding as input [18].\nIn kernel methods, \u03c6 is fixed a priori and defined with respect to a nonlinear kernel [28]. In contrast, our feature map \u03c6 : R2 \u2192 Rh and our classifier \u03c8 : Rh \u2192 {0, 1} are both multilayer perceptrons, which are learned jointly from data. Figure 4 illustrates the proposed architecture, which we term the Neural Causation Coefficient (NCC). In short, to classify a sample Si as causal or anticausal, NCC maps each point (xij , yij) in the sample Si to the representation \u03c6(xij , yij) \u2208 Rh, computes the embedding vector \u03c6Si := 1 mi \u2211mi j=1 \u03c6(xij , yij) across all points (xij , yij) \u2208 Si, and classifies the embedding vector \u03c6Si \u2208 Rh as causal or anticausal using the neural network classifier \u03c8. Importantly, the proposed neural architecture is not restricted to causeeffect inference, and can be used to represent and learn from general distributions.\nNCC has some attractive properties. First, predicting the cause-effect relation for a new set of samples at test time can be done efficiently with a single forward pass through the aggregate network. The complexity of this operation is linear in the number of samples. In contrast, the computational complexity of the state-of-the-art (kernel-based additive noise models) is cubic in the number of samples. Second, NCC can be trained using mixtures of different causal and anticausal generative models, such as linear, non-linear, noisy, and deterministic mechanisms linking causes to their effects. This rich training allows NCC to learn a diversity of causal footprints simultaneously. Third, for differentiable activation functions, NCC is a differentiable function. This allows us to embed NCC into larger neural architectures or to use it as a regularization term to encourage the learning of causal or anticausal patterns.\nThe flexibility of NCC comes at a cost. In practice, labeled cause-effect data as in Equation (2) is scarce and laborious to collect. Because of this, we follow [18] and train\nNCC on artificially generated data. This turns out to be advantageous as it gives us easy access to unlimited data. In the following, we describe the process to generate synthetic cause-effect data along with the training procedure for NCC, and demonstrate the performance of NCC on real-world cause-effect data."
        },
        {
            "heading": "3.1. Synthesis of training data",
            "text": "Causal signals differ significantly from the correlation structures exploited by modern computer vision algorithms. In particular, since the first and second moments are always symmetrical, causal signals can only be found in high-order moments.\nMore specifically, we will construct n synthetic observational samples Si (see Figure 2), where the ith observational sample contains mi points. The points comprising the observational sample Si = {(xij , yij)}mij=1 are drawn from an heteroscedastic additive noise model yij \u2190 fi(xij)+ vijeij , for all j = 1, . . . ,mi. In this manner, we generalize the homoscedastic noise assumption ubiquitous in previous literature [19].\nThe cause terms xij are drawn from a mixture of ki Gaussians distributions. We construct each Gaussian by sampling its mean from Gaussian(0, ri), its standard deviation from Gaussian(0, si) followed by an absolute value, and its unnormalized mixture weight from Gaussian(0, 1) followed by an absolute value. We sample ki \u223c RandomInteger[1, 5] and ri, si \u223c Uniform[0, 5]. We normalize the mixture weights to sum to one. We normalize {xij}mij=1 to zero mean and unit variance.\nThe mechanism fi is a cubic Hermite spline with support[ min({xij}mij=1)\u2212 std({xij}mij=1) , max({xij}mij=1) + std({xij}mij=1) ] (3)\nand di knots drawn from Gaussian(0, 1), where di \u223c RandomInteger(4, 5). The noiseless effect terms {f(xij)}mij=1 are normalized to have zero mean and unit variance.\nThe noise terms eij are sampled from Gaussian(0, vi), where vi \u223c Uniform[0, 5]. To generalize the ICM, we allow for heteroscedastic noise: we multiply each eij by vij , where vij is the value of a smoothing spline with support defined as in Equation (3) and di random knots drawn from Uniform[0, 5]. The noisy effect terms {yij}mij=1 are normalized to have zero mean and unit variance.\nThis sampling process produces a training set of 2n labeled observational samples\nD = { ({(xij , yij)}mij=1, 0) }n i=1\n\u222a { ({(yij , xij)}mij=1, 1) }n i=1 . (4)"
        },
        {
            "heading": "3.2. Training NCC",
            "text": "We train NCC with two embedding layers and two classification layers followed by a softmax output layer. Each hidden layer is a composition of batch normalization [11], 100 hidden neurons, a rectified linear unit, and 25% dropout [29]. We train for 10000 iterations using RMSProp [9] with the default parameters, where each minibatch is of the form given in Equation (4) and has size 2n = 32. Lastly, we further enforce the symmetry P(X \u2192 Y ) = 1 \u2212 P(Y \u2192 X), by training the composite classifier\n1 2 ( 1\u2212 NCC({(xij , yij)}mij=1) + NCC({(yij , xij)}mij=1) ) ,\n(5)\nwhere NCC({(xij , yij)}mij=1) tends to zero if the classifier believes in Xi \u2192 Yi, and tends to one if the classifier believes in Xi \u2190 Yi. We chose our parameters by monitoring the validation error of NCC on a held-out set of 10000 synthetic observational samples. Using this held-out set, we cross-validated the dropout rate over {0.1, 0.25, 0.3}, the number of hidden layers over {2, 3}, and the number of hidden units in each of the layers over {50, 100, 500}."
        },
        {
            "heading": "3.3. Testing NCC",
            "text": "We test the performance of NCC on the Tu\u0308bingen datasets, version 1.0 [19]. This is a collection of one hundred heterogeneous, hand-collected, real-world cause-effect observational samples that are widely used as a benchmark in the causal inference literature [18]. The NCC model with the highest synthetic held-out validation accuracy correctly classifies the cause-effect direction of 79% of the Tu\u0308bingen datasets observational samples. This result outperforms the previous state-of-the-art on observational cause-effect discovery, which achieves 75% accuracy on this dataset [18].4\n4The accuracies reported in [18] are for version 0.8 of the dataset, so we reran the algorithm from [18] on version 1.0 of the dataset.\nThis validation highlights a crucial fact: even when trained on abstract data, NCC discovers the correct causeeffect relationship in a wide variety of real-world datasets. But: Do these abstract, domain-independent, causal footprints hide in complex image data?"
        },
        {
            "heading": "4. Causal signals in sets of static images",
            "text": "We now have at our disposal all the necessary tools to verify our hypotheses. In the following, we chose to work with the twenty object categories of the Pascal VOC 2012 dataset [4]. We first explain how we use NCC to select the most plausible causal or anticausal features for each object category. We then we show that the selected anticausal features are more likely to be object features, that is, located within the object bounding box, than the selected causal features. This establishes that Hypothesis 2 is true, and, as a consequence, also establish that Hypothesis 1 is true."
        },
        {
            "heading": "4.1. Datasets",
            "text": "Our experiments use a feature extraction network trained on the ImageNet [26] dataset and a classifier network trained on the Pascal VOC 2012 dataset [4]. We then use these networks to identify causal relationships on the subset of the 99,309 MSCOCO images [16] representing objects belonging to the twenty Pascal categories: aeroplane, bicycle, bird, boat, bottle, bus, car, cat, chair, cow, dining table, dog, horse, motorbike, person, potted plant, sheep, sofa, train, and television. These datasets feature heterogeneous images that possibly contain multiple objects from different categories. The objects may appear at different scales and angles, and be partially visible or occluded. In addition to these challenges, we have no control about the confounding and selection bias effects polluting these datasets of images. All images are rescaled to ensure that their shorter side is 224 pixels long, then cropped to the central 224\u00d7224 square."
        },
        {
            "heading": "4.2. Selecting causal and anticausal features",
            "text": "Our first task is to determine which of the features scores computed by the feature extraction neural network represent real world entities that cause the presence of the object of interest (causal features), or are caused by the presence of the object of interest (anticausal features).\nTo that effect, we consider the feature scores computed by a 18-layer ResNet [8] trained on the ImageNet dataset using a proven implementation [7]. Building on top of these features, we use the Pascal VOC2012 dataset to train an independent network with two 512-unit hidden layers to recognize the 20 Pascal VOC2012 categories,\nFor each of the MSCOCO images containing at least one instance of the twenty Pascal VOC 2012 object categories, xj \u2208 R3\u00d7224\u00d7224, let fj = f(xj) \u2208 R512 denote the vector of feature scores (before the ReLU nonlinearity) obtained using the feature extraction network and let cj = c(xj) \u2208 R20\ndenote the vector of log-odds (that is, the output unit activations before the sigmoid nonlinearity) obtained using the classifier network. We use features before their nonlinearity and log odds instead of the class probabilities because NCC is trained on continuous data with full support on R.\nAs depicted in Figure 1, for each category k \u2208 {1 . . . 20} and each feature l \u2208 {1 . . . 512}, we apply NCC to the scatterplot {(fjl, cjk)}mj=1 representing the joint distribution of the scores of feature j and the score of category k. Since these scores are computed by running our neural networks on the image pixels, they are not related by a direct causal relationship. However we know that these scores are highly correlated with the presence of objects and features in the real scene. Therefore, the appearance of a causal relationship between these scores suggests that there is a causal relationship between the real world entities they represent.\nBecause we analyze one feature at a time, the values taken by all other features appear as an additional source of noise, and the observed statistical dependencies are then be much weaker than in the synthetic NCC training data. To avoid detecting causation between independent random variables, we use a variant of NCC trained with an augmented training set: in addition to presenting each scatterplot in both causal directions as in (4), we pick a random permutation \u03c3 to generate an additional uncorrelated example {xi,\u03c3(j), yij}mij=1 with label 12 . We use our best model of this kind which, for validation purposes, achieves 79% accuracy in the Tu\u0308bingen pair benchmark.\nFor each category k \u2208 1 . . . 20, we then record the indices of the top 1% causal and the top 1% anticausal features."
        },
        {
            "heading": "4.3. Verifying Hypothesis 2",
            "text": "In order to verify Hypothesis 2, it is sufficient to show that the top anticausal features are more likely to be object features than the top causal features. For each category k and each feature j, we must therefore determine whether feature j is likely to be an object feature or a context feature.\nThis is relatively easy because we have access to the object bounding boxes and we simply need to determine how much of each feature score j is imputable to the bounding boxes of the objects of category k.\nTo that effect, we prepare two alternate versions of each MSCOCO image xj by blacking out (with zeroes) the pixels located outside the bounding boxes of the category k objects, yielding the object-only image xoj , or by blacking out the pixels located inside the bounding boxes of the category k objects, yielding the context-only image xcj . This process is illustrated in Figure 5c. We then compute the corresponding vectors of feature scores foj = f(x o j) and f c j = f(x c j).\nFor each category k and each feature f we heuristically define the object-feature ratio sol and the context-feature ratio scl as follows:\nsol =\n\u2211m j=1 \u2223\u2223\u2223f cjl \u2212 fjl\u2223\u2223\u2223\u2211m j=1 |fjl| , scl = \u2211m j=1 \u2223\u2223\u2223fojl \u2212 fjl\u2223\u2223\u2223\u2211m j=1 |fjl| .\nIntuitively, features with high object-feature ratio (resp. high context-feature ratio) are those features that react violently when the object (resp. the context) is erased.\nNote that blacking out pixels does not constitute an intervention on the scene represented by the image. This is merely a procedure to impute the contribution of the object bounding boxes to each feature score."
        },
        {
            "heading": "4.4. Results",
            "text": "Figure 6 shows the means and the standard deviations of the object-context ratios (top plot) and the context-feature ratios (bottom plot) estimated on the top 1% anticausal features (blue bars) and the top 1% causal features (green bars) for each of the twenty object categories.\nAs predicted by Hypothesis 2, object features are related to anticausal features: the top 1% anticausal features exhibit a higher object-feature ratio than the top 1% causal features. Since this effect can be observed on all 20 classes of interest, the probability of obtaining such a result by chance would be 2\u221220\u224810\u22126. When we select the top 20% causal and anticausal features, this effect remains consistent across 16 out of 20 classes of interest.\nThis result indicates that anticausal features may be useful for detecting objects locations in a robust manner, regardless of their context. As stated in Hypothesis 2, we could not find a consistent relationship between context features and causal features. Remarkably, we remind the reader that the NCC classifier does not depend on the object categories and was trained using synthetic data unrelated to images. As a sanity check, we did not obtain any such results when replacing the NCC scores with the correlation coefficient or the absolute value of the correlation coefficient.5\n5We also ran preliminary experiments to find causal relationships be-\nTherefore we believe that this result establishes that Hypothesis 2 is true with high certainty. As explained in Section 1.3, verifying Hypothesis 2 in this manner also implies confirms Hypothesis 1."
        },
        {
            "heading": "5. Conclusion",
            "text": "Using a carefully designed experiment, we have established that the high order statistical properties of image datasets contain information about the causal dispositions of objects and, more generally, about causal structure of the real world.\nOur experiment relies on three main components. First, we use synthetic scatterplots to train a binary classifier that identifies plausible causal (X\u2192Y ) and anticausal (X\u2190Y ) relations. Second we hypothesise that the distinction between object features and context features in natural scenes\ntween objects of interest, by computing the NCC scores between the log odds of different objects of interest. The strongest causal relationships that we found were \u201cbus causes car,\u201d \u201cchair causes plant,\u201d \u201cchair causes sofa,\u201d \u201cdining table causes bottle,\u201d \u201cdining table causes chair,\u201d \u201cdining table causes plant,\u201d \u201ctelevision causes chair,\u201d and \u201ctelevision causes sofa.\u201d\nis related to the distinction between features that cause the presence of the object and features that are caused by the presence of the object. Finally, we construct an experiment that leverages static image datasets to establish that this latter hypothesis is true. Thus, we conclude that we must therefore have been able to effectively distinguish which features were causal or anticausal.\nBecause we now know that such a signal exist, we can envision in a reasonable future that computer vision algorithms will be able to perceive the causal structure of the real world and reason about scenes. There is no question that significant algorithmic advances will be necessary to achieve this goal. In particular, we stress the importance of (1) building large, real-world datasets to aid research in causal inference, (2) extending data-driven techniques like NCC to causal inference of more than two variables, and (3) exploring data with explicit causal signals, such as the arrow of time in videos (e.g. [24].)"
        }
    ],
    "title": "Discovering Causal Signals in Images",
    "year": 2017
}