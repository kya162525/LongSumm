{
    "abstractText": "Background Neural word embeddings have been widely used in biomedical Natural Language Processing (NLP) applications as they provide vector representations of words capturing the semantic properties of words and the linguistic relationship between words. Many biomedical applications use different textual resources (e.g., Wikipedia and biomedical articles) to train word embeddings and apply these word embeddings to downstream biomedical applications. However, there has been little work on evaluating the word embeddings trained from these resources. Methods In this study, we provide an empirical evaluation of word embeddings trained from four different resources, namely clinical notes, biomedical publications, Wikipedia, and news. For the former two resources, we trained word embeddings utilizing clinical notes available at Mayo Clinic and articles from the PubMed Central (PMC), respectively. For the latter two resources, we used publicly available pre-trained word embeddings, GloVe and Google News. We performed the evaluation qualitatively and quantitatively. In qualitative evaluation, we manually inspected five most similar medical words to a given set of target medical words, and then analyzed word embeddings through the visualization of those word embeddings. In quantitative evaluation, we conducted both intrinsic and extrinsic evaluation. Intrinsic evaluation directly tested semantic relationships between medical words using four published datasets for measuring semantic similarity between medical terms, i.e., Pedersen, Hliaoutakis, MayoSRS, and UMNSRS. In extrinsic evaluation, we applied word embeddings to downstream biomedical NLP applications, including clinical information extraction (IE), biomedical information retrieval (IR), and relation extraction (RE), using data from shared tasks. Results Qualitative evaluation shows that the word embeddings trained from EHR and PubMed can find more relevant similar medical terms than these from GloVe and Google News. In the intrinsic quantitative evaluation, the semantic similarity captured by the word embeddings trained from EHR are closer to human experts\u2019 judgments on all four tested datasets. In the extrinsic quantitative evaluation, the word embeddings trained from EHR has the best F1 score of 0.900 for the clinical IE task; no word embeddings improve the performance in terms of inference average precision and mean average precision for the biomedical IR task; and the word embeddings trained on Google News has the best overall F1 score of 0.790 for the RE task.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yanshan Wang"
        },
        {
            "affiliations": [],
            "name": "Sijia Liu"
        },
        {
            "affiliations": [],
            "name": "Naveed Afzal"
        },
        {
            "affiliations": [],
            "name": "Majid Rastegar-Mojarad"
        },
        {
            "affiliations": [],
            "name": "Liwei Wang"
        },
        {
            "affiliations": [],
            "name": "Feichen Shen"
        },
        {
            "affiliations": [],
            "name": "Paul Kingsbury"
        },
        {
            "affiliations": [],
            "name": "Hongfang Liu"
        }
    ],
    "id": "SP:9b2776df864ba00506ff958b7d750fa7bc4358c7",
    "references": [
        {
            "authors": [
                "T. Mikolov",
                "W.-t. Yih",
                "G. Zweig"
            ],
            "title": "Linguistic regularities in continuous space word representations.",
            "venue": "in hlt-Naacl,",
            "year": 2013
        },
        {
            "authors": [
                "F. Liu",
                "J. Chen",
                "A. Jagannatha",
                "H. Yu"
            ],
            "title": "Learning for biomedical information extraction: Methodological review of recent advances",
            "venue": "arXiv preprint arXiv:1606.07993, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "O. Levy",
                "Y. Goldberg"
            ],
            "title": "Dependency-based word embeddings.",
            "year": 2014
        },
        {
            "authors": [
                "Y. Wang",
                "L. Wang",
                "M. Rastegar-Mojarad",
                "S. Moon",
                "F. Shen",
                "N. Afzal",
                "S. Liu",
                "Y. Zeng",
                "S. Mehrabi",
                "S. Sohn"
            ],
            "title": "Clinical information extraction applications: A literature review",
            "venue": "Journal of biomedical informatics, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "D. Zeng",
                "K. Liu",
                "S. Lai",
                "G. Zhou",
                "J. Zhao"
            ],
            "title": "Relation classification via convolutional deep neural network.",
            "venue": "in COLING,",
            "year": 2014
        },
        {
            "authors": [
                "T.H. Nguyen",
                "R. Grishman"
            ],
            "title": "Employing word representations and regularization for domain adaptation of relation extraction.",
            "year": 2014
        },
        {
            "authors": [
                "D. Ganguly",
                "D. Roy",
                "M. Mitra",
                "G.J. Jones"
            ],
            "title": "Word embedding based generalized language model for information retrieval",
            "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 2015, pp. 795\u2013798.",
            "year": 2015
        },
        {
            "authors": [
                "D. Tang",
                "F. Wei",
                "N. Yang",
                "M. Zhou",
                "T. Liu",
                "B. Qin"
            ],
            "title": "Learning sentiment-specific word embedding for twitter sentiment classification.",
            "year": 2014
        },
        {
            "authors": [
                "A.L. Maas",
                "R.E. Daly",
                "P.T. Pham",
                "D. Huang",
                "A.Y. Ng",
                "C. Potts"
            ],
            "title": "Learning word vectors for sentiment analysis",
            "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1. Association for Computational Linguistics, 2011, pp. 142\u2013150.",
            "year": 2011
        },
        {
            "authors": [
                "M. Ren",
                "R. Kiros",
                "R. Zemel"
            ],
            "title": "Exploring models and data for image question answering",
            "venue": "Advances in neural information processing systems, 2015, pp. 2953\u20132961.",
            "year": 2015
        },
        {
            "authors": [
                "L. Dong",
                "F. Wei",
                "M. Zhou",
                "K. Xu"
            ],
            "title": "Question answering over freebase with multi-column convolutional neural networks.",
            "year": 2015
        },
        {
            "authors": [
                "D. Yogatama",
                "F. Liu",
                "N.A. Smith"
            ],
            "title": "Extractive summarization by maximizing semantic volume.",
            "venue": "in EMNLP,",
            "year": 2015
        },
        {
            "authors": [
                "A.M. Rush",
                "S. Chopra",
                "J. Weston"
            ],
            "title": "A neural attention model for abstractive sentence summarization",
            "venue": "arXiv preprint arXiv:1509.00685, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "B. Tang",
                "H. Cao",
                "X. Wang",
                "Q. Chen",
                "H. Xu"
            ],
            "title": "Evaluating word representation features in biomedical named entity recognition tasks",
            "venue": "BioMed research international, vol. 2014, 2014. 16",
            "year": 2014
        },
        {
            "authors": [
                "S. Liu",
                "B. Tang",
                "Q. Chen",
                "X. Wang"
            ],
            "title": "Effects of semantic features on machine learning-based drug name recognition systems: word embeddings vs. manually constructed dictionaries",
            "venue": "Information, vol. 6, no. 4, pp. 848\u2013865, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "A.N. Jagannatha",
                "J. Chen",
                "H. Yu"
            ],
            "title": "Mining and ranking biomedical synonym candidates from wikipedia",
            "venue": "Proceedings of the Sixth International Workshop on Health Text Mining and Information Analysis (Louhi), 2015, pp. 142\u2013151.",
            "year": 2015
        },
        {
            "authors": [
                "Z. Jiang",
                "L. Jin",
                "L. Li",
                "M. Qin",
                "C. Qu",
                "J. Zheng",
                "D. Huang"
            ],
            "title": "A crd-wel system for chemical-disease relations extraction",
            "venue": "The fifth BioCreative challenge evaluation workshop, 2015, pp. 317\u2013326.",
            "year": 2015
        },
        {
            "authors": [
                "S. Liu",
                "B. Tang",
                "Q. Chen",
                "X. Wang"
            ],
            "title": "Drug-drug interaction extraction via convolutional neural networks",
            "venue": "Computational and mathematical methods in medicine, vol. 2016, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "Y. Wang",
                "S. Liu",
                "M. Rastegar-Mojarad",
                "L. Wang",
                "F. Shen",
                "F. Liu",
                "H. Liu"
            ],
            "title": "Dependency embeddings and amr embeddings for drug-drug interaction extraction from biomedical texts",
            "venue": "Proceedings of the 8th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics. ACM, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Z. Jiang",
                "L. Li",
                "D. Huang"
            ],
            "title": "A general protein-protein interaction extraction architecture based on word representation and feature selection",
            "venue": "International Journal of Data Mining and Bioinformatics, vol. 14, no. 3, pp. 276\u2013291, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "S.-H. Jo",
                "K.-S. Lee"
            ],
            "title": "Cbnu at trec 2016 clinical decision support track",
            "venue": "Text REtrieval Conference (TREC 2016), 2016.",
            "year": 2016
        },
        {
            "authors": [
                "Y. Wang",
                "M. Rastegar-Mojarad",
                "R.K. Elayavilli",
                "S. Liu",
                "H. Liu"
            ],
            "title": "An ensemble model of clinical information extraction and information retrieval for clinical decision support.",
            "venue": "TREC,",
            "year": 2016
        },
        {
            "authors": [
                "Y. Wu",
                "J. Xu",
                "Y. Zhang",
                "H. Xu"
            ],
            "title": "Clinical abbreviation disambiguation using neural word embeddings",
            "venue": "Proceedings of the 2015 Workshop on Biomedical Natural Language Processing (BioNLP), 2015, pp. 171\u2013176.",
            "year": 2015
        },
        {
            "authors": [
                "H. Gurulingappa",
                "L. Toldo",
                "C. Schepers",
                "A. Bauer"
            ],
            "title": "Megaro, \u201cSemi-supervised information retrieval system for clinical decision support.",
            "venue": "TREC,",
            "year": 2016
        },
        {
            "authors": [
                "F. Shen",
                "Y. Lee"
            ],
            "title": "Knowledge discovery from biomedical ontologies in cross domains",
            "venue": "PloS one, vol. 11, no. 8, p. e0160005, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "F. Shen",
                "H. Liu",
                "S. Sohn",
                "D.W. Larson",
                "Y. Lee"
            ],
            "title": "Predicate oriented pattern analysis for biomedical knowledge discovery",
            "venue": "Intelligent Information Management, vol. 8, no. 03, pp. 66\u201385, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "F. Shen",
                "H. Liu",
                "S. Sohn",
                "D. Larson",
                "Y. Lee"
            ],
            "title": "Bmqgen: Biomedical query generator for knowledge discovery",
            "venue": "Bioinformatics and Biomedicine (BIBM), 2015 IEEE International Conference on, pp. 1092\u20131097, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "T. Pedersen",
                "S.V. Pakhomov",
                "S. Patwardhan",
                "C.G. Chute"
            ],
            "title": "Measures of semantic similarity and relatedness in the biomedical domain",
            "venue": "Journal of biomedical informatics, vol. 40, no. 3, pp. 288\u2013299, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "A. Hliaoutakis"
            ],
            "title": "Semantic similarity measures in mesh ontology and their application to information retrieval on medline",
            "venue": "Master\u2019s thesis, 2005.",
            "year": 2005
        },
        {
            "authors": [
                "S.V. Pakhomov",
                "T. Pedersen",
                "B. McInnes",
                "G.B. Melton",
                "A. Ruggieri",
                "C.G. Chute"
            ],
            "title": "Towards a framework for developing semantic relatedness reference standards",
            "venue": "Journal of biomedical informatics, vol. 44, no. 2, pp. 251\u2013265, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "S. Pakhomov",
                "B. McInnes",
                "T. Adam",
                "Y. Liu",
                "T. Pedersen",
                "G.B. Melton"
            ],
            "title": "Semantic similarity and relatedness between clinical terms: an experimental study",
            "venue": "AMIA annual symposium proceedings, vol. 2010. American Medical Informatics Association, 2010, p. 572.",
            "year": 2010
        },
        {
            "authors": [
                "S.V. Pakhomov",
                "G. Finley",
                "R. McEwan",
                "Y. Wang",
                "G.B. Melton"
            ],
            "title": "Corpus domain effects on distributional semantic modeling of medical terms",
            "venue": "Bioinformatics, vol. 32, no. 23, pp. 3635\u20133644, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "M. Baroni",
                "G. Dinu",
                "G. Kruszewski"
            ],
            "title": "Don\u2019t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors.",
            "year": 2014
        },
        {
            "authors": [
                "H. Rubenstein",
                "J.B. Goodenough"
            ],
            "title": "Contextual correlates of synonymy",
            "venue": "Communications of the ACM, vol. 8, no. 10, pp. 627\u2013633, 1965.",
            "year": 1965
        },
        {
            "authors": [
                "L. Finkelstein",
                "E. Gabrilovich",
                "Y. Matias",
                "E. Rivlin",
                "Z. Solan",
                "G. Wolfman",
                "E. Ruppin"
            ],
            "title": "Placing search in context: The concept revisited",
            "venue": "Proceedings of the 10th international conference on World Wide Web. ACM, 2001, pp. 406\u2013414.",
            "year": 2001
        },
        {
            "authors": [
                "T.K. Landauer",
                "S.T. Dumais"
            ],
            "title": "A solution to plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.",
            "venue": "Psychological review,",
            "year": 1997
        },
        {
            "authors": [
                "A. Almuhareb"
            ],
            "title": "Attributes in lexical acquisition",
            "venue": "Ph.D. dissertation, University of Essex, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "S. Pad\u00f3",
                "M. Lapata"
            ],
            "title": "Dependency-based construction of semantic space models",
            "venue": "Computational Linguistics, vol. 33, no. 2, pp. 161\u2013199, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "T. Schnabel",
                "I. Labutov",
                "D.M. Mimno",
                "T. Joachims"
            ],
            "title": "Evaluation methods for unsupervised word embeddings.",
            "venue": "in EMNLP,",
            "year": 2015
        },
        {
            "authors": [
                "R. Collobert",
                "J. Weston",
                "L. Bottou",
                "M. Karlen",
                "K. Kavukcuoglu",
                "P. Kuksa"
            ],
            "title": "Natural language processing (almost) from scratch",
            "venue": "Journal of Machine Learning Research, vol. 12, no. Aug, pp. 2493\u20132537, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "R. Lebret",
                "R. Collobert"
            ],
            "title": "Word emdeddings through hellinger pca",
            "venue": "arXiv preprint arXiv:1312.5542, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "J. Pennington",
                "R. Socher",
                "C. Manning"
            ],
            "title": "Glove: Global vectors for word representation",
            "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), 2014, pp. 1532\u20131543.",
            "year": 2014
        },
        {
            "authors": [
                "P. Dhillon",
                "J. Rodu",
                "D. Foster",
                "L. Ungar"
            ],
            "title": "Two step cca: A new spectral method for estimating vector models of words",
            "venue": "arXiv preprint arXiv:1206.6403, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "P. Li",
                "T.J. Hastie",
                "K.W. Church"
            ],
            "title": "Very sparse random projections",
            "venue": "Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2006, pp. 287\u2013296.",
            "year": 2006
        },
        {
            "authors": [
                "S. Ghannay",
                "B. Favre",
                "Y. Esteve",
                "N. Camelin"
            ],
            "title": "Word embedding evaluation and combination.",
            "venue": "in LREC,",
            "year": 2016
        },
        {
            "authors": [
                "H. Schwenk"
            ],
            "title": "Cslm-a modular open-source continuous space language modeling toolkit.",
            "venue": "INTERSPEECH,",
            "year": 2013
        },
        {
            "authors": [
                "N. Nayak",
                "G. Angeli",
                "C.D. Manning"
            ],
            "title": "Evaluating word embeddings using a representative suite of practical tasks",
            "venue": "ACL 2016, p. 19, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "Y. Wang",
                "L. Wang",
                "M. Rastegar-Mojarad",
                "S. Liu",
                "F. Shen",
                "H. Liu"
            ],
            "title": "Systematic analysis of free-text family history in electronic health record",
            "venue": "AMIA Summits on Translational Science Proceedings, vol. 2017, p. 104, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "L. v. d. Maaten",
                "G. Hinton"
            ],
            "title": "Visualizing data using t-sne",
            "venue": "Journal of Machine Learning Research, vol. 9, no. Nov, pp. 2579\u20132605, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "P. Bojanowski",
                "E. Grave",
                "A. Joulin",
                "T. Mikolov"
            ],
            "title": "Enriching word vectors with subword information",
            "venue": "arXiv preprint arXiv:1607.04606, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "\u00d6. Uzuner",
                "I. Goldstein",
                "Y. Luo",
                "I. Kohane"
            ],
            "title": "Identifying patient smoking status from medical discharge records",
            "venue": "Journal of the American Medical Informatics Association, vol. 15, no. 1, pp. 14\u201324, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "Y. Wang",
                "E. Atkinson",
                "S. Amin",
                "H. Liu"
            ],
            "title": "A distant supervision paradigm for clinical information extraction",
            "venue": "2018.",
            "year": 2018
        },
        {
            "authors": [
                "K. Roberts",
                "D. Demner-Fushman",
                "E.M. Voorhees",
                "W.R. Hersh"
            ],
            "title": "Overview of the trec 2016 clinical decision support track.",
            "venue": "TREC,",
            "year": 2016
        },
        {
            "authors": [
                "A.E. Johnson",
                "T.J. Pollard",
                "L. Shen",
                "L.-w. H. Lehman",
                "M. Feng",
                "M. Ghassemi",
                "B. Moody",
                "P. Szolovits",
                "L.A. Celi",
                "R.G. Mark"
            ],
            "title": "Mimic-iii, a freely accessible critical care database",
            "venue": "Scientific data, vol. 3, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "T. Strohman",
                "D. Metzler",
                "H. Turtle",
                "W.B. Croft"
            ],
            "title": "Indri: A language model-based search engine for complex queries",
            "venue": "Proceedings of the International Conference on Intelligent Analysis, vol. 2, no. 6. Citeseer, 2005, pp. 2\u20136.",
            "year": 2005
        },
        {
            "authors": [
                "C. Zhai",
                "J. Lafferty"
            ],
            "title": "Two-stage language models for information retrieval",
            "venue": "Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2002, pp. 49\u201356.",
            "year": 2002
        },
        {
            "authors": [
                "E. Yilmaz",
                "E. Kanoulas",
                "J.A. Aslam"
            ],
            "title": "A simple and efficient sampling method for estimating ap and ndcg",
            "venue": "Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2008, pp. 603\u2013610.",
            "year": 2008
        },
        {
            "authors": [
                "I. Segura Bedmar",
                "P. Mart\u0131\u0301nez",
                "M. Herrero Zazo"
            ],
            "title": "Semeval-2013 task 9: Extraction of drug-drug interactions from biomedical texts (ddiextraction 2013).",
            "venue": "Association for Computational Linguistics,",
            "year": 2013
        }
    ],
    "sections": [
        {
            "text": "Background Neural word embeddings have been widely used in biomedical Natural Language Processing (NLP) applications as they provide vector representations of words capturing the semantic properties of words and the linguistic relationship between words. Many biomedical applications use different textual resources (e.g., Wikipedia and biomedical articles) to train word embeddings and apply these word embeddings to downstream biomedical applications. However, there has been little work on evaluating the word embeddings trained from these resources.\nMethods In this study, we provide an empirical evaluation of word embeddings trained from four different resources, namely clinical notes, biomedical publications, Wikipedia, and news. For the former two resources, we trained word embeddings utilizing clinical notes available at Mayo Clinic and articles from the PubMed Central (PMC), respectively. For the latter two resources, we used publicly available pre-trained word embeddings, GloVe and Google News. We performed the evaluation qualitatively and quantitatively. In qualitative evaluation, we manually inspected five most similar medical words to a given set of target medical words, and then analyzed word embeddings through the visualization of those word embeddings. In quantitative evaluation, we conducted both intrinsic and extrinsic evaluation. Intrinsic evaluation directly tested semantic relationships between medical words using four published datasets for measuring semantic similarity between medical terms, i.e., Pedersen, Hliaoutakis, MayoSRS, and UMNSRS. In extrinsic evaluation, we applied word embeddings to downstream biomedical NLP applications, including clinical information extraction (IE), biomedical information retrieval (IR), and relation extraction (RE), using data from shared tasks.\nResults Qualitative evaluation shows that the word embeddings trained from EHR and PubMed can find more relevant similar medical terms than these from GloVe and Google News. In the intrinsic quantitative evaluation, the semantic similarity captured by the word embeddings trained from EHR are closer to human experts\u2019 judgments on all four tested datasets. In the extrinsic quantitative evaluation, the word embeddings trained from EHR has the best F1 score of 0.900 for the clinical IE task; no word embeddings improve the performance in terms of inference average precision and mean average precision for the biomedical IR task; and the word embeddings trained on Google News has the best overall F1 score of 0.790 for the RE task.\n*Corresponding authors.\nar X\niv :1\n80 2.\n00 40\n0v 2\n[ cs\n.I R\n] 9\n2 Conclusion Based on the evaluation results, we can draw the following conclusions. First, the word embeddings\ntrained on EHR and PubMed can capture the semantics of medical terms better than those trained on GloVe and Google News and find more relevant similar medical terms, and are closer to human experts\u2019 judgments, compared to these trained on GloVe and Google News. Second, there does not exist a consistent global ranking of word embedding quality for downstream biomedical NLP applications. However, adding word embeddings as extra features will improve results on most downstream tasks. Finally, the word embeddings trained on biomedical domain corpora do not necessarily have better performance than those trained on other general domain corpora for any downstream biomedical NLP tasks.\nIndex Terms\nword embeddings; natural language processing; biomedical application; deep learning;\nI. INTRODUCTION\nNeural word embeddings have been widely used in Natural Language Processing (NLP) applications as they provide vector representations of words capturing the semantic properties of words and the linguistic relationship between words [1], [2], [3]. There has been an increasing number of studies applying word embeddings in common NLP tasks, such as information extraction (IE) [4], [5], [6], information retrieval (IR) [7], sentiment analysis [8], [9], question answering[10], [11], and text summarization [12], [13]. Recently in the biomedical domain word embeddings have been remarkably utilized in applications like biomedical named entity recognition (NER) [14], [15], medical synonym extraction[16], relation extraction (RE) including chemical-disease relation [17], drugdrug interaction [18], [19] and protein-protein interaction [20], biomedical IR [21], [22] and medical abbreviation disambiguation [23].\nMany biomedical applications use task corpora to train word embeddings or use external data resources such as Wikipedia [21], [24] to train word embeddings based on an implicit assumption that the external resources contain the knowledge that could be used to enhance domain tasks [25], [26], [27]. A number of pre-trained word embeddings are publicly available, such as Google News embeddings 1 and GloVe embeddings 2. These embeddings could capture semantics of general English words from a large corpus. However, one question remains unanswered: Do we need to train word embeddings for a specific NLP task since there are a number of public pre-trained word embeddings? This question becomes more significant for biomedical applications, and particularly more important for the clinical domain. The reason is that few electrical health records (EHRs) data are publicly available due to Health Insurance Portability and Accountability Act (HIPAA) rule while a big volume of biomedical literature data is available online. However, there has been little work on evaluating the word embeddings trained from these textual resources for biomedical applications, to the best of our knowledge.\nIn this study, we provide an empirical evaluation of word embeddings trained from four different resources, namely clinical notes, biomedical publications, Wikipedia, and news. For the former two resources, we utilized\n1https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit 2https://nlp.stanford.edu/projects/glove/\n3 clinical notes from the EHR system at Mayo Clinic and biomedical publications from the PubMed Central (PMC)3 to train word embeddings separately. For the latter two resources, we used publicly available pre-trained word embeddings, GloVe and Google News. We performed the evaluation qualitatively and quantitatively. In qualitative evaluation, we manually inspected five most similar medical words to a given set of target medical words, and then analyzed word embeddings through the visualization of those word embeddings. In quantitative evaluation, we conducted both intrinsic and extrinsic evaluation. Intrinsic evaluation directly tested semantic relationships between medical words using four published datasets for measuring semantic similarity between medical terms, i.e., Pedersen [28], Hliaoutakis [29], MayoSRS [30], and UMNSRS [31], [32]. In extrinsic evaluation, we applied word embeddings to downstream NLP applications in the biomedical domain including clinical IE, biomedical IR, and RE, and measured the performance of word embeddings."
        },
        {
            "heading": "II. RELATED WORK",
            "text": "Due to the successful usage of word embeddings in a variety of NLP applications, there exists recent work on evaluation of word embeddings in representing word semantics. Most of the previous work focuses on evaluating word embeddings generated by different approaches. Baroni et al [33] presented the first systematic evaluation of word embeddings generated by count models (using DISSECT4 on a corpus of 2.8 billion tokens constructed by concatenating ukWaC5 the English GloVe6 and the British National Corpus7), CBOW [1] (using word2vec8 on the same corpus as DISSECT), Distributional Memory model9 (on the same corpus as DISSECT), and Collobert and Weston model10 (on the GloVe), and tested them on fourteen benchmarks in five categories: semantic relatedness (a dataset of semantic benchmarks constructed by asking human subjects to rate the degree of semantic similarity or relatedness between two words on a numerical scale, such as Rubenstein and Goodenough\u2019s dataset [34] and WordSim353 [35]), synonym detection (a dataset containing 80 multiple-choice questions that pair a target term with 4 synonym candidates, such as TOEFL dataset [36]), concept categorization (given a set of nominal concepts, the task is to group them into natural categories, such as Almuhareb-Poesio dataset [37]), selectional preferences (a dataset containing verb-noun pairs that were rated by subjects for the typicality of the noun as a subject or object of the verb, such as Ulrike Pado\u2019s dataset [38]), and analogy (a dataset containing semantic and syntactic analogy questions, such as Mikolov\u2019s dataset [1]). They found that the word2vec model, CBOW, performed the best for almost all the tasks. Schnabel et al [39] trained the CBOW model of word2vec [1], C&W embeddings [40], Hellinger PCA [41], GloVe [42], TSCCA [43] and Sparse Random Projections [44] on a 2008 GloVe dump, and\n3http://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/ 4http://clic.cimec.unitn.it/composes/ 5http://wacky.sslmit.unibo.it/ 6http://en.GloVe.org/ 7http://www.natcorp.ox.ac.uk/ 8https://code.google.com/p/word2vec/ 9http://clic.cimec.unitn.it/dm/ 10http://ronan.collobert.com/senna/\n4 tested on the same fourteen datasets. They also found the CBOW outperformed other embeddings on 10 datasets. In addition to this intrinsic evaluation, they conducted extrinsic evaluation by using the embeddings as input features to two downstream tasks, namely noun phrase chunking and sentiment classification, and found the results of CBOW were also among the best. Ghannay et al [45] conducted a similar intrinsic evaluation, they additionally evaluated the skip-gram models of word2vec [1], CSLM word embeddings [46], dependency-based word embeddings [3], and combined word embeddings on NLP tasks (i.e., Part-Of-Speech Tagging, chunking, named entity recognition, mention detection) and linguistic tasks using Mikolov\u2019s dataset [1] and the WordSim353 dataset[35]. They trained these word embeddings on the Gigaword corpus composed of 4 billion words and found that the dependency-based word embeddings gave the best performance on the NLP tasks and combination of the embeddings yielded significant improvement. In Nayak et al\u2019s study [47], they recommended that the evaluation should test both syntactic and semantic properties of the word embeddings and that the tasks should be closer to real-word applications. However, none of these studies evaluate word embeddings in the biomedical domain and none of these datasets focus on biomedical data.\nAs most of the aforementioned studies evaluate word embeddings in the general (i.e., non-medical) NLP domain, only one recent paper by Pakhomov et al [32] is about evaluating word embeddings in the biomedical domain, to the best of our knowledge. They trained the CBOW model on two biomedical corpora, namely clinical notes and biomedical publications, and one general English corpora, namely GloVe. The word embeddings were evaluated on subsets of UMNSRS dataset, which consisted of pairs of medical terms with the similarity of each pair assessed by medical experts, and on a document retrieval task and a word sense disambiguation task. They found that the semantics captured by the embeddings computed from biomedical publications were on par with that from clinical notes. Inspired by this work, we would like to conduct a complementary study to extend their evaluation of word embeddings by 1) utilizing four datasets to evaluate word embeddings on capturing medical term semantics; 2) conducting a qualitative evaluation; and 3) examining word embeddings on more biomedical application.\nIn this work, we provide a comparison of the quality of word embeddings trained separately from different\nresources. Specifically, our contributions are:\n1) We performed qualitative evaluation where we manually inspected five most similar medical words to a given\nset of target medical words and plotted a visualization of selected medical words from those word embeddings. 2) We performed quantitative evaluation, including extrinsic and intrinsic evaluation. In the intrinsic evaluation,\nwe used four published datasets for measuring semantic similarity between medical terms.\n3) In extrinsic evaluation, we evaluated word embeddings by applying them to three publicly shared biomedical\ntasks, including biomedical IR, NER, and RE, and one institutional clinical NLP task."
        },
        {
            "heading": "III. WORD EMBEDDINGS",
            "text": "Since it has been shown that the word2vec outperforms other approaches in generating good embeddings in general NLP tasks [33], [39], the skip-gram model of word2vec is utilized as the approach for generating word embeddings in this paper. Since no evidence shows the CBOW outperforms the skip-gram model or vice versa, we arbitrarily chose the skip-gram model of word2vec.\n5"
        },
        {
            "heading": "A. Brief Introduction",
            "text": "Word embeddings can be represented as a mapping V \u2192 RD : w 7\u2192 \u03b8 which maps a word w from a vocabulary V to a real-valued vector \u03b8 in an embedding space with a dimension of D. The skip-gram model is an architecture proposed by Mikolov et al [1], which trains embeddings using the negative-sampling procedure. It constructed with the focus word as the single input vector, and the target contextual words are at the output layer. Negative-sampling updates a sample of output words per iteration, and the target output words should be kept in the sample and gets updated while a few non-target words are added as negative samples. Mathematically, given a target word w and its contextual word h, the goal is to maximize the log-likelihood on the training data, i.e.,\nmax J = logP (h|w),\nwhere P (h|w) is the conditional probability in the neural probabilistic language model that is usually defined as:\nP (h|w) = \u03c3(score(w, h)) = e score(w,h)\u2211\nw\u2208V e score(w,h)\n,\nwhere \u03c3()\u0307 is a softmax function that normalize real vector into a probability vector. Accordingly, the log-likelihood function can be written as:\nJ = score(w, h)\u2212 log( \u2211 w\u2208V escore(w,h)).\nNegative-sample is adopted here to avoid expensive computation over |V | words, i.e.,\nJ \u2032 = \u2211\nw,h\u2208D\nlogQ\u03b8(D = 1|w, h) + \u2211\nw,h\u2208D\u2032 logQ\u03b8(D = 0|w, h),\nwhere D is the observed data, D\u2032 is the unobserved data, \u03b8 is the embedding vector, and Q\u03b8(D = 1|w, h) is the probability of w and h being observed. The word embeddings can be computed by maximizing the log-likelihood function."
        },
        {
            "heading": "B. Parameter Settings",
            "text": "We tested dimensions of 20, 60 and 100 for word embeddings trained on EHR and PubMed and chose 100 for EHR and 60 for PubMed according to their performance in our intrinsic evaluation. Similarly, we chose the dimension of 100 for GloVe, and that of 300 for Google News since only 300 dimension was publicly available for Google News. The whole results of using different dimensions for word embeddings are provided in Appendix A. For training word embeddings on the EHR and PubMed, we set the window size to 5 words, the minimum word frequency to 7 (i.e., the words occurred less than 7 times in the corpus were ignored), and the negative sampling to 5. These parameters were selected based on previous studies [1], [3], [19].\nIV. DATA"
        },
        {
            "heading": "A. Text Corpora",
            "text": "We compared word embeddings computed from four different kinds of corpora. The first corpus, denoted as EHR, was from the Electronic Health Record system at Mayo Clinic. It contains textual clinical notes for a cohort\n6 of 113k patients receiving their primary care at Mayo Clinic, spanning a period of 15 years from 1998 to 2013. The vocabulary size |V | of this corpus is 103k. The second corpus, denoted as PubMed, is from a snapshot of the Open Access Subset of PMC in 2016. PMC is an online digital database of freely available full-text biomedical literature. The PubMed contains 1.25 million biomedical articles and 2 million distinct words (i.e., |V |). As comparisons, two additional public pre-trained word embeddings from two general English resources, i.e., Google News embeddings 11 and GloVe embeddings12, were also considered in the evaluation. The Google News embeddings have embeddings for 3 million words from Google News, trained using the wor2vec [1]. The GloVe embeddings have embeddings for 400k words from a snapshot of Wikipedia in 2014 and Gigaword Fifth Edition13, trained using the GloVe model [42]."
        },
        {
            "heading": "B. Pre-processing",
            "text": "The PubMed was pre-processed minimally by removing punctuations (one exception is that we replaced \u2018-\u2019 by \u2018 \u2019 if two words were connected by \u2018-\u2019 and we treated them as one word), lowercasing, and replacing all digits with \u201d7\u201d. We conducted additional pre-processing on the EHR since the narratives written by physicians are more sparse than research articles. Specifically, the section of \u201cFamily history\u201d in the corpus was removed if it was semi-structured [48]. See an example of the \u201dFamily history\u201d section in Table I\nTABLE I: An example of the \u201dFamily history\u201d section.\nMOTHER Stroke/TIA BROTHERS 4 brothers alive 1 brother deceased SISTERS 2 sisters alive DAUGHTERS 1 daughter alive Heart disease\nThe section of \u201dVital Signs\u201d was totally removed since it did not contain contextual information for training\nword embeddings. See an example of the \u201dVital Signs\u201d section in Table II.\nTABLE II: An example of the \u201dFamily history\u201d section.\nHeight: 149.1 cm. Weight: 44.5 kg. BSA(G): 1.3573 M2. BMI: 20.02 KG/M2.\nNote that these pre-processing strategies are specific for the clinical notes at Mayo Clinic. Moreover, we replace all text contractions with their respective complete text. For example, \u201dcan\u2019t\u201d is replaced with \u201dcan not\u201d. For clinical\n11https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing 12http://nlp.stanford.edu/data/glove.6B.zip 13https://catalog.ldc.upenn.edu/LDC2011T07\n7 corpus, we removed all the clinical notes metadata and note sections headers, dates, phone numbers, weight and height information and punctuations from clinical corpus. For PubMed articles, we removed all the websites url, email addresses, twitter handler and punctuations."
        },
        {
            "heading": "V. QUALITATIVE EVALUATION",
            "text": "The first experiment is qualitative evaluation by manually inspecting the five most similar words to a given target word. We used the commonly employed cosine similarity to calculate the most similar words. Suppose w1 and w2 are two words, the similarity between w1 and w2 is defined as\nsimilarity(w1, w2) = \u03b81 \u00b7 \u03b82 \u2016\u03b81\u2016\u2016\u03b82\u2016 , (1)\nwhere \u03b81 and \u03b82 are vector representations for w1 and w2 in the embedding space, respectively. If the target word is a medical phrase s1 consisting of multiple words, i.e., s1 = w1, w2, ..., wn, the similarity function becomes\nsimilarity(s1, w2) = \u03981\u03b8\u03072 \u2016\u03981\u2016\u2016\u03b82\u2016\n(2)\nwhere \u03981 = 1n \u2211n i \u03b8i is the representation for s1 in the embedding space. This is different from Pakhomov et al\u2019s study [32] where only single word terms were considered.\nTable III lists eight target words in three medical categories, i.e., disorder, symptom and drug, and their five most\nsimilar words induced by different word embeddings.\nFor the first target word describing an disorder, diabetes, EHR and PubMed find its synonym, mellitus, in the most similar words while GloVe and Google News fail to find it. EHR finds two terms related to co-morbidities of diabetes, cholesterolemia and dyslipidemia, and a most common modifier term, uncontrolled. PubMed finds terms relevant to co-existing conditions for diabetes, such as cardiovascular (very possibly from cardiovascular disease), nonalcoholic (very possibly from nonalcoholic fatty liver disease), obesity, and polycystic (very possibly from polycystic ovary syndrome which is a hyperandrogenic disorder that is associated with a high-risk of development of Type 2 diabetes). Most of these terms are from medical research topics and thus occur in the PubMed articles. GloVe finds two related terms, hypertension and obesity, while other three terms, i.e., arthritis, cancer and alzheimer, are less relevant disease names. Google News finds two morphological terms, diabetics and diabetic, relevant to the target words, one synonym, diabetes mellitus, and one related disease name, heart disease. We can draw similar conclusions for the second and third disorder words.\nThe dyspnea example in the symptom category demonstrates the advantage of EHR and PubMed. EHR finds palpitations, a common cause of dyspnea, and orthopnea, exertional, and doe (dyspnea on exertion) are synonyms or specific conditions for dyspnea. PubMed finds related symptoms, sweats and orthopnea, a synonym breathlessness, a relevant disorder hypotension, and a term relevant to the symptom rhonchi. Wikipedia finds synonyms shortness and breathlessness, and less relevant symptoms cyanosis and photophobia. Google News finds less relevant symptoms pruritus and rhinorrhea and less relevant disease nasopharyngitis. Similar observations can be found for sore throat and low blood pressure as well.\nThe drug category further differentials the word embeddings. In opioid, EHR finds opiate, benzodiazepine, sedative, polypharmacy, which are very relevant medications. PubMed finds nmda receptor, affective motivational,\n8\n20 10 0 10 20 20 10 0\n10 20 acetylcysteine\nacne activase\nactonel\nadenitis adenosine\nagitation\nagranulocytosis albumin\nalbuterol\nalcohol allergy\nallopurinol\namantadine\namiodarone\namoxil\nampicillin\nanemia\naneurysm\nangina\nanorexia\nanosmia\nanovulation\nantabuse\naphonia\nappendicitis\narteriosclerosis\narthralgia\narthritis\naspirin\nasthma\nataxia\natenolol\natherosclerosis\nativan\navandia\navelox\nbabesiosis\nbacitracin\nbacteremia\nbactroban\nbandemia\nbedwetting\nblanching\nblastomycoses\nbleomycin\nblepharospasm\nbronchitis\nbrucellosis\ncachexia\ncalamine\ncalan\ncandidiasis\ncarbatrol\ncarboplatin\ncardiomyopathies\ncardiomyopathy\ncardizem\ncardura\ncarsickness\ncatapres\ncataract cataracts\ncatatonia\ncatch\ncefaclor\ncefazolin cefepime\ncefoxitin\nceftazidime\nchills\nchloramphenicol\ncholestasis\ncholestyramine\ncipro\ncirrhosis isplatin\nclonus\nclubbing\ncluttering\ncoccidioidomycosis\ncodeine\ncolchicine\ncolitis\ncomatose\nconstipation\nconvulsion\ncoreg\ncortisone\ncoumadin\ncozaar\ncromolyn\ncrowning\ncyanosis\ndeafness\ndehydration\ndementia\ndementias\ndermatitis\ndermatomyositis\ndexamethasone\ndiabetes\nvomitingdiarrhea\ndigoxin\ndilantin diovandizziness\ndoxycyline\ndrooling\ndulcolax\nduragesic\ndysentery\ndyslipidemia\ndyspnea\ndysuria\nearache\nechinacea\nemaciation\nenalapril\nencephalitis\nepilepsy\nerythema\nerythromycin\nesmolol\nethanol\nexophthalmos\nfamvirfatigue\nfibrillation\nflatulence\nflushing\nfosamax\ngarlic\ngastroenteritis\ngiardiasis\nglaucoma\nglomerulosclerosis\nglucagen\nglucophageglucotrol\nglyburide\ngoiter\ngonorrhea\ngranulocytosis\ngrimaces\nhaemorrhoids\nhalitosis\nheadache\nheartburn\nhematemesis\nhemicrania\nhemiplegia\nhemochromatosis\nhemoglobinopathy\nhemophilia\nhemoptysis\nheparin\nhepatitis\nhepatomegalyhepatosplenomegalyherniahernias\nherpes\nhistoplasmosis\nhunger\nhyperacusis\nhyperesthesiahyperextension\nperglycemia\nhyperlipidemia\nhypersomnolence\nhyperthyroidism\nhypoproteinemia\nhypothyroidism\nhytrin\nence\ninfertility\ninfluenza\ninfluenzae\ninsulin\niron\nischemia\nisosorbide\nkeflex\nketamine\nketonuria\nlasix\nlethargy\nleukopenia\nlevaquin\nlevophed\nlidocaine\nlipitor lisinopril\nlisteriosis\nloperamide\nlopid\nlovastatin\nmacule\nmaculopapule\nmalaria\nmalnutrition\nmannerism\nmannitol\nmastodynia\nmedrol meningism\nmeningitis\nmetatarsalgia\nmethadone\nmethotrexate\nmittelschmerz\nmorphine\nmotrin\nmycoses\nmycosis\nmyelosuppression\nmyopathymyositis\nnarcan\nnausea\nnephritis\nneuralgia\nneuropathy\nnitroglycerine\nincontinnoctu ria\nnystagmus\nobesity\nosteoporosis\notitis\npain\npallor\npancreatitis\nparasitemia\nparesis\npenicillin\npepcid\nperitonitis\nphenobarbital\nphotopsia photosensitization\nplague\nplavix\npneumonia\npneu oniae\npolydipsiapolyuria\nprednisolone\nprilosec\nprobenecid\npropofol\npropranolol\nprostatism\nproteinuria\nprotonix\nprozac\npsoriasis\nrabies\nrales\nregurgitation\nreticulocytosis\nrheumatism\nhonchi\nr bitussin\nrogaine\nsandimmune\nschistosomiasis\nsciatica\nscleroderma\nseasickness\nseizures\nsepticemia\nserevent\nsilvade e\nsinemet singulair\nsleeplessness sluggishness\nsmallpox\nsnoring\nspasm\nspiriva\nstarvation\nstridor syncope\nsynthroid\nsyphilis\ntamiflu\nthalassemia\nthirsty\nthrombocytopen a\nthromboembolism\nthrombophilias\nthrombus\ntiredness\ntoothache\ntorticollis\ntrembling\ntremor\ntuberculosis\ntums\ntylenol\nultram\nurolithiasis\nvaccinia\nvancocin\nvasculitis\nvertigo\nvfend\nvicodin\nweakness\nwellbutrin\nwelts\nwheezing\nxanax\nxenical\nzantac\nzetia\nzithromax\nzocor\nzofran\nzoloft\nzovirax (a) EHR\n20 10 0 10 20\n20\n10\n0\n10\n20\nacetylcysteine\nacne\nactivaseactonel\nadenitis\nadenosine\nagitation\nagranulocytosis\nalbumin\nalbuterol\nalcohol\nallergy\nallopurinol\namantadine\namenorrhoea\namiodarone amoxil\nampicillin\namyloidoses\nanemia\naneurysm\nangina\nanorexia\nanosmia\nanovulation\nanoxemia\nantabuse\naphonia\nappendicitis\narteriosclerosis\narthralgia\narthritis\naspirin\nasthma\nataxia\natenolol\natherosclerosis\nativan\navandia\navelox\nbabesiosis\nbacitracin\nbacteremia\nbactroban\nbandemia\nbedwetting\nblanching\nbleomycin\nblepharospasm\nbronchitis\nbrucellosis\ncachexia\ncalamine\ncalan\ncamelpox\ncandidiases\ncandidia is\ncarboplatin\ncardialgia\ncardiomyopathiescardiomyopathy\ncardizem\ncardura\ncarsickness\ncataractcataracts\ncatatonia\ncatch\ncefaclor\nazidime cefazolin cefepime\ncefoxitin ceft\nceftiaxone\nchills\nchloramphenicol\ncholestasis\ncholestyramine\nchyluria\ncipro\ncirrhosis\ncisplatin\nclonus\nclubbing\ncluttering\ncoccidioidomycosis\ncodeine\ncolchicine\ncolitis\ncomatose\nconstipation\nconvulsion\ncoreg\ncorkscrewing\ncortisone\ncoumadin\ncozaar\ncromolyn\ncrowning\ncyan sis deafness\ndehydration\ndementia\ndementias\ndermatitisdermatomyositis\ndexamethasone\ndiabetes\ndiarrhea\ndigoxin\ndilantin diovan\ndizziness\ndoxycyline\ndrooling\ndulcolax\nduragesic\ndysentery\ndyslipidemia\ndyspnea\ndysuria\nearache\nechinacea\nemaciation\nenalapril\nencephalitis epilepsy\nerythema\nerythromycin\nesmolol\nethanol\nexophthalmos\nfatigue\nfibrillation\nflatulence\nflushing\nfosamax\ngarlic\ngastroenteritis\ngiardiasis\nglaucoma\nglomerulosclerosis\nglucagen\nglucophage\nglucotrol\nglyburide\ngoiter\ngonorrhea\ngranulocytosis\ngrimaces\nhyperemesis\nhaemorrhoids\nhalitosis\nheadacheheartburn\nhematemesis hemiballismus\nhemicrania\nhemiplegia\nhemochromatosis\nhemoglobinopathy\nhemophilia\nhemoptysis\nheparin\nhepatitis\nhepatomeg ly\nhepatosplenomegaly\nhernia\nhernias\nherpes\nhistoplasmosis\nhun er\nhyperacusis\nhyperesthesia\nhyperglycemiahyperlipidemia\nhyperoxia\nhypersomnolence\nhyperthyroidism\nhypoproteinemia\nhypothyroidism\nhytrin\nincontinence\ninfertility\ninfluenza\ninfluenzae\ninsulin\niron\nischemia\nischemias\nisosorbide\nkeflex\nketamine\nketonuria\nlasix\nlethargy\nleukopenia\nlevaquin\nlevophed\nlidocaine\nlipitor\nlisinopril\nlisteriosis\nloperamide\nlopid\nlovastatin\nmacule\nmaculopapule malaria\nmalnutrition\nmannerism\nmannitol\nmastodynia\nmedrol\nmeningism\nmeningitis\nmenouria\nmethadone\nmethotrexate\nmorphine\nmotrin\nmycoses\nmycosis\nmyelosuppression\nmyopathy\nyositis\nnarcan\nnausea\nnephritis\nneuralgia neuropathy\nnitroglycerine\nnocturia\nob sity\nopisthotonus\nosteoporosis\notitis\novernutrition\npain\npallor\npancreatitis\nparasitemia\npenicillinpepcid\nperitonitis\nphenobarbital\nphotopsia\nphotosensitization\nplague\nplavix\npneumonia\npneumoniae\npolydipsiapolyuria\nprednisolone\nprilosec\nprobenecid\npropofol\npropranolol\nprostatism\nproteinuria\nprotonixprozac\npsoriasis\npyorrhea\nrabies\nrales\nregurgitation\nreticulocytosis\nrheumatism\nrhonchi\nrobitussin rogaine\nsandimmune\nschistosomiasis\nsciatica\nscleroderma seizures\nsepticemia\nserevent\nsilvadene\nsinemet\nsingulair\nsleeplessness\nsluggishness\nsmallpox\nsnoring\nspasm\nspiriva\nstarvation\nstridor\nsyncope\nsynthroid\nsyphilis\ntamiflu\nthalassemia\nthirsty\nthrombocytopenia\nthromboembolismthrombophilias\nthrombus\ntiredness\ntoothache\ntorticollis\ntrembling\ntremor\ntuberculosis\ntums\ntylenol\nultram\nurolithiasis\nvaccinia\nvancoc\nvasculitis\nvermox\nvertigo\nvfend\nvicodin\nvirilism\nvomiting\nweakness\nwellbutrin\nwelts\nwheezing\nxanax xenical\nzantac\nzetia\nzithromax\nzocor\nzofran\nzoloft\nzovirax\nzoonosis\n(b) PubMed\n15 10 5 0 5 10 15\n10\n5\n0 5 10 acetylcysteine\nacne\nactivase actonel\nadenosine\nagit tion\nagranulocytosis airsickness albumin albuterol\nalcohol\nallergy\nallopurinol amantadine amenorrhoea amiodarone ampicillin\nanemia\naneurysm\nangina\nanorexia\nanosmia anovulation antabuse\nappendicitis arteriosclerosis arthralgia\narthritis\nasthma\nataxia atenolol a herosclerosis ativan avandia babesiosis bacitracin bacteremia bedwetting\nblanching\nbleomycin\nbronchitis\nbrucellosis cachexia calamine calan camelpox\ncandidiasis\ncarboplatin cardiomyopathycardizem cardura\ncataract cataracts\ncatatonia\ncatch\nchills\nchloramphenicol\ncholestasis cipro\ncirrhosis cisplatin clonus\nclubbing\ncluttering\ncoccidioidomycosis codeine colchicine\ncolitis\ncomatose\nconstipation\nconvulsion\ncoreg corkscrewing cortisone coumadin cozaar\ncrowning\ncyanosis\ndeafness\ndehydration\ndementia\ndementias\ndermatitis dermatomyositis dexamethasone\ndiabetes\ndiarrhea\ndigoxin dilantin diovan\ndizziness\ndrooling\nduragesic\ndysentery\ndyslipidemia dyspnea\nearache\nechinacea\nemaciation enalapril\nepilepsy\nerythema erythromycin ethanol exophthalmos famvir\nfatigue\nfibrillation\nflatulence\nflushing\nfosamax garlic\nga troenteritis\ngiardiasis\nglaucoma\nglomerulosclerosis glucophage goiter\ngrimaces\nhyperemesis\nhalitosis\nheadache\nheartburn\nhemiplegia\nhemochromatosis\nhemophilia hemoptysis heparin hepatomegaly hepatosplenomegaly herniahernias histoplasmosis hyperacusis hyperextension hyperglycemia hyperlipidemia hyperthyroidismothyroidism hytrin\nincontinenceinfertility\ninfluenza\ninfluenzae iron ischemiaisosorbide ketamine lasix\nlethargy\nleukopenia levaquin lidocaine lipitor lisinopril\nlisteriosis\nlovastatin mannerism mannitol\nmeningitis\nmethadone methotrexate morphine motrin\nmycosis myopathy myositis narcan\nnausea\nnephritis\nneuralgia\nneuropathy\nnitroglycerine\nnystagmus\nobesity\nosteoporosis\notitis\npain\npallor\npancreatitis paresis penicillin pepcid peritonitis phenobarbital\nplague\nplavix\npneumonia\npneumoniae polydipsia polyuria prednisolone prilosec probenecid propofol propranolol proteinuria protonix\npsoriasis\nrabies\nrales\nregurgitation\nrheumatism\nrobitussin rogaine\nschistosomiasis\nsciatica\nscleroderma\nseasickness\nseizures\ns pt cemia serevent singulair\nsleeplessness\nsluggishness\nsmallpox\nsnoring\nspasm\nsyncope synthroid tamiflu\nthalassemia\nthirsty\nthrombocytopenia thrombo mbolism thrombus\ntir dness\ntoothache\ntorticollis\ntrembling\ntremor tums tylenol\nvaccinia\nvasculitis\nvertigo\nvicodin\nvomiting weakness\nwelts\nwheezing\nxanax xenical zantac zetia zithromax zocor zofran zovirax\ncomatose zoonosis\n(c) GloVe\n30 20 10 0 10 20 30\n20\n15\n10\n5\n0\n5\n10\n15\n20\nacetylcysteine\nacne\nadenosine\nagitation\nagranulocytosis\nairsickness\nlb min\nalbuterol\nalcohol\nallergy\nallopurinol amantadine\namenorrhoea\namiodarone\namoxil\nampicillin\nanemia\naneurysm\nangina\nanorexia\nanosmia\nanovulation\nantabuse\nappendicitis\narteriosclerosis\narthralgia\narthritis\naspirin\nasthma\nataxi\natenolol\natherosclerosis\nativanavandia\nbabesiosis\nbacitracin\nbacteremia\nbedwetting\nblanching\nbleomycin\nblepharospasm\nbronchitis\nbrucellosis\ncachexia\ncalamine\ncandidiasis\ncarboplatin\ncardiomyopathie\ncardiomyopathy\ncardizem\ncarsickness\ncataractcataracts\ncatatonia\ncatch\ncefazolin\ncefepi e ceftazidime\nchills\nchloramphenicol\ncholestasis cholestyramine cipro\ncirrhosis\ncisplatin\nclubbing\ncluttering\ncoccidioidomycosis\ncodeine\ncolchicine\ncolitis\ncomatose\nconstipation\nconvulsio corkscrewing\ncortisone\ncoumadin\ncromolyn\nc owning\ncyanosisdeafness dehydration\ndementia dementias\ndermatitis\ndermatomyositis\ndexamethasone\ndiabetes\ndiarrhea\ndigoxin\ndilantin\ndiovan\ndizziness\ndrooling\ndysentery\ndyslipidemia\ndyspnea\ndysuria\nearache\nechinacea\nemaciation\nenalapril\nencephalitis\nepilepsy\nerythema\nerythromycin\nethanol\nfibrillation\nflatulence\nflushing\nfosamax\ngarlic\ngastroenteritis\ngiardiasis\nglaucoma glomerulosclerosis\nglucophage\nglyburide\ngoiter\ngonorrhea\ngrimaces\nhyperemesis\nhae orrhoidshalitosis\nheadacheheartburn\nhemiplegia\nhemochromatosis\nhemophilia\nhemoptysis\nheparin\nhepatitis\nhepatomegaly\nherniahernias\nherpes\nhistoplasmosis\nhunger\nhyperacusis\nhyperextension\nhyperglycemi\nhyperlipidemia\nhyperoxia\nhyperthyroidismhypothyroidism\nincontinence\ninfertility\ninfluenza\ninfluenzae\ninsulin\niron\nischemia\nisosorbide keflex\nketamine\nlasix\nlethargy\nleukopenia levaquin\nlidocaine\nlipitor\nlisinopril\nlisteriosis\nloperamide\nlovastatin\nmalaria\nmalnutrition\nmannerism\nmannitol\nmeningitis\nmetatarsalgia\nmethadone\nmethotrexate\nmorphine\nmotrin\nmycoses\nmycosis\nmyelosuppression\nmyopathymyositis\nnausea\nnephritis\nneuralgia\nn uropath\nnitroglycerine\nnocturia\nnystagmus\nobesity\nosteoporosis\notitis\novernutrition\npain\npallor\npancreatitis\nparasitemia\nparesis\npenicillin\nperitonitis\nphenobarbital\nplague\nplavix\npneumonia pneumoniae\npolydipsiaolyuria\nprednisolone\nprilosec\nprobenecid\npropofol\npropranolol\nproteinuria\nprozac\npsoriasis\nrabies\nrales\nregurgitation\nrheumatism\nrogaine\nschistosomiasis\nsciatica\nsclerod rm\nseasickness\nseizures\nepticemia\nsleeplessness\nsluggi hne s\nsmallpox\nsnoring\nspasm\nstarvation\nstridor\nsyncope\nsynthroid\nsyphilis\ntamiflu\nthalassemia\nthirsty\nthrombocytopenia thromboembolism\nthrombus\nfatiguetiredness toothache\ntorticollis\ntrembling\ntremor\ntuberculosis\ntums\ntylenol\nultram\nurolithiasis\nvaccinia\nvasculitis\nvertigo\nvicodin\nvomiting\nweakness\nwellbutrin\nwelts\nwheezing\nxanax\nxenical\nzan ac\nzithromax zoloftzovirax\nzoonosis\n(d) Google News\nFig. 1: Example clusters in different word embeddings.\nnaloxone precipitated, hyperlocomotion, whi h are related to th mechanism of action of opioid. GloVe finds analgesic and less relevant anti-inflammatory, and Google News finds opioid-related phrases and relevant term antipsychotics. In aspirin, EHR also finds very clinically relevant used terms and PubMed finds relevant terms in research articles while GloVe and G ogle New only find medication names.\nIt is obvious shown from these target words that EHR and PubMed can capture the semantics of medical terms better than GloVe and Google News and find more rel vant similar edical terms. However, EHR and PubMed find simil r medical terms fr m different perspectives due to their different focuses. EHR contains clinical narratives and thus it is closer to clinical languag . Y t, it contains terms with different morphologies and even typos, such as melitis, ca er nd thraot as listed in Table III. Differently, PubMed contains mo e medical terminology used in medical articles, and finds similar words mostly from a medical research perspective.\nIn order to visualize the semantics of medical terms captured by different word embeddings, we extracted 377 medical terms from the UMNSRS dataset [31], [32] and plotted the word embeddings for these medical terms using t-SNE [49]. Example clusters of medical terms n the word embeddings ar shown n Figure 1. Figure 1a depicts a cluster of symptoms in word embeddings trained on EHR, such as \u201cheartburn\u201d, \u201cvomiting\u201d and \u201cnausea\u201d. As shown in Figure 1b, a cluster of an antibiotic medications can be obviously observed from PubMed embeddings, such as \u201cbacitracin\u201d, \u201ccefoxitin\u201d, and \u201cchloramphenicol\u201d. Figures 1c and 1d illustrate clusters of symptoms in w rd embeddings trained from GloVe and Google News, respectively. Since we did not employ any clustering method, these clusters were intuitively ob erved from the visualization. The visualization of the wh le 377 medical terms can be found in Appendix B.\n9\n10"
        },
        {
            "heading": "VI. QUANTITATIVE EVALUATION",
            "text": "We tested word embeddings with quantitative evaluation to show the qualitative differences between them. We utilized extrinsic and intrinsic evaluation, where the former used four published datasets for measuring semantic similarity between medical terms and the latter used downstream biomedical tasks to evaluate word embeddings.\nA. Intrinsic Evaluation\nWe tested word embeddings on four published biomedical datasets commonly used to measure semantic similarity between medical terms. The first is Pedersen\u2019s dataset [28] that consists of 30 medical term pairs that were scored by physician experts according to their relatedness. The second is Hliaoutakis\u2019s dataset [29] consisting of 34 medical term pairs with similarity scores obtained by human judgments. The third one is MayoSRS dataset, developed by Pakhomov et al [30], which consists of 101 clinical term pairs whose relatedness was determined by nine medical coders and three physicians from the Mayo Clinic. The relatedness of each term pair was assessed based on a four point scale: (4.0) practically synonymous, (3.0) related, (2.0) marginally related and (1.0) unrelated. We evaluated word embeddings on the mean score of the physicians and medical coders. The fourth is UMNSRS similarity dataset, also developed by Pakhomov et al [31], which consists of 566 medical term pairs whose semantic similarity was determined independently by eight medical residents from the University of Minnesota Medical School. The similarity and relatedness of each term pair was annotated based on a continuous scale by having the resident touch a bar on a touch sensitive computer screen to indicate the degree of similarity or relatedness.\nFor each pair of medical terms in the testing datasets, we used Equations 1 and 2 to calculate the semantic similarity for each pair in the embeddings space. Since some medical terms might not exist in the vocabulary of word embeddings, we used fastText [50] to compute word vectors for these out-of-vocabulary medical terms. Pearson correlation coefficient was employed to calculate the correlation between similarity scores from human judgments and those from word embeddings. Table IV lists the results for the four datasets. Overall, the semantic similarity captured by the word embeddings trained on EHR are closer to human experts\u2019 judgments, compared with other word embeddings. PubMed performs worse than EHR but has a comparative result for the UMNSRS dataset. GloVe and Google News are inferior to EHR and PubMed and perform similarly in representing medical semantics. Note that the four datasets and corresponding semantic similarity scores from both human experts and word embeddings are provided in the supplementary Excel file.\n11"
        },
        {
            "heading": "B. Extrinsic Evaluation",
            "text": "Extrinsic evaluations are used to measure the contribution of word embeddings to specific biomedical tasks. In\nthis evaluation, we applied word embeddings to three prevalent shared tasks: clinical IE, biomedical IR, and RE.\n1) Clinical Information Extraction: We evaluated word embeddings on two clinical IE tasks. The first experiment is a shared task while the second is an institutional task. We would like to examine whether our local institutional word embeddings are better than external pre-trained word embeddings.\nIn the first experiment, we evaluated the word embeddings on an institutional information extraction task at Mayo Clinic. In this task, a set of 1000 radiology reports was examined to detect whether a hand and figure/wrist fracture could be identified. Reports were drawn from a cohort of residents of Olmsted County, aged 18 or older, who experienced fractures in 2009-2011. Each report was annotated by a medical expert with multiple years of experience abstracting fractures by assigning \u201c1\u201d if a hand and figure/wrist fracture was found, or \u201c0\u201d otherwise.\nIn our experiment, word embeddings were employed as features for machine learning models and evaluated by precision, recall and F1 scores [52]. For a clinical document d = {w1, w2, .., wM} where wi, i = 1, 2, ...,M is the ith word and M is the total number of words in this document, the feature vector x of document d is defined by\nx = 1\nM M\u2211 i xi,\nwhere xi is the embedding vector for word wi from the word embedding matrix. Then x was utilized as input to the machine learning models. A prevalent machine learning model, Support Vector Machine (SVM), was tested in this experiment. We performed 10-fold cross validation on the dataset and reported means of precision, recall and F1 scores from the cross validation. As comparison, the baseline method used term frequency features as input. Table V reports the means of precision, recall, and F1 score from the cross validation.\nThe word embeddings trained from EHR are superior to other word embeddings on all metrics (precision: 0.974, recall: 0.972, F1 score: 0.972). The fracture dataset in this experiment is curated from the same EHR system as the EHR corpus used to train word embeddings, and thus they have identical sublanguage characteristics embedded in word embeddings. The word embeddings trained on PubMed also have comparable results (precision: 0.946, recall: 0.943, F1 score: 0.942). Since this task is a medical task with specific medical terminologies, word embeddings trained on Google News have the worst performance. However, the word embeddings trained on GloVe are close to those trained on EHR with 0.02 difference on F1 score without statistically significance (p<0.01). This experiment shows that word embeddings trained on local corpus have the best performance for a local task but those trained on external Wikipedia corpus also have comparable performance.\n12\nWe secondly tested word embeddings on the i2b2 (Informatics for Integrating Biology to the Bedside) 2006 smoking status extraction shared task [51]. Participants of this task were asked to develop automatic NLP systems to determine the smoking status of patients from their discharge records in Partners HealthCare. For each discharge record, an automatic system should be able to categorize it into five pre-determined smoking status categories: past smoker, current smoker, smoker, non-smoker, and unknown, where a past and current smoker are distinguished based on temporal expressions in the patient\u2019s medical records. The dataset contains a total of 389 documents, including 35 documents of current smoker, 66 of non-smoker, 36 of past smoker, and 252 of unknown.\nThe experimental results are shown in Table VI. First, it is obviously shown that word embedding features perform better than term frequency features due to the semantics embedded in word embeddings. From Table VI, we can observe that using word embeddings trained on EHR has the best performance with a F1 score of 0.900. This result is interesting since the EHR corpus used to train the word embeddings is from a different institution. The reason might be that the smoking dataset has similar sublanguage characteristics as our EHR corpus. This result indicates that effective word embeddings can be shared across institutions for clinical NLP tasks. Another interesting observation is that the performance of word embeddings trained on Google News is close to that on EHR corpus with a comparable F1 score and a better recall. The performance difference is not statistically significant (p<0.01). This implies that word embeddings trained on a public dataset may not be definitely inferior to these trained on a medical specific dataset for a medical IE task. The likely cause is that the terminology used in smoking status extraction task also appears frequently in news, such as medications and advice for smokers.\n2) Biomedical Information Retrieval: We utilized Text REtreival Conference 2016 Clinical Decision Support (TREC 2016 CDS) track as the experimental data sets for evaluating word embeddings for biomedical IR. TREC 2016 CDS track focuses on biomedical literature retrieval that helps physicians find the precise literature information and make the best clinical decision at the point of care [53]. EHRs from MIMIC-III data set [54] were utilized to generate the query topics. Those topics are categorized into three most common types, namely Diagnosis, Test and Treatment, according to physicians\u2019 information needs, and 10 topics are provided for each type. Each topic is comprised of a note field (admission note), a description field (jargons and clinical abbreviations are removed) and a summary field (simplified version of the description). The participants are required to use only one of these three fields in their submissions and at least one submission must utilize the note field. Submitted systems should retrieve relevant biomedical articles from a given PMC article collection for each given query topic to answer three corresponding clinical questions: What is the patient\u2019s diagnosis? What tests should the patient receive? How should the patient be treated?. Each IR system can retrieve up to 1000 documents per query.\n13\nIn order to make comparison as fair as possible, we first implemented an IR system as a baseline system following [22] and then employed the simplest query expansion using word embeddings. That is, we expanded each query term with five most similar terms from word embeddings. Indri [55] was utilized as our indexing and retrieval tool. The preprocessing included stopword removal and Porter stemming. The stopword list was based on the PubMed stopwords 14. The article-id, title, abstract and body fields of each document were indexed. Language models with two-stage smoothing [56] was used to obtain all the retrieval results. Official metrics, Inferred Normalized Discounted Cumulated Gain (infNDCG) [57], Inferred Average Precision (infAP) [57], Precision at 10 (P@10) and Mean Average Precision (MAP), were utilized to measure the performance.\nTable VII shows the results on the TREC 2016 CDS track. It is interesting that query expansion using word embeddings almost does not improve retrieval performance and even worsen the performance when infAP and MAP were used as metrics. By comparing the retrieval performance of different word embeddings, we observe that EHR and PubMed perform slight better than GloVe and Google News without statistical significance (p<0.01). This result implies that applying different word embeddings trained from different resources has almost no significant difference for biomedical IR tasks. We also note that this may also be due to the query expansion method used in our evaluation.\n3) Relation Extraction: Drug-drug interaction (DDI) extraction is a specific RE task in the biomedical domain. DDI is an unexpected change in a drug\u2019s effect on the human body when the drug and a second drug are coprescribed and taken together. Automatic extracting DDI information from literature is a challenging and important research topic since the volume of the published literature grows rapidly and greatly. In this experiment, we evaluate word embeddings on DDIExtraction 2013 challenge corpus [58]. The dataset for DDIExtraction 2013 was composed of sentences describing DDIs from the DrugBank database and MedLine abstracts. In this dataset, drug entities and DDIs were annotated at the sentence level and each sentence could contain two or more drugs. An RE system should be able to automatically extract DDI drug pairs from a sentence. We leveraged the baseline system introduced in [19] and evaluated word embeddings by concatenating to the baseline features. We utilized Random Forest [59] as the classifier since it had the best performance in [19]. F1 score was used as the evaluation metric.\nTable VIII shows the results on the DDIExtraction 2013 challenge. We can see that the overall performance of word embeddings trained on Google News is the best. The reason is that the semantics of general English terms in the context of drug mentions are more important for determining the drug interactions. For example, in the\n14http://www.ncbi.nlm.nih.gov/books/NBK3827/table/pubmedhelp.T.stopwords/\n14\nsentence \u201cAcarbose may interact with metformin\u201d, the term \u201cinteract\u201d is crucial to classify the relation. Since these crucial terms are generally not medical terminology, word embeddings trained on Google News where the corpus represents general English are able to capture the semantics of these terms. However, the superiority of Google News to other corpora is minor for this task. Another interesting observation is that word embeddings trained from PubMed have the best performance for the DrugBank corpus while these from Google News perform the best for the MedLine corpus. Though MedLine abstracts are from PubMed articles, this result shows that word embeddings trained from the same corpus are not necessarily superior to other embeddings."
        },
        {
            "heading": "VII. CONCLUSION AND DISCUSSION",
            "text": "In this study, we provided an empirical evaluation of word embeddings trained from four different resources, namely clinical notes, biomedical publications, Wikipedia, and news. We performed the evaluation qualitatively and quantitatively. In qualitative evaluation, we selected a set of medical words and evaluated the five most similar medical words. We then analyzed word embeddings through the visualization of those word embeddings. We conducted extrinsic and intrinsic evaluation for quantitative evaluation. Intrinsic evaluation directly tested semantic relationships between medical words using four published datasets for measuring semantic similarity between medical terms while extrinsic evaluation evaluated word embeddings in three downstream biomedical NLP applications, i.e., clinical IE, biomedical IR, and RE.\nBased on the evaluation results, we can draw the following conclusions. First, the word embeddings trained on EHR and PubMed can capture the semantics of medical terms better than those trained on GloVe and Google News and find more relevant similar medical terms. However, EHR finds similar terms vis a vis clinical language while PubMed contains more medical terminology used in medical articles, and finds similar words mostly from a medical research perspective. Second, the medical semantic similarity captured by the word embeddings trained on EHR and PubMed are closer to human experts\u2019 judgments, compared to these trained on GloVe and Google News. Third, there does not exist a consistent global ranking of word embedding quality for downstream biomedical NLP applications. However, adding word embeddings as extra features will improve results on most downstream tasks. Finally, word embeddings trained from biomedical domain corpora do not necessarily have better performance than those trained on other general domain corpora. That is, there might be no significant difference when word embeddings trained from an out-domain corpus were employed for a biomedical NLP application. However, the performance of word embeddings trained from a local corpus is better for local NLP tasks.\nIn a biomedical NLP application, our experiment implicitly show that applying word embeddings trained from corpora in a general domain, such as Wikipedia and news, is not significantly inferior to applying those obtained\n15\nfrom biomedical or clinical domain, which is usually difficult to access due to privacy. This result is consistent with but more general than the conclusion drawn in [32]. Thus, lack of access to a domain-specific corpus is not necessarily a barrier to the use of word embeddings in practical implementations.\nAs a future direction, we would like to evaluate word embeddings on more downstream biomedical NLP applications, such as medial named entity recognition and clinical note summarization. We will investigate whether different word embeddings represent language characteristics differently for a corpus, such as term frequency and medical concepts. We also want to assess word embeddings across health care institutions using different EHR systems and investigate how sublanguage characteristics affect the portability of word embeddings. Moreover, we want to apply clustering methods on word embeddings and compare the word-level and concept-level difference between clusters of medical terms."
        },
        {
            "heading": "VIII. ACKNOWLEDGEMENT",
            "text": "This work has been supported by the National Institute of Health (NIH) grants R01LM011934 and R01GM102282."
        },
        {
            "heading": "APPENDIX A",
            "text": "RESULTS OF USING DIFFERENT DIMENSIONS FOR WORD EMBEDDINGS.\n18"
        },
        {
            "heading": "APPENDIX B",
            "text": "VISUALIZATION OF WORD EMBEDDINGS.\n19\n20\n21"
        }
    ],
    "title": "A Comparison of Word Embeddings for the Biomedical Natural Language Processing",
    "year": 2022
}