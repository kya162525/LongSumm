{
    "abstractText": "This work aims to address the problem of image-based question-answering (QA) with new models and datasets. In our work, we propose to use neural networks and visual semantic embeddings, without intermediate stages such as object detection and image segmentation, to predict answers to simple questions about images. Our model performs 1.8 times better than the only published results on an existing image QA dataset. We also present a question generation algorithm that converts image descriptions, which are widely available, into QA form. We used this algorithm to produce an order-of-magnitude larger dataset, with more evenly distributed answers. A suite of baseline results on this new dataset are also presented.",
    "authors": [
        {
            "affiliations": [],
            "name": "Mengye Ren"
        },
        {
            "affiliations": [],
            "name": "Ryan Kiros"
        },
        {
            "affiliations": [],
            "name": "Richard S. Zemel"
        }
    ],
    "id": "SP:35b0331dfcd2897abd5749b49ff5e2b8ba0f7a62",
    "references": [
        {
            "authors": [
                "O. Vinyals",
                "A. Toshev",
                "S. Bengio",
                "D. Erhan"
            ],
            "title": "Show and tell: A neural image caption generator",
            "venue": "CVPR, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "R. Kiros",
                "R. Salakhutdinov",
                "R.S. Zemel"
            ],
            "title": "Unifying visual-semantic embeddings with multimodal neural language models",
            "venue": "TACL, 2015. 8",
            "year": 2015
        },
        {
            "authors": [
                "A. Karpathy",
                "A. Joulin",
                "L. Fei-Fei"
            ],
            "title": "Deep fragment embeddings for bidirectional image sentence mapping",
            "venue": "NIPS, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "J. Mao",
                "W. Xu",
                "Y. Yang",
                "J. Wang",
                "A.L. Yuille"
            ],
            "title": "Explain images with multimodal recurrent neural networks",
            "venue": "NIPS Deep Learning Workshop, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "J. Donahue",
                "L.A. Hendricks",
                "S. Guadarrama",
                "M. Rohrbach",
                "S. Venugopalan",
                "K. Saenko",
                "T. Darrell"
            ],
            "title": "Long-term recurrent convolutional networks for visual recognition and description",
            "venue": "CVPR, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "X. Chen",
                "C.L. Zitnick"
            ],
            "title": "Learning a recurrent visual representation for image caption generation",
            "venue": "CoRR, vol. abs/1411.5654, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "H. Fang",
                "S. Gupta",
                "F.N. Iandola",
                "R. Srivastava",
                "L. Deng",
                "P. Doll\u00e1r",
                "J. Gao",
                "X. He",
                "M. Mitchell",
                "J.C. Platt",
                "C.L. Zitnick",
                "G. Zweig"
            ],
            "title": "From captions to visual concepts and back",
            "venue": "CVPR, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "K. Xu",
                "J. Ba",
                "R. Kiros",
                "K. Cho",
                "A.C. Courville",
                "R. Salakhutdinov",
                "R.S. Zemel",
                "Y. Bengio"
            ],
            "title": "Show, attend and tell: Neural image caption generation with visual attention",
            "venue": "ICML, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "R. Lebret",
                "P.O. Pinheiro",
                "R. Collobert"
            ],
            "title": "Phrase-based image captioning",
            "venue": "ICML, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "B. Klein",
                "G. Lev",
                "G. Lev",
                "L. Wolf"
            ],
            "title": "Fisher vectors derived from hybrid Gaussian-Laplacian mixture models for image annotations",
            "venue": "CVPR, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "M. Malinowski",
                "M. Fritz"
            ],
            "title": "Towards a visual Turing challenge",
            "venue": "NIPS Workshop on Learning Semantics, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "N. Silberman",
                "D. Hoiem",
                "P. Kohli",
                "R. Fergus"
            ],
            "title": "Indoor segmentation and support inference from RGBD images",
            "venue": "ECCV, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "S. Antol",
                "A. Agrawal",
                "J. Lu",
                "M. Mitchell",
                "D. Batra",
                "C.L. Zitnick",
                "D. Parikh"
            ],
            "title": "VQA: Visual Question Answering",
            "venue": "CoRR, vol. abs/1505.00468, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "M. Malinowski",
                "M. Rohrbach",
                "M. Fritz"
            ],
            "title": "Ask Your Neurons: A Neural-based Approach to Answering Questions about Images",
            "venue": "CoRR, vol. abs/1505.01121, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "H. Gao",
                "J. Mao",
                "J. Zhou",
                "Z. Huang",
                "L. Wang",
                "W. Xu"
            ],
            "title": "Are you talking to a machine? dataset and methods for multilingual image question answering",
            "venue": "CoRR, vol. abs/1505.05612, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "L. Ma",
                "Z. Lu",
                "H. Li"
            ],
            "title": "Learning to answer questions from image using convolutional neural network",
            "venue": "CoRR, vol. abs/1506.00333, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "T. Lin",
                "M. Maire",
                "S. Belongie",
                "J. Hays",
                "P. Perona",
                "D. Ramanan",
                "P. Doll\u00e1r",
                "C.L. Zitnick"
            ],
            "title": "Microsoft COCO: Common Objects in Context",
            "venue": "ECCV, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "X. Chen",
                "H. Fang",
                "T.-Y. Lin",
                "R. Vedantam",
                "S. Gupta",
                "P. Dollar",
                "C.L. Zitnick"
            ],
            "title": "Microsoft COCO captions: Data collection and evaluation server",
            "venue": "CoRR, vol. abs/1504.00325, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "S. Hochreiter",
                "J. Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural Computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.",
            "year": 1997
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "ICLR, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "O. Russakovsky",
                "J. Deng",
                "H. Su",
                "J. Krause",
                "S. Satheesh",
                "S. Ma",
                "Z. Huang",
                "A. Karpathy",
                "A. Khosla",
                "M.S. Bernstein",
                "A.C. Berg",
                "L. Fei-Fei"
            ],
            "title": "Imagenet large scale visual recognition challenge",
            "venue": "IJCV, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "T. Mikolov",
                "K. Chen",
                "G. Corrado",
                "J. Dean"
            ],
            "title": "Efficient estimation of word representations in vector space",
            "venue": "ICLR, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "A. Frome",
                "G.S. Corrado",
                "J. Shlens",
                "S. Bengio",
                "J. Dean",
                "M. Ranzato",
                "T. Mikolov"
            ],
            "title": "DeViSE: A deep visual-semantic embedding model",
            "venue": "NIPS, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "M. Hodosh",
                "P. Young",
                "J. Hockenmaier"
            ],
            "title": "Framing image description as a ranking task: Data, models and evaluation metrics",
            "venue": "J. Artif. Intell. Res. (JAIR), vol. 47, pp. 853\u2013899, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "V. Ordonez",
                "G. Kulkarni",
                "T.L. Berg"
            ],
            "title": "Im2text: Describing images using 1 million captioned photographs",
            "venue": "NIPS, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "D. Klein",
                "C.D. Manning"
            ],
            "title": "Accurate unlexicalized parsing",
            "venue": "ACL, 2003.",
            "year": 2003
        },
        {
            "authors": [
                "N. Chomsky"
            ],
            "title": "Conditions on Transformations",
            "year": 1973
        },
        {
            "authors": [
                "C. Fellbaum",
                "Ed"
            ],
            "title": "WordNet An Electronic Lexical Database",
            "year": 1998
        },
        {
            "authors": [
                "S. Bird"
            ],
            "title": "NLTK: the natural language toolkit",
            "venue": "ACL, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "J. Devlin",
                "S. Gupta",
                "R. Girshick",
                "M. Mitchell",
                "C.L. Zitnick"
            ],
            "title": "Exploring nearest neighbor approaches for image captioning",
            "venue": "CoRR, vol. abs/1505.04467, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "Z. Wu",
                "M. Palmer"
            ],
            "title": "Verb semantics and lexical selection",
            "venue": "ACL, 1994.",
            "year": 1994
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Combining image understanding and natural language interaction is one of the grand dreams of artificial intelligence. We are interested in the problem of jointly learning image and text through a question-answering task. Recently, researchers studying image caption generation [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] have developed powerful methods of jointly learning from image and text inputs to form higher level representations from models such as convolutional neural networks (CNNs) trained on object recognition, and word embeddings trained on large scale text corpora. Image QA involves an extra layer of interaction between human and computers. Here the model needs to pay attention to details of the image instead of describing it in a vague sense. The problem also combines many computer vision sub-problems such as image labeling and object detection.\nIn this paper we present our contributions to the problem: a generic end-to-end QA model using visual semantic embeddings to connect a CNN and a recurrent neural net (RNN), as well as comparisons to a suite of other models; an automatic question generation algorithm that converts description sentences into questions; and a new QA dataset (COCO-QA) that was generated using the algorithm, and a number of baseline results on this new dataset.\nIn this work we assume that the answers consist of only a single word, which allows us to treat the problem as a classification problem. This also makes the evaluation of the models easier and more robust, avoiding the thorny evaluation issues that plague multi-word generation problems."
        },
        {
            "heading": "2 Related Work",
            "text": "Malinowski and Fritz [11] released a dataset with images and question-answer pairs, the DAtaset for QUestion Answering on Real-world images (DAQUAR). All images are from the NYU depth v2 dataset [12], and are taken from indoor scenes. Human segmentation, image depth values, and object labeling are available in the dataset. The QA data has two sets of configurations, which differ by the\nar X\niv :1\n50 5.\n02 07\n4v 4\n[ cs\n.L G\n] 2\n9 N\nov 2\nnumber of object classes appearing in the questions (37-class and 894-class). There are mainly three types of questions in this dataset: object type, object color, and number of objects. Some questions are easy but many questions are very hard to answer even for humans. Since DAQUAR is the only publicly available image-based QA dataset, it is one of our benchmarks to evaluate our models.\nTogether with the release of the DAQUAR dataset, Malinowski and Fritz presented an approach which combines semantic parsing and image segmentation. Their approach is notable as one of the first attempts at image QA, but it has a number of limitations. First, a human-defined possible set of predicates are very dataset-specific. To obtain the predicates, their algorithm also depends on the accuracy of the image segmentation algorithm and image depth information. Second, their model needs to compute all possible spatial relations in the training images. Even though the model limits this to the nearest neighbors of the test images, it could still be an expensive operation in larger datasets. Lastly the accuracy of their model is not very strong. We show below that some simple baselines perform better.\nVery recently there has been a number of parallel efforts on both creating datasets and proposing new models [13, 14, 15, 16]. Both Antol et al. [13] and Gao et al. [15] used MS-COCO [17] images and created an open domain dataset with human generated questions and answers. In Anto et al.\u2019s work, the authors also included cartoon pictures besides real images. Some questions require logical reasoning in order to answer correctly. Both Malinowski et al. [14] and Gao et al. [15] use recurrent networks to encode the sentence and output the answer. Whereas Malinowski et al. use a single network to handle both encoding and decoding, Gao et al. used two networks, a separate encoder and decoder. Lastly, bilingual (Chinese and English) versions of the QA dataset are available in Gao et al.\u2019s work. Ma et al. [16] use CNNs to both extract image features and sentence features, and fuse the features together with another multi-modal CNN.\nOur approach is developed independently from the work above. Similar to the work of Malinowski et al. and Gao et al., we also experimented with recurrent networks to consume the sequential question input. Unlike Gao et al., we formulate the task as a classification problem, as there is no single well- accepted metric to evaluate sentence-form answer accuracy [18]. Thus, we place more focus on a limited domain of questions that can be answered with one word. We also formulate and evaluate a range of other algorithms, that utilize various representations drawn from the question and image, on these datasets."
        },
        {
            "heading": "3 Proposed Methodology",
            "text": "The methodology presented here is two-fold. On the model side we develop and apply various forms of neural networks and visual-semantic embeddings on this task, and on the dataset side we propose new ways of synthesizing QA pairs from currently available image description datasets."
        },
        {
            "heading": "3.1 Models",
            "text": "In recent years, recurrent neural networks (RNNs) have enjoyed some successes in the field of natural language processing (NLP). Long short-term memory (LSTM) [19] is a form of RNN which is easier to train than standard RNNs because of its linear error propagation and multiplicative gatings. Our model builds directly on top of the LSTM sentence model and is called the \u201cVIS+LSTM\u201d model. It treats the image as one word of the question. We borrowed this idea of treating the image as a word from caption generation work done by Vinyals et al. [1]. We compare this newly proposed model with a suite of simpler models in the Experimental Results section.\n1. We use the last hidden layer of the 19-layer Oxford VGG Conv Net [20] trained on ImageNet 2014 Challenge [21] as our visual embeddings. The CNN part of our model is kept frozen during training.\n2. We experimented with several different word embedding models: randomly initialized embedding, dataset-specific skip-gram embedding and general-purpose skip-gram embedding model [22]. The word embeddings are trained with the rest of the model.\n3. We then treat the image as if it is the first word of the sentence. Similar to DeViSE [23], we use a linear or affine transformation to map 4096 dimension image feature vectors to a 300 or 500 dimensional vector that matches the dimension of the word embeddings.\n4. We can optionally treat the image as the last word of the question as well through a different weight matrix and optionally add a reverse LSTM, which gets the same content but operates in a backward sequential fashion.\n5. The LSTM(s) outputs are fed into a softmax layer at the last timestep to generate answers."
        },
        {
            "heading": "3.2 Question-Answer Generation",
            "text": "The currently available DAQUAR dataset contains approximately 1500 images and 7000 questions on 37 common object classes, which might be not enough for training large complex models. Another problem with the current dataset is that simply guessing the modes can yield very good accuracy.\nWe aim to create another dataset, to produce a much larger number of QA pairs and a more even distribution of answers. While collecting human generated QA pairs is one possible approach, and another is to synthesize questions based on image labeling, we instead propose to automatically convert descriptions into QA form. In general, objects mentioned in image descriptions are easier to detect than the ones in DAQUAR\u2019s human generated questions, and than the ones in synthetic QAs based on ground truth labeling. This allows the model to rely more on rough image understanding without any logical reasoning. Lastly the conversion process preserves the language variability in the original description, and results in more human-like questions than questions generated from image labeling.\nAs a starting point we used the MS-COCO dataset [17], but the same method can be applied to any other image description dataset, such as Flickr [24], SBU [25], or even the internet."
        },
        {
            "heading": "3.2.1 Pre-Processing & Common Strategies",
            "text": "We used the Stanford parser [26] to obtain the syntatic structure of the original image description. We also utilized these strategies for forming the questions.\n1. Compound sentences to simple sentences Here we only consider a simple case, where two sentences are joined together with a conjunctive word. We split the orginial sentences into two independent sentences.\n2. Indefinite determiners \u201ca(n)\u201d to definite determiners \u201cthe\u201d. 3. Wh-movement constraints\nIn English, questions tend to start with interrogative words such as \u201cwhat\u201d. The algorithm needs to move the verb as well as the \u201cwh-\u201d constituent to the front of the sentence. For example: \u201cA man is riding a horse\u201d becomes \u201cWhat is the man riding?\u201d In this work we consider the following two simple constraints: (1) A-over-A principle which restricts the movement of a whword inside a noun phrase (NP) [27]; (2) Our algorithm does not move any wh-word that is contained in a clause constituent."
        },
        {
            "heading": "3.2.2 Question Generation",
            "text": "Question generation is still an open-ended topic. Overall, we adopt a conservative approach to generating questions in an attempt to create high-quality questions. We consider generating four types of questions below:\n1. Object Questions: First, we consider asking about an object using \u201cwhat\u201d. This involves replacing the actual object with a \u201cwhat\u201d in the sentence, and then transforming the sentence structure so that the \u201cwhat\u201d appears in the front of the sentence. The entire algorithm has the following stages: (1) Split long sentences into simple sentences; (2) Change indefinite determiners to definite determiners; (3) Traverse the sentence and identify potential answers and replace with \u201cwhat\u201d. During the traversal of object-type question generation, we currently ignore all the prepositional phrase (PP) constituents; (4) Perform wh-movement. In order to identify a possible answer word, we used WordNet [28] and the NLTK software package [29] to get noun categories.\n2. Number Questions: We follow a similar procedure as the previous algorithm, except for a different way to identify potential answers: we extract numbers from original sentences. Splitting compound sentences, changing determiners, and wh-movement parts remain the same.\n3. Color Questions: Color questions are much easier to generate. This only requires locating the color adjective and the noun to which the adjective attaches. Then it simply forms a sentence \u201cWhat is the color of the [object]\u201d with the \u201cobject\u201d replaced by the actual noun.\n4. Location Questions: These are similar to generating object questions, except that now the answer traversal will only search within PP constituents that start with the preposition \u201cin\u201d. We also added rules to filter out clothing so that the answers will mostly be places, scenes, or large objects that contain smaller objects."
        },
        {
            "heading": "3.2.3 Post-Processing",
            "text": "We rejected the answers that appear too rarely or too often in our generated dataset. After this QA rejection process, the frequency of the most common answer words was reduced from 24.98% down to 7.30% in the test set of COCO-QA."
        },
        {
            "heading": "4 Experimental Results",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "Table 1 summarizes the statistics of COCO-QA. It should be noted that since we applied the QA pair rejection process, mode-guessing performs very poorly on COCO-QA. However, COCO-QA questions are actually easier to answer than DAQUAR from a human point of view. This encourages the model to exploit salient object relations instead of exhaustively searching all possible relations. COCO-QA dataset can be downloaded at http://www.cs.toronto.edu/\u02dcmren/ imageqa/data/cocoqa\nHere we provide some brief statistics of the new dataset. The maximum question length is 55, and average is 9.65. The most common answers are \u201ctwo\u201d (3116, 2.65%), \u201cwhite\u201d (2851, 2.42%), and \u201cred\u201d (2443, 2.08%). The least common are \u201ceagle\u201d (25, 0.02%) \u201ctram\u201d (25, 0.02%), and \u201csofa\u201d (25, 0.02%). The median answer is \u201cbed\u201d (867, 0.737%). Across the entire test set (38,948 QAs), 9072 (23.29%) overlap in training questions, and 7284 (18.70%) overlap in training question-answer pairs."
        },
        {
            "heading": "4.2 Model Details",
            "text": "1. VIS+LSTM: The first model is the CNN and LSTM with a dimensionality-reduction weight matrix in the middle; we call this \u201cVIS+LSTM\u201d in our tables and figures.\n2. 2-VIS+BLSTM: The second model has two image feature inputs, at the start and the end of the sentence, with different learned linear transformations, and also has LSTMs going in both the forward and backward directions. Both LSTMs output to the softmax layer at the last timestep. We call the second model \u201c2-VIS+BLSTM\u201d.\n3. IMG+BOW: This simple model performs multinomial logistic regression based on the image features without dimensionality reduction (4096 dimension), and a bag-of-word (BOW) vector obtained by summing all the learned word vectors of the question.\n4. FULL: Lastly, the \u201cFULL\u201d model is a simple average of the three models above.\nWe release the complete details of the models at https://github.com/renmengye/ imageqa-public."
        },
        {
            "heading": "4.3 Baselines",
            "text": "To evaluate the effectiveness of our models, we designed a few baselines.\n1. GUESS: One very simple baseline is to predict the mode based on the question type. For example, if the question contains \u201chow many\u201d then the model will output \u201ctwo.\u201d In DAQUAR, the modes are \u201ctable\u201d, \u201ctwo\u201d, and \u201cwhite\u201d and in COCO-QA, the modes are \u201ccat\u201d, \u201ctwo\u201d, \u201cwhite\u201d, and \u201croom\u201d.\n2. BOW: We designed a set of \u201cblind\u201d models which are given only the questions without the images. One of the simplest blind models performs logistic regression on the BOW vector to classify answers.\n3. LSTM: Another \u201cblind\u201d model we experimented with simply inputs the question words into the LSTM alone.\n4. IMG: We also trained a counterpart \u201cdeaf\u201d model. For each type of question, we train a separate CNN classification layer (with all lower layers frozen during training). Note that this model knows the type of question, in order to make its performance somewhat comparable to models that can take into account the words to narrow down the answer space. However the model does not know anything about the question except the type.\n5. IMG+PRIOR: This baseline combines the prior knowledge of an object and the image understanding from the \u201cdeaf model\u201d. For example, a question asking the color of a white bird flying in the blue sky may output white rather than blue simply because the prior probability of the bird being blue is lower. We denote c as the color, o as the class of the object of interest, and x as the\nimage. Assuming o and x are conditionally independent given the color,\np(c|o, x) = p(c, o|x)\u2211 c\u2208C p(c, o|x) = p(o|c, x)p(c|x)\u2211 c\u2208C p(o|c, x)p(c|x) = p(o|c)p(c|x)\u2211 c\u2208C p(o|c)p(c|x)\n(1)\nThis can be computed if p(c|x) is the output of a logistic regression given the CNN features alone, and we simply estimate p(o|c) empirically: p\u0302(o|c) = count(o,c)count(c) . We use Laplace smoothing on this empirical distribution.\n6. K-NN: In the task of image caption generation, Devlin et al. [30] showed that a nearest neighbors baseline approach actually performs very well. To see whether our model memorizes the training data for answering new question, we include a K-NN baseline in the results. Unlike image caption generation, here the similarity measure includes both image and text. We use the bag-ofwords representation learned from IMG+BOW, and append it to the CNN image features. We use Euclidean distance as the similarity metric; it is possible to improve the nearest neighbor result by learning a similarity metric."
        },
        {
            "heading": "4.4 Performance Metrics",
            "text": "To evaluate model performance, we used the plain answer accuracy as well as the Wu-Palmer similarity (WUPS) measure [31, 32]. The WUPS calculates the similarity between two words based on their longest common subsequence in the taxonomy tree. If the similarity between two words is less than a threshold then a score of zero will be given to the candidate answer. Following Malinowski and Fritz [32], we measure all models in terms of accuracy, WUPS 0.9, and WUPS 0.0."
        },
        {
            "heading": "4.5 Results and Analysis",
            "text": "Table 2 summarizes the learning results on DAQUAR and COCO-QA. For DAQUAR we compare our results with [32] and [14]. It should be noted that our DAQUAR results are for the portion of the dataset (98.3%) with single-word answers. After the release of our paper, Ma et al. [16] claimed to achieve better results on both datasets.\nFrom the above results we observe that our model outperforms the baselines and the existing approach in terms of answer accuracy and WUPS. Our VIS+LSTM and Malinkowski et al.\u2019s recurrent neural network model [14] achieved somewhat similar performance on DAQUAR. A simple average of all three models further boosts the performance by 1-2%, outperforming other models.\nIt is surprising to see that the IMG+BOW model is very strong on both datasets. One limitation of our VIS+LSTM model is that we are not able to consume image features as large as 4096 dimensions at one time step, so the dimensionality reduction may lose some useful information. We tried to give IMG+BOW a 500 dim. image vector, and it does worse than VIS+LSTM (\u224848%).\nBy comparing the blind versions of the BOW and LSTM models, we hypothesize that in Image QA tasks, and in particular on the simple questions studied here, sequential word interaction may not be as important as in other natural language tasks.\nIt is also interesting that the blind model does not lose much on the DAQUAR dataset, We speculate that it is likely that the ImageNet images are very different from the indoor scene images, which are mostly composed of furniture. However, the non-blind models outperform the blind models by a large margin on COCO-QA. There are three possible reasons: (1) the objects in MS-COCO resemble the ones in ImageNet more; (2) MS-COCO images have fewer objects whereas the indoor scenes have considerable clutter; and (3) COCO-QA has more data to train complex models.\nThere are many interesting examples but due to space limitations we can only show a few in Figure 1 and Figure 3; full results are available at http://www.cs.toronto.edu/\u02dcmren/ imageqa/results. For some of the images, we added some extra questions (the ones have an \u201ca\u201d in the question ID); these provide more insight into a model\u2019s representation of the image and question information, and help elucidate questions that our models may accidentally get correct. The parentheses in the figures represent the confidence score given by the softmax layer of the respective model.\nModel Selection: We did not find that using different word embedding has a significant impact on the final classification results. We observed that fine-tuning the word embedding results in better performance and normalizing the CNN hidden image features into zero-mean and unit-variance helps achieve faster training time. The bidirectional LSTM model can further boost the result by a little.\nObject Questions: As the original CNN was trained for the ImageNet challenge, the IMG+BOW benefited significantly from its single object recognition ability. However, the challenging part is to consider spatial relations between multiple objects and to focus on details of the image. Our models only did a moderately acceptable job on this; see for instance the first picture of Figure 1 and the fourth picture of Figure 3. Sometimes a model fails to make a correct decision but outputs the most salient object, while sometimes the blind model can equally guess the most probable objects based on the question alone (e.g., chairs should be around the dining table). Nonetheless, the FULL model improves accuracy by 50% compared to IMG model, which shows the difference between pure object classification and image question answering.\nCounting: In DAQUAR, we could not observe any advantage in the counting ability of the IMG+BOW and the VIS+LSTM model compared to the blind baselines. In COCO-QA there is some observable counting ability in very clean images with a single object type. The models can sometimes count up to five or six. However, as shown in the second picture of Figure 3, the ability is fairly weak as they do not count correctly when different object types are present. There is a lot of room for improvement in the counting task, and in fact this could be a separate computer vision problem on its own.\nColor: In COCO-QA there is a significant win for the IMG+BOW and the VIS+LSTM against the blind ones on color-type questions. We further discovered that these models are not only able to recognize the dominant color of the image but sometimes associate different colors to different objects, as shown in the first picture of Figure 3. However, they still fail on a number of easy\nexamples. Adding prior knowledge provides an immediate gain on the IMG model in terms of accuracy on Color and Number questions. The gap between the IMG+PRIOR and IMG+BOW shows some localized color association ability in the CNN image representation."
        },
        {
            "heading": "5 Conclusion and Current Directions",
            "text": "In this paper, we consider the image QA problem and present our end-to-end neural network models. Our model shows a reasonable understanding of the question and some coarse image understanding, but it is still very na\u0131\u0308ve in many situations. While recurrent networks are becoming a popular choice for learning image and text, we showed that a simple bag-of-words can perform equally well compared to a recurrent network that is borrowed from an image caption generation framework [1]. We proposed a more complete set of baselines which can provide potential insight for developing more sophisticated end-to-end image question answering systems. As the currently available dataset is not large enough, we developed an algorithm that helps us collect large scale image QA dataset from image descriptions. Our question generation algorithm is extensible to many image description datasets and can be automated without requiring extensive human effort. We hope that the release of the new dataset will encourage more data-driven approaches to this problem in the future.\nImage question answering is a fairly new research topic, and the approach we present here has a number of limitations. First, our models are just answer classifiers. Ideally we would like to permit longer answers which will involve some sophisticated text generation model or structured output. But this will require an automatic free-form answer evaluation metric. Second, we are only focusing on a limited domain of questions. However, this limited range of questions allow us to study the results more in depth. Lastly, it is also hard to interpret why the models output a certain answer. By comparing our models with some baselines we can roughly infer whether they understood the image. Visual attention is another future direction, which could both improve the results (based on recent successes in image captioning [8]) as well as help explain the model prediction by examining the attention output at every timestep."
        },
        {
            "heading": "Acknowledgments",
            "text": "We would like to thank Nitish Srivastava for the support of Toronto Conv Net, from which we extracted the CNN image features. We would also like to thank anonymous reviewers for their valuable and helpful comments."
        },
        {
            "heading": "A Supplementary Material",
            "text": "A.1 Question Generation: Syntax Tree Example\nA.2 Post-Processing of COCO-QA Detail\nFirst, answers that appear less than a frequency threshold are discarded. Second, we enroll a QA pair one at a time. The probability of enrolling the next QA pair (q, a) is:\np(q, a) =\n{ 1 if count(a) \u2264 K\nexp ( \u2212 count(a)\u2212K[K2 ) otherwise (2)\nwhere count(a) denotes the current number of enrolled QA pairs that have a as the ground truth answer, and K, K2 are some constants with K \u2264 K2. In the COCO-QA generation we chose K = 100 and K2 = 200.\nA.3 More Sample Questions and Responses"
        }
    ],
    "title": "Exploring Models and Data for Image Question Answering",
    "year": 2015
}