{
    "abstractText": "We present StarSpace, a general-purpose neural embedding model that can solve a wide variety of problems: labeling tasks such as text classification, ranking tasks such as information retrieval/web search, collaborative filtering-based or content-based recommendation, embedding of multirelational graphs, and learning word, sentence or document level embeddings. In each case the model works by embedding those entities comprised of discrete features and comparing them against each other \u2013 learning similarities dependent on the task. Empirical results on a number of tasks show that StarSpace is highly competitive with existing methods, whilst also being generally applicable to new cases where those methods are not.",
    "authors": [],
    "id": "SP:a22c66e77f5ced58fed2e732a40d5d964a2e09fe",
    "references": [
        {
            "authors": [
                "B. Bai",
                "J. Weston",
                "D. Grangier",
                "R. Collobert",
                "K. Sadamasa",
                "Y. Qi",
                "O. Chapelle",
                "K. Weinberger"
            ],
            "title": "Supervised semantic indexing",
            "venue": "Proceedings of the 18th ACM conference on Information and knowledge management, 187\u2013196. ACM.",
            "year": 2009
        },
        {
            "authors": [
                "Y. Bengio",
                "R. Ducharme",
                "P. Vincent",
                "C. Jauvin"
            ],
            "title": "A neural probabilistic language model",
            "venue": "Journal of machine learning research 3(Feb):1137\u20131155.",
            "year": 2003
        },
        {
            "authors": [
                "P. Bojanowski",
                "E. Grave",
                "A. Joulin",
                "T. Mikolov"
            ],
            "title": "Enriching word vectors with subword information",
            "venue": "Transactions of the Association for Computational Linguistics 5:135\u2013146.",
            "year": 2017
        },
        {
            "authors": [
                "A. Bordes",
                "J. Weston",
                "R. Collobert",
                "Y Bengio"
            ],
            "title": "Learning structured embeddings of knowledge bases",
            "venue": "In AAAI,",
            "year": 2011
        },
        {
            "authors": [
                "A. Bordes",
                "N. Usunier",
                "A. Garcia-Duran",
                "J. Weston",
                "O. Yakhnenko"
            ],
            "title": "Translating embeddings for modeling multi-relational data",
            "venue": "Advances in neural information processing systems, 2787\u20132795.",
            "year": 2013
        },
        {
            "authors": [
                "A. Bordes",
                "X. Glorot",
                "J. Weston",
                "Y. Bengio"
            ],
            "title": "A semantic matching energy function for learning with multirelational data",
            "venue": "Machine Learning 94(2):233\u2013259.",
            "year": 2014
        },
        {
            "authors": [
                "D. Chen",
                "A. Fisch",
                "J. Weston",
                "A. Bordes"
            ],
            "title": "Reading Wikipedia to answer open-domain questions",
            "venue": "Association for Computational Linguistics (ACL).",
            "year": 2017
        },
        {
            "authors": [
                "R. Collobert",
                "J. Weston",
                "L. Bottou",
                "M. Karlen",
                "K. Kavukcuoglu",
                "P. Kuksa"
            ],
            "title": "Natural language processing (almost) from scratch",
            "venue": "Journal of Machine Learning Research 12(Aug):2493\u20132537.",
            "year": 2011
        },
        {
            "authors": [
                "A. Conneau",
                "H. Schwenk",
                "L. Barrault",
                "Y. Lecun"
            ],
            "title": "Very deep convolutional networks for natural language processing",
            "venue": "arXiv preprint arXiv:1606.01781.",
            "year": 2016
        },
        {
            "authors": [
                "A. Conneau",
                "D. Kiela",
                "H. Schwenk",
                "L. Barrault",
                "A. Bordes"
            ],
            "title": "Supervised learning of universal sentence representations from natural language inference data",
            "venue": "arXiv preprint arXiv:1705.02364.",
            "year": 2017
        },
        {
            "authors": [
                "J. Duchi",
                "E. Hazan",
                "Y. Singer"
            ],
            "title": "Adaptive subgradient methods for online learning and stochastic optimization",
            "venue": "Journal of Machine Learning Research 12(Jul):2121\u2013 2159.",
            "year": 2011
        },
        {
            "authors": [
                "A. Garcia-Duran",
                "A. Bordes",
                "N. Usunier"
            ],
            "title": "Composing relationships with translations",
            "venue": "Ph.D. Dissertation, CNRS, Heudiasyc.",
            "year": 2015
        },
        {
            "authors": [
                "K. Goldberg",
                "T. Roeder",
                "D. Gupta",
                "C. Perkins"
            ],
            "title": "Eigentaste: A constant time collaborative filtering algorithm",
            "venue": "Information Retrieval 4(2):133\u2013151.",
            "year": 2001
        },
        {
            "authors": [
                "K.M. Hermann",
                "D. Das",
                "J. Weston",
                "K. Ganchev"
            ],
            "title": "Semantic frame identification with distributed word representations",
            "venue": "ACL (1), 1448\u20131458.",
            "year": 2014
        },
        {
            "authors": [
                "R. Jenatton",
                "N.L. Roux",
                "A. Bordes",
                "G.R. Obozinski"
            ],
            "title": "A latent factor model for highly multi-relational data",
            "venue": "Advances in Neural Information Processing Systems, 3167\u20133175.",
            "year": 2012
        },
        {
            "authors": [
                "A. Joulin",
                "E. Grave",
                "P. Bojanowski",
                "T. Mikolov"
            ],
            "title": "Bag of tricks for efficient text classification",
            "venue": "arXiv preprint arXiv:1607.01759.",
            "year": 2016
        },
        {
            "authors": [
                "R. Kadlec",
                "O. Bajgar",
                "J. Kleindienst"
            ],
            "title": "Knowledge base completion: Baselines strike back",
            "venue": "arXiv preprint arXiv:1705.10744.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Koren",
                "R. Bell"
            ],
            "title": "Advances in collaborative filtering",
            "venue": "Recommender systems handbook. Springer. 77\u2013 118.",
            "year": 2015
        },
        {
            "authors": [
                "N.D. Lawrence",
                "R. Urtasun"
            ],
            "title": "Non-linear matrix factorization with gaussian processes",
            "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, 601\u2013608. ACM.",
            "year": 2009
        },
        {
            "authors": [
                "J. Lehmann",
                "R. Isele",
                "M. Jakob",
                "A. Jentzsch",
                "D. Kontokostas",
                "P.N. Mendes",
                "S. Hellmann",
                "M. Morsey",
                "P. Van Kleef",
                "S Auer"
            ],
            "title": "Dbpedia\u2013a large-scale, multilingual knowledge base extracted from wikipedia. Semantic Web 6(2):167\u2013195",
            "year": 2015
        },
        {
            "authors": [
                "T. Mikolov",
                "K. Chen",
                "G. Corrado",
                "J. Dean"
            ],
            "title": "Efficient estimation of word representations in vector space",
            "venue": "arXiv preprint arXiv:1301.3781.",
            "year": 2013
        },
        {
            "authors": [
                "M. Nickel",
                "L. Rosasco",
                "T. A Poggio"
            ],
            "title": "Holographic embeddings of knowledge graphs",
            "year": 2016
        },
        {
            "authors": [
                "M. Nickel",
                "V. Tresp",
                "H.-P. Kriegel"
            ],
            "title": "A threeway model for collective learning on multi-relational data",
            "venue": "Proceedings of the 28th international conference on machine learning (ICML-11), 809\u2013816.",
            "year": 2011
        },
        {
            "authors": [
                "B. Recht",
                "C. Re",
                "S. Wright",
                "F. Niu"
            ],
            "title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent",
            "venue": "Advances in neural information processing systems, 693\u2013701.",
            "year": 2011
        },
        {
            "authors": [
                "S. Rendle"
            ],
            "title": "Factorization machines",
            "venue": "Data Mining (ICDM), 2010 IEEE 10th International Conference on, 995\u2013 1000. IEEE.",
            "year": 2010
        },
        {
            "authors": [
                "Y. Shen",
                "P.-S. Huang",
                "M.-W. Chang",
                "J. Gao"
            ],
            "title": "Modeling large-scale structured relationships with shared memory for knowledge base completion",
            "venue": "Proceedings of the 2nd Workshop on Representation Learning for NLP, 57\u201368.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Shi",
                "A. Karatzoglou",
                "L. Baltrunas",
                "M. Larson",
                "N. Oliver",
                "A. Hanjalic"
            ],
            "title": "Climf: learning to maximize reciprocal rank with collaborative less-is-more filtering",
            "venue": "Proceedings of the sixth ACM conference on Recommender systems, 139\u2013146. ACM.",
            "year": 2012
        },
        {
            "authors": [
                "J. Weston",
                "S. Bengio",
                "N. Usunier"
            ],
            "title": "Wsabie: Scaling up to large vocabulary image annotation",
            "venue": "IJCAI, volume 11, 2764\u20132770.",
            "year": 2011
        },
        {
            "authors": [
                "J. Weston",
                "A. Bordes",
                "O. Yakhnenko",
                "N. Usunier"
            ],
            "title": "Connecting language and knowledge bases with embedding models for relation extraction",
            "venue": "arXiv preprint arXiv:1307.7973.",
            "year": 2013
        },
        {
            "authors": [
                "J. Weston",
                "S. Chopra",
                "K. Adams"
            ],
            "title": " tagspace: Semantic embeddings from hashtags",
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 1822\u20131827.",
            "year": 2014
        },
        {
            "authors": [
                "Y. Xiao",
                "K. Cho"
            ],
            "title": "Efficient character-level document classification by combining convolution and recurrent layers",
            "venue": "arXiv preprint arXiv:1602.00367.",
            "year": 2016
        },
        {
            "authors": [
                "X. Zhang",
                "Y. LeCun"
            ],
            "title": "Text understanding from scratch",
            "venue": "arXiv preprint arXiv:1502.01710.",
            "year": 2015
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :1\n70 9.\n03 85\n6v 5\n[ cs\n.C L\n] 2\n1 N\nov 2"
        },
        {
            "heading": "1 Introduction",
            "text": "We introduce StarSpace, a neural embedding model that is general enough to solve a wide variety of problems:\n\u2022 Text classification, or other labeling tasks, e.g. sentiment classification.\n\u2022 Ranking of sets of entities, e.g. ranking web documents given a query.\n\u2022 Collaborative filtering-based recommendation, e.g. recommending documents, music or videos.\n\u2022 Content-based recommendation where content is defined with discrete features, e.g. words of documents.\n\u2022 Embedding graphs, e.g. multi-relational graphs such as Freebase.\n\u2022 Learning word, sentence or document embeddings.\nStarSpace can be viewed as a straight-forward and efficient strong baseline for any of these tasks. In experiments it is shown to be on par with or outperforming several competing methods, whilst being generally applicable to cases where many of those methods are not.\nThe method works by learning entity embeddings with discrete feature representations from relations among collections of those entities directly for the task of ranking or classification of interest. In the general case, StarSpace embeds entities of different types into a vectorial embedding space,\nCopyright c\u00a9 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nhence the \u201cstar\u201d (\u201c*\u201d, meaning all types) and \u201cspace\u201d in the name, and in that common space compares them against each other. It learns to rank a set of entities, documents or objects given a query entity, document or object, where the query is not necessarily of the same type as the items in the set.\nWe evaluate the quality of our approach on six different tasks, namely text classification, link prediction in knowledge bases, document recommendation, article search, sentence matching and learning general sentence embeddings. StarSpace is available as an open-source project at https: //github.com/facebookresearch/Starspace."
        },
        {
            "heading": "2 Related Work",
            "text": "Latent text representations, or embeddings, are vectorial representations of words or documents, traditionally learned in an unsupervised way over large corpora. Work on neural embeddings in this domain includes (Bengio et al. 2003), (Collobert et al. 2011), word2vec (Mikolov et al. 2013) and more recently fastText (Bojanowski et al. 2017). In our experiments we compare to word2vec and fastText as representative scalable models for unsupervised embeddings; we also compare on the SentEval tasks (Conneau et al. 2017) against a wide range of unsupervised models for sentence embedding.\nIn the domain of supervised embeddings, SSI (Bai et al. 2009) and WSABIE (Weston, Bengio, and Usunier 2011) are early approaches that showed promise in NLP and information retrieval tasks ((Weston et al. 2013), (Hermann et al. 2014)). Several more recent works including (Tang, Qin, and Liu 2015), (Zhang and LeCun 2015), (Conneau et al. 2016), TagSpace (Weston, Chopra, and Adams 2014) and fastText (Joulin et al. 2016) have yielded good results on classification tasks such as sentiment analysis or hashtag prediction.\nIn the domain of recommendation, embedding models have had a large degree of success, starting from SVD (Goldberg et al. 2001) and its improvements such as SVD++ (Koren and Bell 2015), as well as a host of other techniques, e.g. (Rendle 2010; Lawrence and Urtasun 2009; Shi et al. 2012). Many of those methods have focused on the collaborative filtering setup where user IDs and movie IDs have individual embeddings, such as in the Netflix challenge setup (see e.g., (Koren and Bell 2015), and so new users or items cannot naturally be incorporated. We show\nhow StarSpace can naturally cater for both that setting and the content-based setting where users and items are represented as features, and hence have natural out-of-sample extensions rather than considering only a fixed set.\nPerforming link prediction in knowledge bases (KBs) with embedding-based methods has also shown promising results in recent years. A series of work has been done in this direction, such as (Bordes et al. 2013) and (GarciaDuran, Bordes, and Usunier 2015). In our work, we show that StarSpace can be used for this task as well, outperforming several methods, and matching the TransE method presented in (Bordes et al. 2013)."
        },
        {
            "heading": "3 Model",
            "text": "The StarSpace model consists of learning entities, each of which is described by a set of discrete features (bag-offeatures) coming from a fixed-length dictionary. An entity such as a document or a sentence can be described by a bag of words or n-grams, an entity such as a user can be described by the bag of documents, movies or items they have liked, and so forth. Importantly, the StarSpace model is free to compare entities of different kinds. For example, a user entity can be compared with an item entity (recommendation), or a document entity with label entities (text classification), and so on. This is done by learning to embed them in the same space such that comparisons are meaningful \u2013 by optimizing with respect to the metric of interest.\nDenoting the dictionary of D features as F which is a D\u00d7 d matrix, where Fi indexes the i\nth feature (row), yielding its d-dimensional embedding, we embed an entity a with\u2211\ni\u2208a Fi. That is, like other embedding models, our model starts by assigning a d-dimensional vector to each of the discrete features in the set that we want to embed directly (which we call a dictionary, it can contain features like words, etc.). Entities comprised of features (such as documents) are represented by a bag-of-features of the features in the dictionary and their embeddings are learned implicitly. Note an entity could consist of a single (unique) feature like a single word, name or user or item ID if desired.\nTo train our model, we need to learn to compare entities. Specifically, we want to minimize the following loss function:\u2211\n(a,b)\u2208E+\nb\u2212\u2208E\u2212\nLbatch(sim(a, b), sim(a, b\u22121 ), . . . , sim(a, b \u2212 k ))\nThere are several ingredients to this recipe:\n\u2022 The generator of positive entity pairs (a, b) coming from the set E+. This is task dependent and will be described subsequently.\n\u2022 The generator of negative entities b\u2212i coming from the set E\u2212. We utilize a k-negative sampling strategy (Mikolov et al. 2013) to select k such negative pairs for each batch update. We select randomly from within the set of entities that can appear in the second argument of the similarity function (e.g., for text labeling tasks a are documents and b are labels, so we sample b\u2212 from the set of labels). An analysis of the impact of k is given in Sec. 4.\n\u2022 The similarity function sim(\u00b7, \u00b7). In our system, we have implemented both cosine similarity and inner product, and selected the choice as a hyperparameter. Generally, they work similarly well for small numbers of label features (e.g. for classification), while cosine works better for larger numbers, e.g. for sentence or document similarity.\n\u2022 The loss function Lbatch that compares the positive pair (a, b) with the negative pairs (a, b\u2212i ), i = 1, . . . , k. We also implement two possibilities: margin ranking loss (i.e. max(0, \u00b5\u2212 sim(a, b), where \u00b5 is the margin parameter), and negative log loss of softmax. All experiments use the former as it performed on par or better.\nWe optimize by stochastic gradient descent (SGD), i.e., each SGD step is one sample from E+ in the outer sum, using Adagrad (Duchi, Hazan, and Singer 2011) and hogwild (Recht et al. 2011) over multiple CPUs. We also apply a max norm of the embeddings to restrict the vectors learned to lie in a ball of radius r in space Rd, as in other works, e.g. (Weston, Bengio, and Usunier 2011).\nAt test time, one can use the learned function sim(\u00b7, \u00b7) to measure similarity between entities. For example, for classification, a label is predicted at test time for a given input a\nusing max b\u0302 sim(a, b\u0302) over the set of possible labels b\u0302. Or in general, for ranking one can sort entities by their similarity. Alternatively the embedding vectors can be used directly for some other downstream task, e.g., as is typically done with word embedding models. However, if sim(\u00b7, \u00b7) directly fits the needs of your application, this is recommended as this is the objective that StarSpace is trained to be good at.\nWe now describe how this model can be applied to a wide variety of tasks, in each case describing how the generators E+ and E\u2212 work for that setting.\nMulticlass Classification (e.g. Text Classification) The positive pair generator comes directly from a training set of labeled data specifying (a, b) pairs where a are documents (bags-of-words) and b are labels (singleton features). Negative entities b\u2212 are sampled from the set of possible labels.\nMultilabel Classification In this case, each document a can have multiple positive labels, one of them is sampled as b at each SGD step to implement multilabel classification.\nCollaborative Filtering-based Recommendation The training data consists of a set of users, where each user is described by a bag of items (described as unique features from the dictionary) that the user likes. The positive pair generator picks a user, selects a to be the unique singleton feature for that user ID, and a single item that they like as b. Negative entities b\u2212 are sampled from the set of possible items."
        },
        {
            "heading": "Collaborative Filtering-based Recommendation with",
            "text": "out-of-sample user extension One problem with classical collaborative filtering is that it does not generalize to new users, as a separate embedding is learned for each user ID. Using the same training data as before, one can learn an alternative model using StarSpace. The positive pair generator\ninstead picks a user, selects a as all the items they like except one, and b as the left out item. That is, the model learns to estimate if a user would like an item by modeling the user not as a single embedding based on their ID, but by representing the user as the sum of embeddings of items they like.\nContent-based Recommendation This task consists of a set of users, where each user is described by a bag of items, where each item is described by a bag of features from the dictionary (rather than being a unique feature). For example, for document recommendation, each user is described by the bag-of-documents they like, while each document is described by the bag-of-words it contains. Now a can be selected as all of the items except one, and b as the left out item. The system now extends to both new items and new users as both are featurized.\nMulti-Relational Knowledge Graphs (e.g. Link Prediction) Given a graph of (h, r, t) triples, consisting of a head concept h, a relation r and a tail concept t, e.g. (Beyonce\u0301, born-in, Houston), one can learn embeddings of that graph. Instantiations of h, r and t are all defined as unique features in the dictionary. We select uniformly at random either: (i) a consists of the bag of features h and r, while b consists only of t; or (ii) a consists of h, and b consists of r and t. Negative entities b\u2212 are sampled from the set of possible concepts. The learnt embeddings can then be used to answer link prediction questions such as (Beyonce\u0301, born-in, ?) or (?, born-in, Houston) via the learnt function sim(a, b).\nInformation Retrieval (e.g. Document Search) and Document Embeddings Given supervised training data consisting of (search keywords, relevant document) pairs one can directly train an information retrieval model: a contains the search keywords, b is a relevant document and b\u2212 are other irrelevant documents. If only unsupervised training data is available consisting of a set of unlabeled documents, an alternative is to select a as random keywords from the document and b as the remaining words. Note that both these approaches implicitly learn document embeddings which could be used for other purposes.\nLearning Word Embeddings We can also use StarSpace to learn unsupervised word embeddings using training data consisting of raw text. We select a as a window of words (e.g., four words, two either side of a middle word), and b as the middle word, following (Collobert et al. 2011; Mikolov et al. 2013; Bojanowski et al. 2017).\nLearning Sentence Embeddings Learning word embeddings (e.g. as above) and using them to embed sentences does not seem optimal when you can learn sentence embeddings directly. Given a training set of unlabeled documents, each consisting of sentences, we select a and b as a pair of sentences both coming from the same document; b\u2212 are sentences coming from other documents. The intuition is that semantic similarity between sentences is shared\nwithin a document (one can also only select sentences within a certain distance of each other if documents are very long). Further, the embeddings will automatically be optimized for sets of words of sentence length, so train time matches test time, rather than training with short windows as typically learned with word embeddings \u2013 window-based embeddings can deteriorate when the sum of words in a sentence gets too large.\nMulti-Task Learning Any of these tasks can be combined, and trained at the same time if they share some features in the base dictionary F . For example one could combine supervised classification with unsupervised word or sentence embedding, to give semi-supervised learning."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "Text Classification",
            "text": "We employ StarSpace for the task of text classification and compare it with a host of competing methods, including fastText, on three datasets which were all previously used in (Joulin et al. 2016). To ensure fair comparison, we use an identical dictionary to fastText and use the same implementation of n-grams and pruning (those features are implemented in our open-source distribution of StarSpace). In these experiments we set the dimension of embeddings to be 10, as in (Joulin et al. 2016).\nWe use three datasets:\n\u2022 AG news1 is a 4 class text classification task given title and description fields as input. It consists of 120K training examples, 7600 test examples, 4 classes, \u223c100K words and 5M tokens in total.\n\u2022 DBpedia (Lehmann et al. 2015) is a 14 class classification problem given the title and abstract of Wikipedia articles as input. It consists of 560K training examples, 70k test examples, 14 classes, \u223c800K words and 32M tokens in total.\n\u2022 The Yelp reviews dataset is obtained from the 2015 Yelp Dataset Challenge2. The task is to predict the full number of stars the user has given (from 1 to 5). It consists of 1.2M training examples, 157k test examples, 5 classes, \u223c500K words and 193M tokens in total.\nResults are given in Table 2. Baselines are quoted from the literature (some methods are only reported on AG news and DBPedia, others only on Yelp15). StarSpace outperforms a number of methods, and performs similarly to fastText. We measure the training speed for n-grams > 1 in Table 3. fastText and StarSpace are both efficient compared to deep learning approaches, e.g. (Zhang and LeCun 2015) takes 5h per epoch on DBpedia, 375x slower than StarSpace. Still, fastText is faster than StarSpace. However, as we will see in the following sections, StarSpace is a more general system.\n1 http://www.di.unipi.it/\u0153gulli/AG_corpus_of_news_articles.html 2 https://www.yelp.com/dataset_challenge"
        },
        {
            "heading": "Content-based Document Recommendation",
            "text": "We consider the task of recommending new documents to a user given their past history of liked documents. We follow a very similar process described in (Weston, Chopra, and Adams 2014) in our experiment. The data for this task is comprised of anonymized two-weeks long interaction histories for a subset of people on a popular social networking service. For each of the 641,385 people considered, we collected the text of public articles that s/he clicked to read, giving a total of 3,119,909 articles. Given the person\u2019s trailing (n\u2212 1) clicked articles, we use our model to predict the n\u2019th article by ranking it against 10,000 other unrelated articles, and evaluate using ranking metrics. The score of the n\u2019th article is obtained by applying StarSpace: the input a is the previous (n\u22121) articles, and the output b is the n\u2019th candidate article. We measure the results by computing hits@k, i.e. the proportion of correct entities ranked in the top k for k = 1, 10, 20, and the mean predicted rank of the clicked article among the 10,000 articles.\nAs this is not a classification task (i.e. there are not a fixed set of labels to classify amongst, but a variable set of never seen before documents to rank per user) we cannot use supervised classification models directly. Starspace however can deal directly with this task, which is one of its major benefits. Following (Weston, Chopra, and Adams 2014), we hence use the following models as baselines:\n\u2022 Word2vec model. We use the publicly available word2vec model trained on Google News articles3, and use the word embeddings to generate article embeddings (by bag-ofwords) and users\u2019 embedding (by bag-of-articles in users\u2019 click history). We then use cosine similarity for ranking.\n\u2022 Unsupervised fastText model. We try both the previously trained publicly available model on Wikipedia4, and train on our own dataset. Unsupervised fastText is an enhancement of word2Vec that also includes subwords.\n\u2022 Linear SVM ranker, using either bag-of-words features or fastText embeddings (component-wise multiplication of a\u2019s and b\u2019s features, which are of the same dimension).\n\u2022 Tagspace model trained on a hashtag task, and then the embeddings are used for document recommendation, a reproduction of the setup in (Weston, Chopra, and Adams 2014). In that work, the Tagspace model was shown to outperform word2vec.\n\u2022 TFIDF bag-of-words cosine similarity model.\nFor fair comparison, we set the dimension of all embedding models to be 300. We show the results of our StarSpace model comparing with the baseline models in Table 1. Training time for StarSpace and fastText (Bojanowski et al. 2017) trained on our dataset is also provided.\n3 https://code.google.com/archive/p/word2vec/ 4 https://github.com/facebookresearch/fastText/blob/master/\npretrained-vectors.md\nTagspace was previously shown to provide superior performance to word2vec, and we observe the same result here. Unsupervised FastText, which is an enhancement of word2vec is also slightly inferior to Tagspace, but better than word2vec. However, StarSpace, which is naturally more suited to this task, outperforms all those methods, including Tagspace and SVMs by a significant margin. Overall, from the evaluation one can see that unsupervised methods of learning word embeddings are inferior to training specifically for the document recommendation task at hand, which StarSpace does."
        },
        {
            "heading": "Link Prediction: Embedding Multi-relation Knowledge Graphs",
            "text": "We show that one can also use StarSpace on tasks of knowledge representation. We use the Freebase 15k dataset from (Bordes et al. 2013), which consists of a collection of triplets (head, relation type, tail) extracted from Freebase5. This data set can be seen as a 3-mode tensor depicting ternary relationships between synsets. There are 14,951 concepts (mids) and 1,345 relation types among them. The training set contains 483,142 triplets, the validation set 50,000 and the test set 59,071. As described in (Bordes et al. 2013), evaluation is performed by, for each test triplet, removing the head and replacing by each of the entities in the dictionary in turn. Scores for those corrupted triplets are first computed by the models and then sorted; the rank of the correct entity is finally stored. This whole procedure is repeated while removing the tail instead of the head. We report the mean of those predicted ranks and the hits@10. We also conduct a filtered evaluation that is the same, except all other valid heads or tails from the train or test set are discarded in the ranking, following (Bordes et al. 2013).\nWe compare with a number of methods, including transE\n5 http://www.freebase.com\npresented in (Bordes et al. 2013). TransE was shown to outperform RESCAL (Nickel, Tresp, and Kriegel 2011), RFM (Jenatton et al. 2012), SE (Bordes et al. 2011) and SME (Bordes et al. 2014) and is considered a standard benchmark method. TransE uses an L2 similarity ||head + relation - tail||2 and SGD updates with single entity corruptions of head or tail that should have a larger distance. In contrast, StarSpace uses a dot product, k-negative sampling, and two different embeddings to represent the relation entity, depending on whether it appears in a or b.\nThe results are given in Table 4. Results for SE, SME and LFM are reported from (Bordes et al. 2013) and optimize the dimension from the choices 20, 50 and 75 as a hyperparameter. RESCAL is reported from (Nickel et al. 2016). For TransE we ran it ourselves so that we could report the results for different embedding dimensions, and because we obtained better results by fine tuning it than previously reported. Comparing TransE and StarSpace for the same embedding dimension, these two methods then give similar performance. Note there are some recent improved results on this dataset using larger embeddings (Kadlec, Bajgar, and Kleindienst 2017) or more complex, but less general, methods (Shen et al. 2017).\nInfluence of k In this section, we ran experiments on the Freebase 15k dataset to illustrate the complexity of our model in terms of the number of negative search examples. We set dim = 50, and the max training time of the algorithm to be 1 hour for all experients. We report the number of epochs the algorithm completes within the time limit and the best filtered hits@10 result over possible learning rate choices, for different k (number of negatives searched for each positive training example). We set k = [1, 5, 10, 25, 50, 100, 250, 500, 1000].\nThe result is presented in Table 5. We observe that the number of epochs finished within the 1 hour training time\nconstraint is close to an inverse linear function of k. In this particular setup, [1, 100] is a good range of k and the best result is achieved at K = 50."
        },
        {
            "heading": "Wikipedia Article Search & Sentence Matching",
            "text": "In this section, we apply our model on a Wikipedia article search and a sentence match problem. We use the Wikipedia dataset introduced by (Chen et al. 2017), which is the 2016- 12-21 dump of English Wikipedia. For each article, only the plain text is extracted and all structured data sections such as lists and figures are stripped. It contains a total of 5,075,182 articles with 9,008,962 unique uncased token types. The dataset is split into 5,035,182 training examples, 10,000 validation examples and 10,000 test examples. We then consider the following evaluation tasks:\n\u2022 Task 1: given a sentence from a Wikipedia article as a search query, we try to find the Wikipedia article it came from. We rank the true Wikipedia article (minus the sentence) against 10,000 other Wikipedia articles using ranking evaluation metrics. This mimics a web search like scenario where we would like to search for the most relevant Wikipedia articles (web documents). Note that we effectively have supervised training data for this task.\n\u2022 Task 2: pick two random sentences from a Wikipedia article, use one as the search query, and try to find the other sentence coming from the same original document. We rank the true sentence against 10,000 other sentences from different Wikipedia articles. This fits the scenario where we want to find sentences that are closely semantically related by topic (but do not necessarily have strong word overlap). Note also that we effectively have supervised\ntraining data for this task.\nWe can train our Starspace model in the following way: each update step selects a Wikipedia article from our training set. Then, one random sentence is picked from the article as the input, and for Task 2 another random sentence (different from the input) is picked from the article as the label (otherwise the rest of the article for Task 1). Negative entities can be selected at random from the training set. In the case of training for Task 1, for label features we use a feature dropout probability of 0.8 which both regularizes and greatly speeds up training. We also try StarSpace word-level training, and multi-tasking both sentence and word-level for Task 2.\nWe compare StarSpace with the publicly released fastText model, as well as a fastText model trained on the text of our dataset.6 We also compare to a TFIDF baseline. For fair comparison, we set the dimension of all embedding models to be 300. The results for tasks 1 and 2 are summarized in Table 6 and 7 respectively. StarSpace outperforms TFIDF and fastText by a significant margin, this is because StarSpace can train directly for the tasks of interest whereas it is not in the declared scope of fastText. Note that StarSpace wordlevel training, which is similar to fastText in method, obtains similar results to fastText. Crucially, it is StarSpace\u2019s ability to do sentence and document level training that brings the performance gains.\nA comparison of the predictions of StarSpace and fastText on the article search task (Task 1) on a few random queries\n6FastText training is unsupervised even on our dataset since its original design does not support directly using supervised data here.\nare given in Table 8. While fastText results are semantically in roughly the right part of the space, they lack finer precision. For example, the first query is looking for articles about an olympic skater, which StarSpace correctly understands whereas fastText picks an olympic gymnast. Note that the query does not specifically mention the word skater, StarSpace can only understand this by understanding related phrases, e.g. the phrase \u201cBlue Swords\u201d refers to an international figure skating competition. The other two examples given yield similar conclusions."
        },
        {
            "heading": "Learning Sentence Embeddings",
            "text": "In this section, we evaluate sentence embeddings generated by our model and use SentEval7 which is a tool from (Conneau et al. 2017) for measuring the quality of general purpose sentence embeddings. We use a total of 14 transfer tasks including binary classification, multi-class classification, entailment, paraphrase detection, semantic relatedness and semantic textual similarity from SentEval. Detailed description of these transfer tasks and baseline models can be found in (Conneau et al. 2017).\n7 https://github.com/facebookresearch/SentEval\nWe train the following models on the Wikipedia Task 2 from the previous section, and evaluate sentence embeddings generated by those models:\n\u2022 StarSpace trained on word level.\n\u2022 StarSpace trained on sentence level.\n\u2022 StarSpace trained (multi-tasked) on both word and sentence level.\n\u2022 Ensemble of StarSpace models trained on both word and sentence level: we train a set of 13 models, multi-tasking on Wikipedia sentence match and word-level training then concatenate all embeddings together to generate a 13 \u00d7 300 = 3900 dimension embedding for each word.\nWe present the results in Table 9 and Table 10. StarSpace performs well, outperforming many methods on many of the tasks, although no method wins outright across all tasks. Particularly on the STS (Semantic Textual Similarity) tasks Starspace has very strong results. Please refer to (Conneau et al. 2017) for further results and analysis of these datasets."
        },
        {
            "heading": "5 Discussion and Conclusion",
            "text": "In this paper, we propose StarSpace, a method of embedding and ranking entities using the relationships between entities, and show that the method we propose is a general system capable of working on many tasks:\n\u2022 Text Classification / Sentiment Analysis: we show that our method achieves good results, comparable to fastText (Joulin et al. 2016) on three different datasets.\n\u2022 Content-based Document recommendation: it can directly solve these tasks well, whereas applying off-the-shelf fastText, Tagspace or word2vec gives inferior results.\n\u2022 Link Prediction in Knowledge Bases: we show that our method outperforms several methods, and matches TransE (Bordes et al. 2013) on Freebase 15K.\n\u2022 Wikipedia Search and Sentence Matching tasks: it outperforms off-the-shelf embedding models due to directly training sentence and document-level embeddings.\n\u2022 Learning Sentence Embeddings: It performs well on the 14 SentEval transfer tasks of (Conneau et al. 2017) compared to a host of embedding methods.\nStarSpace should also be highly applicable to other tasks we did not evaluate here such as other classification, ranking, retrieval or metric learning tasks. Importantly, what is more general about our method compared to many existing embedding models is: (i) the flexibility of using features to represent labels that we want to classify or rank, which enables it to train directly on a downstream prediction/ranking task; and (ii) different ways of selecting positives and negatives suitable for those tasks. Choosing the wrong generators E+ and E\u2212 gives greatly inferior results, as shown e.g. in Table 7.\nFuture work will consider the following enhancements: going beyond discrete features, e.g. to continuous features, considering nonlinear representations and experimenting with other entities such as images. Finally, while our model\nis relatively efficient, we could consider hierarchical classification schemes as in FastText to try to make it more efficient; the trick here would be to do this while maintaining the generality of our model which is what makes it so appealing."
        },
        {
            "heading": "6 Acknowledgement",
            "text": "We would like to thank Timothee Lacroix for sharing with us his implementation of TransE. We also thank Edouard Grave, Armand Joulin and Arthur Szlam for helpful discussions on the StarSpace model."
        },
        {
            "heading": "Information Retrieval 4(2):133\u2013151.",
            "text": "Hermann, K. M.; Das, D.; Weston, J.; and Ganchev, K. 2014. Semantic frame identification with distributed word representations. In ACL (1), 1448\u20131458.\nJenatton, R.; Roux, N. L.; Bordes, A.; and Obozinski, G. R. 2012. A latent factor model for highly multi-relational data. In Advances in Neural Information Processing Systems, 3167\u20133175.\nJoulin, A.; Grave, E.; Bojanowski, P.; and Mikolov, T. 2016. Bag of tricks for efficient text classification. arXiv preprint arXiv:1607.01759.\nKadlec, R.; Bajgar, O.; and Kleindienst, J. 2017. Knowledge base completion: Baselines strike back. arXiv preprint arXiv:1705.10744.\nKoren, Y., and Bell, R. 2015. Advances in collaborative filtering. In Recommender systems handbook. Springer. 77\u2013 118.\nLawrence, N. D., and Urtasun, R. 2009. Non-linear matrix factorization with gaussian processes. In Proceedings of the 26th Annual International Conference on Machine Learning, 601\u2013608. ACM.\nLehmann, J.; Isele, R.; Jakob, M.; Jentzsch, A.; Kontokostas, D.; Mendes, P. N.; Hellmann, S.; Morsey, M.; Van Kleef, P.; Auer, S.; et al. 2015. Dbpedia\u2013a large-scale, multilingual knowledge base extracted from wikipedia. Semantic Web 6(2):167\u2013195.\nMikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.\nNickel, M.; Rosasco, L.; Poggio, T. A.; et al. 2016. Holographic embeddings of knowledge graphs.\nNickel, M.; Tresp, V.; and Kriegel, H.-P. 2011. A threeway model for collective learning on multi-relational data. In Proceedings of the 28th international conference on machine learning (ICML-11), 809\u2013816.\nRecht, B.; Re, C.; Wright, S.; and Niu, F. 2011. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In Advances in neural information processing systems, 693\u2013701.\nRendle, S. 2010. Factorization machines. In Data Mining (ICDM), 2010 IEEE 10th International Conference on, 995\u2013 1000. IEEE.\nShen, Y.; Huang, P.-S.; Chang, M.-W.; and Gao, J. 2017. Modeling large-scale structured relationships with shared memory for knowledge base completion. In Proceedings of the 2nd Workshop on Representation Learning for NLP, 57\u201368.\nShi, Y.; Karatzoglou, A.; Baltrunas, L.; Larson, M.; Oliver, N.; and Hanjalic, A. 2012. Climf: learning to maximize reciprocal rank with collaborative less-is-more filtering. In Proceedings of the sixth ACM conference on Recommender systems, 139\u2013146. ACM.\nTang, D.; Qin, B.; and Liu, T. 2015. Document modeling\nwith gated recurrent neural network for sentiment classification. In EMNLP, 1422\u20131432.\nWeston, J.; Bengio, S.; and Usunier, N. 2011. Wsabie: Scaling up to large vocabulary image annotation. In IJCAI, volume 11, 2764\u20132770.\nWeston, J.; Bordes, A.; Yakhnenko, O.; and Usunier, N. 2013. Connecting language and knowledge bases with embedding models for relation extraction. arXiv preprint arXiv:1307.7973.\nWeston, J.; Chopra, S.; and Adams, K. 2014. # tagspace: Semantic embeddings from hashtags. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 1822\u20131827.\nXiao, Y., and Cho, K. 2016. Efficient character-level document classification by combining convolution and recurrent layers. arXiv preprint arXiv:1602.00367.\nZhang, X., and LeCun, Y. 2015. Text understanding from scratch. arXiv preprint arXiv:1502.01710."
        }
    ],
    "year": 2017
}