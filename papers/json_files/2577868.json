{
    "abstractText": "In this work we introduce a simple, robust approach to hierarchically training an agent in the setting of sparse reward tasks. The agent is split into a low-level and a high-level policy. The low-level policy only accesses internal, proprioceptive dimensions of the state observation. The low-level policies are trained with a simple reward that encourages changing the values of the non-proprioceptive dimensions. Furthermore, it is induced to be periodic with the use a \u201cphase function.\u201d The high-level policy is trained using a sparse, task-dependent reward, and operates by choosing which of the low-level policies to run at any given time. Using this approach, we solve difficult maze and navigation tasks with sparse rewards using the Mujoco Ant and Humanoid agents and show improvement over recent hierarchical methods.",
    "authors": [
        {
            "affiliations": [],
            "name": "Kenneth Marino"
        },
        {
            "affiliations": [],
            "name": "Abhinav Gupta"
        },
        {
            "affiliations": [],
            "name": "Rob Fergus"
        }
    ],
    "id": "SP:1ade32ec7a65add6c4530a90ed62d4593aed3f48",
    "references": [
        {
            "authors": [
                "Pierre-Luc Bacon",
                "Jean Harb",
                "Doina Precup"
            ],
            "title": "The option-critic architecture",
            "venue": "In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February",
            "year": 2017
        },
        {
            "authors": [
                "Peter Dayan",
                "Geoffrey E. Hinton"
            ],
            "title": "Feudal reinforcement learning. In Advances in Neural Information Processing Systems 5, [NIPS Conference, Denver, Colorado, USA, November 30 - December",
            "year": 1992
        },
        {
            "authors": [
                "Thomas G. Dietterich"
            ],
            "title": "State abstraction in MAXQ hierarchical reinforcement learning",
            "venue": "In Advances in Neural Information Processing Systems 12, [NIPS Conference,",
            "year": 1999
        },
        {
            "authors": [
                "Benjamin Eysenbach",
                "Abhishek Gupta",
                "Julian Ibarz",
                "Sergey Levine"
            ],
            "title": "Diversity is all you need: Learning skills without a reward function",
            "venue": "arXiv preprint arXiv:1802.06070,",
            "year": 2018
        },
        {
            "authors": [
                "Carlos Florensa",
                "Yan Duan",
                "Pieter Abbeel"
            ],
            "title": "Stochastic neural networks for hierarchical reinforcement learning",
            "venue": "arXiv preprint arXiv:1704.03012,",
            "year": 2017
        },
        {
            "authors": [
                "Kevin Frans",
                "Jonathan Ho",
                "Xi Chen",
                "Pieter Abbeel",
                "John Schulman"
            ],
            "title": "Meta learning shared hierarchies",
            "venue": "arXiv preprint arXiv:1710.09767,",
            "year": 2017
        },
        {
            "authors": [
                "Karol Gregor",
                "Danilo Jimenez Rezende",
                "Daan Wierstra"
            ],
            "title": "Variational intrinsic control",
            "venue": "arXiv preprint arXiv:1611.07507,",
            "year": 2016
        },
        {
            "authors": [
                "Tuomas Haarnoja",
                "Kristian Hartikainen",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Latent space policies for hierarchical reinforcement learning",
            "venue": "In Proceedings of the 35th International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Tuomas Haarnoja",
                "Kristian Hartikainen",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Latent space policies for hierarchical reinforcement learning",
            "venue": "arXiv preprint arXiv:1804.02808,",
            "year": 2018
        },
        {
            "authors": [
                "Karol Hausman",
                "Jost Tobias Springenberg",
                "Ziyu Wang",
                "Nicolas Heess",
                "Martin Riedmiller"
            ],
            "title": "Learning an embedding space for transferable robot",
            "year": 2018
        },
        {
            "authors": [
                "Nicolas Heess",
                "Greg Wayne",
                "Yuval Tassa",
                "Timothy Lillicrap",
                "Martin Riedmiller",
                "David Silver"
            ],
            "title": "Learning and transfer of modulated locomotor controllers",
            "venue": "arXiv preprint arXiv:1610.05182,",
            "year": 2016
        },
        {
            "authors": [
                "Daniel Holden",
                "Taku Komura",
                "Jun Saito"
            ],
            "title": "Phase-functioned neural networks for character control",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2017
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Ilya Kostrikov"
            ],
            "title": "Pytorch implementations of reinforcement learning algorithms",
            "venue": "https:// github.com/ikostrikov/pytorch-a2c-ppo-acktr,",
            "year": 2018
        },
        {
            "authors": [
                "Eve Marder",
                "Dirk Bucher"
            ],
            "title": "Central pattern generators and the control of rhythmic movements",
            "venue": "Current biology,",
            "year": 2001
        },
        {
            "authors": [
                "Volodymyr Mnih",
                "Koray Kavukcuoglu",
                "David Silver",
                "Alex Graves",
                "Ioannis Antonoglou",
                "Daan Wierstra",
                "Martin Riedmiller"
            ],
            "title": "Playing atari with deep reinforcement learning",
            "venue": "arXiv preprint arXiv:1312.5602,",
            "year": 2013
        },
        {
            "authors": [
                "Volodymyr Mnih",
                "Adria Puigdomenech Badia",
                "Mehdi Mirza",
                "Alex Graves",
                "Timothy Lillicrap",
                "Tim Harley",
                "David Silver",
                "Koray Kavukcuoglu"
            ],
            "title": "Asynchronous methods for deep reinforcement learning",
            "venue": "In International conference on machine learning,",
            "year": 2016
        },
        {
            "authors": [
                "Shakir Mohamed",
                "Danilo Jimenez Rezende"
            ],
            "title": "Variational information maximisation for intrinsically motivated reinforcement learning",
            "venue": "In Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "Ofir Nachum",
                "Shane Gu",
                "Honglak Lee",
                "Sergey Levine"
            ],
            "title": "Data-efficient hierarchical reinforcement learning",
            "venue": "arXiv preprint arXiv:1805.08296,",
            "year": 2018
        },
        {
            "authors": [
                "Xue Bin Peng",
                "Glen Berseth",
                "Michiel Van de Panne"
            ],
            "title": "Terrain-adaptive locomotion skills using deep reinforcement learning",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2016
        },
        {
            "authors": [
                "Xue Bin Peng",
                "Glen Berseth",
                "KangKang Yin",
                "Michiel Van De Panne"
            ],
            "title": "Deeploco: Dynamic locomotion skills using hierarchical deep reinforcement learning",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2017
        },
        {
            "authors": [
                "Xue Bin Peng",
                "Pieter Abbeel",
                "Sergey Levine",
                "Michiel van de Panne"
            ],
            "title": "Deepmimic: Example-guided deep reinforcement learning of physics-based character skills",
            "venue": "arXiv preprint arXiv:1804.02717,",
            "year": 2018
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov"
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347,",
            "year": 2017
        },
        {
            "authors": [
                "Arjun Sharma",
                "Kris M Kitani"
            ],
            "title": "Phase-parametric policies for reinforcement learning in cyclic environments",
            "venue": "In AAAI Conference on Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Richard S Sutton",
                "Doina Precup",
                "Satinder Singh"
            ],
            "title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning",
            "venue": "Artificial intelligence,",
            "year": 1999
        },
        {
            "authors": [
                "Emanuel Todorov",
                "Tom Erez",
                "Yuval Tassa"
            ],
            "title": "Mujoco: A physics engine for model-based control",
            "venue": "In Intelligent Robots and Systems (IROS),",
            "year": 2012
        },
        {
            "authors": [
                "Alexander Sasha Vezhnevets",
                "Simon Osindero",
                "Tom Schaul",
                "Nicolas Heess",
                "Max Jaderberg",
                "David Silver",
                "Koray Kavukcuoglu"
            ],
            "title": "Feudal networks for hierarchical reinforcement learning",
            "venue": "In Proceedings of the 34th International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [],
            "title": "Learning from delayed rewards",
            "venue": "PhD thesis, King\u2019s College,",
            "year": 1989
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The notion of hierarchy is fundamental to AI systems. Effective approaches to perceptual tasks such as vision or audio rely on hierarchical neural network architectures (e.g. convnets and wavenets). Hierarchical methods are also a natural approach to solving reinforcement learning problems where reward is sparse. Consider an environment where a legged robot has to navigate a maze through winding paths to one of several possible goals. The sequence of actions (at the level of joint controls) required for this agent to ever see a reward is too long and complex for na\u0131\u0308ve reinforcement learning methods to find. Hierarchical controllers provide an intuitive solution to this problem: first learn a low-level controller that is able to handle low-level tasks such as locomotion and balance, and then learn a high-level controller that controls the inputs to that low-level controller and handles the high-level planning tasks (e.g. which path to take in the maze).\nWhile conceptually straightforward, hierarchical approaches have been difficult to realize in practice for several reasons: (i) the lack of constraints on intermediate representations; (ii) the difficulty of learning a high-level controller when the low-level policies shifts, and (iii) the continued problem of sample-inefficiency when training both low-level and high-level controller jointly with sparse rewards. While there has been some progress recently, there is still no general hierarchical reinforcement learning method that is robust across domains, easy to implement, and effective.\nThe approach in this work avoids these problems by simplifying the high-level optimization problem to choosing from a set of independently trained low-level controllers. The problem then becomes how to train a variety of controllers which are generally useful for an agent acting in the world and are sufficiently diverse so that the high-level controller will always have access to the building blocks it needs to solve the high-level task.\n\u2217Work done as an intern at Facebook AI Research. See videos and more at https://sites.google.com/view/hrl-ep3\nThe basic method of learning our low-level skills is as follows. First, as with Heess et al. (2016), we factorize the state space into components that describe the internal and external properties of the agent. The former carries information needed for proprioception (e.g. internal joint angles) and is exclusively used by the low-level policy and the latter is used exclusively by the high-level policy. Second, we provide a weak supervisory signal to the low-level policies that rewards them for changes in the external part of the space. We do not specify how; the low-level agent is rewarded for any change in the perceptual features. The intuition is that a policy is useful to an agent if following it changes the external world or the agent\u2019s place in it, (e.g. by changing position or moving objects). Finally, we build into the agent a notion of repeated cycles of action. Biological systems are known to possess central pattern generators (CPGs) that produce rhythmic outputs, assisting with perception, locomotion, and self-regulation (Marder & Bucher, 2001). Taking inspiration from this, we give our agent access to a \u201cphase function\u201d and reward it for acting in consistent, cyclic ways.\nUsing this method of training, we train a number of low-level controllers on Mujoco locomotion tasks and are able to learn a diverse and effective set of low-level policies. We then show that these policies are easily controllable by our high-level controller and sufficient for solving the final task. We show impressive results on a variety of difficult Mujoco maze and navigation tasks with sparse rewards and demonstrate that our method compares favorably to other recent hierarchical RL approaches. Finally, we demonstrate that our method is robust to choice of RL algorithm."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "There is an extensive literature on hierarchical approaches to reinforcement learning (Sutton et al., 1999; Dayan & Hinton, 1992; Dietterich, 1999). Options (Sutton et al., 1999) are perhaps the most standard framing for hierarchical RL, formalizing the notion of a subroutine in an MDP. Our lowlevel networks are options with a deterministic return condition. While earlier works used prespecified subroutines, and focused on learning when to call them, recent works, for example Bacon et al. (2017); Vezhnevets et al. (2017); Nachum et al. (2018), have demonstrated success in learning multiple levels of hierarchy end-to-end from only task reward.\nStill, in more complex settings such as the ones discussed in this work, with high dimensional action spaces, and especially with sparse rewards, performing unsupervised pre-training, or using auxiliary tasks during learning or pre-training have led to better results (Heess et al., 2016; Frans et al., 2017; Florensa et al., 2017; Haarnoja et al., 2018b; Eysenbach et al., 2018; Hausman et al., 2018).\nOne line of work in this direction starts from a variational inference approach to intrinsic motivation and exploration (Mohamed & Rezende, 2015; Gregor et al., 2016; Florensa et al., 2017; Eysenbach et al., 2018). They use an actor parameterized by state and a latent vector in such a way that the latent vector is predictable from a final state or a sequence of states the actor visits, but otherwise, the actions have high entropy. The latter of these works use this approach for designing hierarchical policies.\nIn Florensa et al. (2017), they use a pre-training task, and the prediction cost is an auxiliary reward. The main differences from our work is that we do not try to use stochastic neural networks, and we do not try to compress the one-hot representation of the low level networks into a dense vector during low level training. Instead we keep them fully separate. We also do not attempt to impose any regularization to encourage the low level networks to be diverse. These can be considered simplifications; what we show is that the simple thing works quite well.\nIn Eysenbach et al. (2018) the low-level policy is trained without using any environmental reward by pre-training using the unsupervised variational objective; they then training a high-level actor to issue commands via the latent vector. This latter work is in some senses more general than ours, in that a user is not required to separate out the proprioceptive variables from the external sensory variables. In addition, the formulation directly encourages diverse and controllable low-level policies. Nevertheless, both these works were validated mostly in the same settings we use here, where the separation between variables is natural, and the reward in equation 3.2 is well motivated. In these settings our method is simpler, and performs well in comparison (see Figure 7a).\nOur work is most closely related to Heess et al. (2016). We operate under the same basic philosophy, separating the agent into a low-level component that only sees \u201cproprioceptive\u201d information, and works at a fine time scale, and a coarse agent that has access to \u201csensory\u201d information. Like\nthat work, we evaluate or method in the setting of locomotive agents. However, there are several differences between Heess et al. (2016) and this work. There, the low-level component is trained along with a provisional high-level component to solve a set of high-level tasks designed for the agent and transfer tasks, whereas in this work, the low-level controller is trained with only the objective \u201cchange the sensory responses\u201d in each setting. Furthermore, our underlying architecture is different. Whereas in Heess et al. (2016) the high-level agent interacts with the low-level component via continuous control vectors, in this work, we sample one ofN low-level policies, as in Frans et al. (2017). In addition, whereas they train their low-level actor with only the current state, our low-level actor is shown to benefit from using a pattern generator. Finally, because of the structure of our lowlevel task, we need to ensure that the low-level policies are diverse and controllable (which in Heess et al. (2016) is given by the design of the pre-training task), and we do so by using multiple random instantiations of the \u201cchange the sensory response\u201d policies that maximize equation 3.2. Our work can thus be considered in some ways a simplification and generalization of Heess et al. (2016).\nThe use of phase information in the low-level actor recalls Holden et al. (2017); Sharma & Kitani (2018). In the former, a neural network is used to control the gait of a video game character. The weights of the neural network are a function of a phase variable whose cycle corresponds to a cycle of the gait, and are trained as regressors on motion-capture data. The latter work takes this approach to RL problems that have an explicit cyclic structure, in particular the walker and hopper environments from MuJoCo. Phase has also been used in similar way in Peng et al. (2018; 2017; 2016). While we share a similar motivation with these, our low-level actor processes the phases in a different way: instead of a phase variable directly modulating the weight of the network, we concatenate an embedded phase index to the input of the network."
        },
        {
            "heading": "3 METHODOLOGY",
            "text": "Section 3.1 describes the division of the state space into the proprioceptive or internal state and the external state. In section 3.2 we describe the reward function which we use to train the low-level policies. These policies take in the proprioceptive state and the reward maximizes displacement in the external state. Section 3.3 describes how we augment the low-level policy by adding an additional input that tells the agent where it is in a \u201cphase cycle\u201d and constraining the motion of the agent to match that phase cycle. Finally, in section 3.4 we describe how we learn a high-level controller by learning to choose when and which of the trained low-level policies should be used."
        },
        {
            "heading": "3.1 PROPRIOCEPTIVE AND EXTERNAL STATES",
            "text": "We divide the agent\u2019s sensory output into two sets of variables. The proprioceptive states, sp, are the measurements of the agents own body that can be directly affected by control. The external states, se are measurements of the environment outside of the agents direct control.\nThe proprioceptive part of the state is fed into the low-level policy network. This is the part of the state that gives the agent the information about the configuration of its own body and actuators. The low-level policy is independent of the external state as the low-level policies are intended to be generally usefully in moving in the external state independently of the exact location.\nThe external part of the state is fed into the high-level policy network. The high-level policy is responsible for things like moving to particular coordinates in the world, and so the external state is necessary. The high-level policy does not need the low-level states such as joint locations as that is handled by the low-level policy."
        },
        {
            "heading": "3.2 LOW-LEVEL POLICY TRAINING",
            "text": "We would like a set of low-level policies that are diverse, effective, and controllable. Here \u201ceffective\u201d means that they can make significant changes to the environment. \u201cDiverse\u201d means they make different changes, hopefully covering all of the things the agent will need to be able to do in its test task. Finally \u201ccontrollable\u201d means that the low-level policies are easy for the high-level policy to use, for example because they are predictable and their effect is independent of the external state. Empirically, we have found that simply instantiating multiple randomly initialized neural networks trained to change the external perception suffices.\nFormally, our basic low-level reward at each time step is\nRdisp(st, at) = ||set+1 \u2212 set ||2\nUsing this reward scheme, we train a number of agents (16 in the experiments below). We use the same reward for each agent, and across environments (except regularized with the standard environment-specific control and survival rewards). As we show in Section 4.1, changing only the random seed gives us a wide variety of policies that maximize the displacement of the agent."
        },
        {
            "heading": "3.3 PHASE-CONDITIONED POLICIES",
            "text": "One important aspect of our training procedure for our low-level policies is time index or phase inputs. We expect good movement policies to be at least roughly periodic, and phase inputs allow the model to more easily achieve periodicity. Formally, we denote our phase-conditioned policy as \u03c0(sp, \u03c6), where sp is our proprioceptive observation and \u03c6 \u2208 {0, 1, . . .K \u2212 1} is the phase index, whereK is the length of the period (in all our experiments, we chooseK = 10). At each time-step t, during training and evaluation, we receive our new observation sp, and phase index \u03c6 = t (modK).\nIn particular, in our phase-conditioned networks, we take as input sp and a vector b\u03c6 \u2208 Rd, where b\u03c6 is a learned parameter, like a bias term, for each possible phase indices \u03c6. d = 16 in our experiments.\nFor our phase-conditioned policies, we also want to encourage the agents actions and internal states to be cyclic during training; at any given time step, the current action and proprioceptive state should match the one from the last cycle. Just as after one phase period, a pendulum is in the same state as it was before, during any given cycle, the behavior of the agent should be identical.\nFormally, the reward maximization with a cyclic constraint can be written as:\nargmax at\nR(st, at)\nsubject to ||spt \u2212 s p t\u2212K ||2 \u2264 \u03c3s, ||at \u2212 at\u2212K ||2 \u2264 \u03c3a\nThis can be changed to be a cyclic loss as\nargmax at\nR(st, at)\u2212 \u03bbs||spt \u2212 s p t\u2212K ||2 \u2212 \u03bba||at \u2212 at\u2212K ||2\nFor the phase conditioned policies, we include these losses in the reward function during training. In contrast to Holden et al. (2017); Sharma & Kitani (2018), the model need not be exactly periodic."
        },
        {
            "heading": "3.4 HIERARCHICAL TRAINING",
            "text": "Once we have learned a variety of low-level policies, we want to learn how and when to use these low-level policies in a way that lets us solve much more complex task with sparse rewards.\nThe way we compose our low-level policies is quite simple. Similar to Eysenbach et al. (2018) and many other works, at each time step, our high-level policy chooses one of our low-level policies to run. Given N trained low-level policies \u03c0ll(spt , \u03c6), our high-level policy is\n\u03b8 = \u03c0hl(se)\nThis is actually just a choice from among our low-level policies.\nWe go through the complete formulation for our high-level policy in Appendix A."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "In our experiments, we test on a variety of difficult sparse reward problems simulated through Mujoco (Todorov et al., 2012). We use two popular and challenging agents: Ant and Humanoid.\nIn section 4.1 we show some results from our low-level training on Ant and Humanoid. In section 4.2 we show the results of our high-level policies on several different mazes and a navigation task."
        },
        {
            "heading": "4.1 LOW-LEVEL CONTROL",
            "text": "As described in Section 3.2 and Section 3.3, we train a number of low-level policies using our simple reward function. We train Ant and Humanoid in the default flat plane environments during this stage. Figure 1 shows the reward curves of our phase-conditioned low-level policies compared to standard networks. All networks are trained using PPO (Schulman et al., 2017). See Appendix D for more training and network details.\nIn Figure 2 we show the traces of our low-level policies learned on Ant and Humanoid for 100 time steps. Each trajectory represents a different low-level policy. We can see that we learn a great variety of movements, more than sufficient for the high-level policy. We see that Humanoid moves more slowly, both because Humanoid is much more difficult to control, and because we use the standard OpenAI Gym (Brockman et al., 2016) simulator timesteps, which is shorter for Humanoid."
        },
        {
            "heading": "4.2 MAZES AND NAVIGATION",
            "text": "The high-level policy network is trained according to Section 4.2, also using PPO. See Appendix E for more training and network details.\nThe first high-level environment that we evaluate on is the Cross Maze environment with fixed goals from Haarnoja et al. (2018a). As described in that work, the Cross Maze has three different goals along three different paths. In their version of this environment, three different environments are used for each of the three different goal locations and thus a different model is learned for each version of the maze. In Figure 3 we compare our high-level policy to the published numbers from their\nmethod (SAC-LSP) and their baselines (see Haarnoja et al. (2018a) Figure 5). We used a different value for the length of the timestep for Ant in the simulator, so to make a fair comparison, we multiply our number of frames by 2.5 so that our x-axis compares the same amount of simulator time for high-level training. This comparison favors their method as we receive 2.5x fewer observations.\nWe can see that our method with a phase-input low-level policy converges faster than SAC-LSP and converges to a smaller final distance to goal, meaning that our method is both more efficient and more consistent on the task. Our method also has much smaller standard deviation across trials.\nOne major limitation of this environment is that from the perspective of each model, there is just one fixed goal. So to solve it, the policy need only find the one fixed goal for that version of the environment and learn a policy to consistently move to it.\nA much more difficult environment would be one where the goal is randomly chosen from a set of available options each episode. To solve this kind of maze, the policy would have to explore enough to find each goal during the fraction of episodes where it appears, and then consistent learn and keep separate paths of actions in its memory depending on the random goal location for the episode. We refer to these kinds of mazes as \u201crandom goal\u201d mazes. All of the maze results except for Figure 3 use random goals.\nFigure 4 shows the training curves for two random goal mazes: the cross maze from before and the \u201cskull maze.\u201d See Figure 5 for the layout of these mazes. The skull maze has four possible goals instead of three and also forces the agent to make a decision immediately about which way to move, either to the top, left, or right corridor.\nWe compare our method to baselines similar to those used in Haarnoja et al. (2018a), all trained with PPO as is our method. The baseline models are either trained with or without the phase conditioning, and either from scratch, or finetuned (meaning that we initialize the network using a network trained on our low-level objective). We also give some of the baselines more information by also giving them a velocity reward during high-level training (meaning they are rewarded for movement of the agent).\nEven with velocity rewards and pre-training on the low-level objective (both of which could be considered exploration bonuses), all of the baselines fail to get close to the goal locations. The problem of exploration and consistent navigation to different random goals is too difficult to learn from scratch.\nFigure 5 shows traces of Ant navigating the cross and skull mazes. Most traces reach the goal, although some terminate early. All move fairly decisively towards the correct goal.\nIn Figure 6, we show the results using Humanoid on a smaller version of the cross maze. Because the PPO without phase fails at the low-level policy, we omit all non-phase results. The low-level Humanoid control problem is much harder, and moves less quickly (as we saw in Figure 2), so our method does not do as well as it did on Ant. But it is still able to successfully reach the goal much of the time, and again the baselines again fail to learn the maze. To our knowledge, ours is the only work that shows results on a Humanoid maze task.\nIn Appendix F Table 4, we also show the rates of success of these methods at reaching the random goals. Here it is even more obvious that na\u0131\u0308ve RL methods fail completely in this task.\nIn Figure 7a, we compare to Eysenbach et al. (2018) which is similar to our method in the way it composes low-level policies, but generates these using an entropy method. We compare to their results on the Ant Navigation task (see section 5.2 of their paper) in which Ant has to reach four different waypoints and then travel back to the original waypoint. The agent receives reward after reaching each waypoint, so it is another sparse reward task. Because the plot in their paper measures by walltime instead of timesteps, we show their maximum result (DIAYN) as a dotted horizontal line. While their method is only able to achieve an average of about 2.5 waypoints after 15 hours on their hardware, ours can consistently get to all 5 in less time than that, running serially on CPU.\nIn Figure 7b, we look at the effect of the choice of algorithm on our high-level training. We compare PPO with A2C (Mnih et al., 2016) using the implementation from Kostrikov (2018), and our own implementation of DQN (Mnih et al., 2013; Watkins, 1989). See Appendix C for more implimentation and hyperparameter details. We end DQN early as it took much longer in walltime, but we can see that the choice of algorithm is not critical for successfully training. This suggests that our method is fairly robust to the choice of RL algorithm."
        },
        {
            "heading": "5 DISCUSSION",
            "text": "In this work we present a simple, yet effective way of solving difficult sparse reward RL problems. We first train a set of proprioceptive low-level agents on an intuitive reward and then combine them by training a high-level policy to select the appropriate sequence of low-level policies. With this method we solve difficult Mujoco navigation tasks with sparse rewards that appear impossible to solve using standard RL algorithms.\nWe show that our method performs well in difficult sparse reward problems in Mujoco locomotion tasks. However, it can be applied to any environment where we have a strong notion of internal and external observation. Our reward function encourages behaviors of the agent that change the robot\u2019s state in the environment (i.e. \u201ckeep moving\u201d) or changes something else external to the agent. This notion exists in environments beyond our Mujoco locomotion problems. Some possible applications for our method are Atari and video game environments with sparse rewards (e.g. Montezuma\u2019s Revenge) where to receive rewards the agent must complete a long sequence of actions such as finding a key in one area and bringing it to a lock in another. It could also have potential applications\nfor robotics such as in navigation and manipulation where there are not dense rewards or there is a high labeling cost to those rewards.\nIn this work, we do not show any results where we fine-tune the low-level policies during highlevel training. This is relatively straightforward, and we have implemented this. We omitted these results as they did not substantially improve the results. For other problems, however, there may be cases where this is useful. For example, if the friction of the ground or some other property of the environment changed between the high-level and low-level environment fine-tuning might be useful."
        },
        {
            "heading": "A HIGH LEVEL RL FORMULATION DETAILS",
            "text": "Our high-level action space is\n\u0398 = {\u03b81, \u03b82, . . . \u03b8N} \u0398 = {\u03c0 = \u03c0ll1 (set , \u03c6), \u03c0 = \u03c0ll2 (set , \u03c6), . . . \u03c0 = \u03c0llN (set , \u03c6)}\nAs is standard, we add a slowness parameter to the high-level network. That is, the high-level policy makes a choice of which low-level policy to run for the next T time-steps. In our experiments, we choose T = 10. So our optimization for our sparse reward task is\nargmax \u03b8 T\u22121\u2211 i=0 Rtask(st+i, \u03b8) \u2261 argmax j T\u22121\u2211 i=0 Rtask(st+i, \u03c0 ll j (s e t , \u03c6))"
        },
        {
            "heading": "B BASIC ALGORITHM",
            "text": "Algorithm 1 Our method 1: procedure TRAIN POLICIES 2: Train low-level policies: 3: for i in N (or in parallel): 4: initialize(\u03c0lli ) 5: train(\u03c0lli , envll, Tll, algo, objectivell) 6: Train high-level policy: 7: initialize(\u03c0hl) 8: train(\u03c0hl, envhl, Thl, algo, objectivehl)\nIn Algorithm 1 above, we give the high-level algorithm we use. We first train each of ourN low-level policies \u03c0lli for Tll steps, using our RL algorithm algo, on the low-level objective and environment. We then train our high-level policiy \u03c0hl for Thl using RL algorithm algo on the high-level objective and using the high-level environment. Remember that our high-level policy takes our N low-level policies as input and trains to choose which one to run.\nC IMPLEMENTATION DETAILS AND HYPERPARAMETERS\nFor all RL algorithms in the paper (except for DQN in Figure 7b and the values from other works in Figure 3 and Figure 7a), we used the implementation of Kostrikov (2018). For DQN, we wrote our own simple implementation. The hyperparameters for these three algorithms are shown in Tables 1, 2 and 3 We use the ADAM (Kingma & Ba, 2014) optimizer."
        },
        {
            "heading": "D LOW-LEVEL TRAINING AND NETWORK DETAILS",
            "text": "During low-level training we train 80 policies using different random seeds. This is to give us 16 low-level policies for 5 variance runs of our high-level policy. The variance plots in Figure 1 use all 80 runs.\nFor our Ant models, we use a 3-layer MLP with tanh activation functions and a hidden size of 32. For Humanoid we add skip connections between layers and decrease the hidden size to 16.\nFor all models, we use a vector of size 16 for our phase learned parameters b\u03c6.\nWe choose the cyclic constraint multipliers for state (\u03bbs) and action (\u03bba) to be 0.05 and 0.01 respectively."
        },
        {
            "heading": "E HIGH-LEVEL TRAINING",
            "text": "For all high-level result plots, we run 5 independent runs with different random seeds for each result. For any model using pre-trained low-level policies, we use different random low-level policies for each run. For our method, we use N = 16 low-level policies for our high-level training.\nWe give the methods the goal index as a one-hot vector to the high-level policies and baselines."
        },
        {
            "heading": "F SUCCESS RATES ON MAZE TASKS",
            "text": "To give another way of looking at the results on the maze tasks, we show below the average success rate on our three random goal maze environments averaged across runs across the last 100 episodes of training."
        }
    ],
    "year": 2019
}