{
    "abstractText": "We present two simple modifications to the models in the popular Word2Vec tool, in order to generate embeddings more suited to tasks involving syntax. The main issue with the original models is the fact that they are insensitive to word order. While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems. We show improvements in part-ofspeech tagging and dependency parsing using our proposed models.",
    "authors": [
        {
            "affiliations": [],
            "name": "Wang Ling"
        },
        {
            "affiliations": [],
            "name": "Chris Dyer"
        },
        {
            "affiliations": [],
            "name": "Alan Black"
        },
        {
            "affiliations": [],
            "name": "Isabel Trancoso"
        }
    ],
    "id": "SP:e3c50fd7c487e52bf02726c359b25cb89c464587",
    "references": [
        {
            "authors": [
                "Jacob Andreas",
                "Dan Klein."
            ],
            "title": "How much do word embeddings encode about syntax",
            "venue": "Proceedings of ACL.",
            "year": 2014
        },
        {
            "authors": [
                "Mohit Bansal",
                "Kevin Gimpel",
                "Karen Livescu."
            ],
            "title": "Tailoring continuous word representations for dependency parsing",
            "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.",
            "year": 2014
        },
        {
            "authors": [
                "Danqi Chen",
                "Christopher D Manning."
            ],
            "title": "A fast and accurate dependency parser using neural networks",
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 740\u2013750.",
            "year": 2014
        },
        {
            "authors": [
                "Ronan Collobert",
                "Jason Weston",
                "L\u00e9on Bottou",
                "Michael Karlen",
                "Koray Kavukcuoglu",
                "Pavel Kuksa."
            ],
            "title": "Natural language processing (almost) from scratch",
            "venue": "The Journal of Machine Learning Research, 12:2493\u2013 2537.",
            "year": 2011
        },
        {
            "authors": [
                "Paramveer Dhillon",
                "Dean P Foster",
                "Lyle H Ungar."
            ],
            "title": "Multi-view learning of word embeddings via cca",
            "venue": "Advances in Neural Information Processing Systems, pages 199\u2013207.",
            "year": 2011
        },
        {
            "authors": [
                "Dumitru Erhan",
                "Yoshua Bengio",
                "Aaron Courville",
                "PierreAntoine Manzagol",
                "Pascal Vincent",
                "Samy Bengio"
            ],
            "title": "Why does unsupervised pre-training help deep learning",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2010
        },
        {
            "authors": [
                "Manaal Faruqui",
                "Chris Dyer."
            ],
            "title": "Improving vector space word representations using multilingual correlation",
            "venue": "Proceedings of EACL.",
            "year": 2014
        },
        {
            "authors": [
                "Kevin Gimpel",
                "Nathan Schneider",
                "Brendan O\u2019Connor",
                "Dipanjan Das",
                "Daniel Mills",
                "Jacob Eisenstein",
                "Michael Heilman",
                "Dani Yogatama",
                "Jeffrey Flanigan",
                "Noah A Smith"
            ],
            "title": "Part-of-speech tagging for twitter: Annotation, features, and experiments",
            "year": 2011
        },
        {
            "authors": [
                "Yoav Goldberg",
                "Omer Levy"
            ],
            "title": "word2vec explained: deriving mikolov et al.\u2019s negativesampling word-embedding method",
            "venue": "arXiv preprint arXiv:1402.3722",
            "year": 2014
        },
        {
            "authors": [
                "Jiang Guo",
                "Wanxiang Che",
                "Haifeng Wang",
                "Ting Liu."
            ],
            "title": "Revisiting embedding features for simple semisupervised learning",
            "venue": "Proceedings of EMNLP.",
            "year": 2014
        },
        {
            "authors": [
                "Geoffrey E Hinton",
                "Ruslan Salakhutdinov."
            ],
            "title": "A better way to pretrain deep boltzmann machines",
            "venue": "Advances in Neural Information Processing Systems, pages 2447\u20132455.",
            "year": 2012
        },
        {
            "authors": [
                "Eric H Huang",
                "Richard Socher",
                "Christopher D Manning",
                "Andrew Y Ng."
            ],
            "title": "Improving word representations via global context and multiple word prototypes",
            "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-",
            "year": 2012
        },
        {
            "authors": [
                "Nal Kalchbrenner",
                "Phil Blunsom."
            ],
            "title": "Recurrent continuous translation models",
            "venue": "EMNLP, pages 1700\u20131709.",
            "year": 2013
        },
        {
            "authors": [
                "Lingpeng Kong",
                "Nathan Schneider",
                "Swabha Swayamdipta",
                "Archna Bhatia",
                "Chris Dyer",
                "Noah A. Smith."
            ],
            "title": "A dependency parser for tweets",
            "venue": "Proc. of EMNLP, pages 1001\u20131012, Doha, Qatar, October.",
            "year": 2014
        },
        {
            "authors": [
                "Omer Levy",
                "Yoav Goldberg."
            ],
            "title": "Dependencybased word embeddings",
            "venue": "Proceedings of the 52nd",
            "year": 2014
        },
        {
            "authors": [
                "Shujie Liu",
                "Nan Yang",
                "Mu Li",
                "Ming Zhou."
            ],
            "title": "A recursive recurrent neural network for statistical machine translation",
            "venue": "Proceedings of ACL, pages 1491\u2013 1500.",
            "year": 2014
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Ilya Sutskever",
                "Kai Chen",
                "Greg S Corrado",
                "Jeff Dean."
            ],
            "title": "Distributed representations of words and phrases and their compositionality",
            "venue": "Advances in Neural Information Processing Systems, pages 3111\u20133119.",
            "year": 2013
        },
        {
            "authors": [
                "Olutobi Owoputi",
                "Brendan O\u2019Connor",
                "Chris Dyer",
                "Kevin Gimpel",
                "Nathan Schneider",
                "Noah A Smith"
            ],
            "title": "Improved part-of-speech tagging for online conversational text with word clusters",
            "venue": "In HLT-NAACL,",
            "year": 2013
        },
        {
            "authors": [
                "Ilya Sutskever",
                "Oriol Vinyals",
                "Quoc VV Le."
            ],
            "title": "Sequence to sequence learning with neural networks",
            "venue": "Advances in Neural Information Processing Systems, pages 3104\u20133112.",
            "year": 2014
        },
        {
            "authors": [
                "Joseph Turian",
                "Lev Ratinov",
                "Yoshua Bengio."
            ],
            "title": "Word representations: A simple and general method for semi-supervised learning",
            "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL \u201910, pages 384\u2013394, Strouds-",
            "year": 2010
        },
        {
            "authors": [
                "Yi Yang",
                "Jacob Eisenstein."
            ],
            "title": "Unsupervised multi-domain adaptation with feature embeddings",
            "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.",
            "year": 2015
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Word representations learned from neural language models have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These low-dimensional representations are learned as parameters in a language model and trained to maximize the likelihood of a large corpus of raw text. They are then incorporated as features along side hand-engineered features (Turian et al., 2010), or used to initialize the parameters of neural networks targeting tasks for which substantially less training data is available (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014).\nOne of the most widely used tools for building word vectors are the models described in (Mikolov et al., 2013), implemented in the Word2Vec tool,\nin particular the \u201cskip-gram\u201d and the \u201ccontinuous bag-of-words\u201d (CBOW) models. These two models make different independence and conditioning assumptions; however, both models discard word order information in how they account for context. Thus, embeddings built using these models have been shown to capture semantic information between words, and pre-training using these models has been shown to lead to major improvements in many tasks (Collobert et al., 2011). While more sophisticated approaches have been proposed (Dhillon et al., 2011; Huang et al., 2012; Faruqui and Dyer, 2014; Levy and Goldberg, 2014; Yang and Eisenstein, 2015), Word2Vec remains a popular choice due to their efficiency and simplicity.\nHowever, as these models are insensitive to word order, embeddings built using these models are suboptimal for tasks involving syntax, such as part-ofspeech tagging or dependency parsing. This is because syntax defines \u201cwhat words go where?\u201d, while semantics than \u201cwhat words go together\u201d. Obviously, in a model where word order is discarded, the many syntactic relations between words cannot be captured properly. For instance, while most words occur with the word the, only nouns tend to occur exactly afterwords (e.g. the cat). This is supported by empirical evidence that suggests that order-insensitivity does indeed lead to substandard syntactic representations (Andreas and Klein, 2014; Bansal et al., 2014), where systems using pre-trained with Word2Vec models yield slight improvements while the computationally far more expensive which use word order information embeddings of Collobert et al. (2011) yielded much better results.\nIn this work, we describe two simple modifications to Word2Vec, one for the skip-gram model and one for the CBOW model, that improve the quality of the embeddings for syntax-based tasks1. Our goal is to improve the final embeddings while maintaining the simplicity and efficiency of the original models. We demonstrate the effectiveness of our approaches by training, on commodity hardware, on datasets containing more than 50 million sentences and over 1 billion words in less than a day, and show that our methods lead to improvements when used in state-of-the-art neural network systems for part-of-speech tagging and dependency parsing, relative to the original models."
        },
        {
            "heading": "2 Word2Vec",
            "text": "The work in (Mikolov et al., 2013) is a popular choice for pre-training the projection matrix W \u2208 <d\u00d7|V | where d is the embedding dimension with the vocabulary V . As an unsupervised task that is trained on raw text, it builds word embeddings by maximizing the likelihood that words are predicted from their context or vice versa. Two models were defined, the skip-gram model and the continuous bag-of-words model, illustrated in Figure 1.\nThe skip-gram model\u2019s objective function is to maximize the likelihood of the prediction of contextual words given the center word. More formally, given a document of T words, we wish to maximize\nL = 1 T T\u2211 t=1 \u2211 \u2212c\u2264j\u2264c, j 6=0 log p(wt+j | wt) (1)\nWhere c is a hyperparameter defining the window of context words. To obtain the output probability p(wo|wi), the model estimates a matrix O \u2208 <|V |\u00d7dw , which maps the embeddings rwi into a |V |-dimensional vector owi . Then, the probability of predicting the word wo given the word wi is defined as:\np(wo | wi) = eowi (wo)\u2211 w\u2208V e owi (w) (2)\nThis is referred as the softmax objective. However, for larger vocabularies it is inefficient to compute\n1The code developed in this work is made available in https://github.com/wlin12/wang2vec.\nowi , since this requires the computation of a |V |\u00d7dw matrix multiplication. Solutions for problem are addressed in the Word2Vec by using the hierarchical softmax objective function or resorting to negative sampling (Goldberg and Levy, 2014).\nThe CBOW model predicts the center word wo given a representation of the surrounding words w\u2212c, ..., w\u22121, w1, wc. Thus, the output vector ow\u2212c,...,w\u22121,w1,wc is obtained from the product of the matrix O \u2208 <|V |\u00d7dw with the sum of the embeddings of the context words \u2211 \u2212c\u2264j\u2264c,j 6=0 rwj .\nWe can observe that in both methods, the order of the context words does not influence the prediction output. As such, while these methods may find similar representations for semantically similar words, they are less likely to representations based on the syntactic properties of the words."
        },
        {
            "heading": "3 Structured Word2Vec",
            "text": "To account for the lack of order-dependence in the above models, we propose two simple modifications to these methods that include ordering information, which we expect will lead to more syntacticallyoriented embeddings. These models are illustrated in Figure 2."
        },
        {
            "heading": "3.1 Structured Skip-gram Model",
            "text": "The skip-gram model uses a single output matrix O \u2208 <|V |\u00d7d to predict every contextual word w\u2212c, ..., w\u22121, w1, ..., wc, given the embeddings of the center word w0. Our approach adapts the model so that it is sensitive to the positioning of the words. It defines a set of c \u00d7 2 output predictors O\u2212c, ..., O\u22121, O1, Oc, with size O \u2208 <(|V |)\u00d7d. Each of the output matrixes is dedicated to predicting the\noutput for a specific relative position to the center word. When making a prediction p(wo | wi), we select the appropriate output matrix Oo\u2212i to project the word embeddings to the output vector. Note, that the number of operations that must be performed for the forward and backward passes in the network remains the same, as we are simply switching the output layer O for each different word index."
        },
        {
            "heading": "3.2 Continuous Window Model",
            "text": "The Continuous Bag-Of-Words words model defines a window of words w\u2212c, ..., wc with size c, where the prediction of the center word w0 is conditioned on the remaining words w\u2212c, ..., w\u22121, w1, ..., wc. The prediction matrix O \u2208 <(|V |)\u00d7d is fed with the sum of the embeddings of the context words. As such, the order of the contextual words does not influence the prediction of the center word. Our approach defines a different output predictor O \u2208 <(|V |\u00d72cd which receives as input a (2c \u00d7 d)-dimensional vector that is the concatenation of the embeddings of the context words in the order they occur [e(w\u2212c), . . . , e(w\u22121), e(w1), . . . , e(wc)]. As matrix O defines a parameter for the word embeddings for each relative position, this allows the words to be treated differently depending on where they occur. This model, denoted as CWindow, is essentially the window-based model described in (Collobert et al., 2011), with the exception that we do not project the vector of word embeddings into a window embedding before making the final prediction.\nIn both models, we are increasing the number of parameters of matrix O by a factor of c \u00d7 2, which can lead to sparcity problems when training on small datasets. However, these models are generally trained on datasets in the order of 100 millions of words, where these issues are not as severe."
        },
        {
            "heading": "4 Experiments",
            "text": "We conducted experiments in two mainstream syntax-based tasks part-of-speech Tagging and Dependency parsing. Part-of-speech tagging is a word labeling task, where each word is to be labelled with its corresponding part-of-speech. In dependency parsing, the goal is to predict a tree built of syntactic relations between words. In both tasks, it has been\nshown that pre-trained embeddings can be used to achieve better generalization (Collobert et al., 2011; Chen and Manning, 2014)."
        },
        {
            "heading": "4.1 Building Word Vectors",
            "text": "We built vectors for English in two very different domains. Firstly, we used an English Wikipedia dump containing 1,897 million words (60 million sentences), collected in September of 2014. We built word embeddings using the original and our proposed methods on this dataset. These embeddings will be denoted as WIKI(L). Then, we took a sample of 56 million English tweets with 847 million words collected in (Owoputi et al., 2013), and applied the same procedure to build the TWITTER embeddings. Finally, we also use the Wikipedia documents, with 16 million words, provided in the Word2Vec package for contrastive purposes, denoted as WIKI(S). As preprocessing, the text was lowercased and groups of contiguous digits were replaced by a special word. For all corpora, we trained the network with a c = 5, with a negative sampling value of 10 and filter out words with less than 40 instances. WIKI(L) and TWITTER have a vocabulary of 424,882 and 216,871 types, respectively, and embeddings of 50 dimensions.\nTable 1 shows the similarity for a few selected keywords for each of the different embeddings. We can see hints that our models tend to group words that are more syntactically related. In fact, for the word breaking, the CWindow model\u2019s top five words are exclusively composed by verbs in the continuous form, while the Structured Skip-gram model tends to combine these with other forms of the verb break. The original models tend to be less keen on\npreserving such properties. As for the TWITTER embeddings, we can observe that our adapted embeddings are much better at finding lexical variations of the words, such as a-mazing, resembling the results obtained using brown clusters (Owoputi et al., 2013). Finally, for the query person, we can see that our models tend to associate this term to other words in the same class, such as man, woman and child, while original models tend to include unrelated words, such as if and right-thinking.\nIn terms of computation speed, the Skip-gram and CBOW models, achieve a processing rate of 71.35k and 342.17k words per second, respectively. The Structured Skip-gram and CWindow models can process 34.64k and 124.43k words per second, respectively. There is a large drop in computational speed in the CWindow model compared to the CBOW model, as it uses a larger output matrix, which grows with the size of the window. The Structured Skip-gram model processes words at almost half the speed of the Skip-gram model. This is explained by the fact that the Skip-gram model subsamples context words, varying the size of the window size stochastically, so that words closer to the\ncenter word are sampled more frequently. That is, when defining a window size of 5, the actual window size used for each sample is a random value between 1 and 5. As we use a separate output layer for each position, we did not find this property to be useful as it provides less training samples for output matrixes with higher indexes. While our models are slower they are still suitable for processing large datasets as all the embeddings we use were all built within a day."
        },
        {
            "heading": "4.2 Part-Of-Speech Tagging",
            "text": "We reimplemented the window-based model proposed in (Collobert et al., 2011), which defines a 3-layer perceptron. In this network, words are first projected into embeddings, which are concatenated and projected into a window embedding. These are finally projected into an output layer with size of the POS tag vocabulary, followed by a softmax. In our experiments, we used a window size of 5, word embeddings of size 50 and window embeddings of size 500. Word embeddings were initialized using the pre-trained vectors and these parameters are updated as the rest of the network. Additionally, we also add a capitalization feature which indicates whether the first letter of the work is uppercased, as all word features are lowercased words. Finally, for words unseen in the training set and in the pre-trained embeddings, we replace them with a special unknown token, which is also modelled as a word type with a set of 50 parameters. At training time, we stochastically replace word types that only occur once in the training dataset with the unknown token. Evaluation is performed with the part-of-speech tag accuracy, which denotes the percentage of words labelled correctly.\nExperiments are performed on two datasets, the English Penn Treebank (PTB) dataset using the standard train, dev and test splits, and the ARK dataset (Gimpel et al., 2011), with 1000 training, 327 dev and 500 labelled English tweets from Twitter. For the PTB dataset, we use the WIKI(L) embeddings and use TWITTER embeddings for the ARK dataset. Finally, the set of parameters with the highest accuracy in the dev set are used to report the score for the test set.\nResults are shown in Table 2, where we observe that our adapted models tend to yield better re-\nsults than the original models in both datasets. In the Twitter dataset, our results slightly higher than the accuracy reported using only Brown clusters in (Owoputi et al., 2013), which was 89.50. We also try initializing our embeddings with those in (Collobert et al., 2011), which are in the \u201cSenna\u201d row. Even though results are higher in our models, we cannot conclude that our method is better as they are trained crawls from Wikipedia in different time periods. However, it is a good reference to show that our embeddings are on par with those learned using more sophisticated models."
        },
        {
            "heading": "4.3 Dependency Parsing",
            "text": "The evaluation on dependency parsing is performed on the English PTB, with the standard train, dev and test splits with Stanford Dependencies. We use neural network defined in (Chen and Manning, 2014), with the default hyper-parameters2 and trained for 5000 iterations. The word projections are initialized using WIKI(L) embeddings. Evaluation is performed with the labelled (LAS) and unlabeled (UAS) attachment scores.\nIn Table 3, we can observe that results are consistent with those in part-of-speech tagging, where our models obtain higher scores than the original models and with competitive results compared to Senna embeddings. This suggests that our models are suited at learning syntactic relations between words."
        },
        {
            "heading": "5 Conclusions",
            "text": "In this work, we present two modifications to the original models in Word2Vec that improve the word embeddings obtained for syntactically motivated\n2Found in http://nlp.stanford.edu/software/nndep.shtml\ntasks. This is done by introducing changes that make the network aware of the relative positioning of context words. With these models we obtain improvements in two mainstream NLP tasks, namely partof-speech tagging and dependency parsing, and results generalize in both clean and noisy domains."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was partially supported by FCT (INESCID multiannual funding) through the PIDDAC Program funds, and also through projects CMUPT/HuMach/0039/2008 and CMU-PT/0005/2007. The PhD thesis of Wang Ling is supported by FCT grant SFRH/BD/51157/2010. The authors also wish to thank the anonymous reviewers for many helpful comments."
        }
    ],
    "title": "Two/Too Simple Adaptations of Word2Vec for Syntax Problems",
    "year": 2015
}