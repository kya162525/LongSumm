{
    "abstractText": "Much effort has been devoted to evaluate whether multi-task learning can be leveraged to learn rich representations that can be used in various Natural Language Processing (NLP) down-stream applications. However, there is still a lack of understanding of the settings in which multi-task learning has a significant effect. In this work, we introduce a hierarchical model trained in a multi-task learning setup on a set of carefully selected semantic tasks. The model is trained in a hierarchical fashion to introduce an inductive bias by supervising a set of low level tasks at the bottom layers of the model and more complex tasks at the top layers of the model. This model achieves state-of-the-art results on a number of tasks, namely Named Entity Recognition, Entity Mention Detection and Relation Extraction without hand-engineered features or external NLP tools like syntactic parsers. The hierarchical training supervision induces a set of shared semantic representations at lower layers of the model. We show that as we move from the bottom to the top layers of the model, the hidden states of the layers tend to represent more complex semantic information.",
    "authors": [
        {
            "affiliations": [],
            "name": "Victor Sanh"
        },
        {
            "affiliations": [],
            "name": "Thomas Wolf"
        },
        {
            "affiliations": [],
            "name": "Sebastian Ruder"
        }
    ],
    "id": "SP:c3d8d98847bd33fa48bc6448316f78ed7f131afe",
    "references": [
        {
            "authors": [
                "S. Arora",
                "Y. Liang",
                "T. Ma"
            ],
            "title": "A simple but tough-to-beat baseline for sentence embeddings",
            "year": 2017
        },
        {
            "authors": [
                "I. Augenstein",
                "S. Ruder",
                "A. S\u00f8gaard"
            ],
            "title": "Multi-task Learning of Pairwise Sequence Classification Tasks Over Disparate Label Spaces",
            "venue": "Proceedings of NAACL-HLT 2018.",
            "year": 2018
        },
        {
            "authors": [
                "M. Bansal",
                "D. Klein"
            ],
            "title": "Coreference semantics from web features",
            "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL \u201912, 389\u2013398. Stroudsburg, PA, USA: Association for Computational Linguistics.",
            "year": 2012
        },
        {
            "authors": [
                "G. Bekoulis",
                "J. Deleu",
                "T. Demeester",
                "C. Develder"
            ],
            "title": "Joint entity recognition and relation extraction as a multi-head selection problem",
            "year": 2018
        },
        {
            "authors": [
                "Y. Bengio",
                "J. Louradour",
                "R. Collobert",
                "J. Weston"
            ],
            "title": "Curriculum learning",
            "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, ICML \u201909, 41\u201348. New York, NY, USA: ACM.",
            "year": 2009
        },
        {
            "authors": [
                "J. Bingel",
                "A. S\u00f8gaard"
            ],
            "title": "Identifying beneficial task relations for multi-task learning in deep neural networks",
            "venue": "CoRR abs/1702.08303.",
            "year": 2017
        },
        {
            "authors": [
                "R. Caruana"
            ],
            "title": "Multitask learning: A knowledge-based source of inductive bias",
            "venue": "Proceedings of the Tenth International Conference on Machine Learning.",
            "year": 1993
        },
        {
            "authors": [
                "R. Caruana"
            ],
            "title": "Multitask learning",
            "venue": "Mach. Learn. 28(1):41\u201375.",
            "year": 1997
        },
        {
            "authors": [
                "J.P.C. Chiu",
                "E. Nichols"
            ],
            "title": "Named Entity Recognition with Bidirectional LSTM-CNNs",
            "year": 2015
        },
        {
            "authors": [
                "K. Clark",
                "C.D. Manning"
            ],
            "title": "Entity-centric coreference resolution with model stacking",
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 1405\u20131415. Association for Compu-",
            "year": 2015
        },
        {
            "authors": [
                "A. Conneau",
                "D. Kiela",
                "H. Schwenk",
                "L. Barrault",
                "A. Bordes"
            ],
            "title": "Supervised learning of universal sentence representations from natural language inference data",
            "venue": "CoRR abs/1705.02364.",
            "year": 2017
        },
        {
            "authors": [
                "A. Conneau",
                "G. Kruszewski",
                "G. Lample",
                "L. Barrault",
                "M. Baroni"
            ],
            "title": "What you can cram into a single vector: Probing sentence embeddings for linguistic properties",
            "venue": "CoRR abs/1805.01070.",
            "year": 2018
        },
        {
            "authors": [
                "B. Dhingra",
                "Q. Jin",
                "Z. Yang",
                "W.W. Cohen",
                "R. Salakhutdinov"
            ],
            "title": "Neural models for reasoning over multiple mentions using coreference",
            "venue": "CoRR abs/1804.05922.",
            "year": 2018
        },
        {
            "authors": [
                "G. Doddington",
                "A. Mitchell",
                "M. Przybocki",
                "L. Ramshaw",
                "S. Strassel",
                "R. Weischedel"
            ],
            "title": "The automatic content extraction (ace) program-tasks, data, and evaluation",
            "venue": "2.",
            "year": 2004
        },
        {
            "authors": [
                "G. Durrett",
                "D. Klein"
            ],
            "title": "Easy victories and uphill battles in coreference resolution",
            "venue": "EMNLP 2013 - 2013 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference, 1971\u20131982.",
            "year": 2013
        },
        {
            "authors": [
                "G. Durrett",
                "D. Klein"
            ],
            "title": "A joint model for entity analysis: Coreference, typing, and linking",
            "venue": "Transactions of the Association for Computational Linguistics, volume 2, 477\u2013490.",
            "year": 2014
        },
        {
            "authors": [
                "R.M. French"
            ],
            "title": "Catastrophic forgetting in connectionist networks",
            "venue": "Trends in Cognitive Sciences 3(4):128 \u2013 135.",
            "year": 1999
        },
        {
            "authors": [
                "K. Hashimoto",
                "C. Xiong",
                "Y. Tsuruoka",
                "R. Socher"
            ],
            "title": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Jernite",
                "S.R. Bowman",
                "D. Sontag"
            ],
            "title": "Discourse-based objectives for fast unsupervised sentence representation learning",
            "venue": "CoRR abs/1705.00557.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Ji",
                "C. Tan",
                "S. Martschat",
                "Y. Choi",
                "N.A. Smith"
            ],
            "title": "Dynamic Entity Representations in Neural Language Models",
            "year": 2017
        },
        {
            "authors": [
                "A. Katiyar",
                "C. Cardie"
            ],
            "title": "Going out on a limb: Joint Extraction of Entity Mentions and Relations without Dependency Trees",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 917\u2013928.",
            "year": 2017
        },
        {
            "authors": [
                "R. Kiros",
                "Y. Zhu",
                "R. Salakhutdinov",
                "R.S. Zemel",
                "A. Torralba",
                "R. Urtasun",
                "S. Fidler"
            ],
            "title": "Skip-thought vectors",
            "venue": "arXiv preprint arXiv:1506.06726.",
            "year": 2015
        },
        {
            "authors": [
                "J.D. Lafferty",
                "A. McCallum",
                "F.C.N. Pereira"
            ],
            "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
            "venue": "282\u2013289.",
            "year": 2001
        },
        {
            "authors": [
                "G. Lample",
                "M. Ballesteros",
                "S. Subramanian",
                "K. Kawakami",
                "C. Dyer"
            ],
            "title": "Neural Architectures for Named Entity Recognition",
            "year": 2016
        },
        {
            "authors": [
                "K. Lee",
                "L. He",
                "M. Lewis",
                "L. Zettlemoyer"
            ],
            "title": "End-to-end Neural Coreference Resolution",
            "year": 2017
        },
        {
            "authors": [
                "Q. Li",
                "H. Ji"
            ],
            "title": "Incremental Joint Extraction of Entity Mentions and Relations",
            "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014) 402\u2013 412.",
            "year": 2014
        },
        {
            "authors": [
                "W. Ling",
                "T. Lu\u0131\u0301s",
                "L. Marujo",
                "R.F. Astudillo",
                "S. Amir",
                "C. Dyer",
                "A.W. Black",
                "I. Trancoso"
            ],
            "title": "Finding function in form: Compositional character models for open vocabulary word representation",
            "venue": "CoRR abs/1508.02096",
            "year": 2015
        },
        {
            "authors": [
                "B. Mccann",
                "N.S. Keskar",
                "C. Xiong",
                "R. Socher"
            ],
            "title": "The Natural Language Decathlon : Multitask Learning as Question Answering",
            "venue": "(Nips).",
            "year": 2018
        },
        {
            "authors": [
                "T.M. Mitchell"
            ],
            "title": "The need for biases in learning generalizations",
            "venue": "Technical report.",
            "year": 1980
        },
        {
            "authors": [
                "M. Miwa",
                "M. Bansal"
            ],
            "title": "End-to-end relation extraction using lstms on sequences and tree structures",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 1105\u20131116. Association for Computational Linguistics.",
            "year": 2016
        },
        {
            "authors": [
                "J. Pennington",
                "R. Socher",
                "C.D. Manning"
            ],
            "title": "Glove: Global vectors for word representation",
            "venue": "Empirical Methods in Natural Language Processing (EMNLP), 1532\u20131543.",
            "year": 2014
        },
        {
            "authors": [
                "M.E. Peters",
                "M. Neumann",
                "M. Iyyer",
                "M. Gardner",
                "C. Clark",
                "K. Lee",
                "L. Zettlemoyer"
            ],
            "title": "Deep contextualized word representations",
            "year": 2018
        },
        {
            "authors": [
                "S. Pradhan",
                "A. Moschitti",
                "N. Xue",
                "O. Uryupina",
                "Y. Zhang"
            ],
            "title": "Conll-2012 shared task: Modeling multilingual unrestricted coreference in ontonotes",
            "venue": "EMNLP-CoNLL Shared Task.",
            "year": 2012
        },
        {
            "authors": [
                "S. Pradhan",
                "A. Moschitti",
                "N. Xue",
                "H. Tou Ng",
                "A. Bjrkelund",
                "O. Uryupina",
                "Y. Zhang",
                "Z. Zhong"
            ],
            "title": "Towards robust linguistic analysis using ontonotes",
            "venue": "143\u2013152.",
            "year": 2013
        },
        {
            "authors": [
                "A. Rahman",
                "V. Ng"
            ],
            "title": "Supervised models for coreference resolution",
            "year": 2009
        },
        {
            "authors": [
                "S. Ruder",
                "J. Bingel",
                "I. Augenstein",
                "A. S\u00f8gaard"
            ],
            "title": "Learning what to share between loosely related tasks",
            "venue": "arXiv preprint arXiv:1705.08142.",
            "year": 2017
        },
        {
            "authors": [
                "S. Ruder"
            ],
            "title": "An Overview of Multi-Task Learning in Deep Neural Networks",
            "venue": "arXiv (June).",
            "year": 2017
        },
        {
            "authors": [
                "S. Singh",
                "S. Riedel",
                "B. Martin",
                "J. Zheng",
                "A. McCallum"
            ],
            "title": "Joint Inference of Entities, Relations, and Coreference",
            "venue": "Automated Knowledge Base Construction (AKBC CIKM 2013) 1\u20136.",
            "year": 2013
        },
        {
            "authors": [
                "A. S\u00f8gaard",
                "Y. Goldberg"
            ],
            "title": "Deep multi-task learning with low level tasks supervised at lower layers",
            "venue": "ACL.",
            "year": 2016
        },
        {
            "authors": [
                "E. Strubell",
                "P. Verga",
                "D. Belanger",
                "A. McCallum"
            ],
            "title": "Fast and Accurate Entity Recognition with Iterated Dilated Convolutions",
            "year": 2017
        },
        {
            "authors": [
                "S. Subramanian",
                "A. Trischler",
                "Y. Bengio",
                "C.J. Pal"
            ],
            "title": "Learning General Purpose Distributed Sentence Representations Via Large Scale Multi-Task Learning",
            "year": 2018
        },
        {
            "authors": [
                "Z. Yang",
                "P. Blunsom",
                "C. Dyer",
                "W. Ling"
            ],
            "title": "Referenceaware language models",
            "venue": "CoRR abs/1611.01628.",
            "year": 2016
        },
        {
            "authors": [
                "Z. Yang",
                "R. Salakhutdinov",
                "W. Cohen"
            ],
            "title": "Multi-Task Cross-Lingual Sequence Tagging from Scratch",
            "year": 2016
        }
    ],
    "sections": [
        {
            "heading": "Introduction",
            "text": "Recent Natural Language Processing (NLP) models heavily rely on rich distributed representations (typically word or sentence embeddings) to achieve good performance. One example are so-called \u201cuniversal representations\u201d (Conneau et al. 2017) which are expected to encode a varied set of linguistic features, transferable to many NLP tasks. This kind of rich word or sentence embeddings can be learned by leveraging the training signal from different tasks in a multi-task setting. It is known that a model trained in a multi-task framework can take advantage of inductive transfer between the tasks, achieving a better generalization performance (Caruana 1993). Recent works in sentence embeddings (Subramanian et al. 2018; Jernite, Bowman, and Sontag 2017) indicate that complementary aspects of the sentence (e.g. syntax, sentence length, word order) should be encoded in order for the model to produce sentence embeddings that are able to generalize over a wide range of tasks. Complementary aspects in representations can be naturally encoded by training a model on a set of diverse\nCopyright c\u00a9 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\ntasks, such as, machine translation, sentiment classification or natural language inference. Although, (i) the selection of this diverse set of tasks, as well as, (ii) the control of the interactions between them are of great importance, a deeper understanding of (i) and (ii) is missing as highlighted in the literature (Caruana 1997; Mitchell 1980; Ruder 2017). This work explores this line of research by combining, in a single model, four fundamental semantic NLP tasks: Named Entity Recognition, Entity Mention Detection (also sometimes referred as mention detection), Coreference Resolution and Relation Extraction. This selection of tasks is motivated by the inter-dependencies these tasks share. In Table 1, we give three simple examples to exemplify the reasons why these tasks should benefit from each other. For instance, in the last example knowing that the company and Dell are referring to the same real world entity, Dell is more likely to be an organization than a person.\nSeveral prior works (Yang, Salakhutdinov, and Cohen 2016; Bingel and S\u00f8gaard 2017) avoid the question of the linguistic hierarchies between NLP tasks. We argue that some tasks (so-called \u201clow level\u201d tasks) are simple and require a limited amount of modification to the input of the model while other tasks (so-called \u201chigher level\u201d tasks) require a deeper processing of the inputs and likely a more complex architecture. Following (Hashimoto et al. 2017; S\u00f8gaard and Goldberg 2016), we therefore introduce a hierarchy between the tasks so that low level tasks are supervised at lower levels of the architecture while keeping more com-\nar X\niv :1\n81 1.\n06 03\n1v 2\n[ cs\n.C L\n] 2\n6 N\nov 2\n01 8\nplex interactions at deeper layers. Unlike previous works (Li and Ji 2014; Miwa and Bansal 2016), our whole model can be trained end-to-end without any external linguistic tools or hand-engineered features while giving stronger results on both Relation Extraction and Entity Mention Detection.\nOur main contributions are the following: (1) we propose a multi-task architecture combining four different tasks that have not been explored together to the best of our knowledge. This architecture uses neural networks and does not involve external linguistic tools or hand-engineered features. We also propose a new sampling strategy for multi-task learning, proportional sampling. (2) We show that this architecture can lead to state-of-the art results on several tasks e.g. Named Entity Recognition, Relation Extraction and Entity Mention Detection while using simple models for each of these tasks. This suggests that the information encoded in the embeddings is rich and covers a variety of linguistic phenomena. (3) We study and give insights on the influence of multi-task learning on (i) the speed of training and (ii) the type of biases learned in the hierarchical model."
        },
        {
            "heading": "Model",
            "text": "In this section, we describe our model beginning at the lower levels of the architecture and ascending to the top layers. Our model introduces a hierarchical inductive bias between the tasks by supervising low-level tasks (that are assumed to require less knowledge and language understanding) at the bottom layers of the model architecture and supervising higher-level tasks at higher layers. The architecture of the model is shown in Figure 1. Following (Hashimoto et al. 2017), we use shortcut connections so that top layers can have access to bottom layer representations."
        },
        {
            "heading": "Words embeddings",
            "text": "Our model encodes words wt of an input sentence s = (w1, w2, ..., wn) as a combination of three different types of embeddings. We denote the concatenation of the these three embeddings ge. Pre-trained word embeddings: We use GloVe (Pennington, Socher, and Manning 2014) pre-trained word level embeddings. These embeddings are fine-tuned during training. Pre-trained contextual word embeddings: We also use contextualized ELMo embeddings (Peters et al. 2018). These word embeddings differ from GloVe word embeddings in that each token is represented by a vector that is a function of the whole sentence (a word can thus have different representations depending on the sentence it is extracted from). These representations are given by the hidden states of a bidirectional language model. ELMo embeddings have been shown to give state-of-the-art results in multiple NLP tasks (Peters et al. 2018). Character-level word embeddings: Following (Chiu and Nichols 2015; Lample et al. 2016), we use character-level word embeddings to extract character-level features. Specifically, we use a convolutional neural network (CNN) (followed by a max pooling layer) for the ease of training since Recurrent Neural Network-based encodings do not significantly outperform CNNs while being computationally more expensive to train (Ling et al. 2015)."
        },
        {
            "heading": "Named Entity Recognition (NER)",
            "text": "The first layers of our model are supervised by Named Entity Recognition labels. NER aims to identify mentions of named entities in a sequence and classify them into predefined categories. In accordance with previous work (Chiu and Nichols 2015; Lample et al. 2016) the tagging module contains an RNN-based encoding layer followed by a sequence tagging module based on a conditional random field (Lafferty, McCallum, and Pereira 2001). We use multi-layer bi-directional LSTMs (Long Short-Term Memory) as encoders. The encoders take as input the concatenated word embeddings ge and produce (sequence) embeddings gner. Specifically, gner are the concatentation of the backward and forward hidden states of the top layer of the biLSTMs, which are then fed to the sequence tagging layer.\nWe adopt the BILOU (Beginning, Inside, Last, Outside, Unit) tagging scheme. The tagging decisions are modeled using a CRF, which explicitly reasons about interactions between neighbour tokens tags."
        },
        {
            "heading": "Entity Mention Detection (EMD)",
            "text": "A second group of layers of our model are supervised using Entity Mention Detection labels. EMD is similar in spirit to NER but more general as it aims to identify all the mentions related to a real life entity, whereas NER only focuses on the named entities. Let us consider an example: [The men]PERS held on [the sinking vessel]V EH until [the passenger ship]V EH was able to reach them from [Corsica]GPE . Here, NER annotations would only tag Corsica, while EMD requires a deeper understanding of the entities in the sentence.\nWe formulate Mention Detection as a sequence tagging task using a BILOU scheme. We use a multi-layer biLSTM followed by a CRF tagging layer. We adopt shortcut connections so that each layer can build on top of the representations extracted by the lower layers in a hierarchical fashion. The encoder thus takes as input the concatenation of the lower layer representations [ge, gner] and outputs sequence embeddings denoted by gemd.\nTo be able to compare our results with previous works (Bekoulis et al. 2018; Miwa and Bansal 2016; Katiyar and Cardie 2017) on EMD, we identify the head of the entity mention rather than the whole mention."
        },
        {
            "heading": "Coreference Resolution (CR)",
            "text": "Ascending one layer higher in our model, CR is the task of identifying mentions that are referring to the same real life entity and cluster them together (typically at the level of a few sentences). For instance, in the example My mom tasted the cake. She said it was delicious, there are two clusters: (My mom, She) and (the cake, it). CR is thus a task which requires a form of semantic representation to cluster the mentions pointing to the same entity.\nWe use the model proposed in (Lee et al. 2017). This model considers all the spans in a document as potential mentions and learns to distinguish the candidate coreferent mentions from the other spans using a mention scorer to prune the number of mentions. The output of the mention scorer is fed to a mention pair scorer, which decides whether identified candidates mentions are coreferent. The main elements introduced in (Lee et al. 2017) are the use of span embeddings to combine context-dependent boundary representation and an attention mechanism over the span to point to the mention\u2019s head. This model is trained fully end-to-end without relying on external parser pre-processing.\nThe encoder takes as input the concatenation of [ge, gemd] and outputs representations denoted as gcr, which are then fed to the mention pair scorer."
        },
        {
            "heading": "Relation Extraction (RE)",
            "text": "The last supervision of our model is given by Relation Extraction (RE). RE aims at identifying semantic relational structures between entity mentions in unstructured text. Traditional systems treat this task as two pipelined tasks: (i) identifying mentions and (ii) classifying the relations between identified mentions. We use the Joint Resolution Model proposed by Bekoulis et al. (2018) in which the selection of the mentions and classification of the relation between these mentions are performed jointly. Following previous work (Li and Ji 2014; Katiyar and Cardie 2017; Bekoulis et al. 2018), we only consider relations between the last token of the head mentions involved in the relation. Redundant relations are therefore not classified.\nThe RE encoder is a multi-layer BiLSTM which takes as input [ge, gemd] and outputs a representation denoted gre. These contextualized representations are fed to a feedforward neural network. More specifically, considering two token\u2019s contextualized representations, gi and gj , both of size\nRl, we compute a vector score:\nt(wi, wj) = V \u03c6(Ugj +Wgi + b) (1)\nwhere U \u2208 Rd\u00d7l, W \u2208 Rd\u00d7l, b \u2208 Rd, and V \u2208 Rr\u00d7d are learned transformation weights, l is the size of the embeddings output by the encoder, d is the size of the hidden layer of the feedforward network, r is the number of possible relations, and \u03c6 is a non-linear activation function. The relation probabilities are then estimated as p = \u03c3(t(wi, wj)) \u2208 Rr where pk (1 \u2264 k \u2264 r) is the probability that token wi and token wj are respectively labeled as ARG1 and ARG2 in a relation of type k. The model predictions are computed by thresholding estimated probabilities. The parameters of the model V , U , W , and b are trained by minimizing a crossentropy loss.\nIn this formulation, a mention may be involved in several relations at the same time (for instance being the ARG1 and the ARG2 in two respective relations), which can occur in real life examples. If we replaced the sigmoid function by a softmax function, this is not possible.\nIn the model, the CR and RE modules are both on the same level. We did not find it helpful to introduce a hierarchical relation between these two tasks as they both rely on deeper semantic modeling, i.e. both trying to link mentions."
        },
        {
            "heading": "Experiment setting",
            "text": ""
        },
        {
            "heading": "Datasets and evaluation metrics",
            "text": "We use labeled data from different sources to train and evaluate our model. For NER, we use the English portion of OntoNotes 5.0 (Pradhan et al. 2013). Following Strubell et al. (2017), we use the same data split as used for coreference resolution in the CoNLL-2012 shared task (Pradhan et al. 2012). We report the performance on NER using span level F1 score on the test set. The dataset covers a large set of document types (including telephone conversations, web text, broadcast news and translated documents), and a diverse set of 18 entity types (including PERSON, NORP, FACILITY, ORGANIZATION, GPE). Statistics of the corpus are detailed in Table 2. We also report performance on more commonly used CoNLL2003 NER dataset.\nFor CR, EMD and RE, we use the Automatic Content Extraction (ACE) program ACE05 corpus (Doddington et al. 2004). The ACE05 corpus is one of the largest corpus annotated with CR, EMD and RE making it a compelling dataset for multi-task learning. Mention tags in ACE05 cover 7 types of entities such as Person, Organization, or Geographical Entities. For each entity, both the mention boundaries and the head spans are annotated. ACE05 also introduces 6 relation types (including OrganizationAffiliation (ORG-AFF), GEN-Affiliation (GEN-AFF), and Part-Whole (PART-WHOLE)). We use the same data splits as previous work (Li and Ji 2014; Miwa and Bansal 2016; Katiyar and Cardie 2017) for both RE and EMD and report F1-scores, Precision, and Recall. We consider an entity mention correct if the model correctly predicted both the mention\u2019s head and its type. We consider a relation correct if the model correctly predicted the heads of the two arguments and the relation type.\nFor CR, we use different splits to be able to compare to previous work (Bansal and Klein 2012; Durrett and Klein 2014). These splits (introduced in (Rahman and Ng 2009)) use the whole ACE05 dataset leaving 117 documents for test while having 482 documents for training (as in (Bansal and Klein 2012), we randomly split the training into a 70/30 ratio to form a validation set). We evaluate coreference on both splits. We compare all coreference systems using the commonly used metrics: MUC, B3, CEAFe (CEAF\u03c64) as well as the average F1 of the three metrics as computed by the official CoNLL-2012 scorer. Note that Durrett and Klein make use of external NLP tools including an automatic parser (Durrett and Klein 2013).\nWe compare our model to several previous systems that have driven substantial improvements over the past few years both using graphical models or neural-net-based models. These are the strongest baselines to the best of our knowledge."
        },
        {
            "heading": "Training Details",
            "text": "Subramanian et al. (2018) observe that there is no clear consensus on how to correctly train a multi-task model. Specifically, there remain many open questions such as \u201cwhen should the training schedule switch from one task to another task?\u201d or \u201cshould each task be weighted equally?\u201d One of the main issues that arises when training a multi-task model is catastrophic forgetting (French 1999) where the model abruptly forgets part of the knowledge related to a previously learned task as a new task is learned. This phenomenon is especially present when multiple tasks are trained sequentially.\nWe selected the simple yet effective training method described in (S\u00f8gaard and Goldberg 2016; Ruder et al. 2017): after each parameter update, a task is randomly selected and a batch of the dataset attached to this task is also sampled at random to train the model. This process is repeated until convergence (the validation metrics do not improve anymore). We tested both uniform and proportional sampling and found that proportional sampling performs better both in terms of performance and speed of convergence. In proportional sampling, the probability of sampling a task is proportional to the relative size of each dataset compared to the cumulative size of all the datasets. Note that unlike (Subramanian et al. 2018), the updates for a particular task affect the layers associated with this task and all the layers below but not the layers above."
        },
        {
            "heading": "Results and Discussion",
            "text": ""
        },
        {
            "heading": "Overall Performance",
            "text": "In this section, we present our main results on each task and dataset. The hierarchical model and multi-task learning framework presented in this work achieved state-of-the-art results on three tasks, namely NER (+0.52), EMD (+3.8) and RE (+6.8). Table 3 summarizes the results and introduces each setups\u2019 abbreviation (alphabetical letters). In the following subsections, we highlight a few useful observations.\nTo be able to compare our work on CR with the various baselines, we report results using different settings and splits. More precisely, GM indicates that gold mentions were used for evaluation and that coreference was trained using the ACE05 splits introduced in (Rahman and Ng 2009).\nUsing gold mentions is impossible in real settings so we also relax this condition leading to a more challenging task in which we make no use of external tools or metadata (such as speaker ID used by some systems (Clark and Manning 2015)). Comparing setups A and A-GM shows how the supervision from one module (e.g. CR) can flow through the entire architecture and impact other tasks\u2019 performance: RE\u2019s F1 score drops by \u223c1 point on A. Note that the GM setup impacts the training exit condition (the validation metrics stop improving) and the evaluation metrics (it is well known that using gold mentions at evaluation time improves CR\u2019s performance). Similarly, the A-GM setup leads to the state-of-the-art on EMD and RE. It increases the F1 by\u223c1.5 points for EMD and \u223c1 point for RE (A vs. A-GM). This suggests that having different type of information on different sentences brings richer information than having multiple types of information on the same sentences (setup ACoNLL2012 -see Table 4- supports this claim as CR trained on another dataset leads to comparable performance on the three other tasks).\nTo analyze which components of the model are driving the improvements and understand the interactions between the different tasks and modules, we performed an ablation study summarized in the following paragraphs and on Tables 3 and 5.\nSingle Task vs. Full Models The largest difference between a single-task and a multi-task setup is observed on the RE task (A vs. D on Table 3), while the results on NER are similar in multi-task and single-task setups (B vs. A & A-GM). This further highlights how the RE module can be sensitive to information learned from other tasks. Results on EMD are in the middle, with the single task setup giving higher score than a multi-task-setup except for A-GM and I. More surprisingly, CR can give slightly better results when being single-task-trained (A vs. E).\nProgressively adding tasks To better understand the contribution of each module, we vary the number of tasks in our training setup. The experiments show that training using RE helps both for NER and for EMD. Adding RE supervision leads to an increase of\u223c1 point on NER while boosting both precision and recall on EMD (F vs. I). CR and RE can help NER as shown by comparing setups A and F: recall and F1\nfor NER are \u223c1 point stronger, while the impact on EMD is negative. Finally, training using CR supervision boosts NER (F vs. J) by increasing NER\u2019s recall while lowering EMD\u2019s precision and F1. In other words the information flowing along the hierarchical model (e.g. stacking of encoders and shortcut connections) enables higher levels\u2019 supervision to train lower levels to learn better representations. More generally, whenever the task RE is combined with another task, it always increases the F1 score (most of the improvement coming from the precision) by 2\u20136 F1 points.\nExperimenting with the hierarchy order Comparing setups F vs. K and A vs. L in which we switched NER and EMD, provides evidence for the hierarchical relation between NER and EMD: supervising EMD at a lower level than NER is detrimental to the overall performance. This supports our intuition that the hierarchy should follow the intrinsic difficulty of the tasks.\nComparison to other canonical datasets We also compare our model on two other canonical datasets for NER (CoNLL-2003) and CR (CoNLL-2012). Details are reported in Table 4. We did not tune hyperparameters, keeping the same hyperparameters as used in the previous experiments. We reach performance comparable to previous work and the other tasks, demonstrating that our improvements are not dataset-dependent.\nEffect of the embeddings We perform an ablation study on the words and character embeddings ge. Results are reported in Table 5. As expected, contextualized ELMo embeddings have a noticeable effect on each metrics. Removing ELMo leads to a \u223c4 F1 points drop on each task. Furthermore, character-level embeddings, which are sensitive to morphological features such as prefixes and suffixes and capitalization, also have a strong impact, in particular on NER, RE and CR. Removing character-level embeddings\ndoes not affect EMD suggesting that the EMD module can compensate for this information. The main improvements on the CR task stem from the increase in B3 and Ceafe metrics. Note that the strong effect of removing a type of embedding is also a consequence of using shortcut connections: removing an embedding has a direct impact on the input to each task\u2019s module.\nWhat did the embeddings and encoders learn? High scores on a specific task suggest that the representations learned by the encoders (and the embeddings) have somehow managed to capture relevant linguistic or statistical features for the task. However, using complex archi-\ntectures makes it difficult to understand what is actually encoded in the embeddings and hidden states and what type of linguistic information, if any, is being used by the model to make a particular prediction. To further understand our architecture, we analyze the inductive biases encoded in the embeddings and hidden states of the various layers. We follow Conneau et al. (2018) who introduced 10 different probing tasks1 to analyze the quality of sentence embeddings. These tasks aim at evaluating a wide set of linguistic properties from surface information, to syntactic information through semantic information.\nWe use a simple logistic regression classifier, which takes the sentence embeddings as inputs and predicts the linguistic property. We study both the word embeddings (ge) and the hidden state representations (biLSTM encoders) specific to each module in our model. The sentence embedding of an input sequence of length L is computed from the L hidden states of an encoder by taking the maximum value over each dimension of the last layer activations as done in (Conneau et al. 2017). Sentence embeddings are obtained from word and character-level embeddings by max-pooling over a sentence\u2019s words. Averaging word embeddings is known to be a strong baseline for sentence embeddings (Arora, Liang, and Ma 2017) and we also report the results of this simpler procedure in Table 6.\nResults We compare our results with two baselines from (Conneau et al. 2018): bag-of-words computed from FastText embeddings and SkipThought sentence embeddings (Kiros et al. 2015). We compare the base word embeddings ge of our model with the first baseline and the output of the task-specific encoders to the second baseline. A first observation is that the word embeddings already encode rich representations, having an accuracy higher than 70% on seven of the ten probing tasks. We suspect that shortcut connections are key to this good performances by allowing high level tasks to encode rich representations. The good performance on Bigram Shift (compared to BoV-FastText: +38.8) likely comes from the use of ELMo embeddings which are sensitive to word order. The same argument may also explain the strong performance on Sentence Length.\nThere are significant discrepancies between the results of the word embeddings ge and the encoder representations, indicating that the learned linguistic features are different between these two types of embeddings. Averaging the base embeddings surpasses encoder embeddings on almost all the probing tasks (except Coordination Inversion). The difference is particularly high on the Word Content task in which the results of the encoders embeddings barely rise above 11.0, indicating that the ability to recover a specific word is not a useful feature for our four semantic tasks.\nThe performance of the encoder representation is stronger on semantic probing tasks, compared to the low signal for surface and syntatic tasks. The only exception is the Sentence Length which suggest that this linguistic aspect is naturally encoded. The performances of the NER and EMD encoders are generally in the same range supporting the fact\n1A probing task is a classification task that focuses on a well identified linguistic property.\nthat these two tasks are similar in nature. Finally, we observe that the highest scores for encoder representations always stem from the coreference encoder suggesting that CR is both the highest level task and that the CR module requires rich and diverse representations to make a decision.\nMulti-task learning accelerating training It is also interesting to understand the influence of a multitask learning framework on the training time of the model. In the following section, we compare the speed of training in terms of number of parameter updates (a parameter update being equal to a back-propagation pass) for each of the tasks in the multi-task framework against a single-task framework. The speed of training is defined as the number of updates necessary to reach convergence based on the validation metric.\nResults are presented in Table 7 for the best performing multi-task model (A-GM). The multi-task framework needs less updates to reach comparable (or higher) F1 score in most cases except on the RE task. This supports the intuition that knowledge gathered from one task is beneficial to the other tasks in the hierarchical architecture of our model."
        },
        {
            "heading": "Related work",
            "text": "Our work is most related to Hashimoto et al. (2017) who develop a joint hierarchical model trained on syntactic and semantic tasks. The top layers of this model are supervised by semantic relatedness and textual entailment between two input sentences, implying that the lower layers need to be run two times on different input sentences. Our choice of tasks avoids such a setup. Our work also adopts a different approach to multi-task training (proportional sampling) therefore avoiding the use of complex regularization schemes to prevent catastrophic forgetting. Our results show that strong performances can be reached without these ingredients. In addition, the tasks examined in this work are more focused on learning semantic representations, thereby reducing the need to learn surface and syntactic information, as evidenced by the linguistic probing tasks.\nUnlike (Hashimoto et al. 2017) and other previous work (Katiyar and Cardie 2017; Bekoulis et al. 2018; Augenstein, Ruder, and S\u00f8gaard 2018), we do not learn label embeddings, meaning that the (supervised) output/prediction of a layer is not directly fed to the following layer through an embedding learned during training. Nonetheless, sharing embeddings and stacking hierarchical encoders allows us to share the supervision from each task along the full structure of our model and achieve state-of-the-art performance.\nUnlike some studies on multi-task learning such as (Subramanian et al. 2018), each task has its own contextualized encoder (multi-layer BiLSTM) rather than a shared one, which we found to improve the performance.\nOur work is also related to S\u00f8gaard and Goldberg (2016) who propose to cast a cascade architecture into a multi-task learning framework. However, this work was focused on syntactic tasks and concluded that adding a semantic task like NER to a set of syntactic tasks does not bring any improvement. This confirms the intuition that multi-task learn-\ning is mostly effective when tasks are related and that syntactic and semantic tasks may be too distant to take advantage of shared representations. The authors used a linear classifier with a softmax activation, relying on the richness on the embeddings. As a consequence the sequence tagging decisions are made independently for each token, in contrast to our work.\nOne central question in multi-task learning is the training procedure. Several schemes have been proposed in the literature. Hashimoto et al. (2017) train their hierarchical model following the model\u2019s architecture from bottom to top: the trainer successively goes through the whole dataset for each task before moving to the task of the following level. The underlying hypothesis is that the model should perform well on low-level tasks before being trained in more complicated tasks. Hashimoto et al. avoid catastrophic forgetting by introducing successive regularization using slack constraints on the parameters. Subramanian et al. (2018) adopt a simpler strategy for each parameter update: a task is randomly selected and a batch of the associated dataset is sampled for the current update. More recently, Mccann et al. (2018) explored various batch-level sampling strategies and showed that an anti-curriculum learning strategy (Bengio et al. 2009) is most effective. In contrast, we propose a novel proportional sampling strategy, which we find to be more effective.\nRegarding the selection of the set of tasks, our work is closest to (Durrett and Klein 2014; Singh et al. 2013). Durrett and Klein (2014) combine coreference resolution, entity linking (sometimes referred to as Wikification) and mention detection. Singh et al. (2013) combine entity tagging, coreference resolution and relation extraction. These two works are based on graphical models with hand-engineered factors.\nWe are using a neural-net-based approach fully trainable in an end-to-end fashion, with no need for external NLP tools (such as in (Durrett and Klein 2014)) or hand-engineered features. Coreference resolution is rarely used in combination with other tasks. The main work we are aware of is (Dhingra et al. 2018), which uses coreference clusters to improve reading comprehension and the works on language modeling by Ji et al. (2017) and Yang et al. (2016).\nRegarding the combination of entity mention detection and relation, we refer to our baselines detailed above. Here again, our predictors do not require additional features like dependency trees (Miwa and Bansal 2016) or handengineered heuristics (Li and Ji 2014)."
        },
        {
            "heading": "Conclusion",
            "text": "We proposed a hierarchically supervised multi-task learning model focused on a set of semantic task. This model achieved state-of-the-art results on the tasks of Named Entity Recognition, Entity Mention Detection and Relation Extraction and competitive results on Coreference Resolution while using simpler training and regularization procedures than previous works. The tasks share common embeddings and encoders allowing an easy information flow from the lowest level to the top of the architecture. We analyzed several aspects of the representations learned by this model as well as the effect of each tasks on the overall performances of the model."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank Gianis Bekoulis and Victor Quach for helpful feedbacks and the anonymous reviewers for constructive comments on the paper."
        }
    ],
    "title": "A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks",
    "year": 2018
}