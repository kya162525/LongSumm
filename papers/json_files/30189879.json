{
    "abstractText": "By capturing statistical patterns in large corpora, machine learning has enabled significant advances in natural language processing, including in machine translation, question answering, and sentiment analysis. However, for agents to intelligently interact with humans, simply capturing the statistical patterns is insufficient. In this paper we investigate if, and how, grounded compositional language can emerge as a means to achieve goals in multi-agent populations. Towards this end, we propose a multi-agent learning environment and learning methods that bring about emergence of a basic compositional language. This language is represented as streams of abstract discrete symbols uttered by agents over time, but nonetheless has a coherent structure that possesses a defined vocabulary and syntax. We also observe emergence of nonverbal communication such as pointing and guiding when language communication is unavailable.",
    "authors": [
        {
            "affiliations": [],
            "name": "Igor Mordatch"
        },
        {
            "affiliations": [],
            "name": "Pieter Abbeel"
        }
    ],
    "id": "SP:78997e1c5c25382c0c66ca78cf4173c224d52a51",
    "references": [
        {
            "authors": [
                "J. Andreas"
            ],
            "title": "and Klein",
            "venue": "D.",
            "year": 2016
        },
        {
            "authors": [
                "Dragan Andreas",
                "J. Klein 2017] Andreas",
                "A. Dragan",
                "D. Klein"
            ],
            "title": "Translating neuralese",
            "year": 2017
        },
        {
            "authors": [
                "Cho Bahdanau",
                "D. Bengio 2014] Bahdanau",
                "K. Cho",
                "Y. Bengio"
            ],
            "title": "Neural machine translation by jointly learning to align and translate",
            "venue": "arXiv preprint arXiv:1409.0473",
            "year": 2014
        },
        {
            "authors": [
                "K. Beuls"
            ],
            "title": "and Steels",
            "venue": "L.",
            "year": 2013
        },
        {
            "authors": [
                "J. Bratman",
                "M. Shvartsman",
                "Lewis"
            ],
            "title": "R",
            "venue": "L.; and Singh, S.",
            "year": 2010
        },
        {
            "authors": [
                "B. Dhingra",
                "L. Li",
                "X. Li",
                "J. Gao",
                "Y.-N. Chen",
                "F. Ahmed",
                "Deng"
            ],
            "title": "L",
            "venue": "2016. End-to-End Reinforcement Learning of Dialogue Agents for Information Access. arXiv:1609.00777 [cs]. arXiv:",
            "year": 1609
        },
        {
            "authors": [
                "A. Dosovitskiy"
            ],
            "title": "and Koltun",
            "venue": "V.",
            "year": 2016
        },
        {
            "authors": [
                "Berg-Kirkpatrick Durrett",
                "G. Klein 2016] Durrett",
                "T. Berg-Kirkpatrick",
                "D. Klein"
            ],
            "title": "Learning-based single-document summarization with compression and anaphoricity",
            "year": 2016
        },
        {
            "authors": [
                "Foerster, J.N.",
                "Assael"
            ],
            "title": "Y",
            "venue": "M.; de Freitas, N.; and Whiteson, S.",
            "year": 2016
        },
        {
            "authors": [
                "M.C. Frank",
                "Goodman"
            ],
            "title": "N",
            "venue": "D.",
            "year": 2012
        },
        {
            "authors": [
                "J. Gauthier"
            ],
            "title": "and Mordatch",
            "venue": "I.",
            "year": 2016
        },
        {
            "authors": [
                "Liang Golland",
                "D. Klein 2010] Golland",
                "P. Liang",
                "D. Klein"
            ],
            "title": "A game-theoretic approach to generating spatial descriptions",
            "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2010
        },
        {
            "authors": [
                "Grice"
            ],
            "title": "H",
            "venue": "P.",
            "year": 1975
        },
        {
            "authors": [
                "Gu Jang",
                "E. Poole 2016] Jang",
                "S. Gu",
                "B. Poole"
            ],
            "title": "Categorical Reparameterization with GumbelSoftmax. ArXiv e-prints",
            "year": 2016
        },
        {
            "authors": [
                "Griffiths Kirby",
                "S. Smith 2014] Kirby",
                "T. Griffiths",
                "K. Smith"
            ],
            "title": "Iterated learning and the evolution of language. Current opinion in neurobiology 28:108\u2013114",
            "year": 2014
        },
        {
            "authors": [
                "B.M. Lake",
                "T.D. Ullman",
                "J.B. Tenenbaum",
                "Gershman"
            ],
            "title": "S",
            "venue": "J.",
            "year": 2016
        },
        {
            "authors": [
                "Peysakhovich Lazaridou",
                "A. Baroni 2016] Lazaridou",
                "A. Peysakhovich",
                "M. Baroni"
            ],
            "title": "Multi-agent cooperation and the emergence of (natural) language. arXiv preprint arXiv:1612.07182",
            "year": 2016
        },
        {
            "authors": [
                "A. Lazaridou",
                "N.T. Pham",
                "Baroni"
            ],
            "title": "M",
            "venue": "2016. Towards MultiAgent Communication-Based Language Learning. arXiv:",
            "year": 1605
        },
        {
            "authors": [
                "Littman"
            ],
            "title": "M",
            "venue": "L.",
            "year": 1994
        },
        {
            "authors": [
                "C.J. Maddison",
                "A. Mnih",
                "Teh"
            ],
            "title": "Y",
            "venue": "W.",
            "year": 2016
        },
        {
            "authors": [
                "M.A. Nowak",
                "J.B. Plotkin",
                "Jansen"
            ],
            "title": "V",
            "venue": "A. A.",
            "year": 2000
        },
        {
            "authors": [
                "Silver"
            ],
            "title": "The predictron: End-to-end learning and planning",
            "venue": "arXiv preprint arXiv:1612.08810",
            "year": 2016
        },
        {
            "authors": [
                "R. Socher",
                "A. Perelygin",
                "J.Y. Wu",
                "J. Chuang",
                "C.D. Manning",
                "Ng"
            ],
            "title": "A",
            "venue": "Y.; Potts, C.; et al.",
            "year": 2013
        },
        {
            "authors": [
                "Szlam Sukhbaatar",
                "S. Fergus 2016] Sukhbaatar",
                "A. Szlam",
                "R. Fergus"
            ],
            "title": "Learning multiagent communication with backpropagation",
            "venue": "In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016,",
            "year": 2016
        },
        {
            "authors": [
                "I. Sutskever",
                "O. Vinyals",
                "Le"
            ],
            "title": "Q",
            "venue": "V.",
            "year": 2014
        },
        {
            "authors": [
                "Teh"
            ],
            "title": "Y",
            "venue": "W.",
            "year": 2011
        },
        {
            "authors": [
                "Xu Ullman",
                "T. Goodman 2016] Ullman",
                "Y. Xu",
                "N. Goodman"
            ],
            "title": "The pragmatics of spatial language",
            "venue": "In Proceedings of the Cognitive Science Society",
            "year": 2016
        },
        {
            "authors": [
                "A. Vogel",
                "A. G\u00f3mez Emilsson",
                "Frank"
            ],
            "title": "M",
            "venue": "C.; Jurafsky, D.; and Potts, C.",
            "year": 2014
        },
        {
            "authors": [
                "Wang"
            ],
            "title": "S",
            "venue": "I.; Liang, P.; and Manning, C.",
            "year": 2016
        }
    ],
    "sections": [
        {
            "heading": "Introduction",
            "text": "Development of agents that are capable of communication and flexible language use is one of the long-standing challenges facing the field of artificial intelligence. Agents need to develop communication if they are to successfully coordinate as a collective. Furthermore, agents will need some language capacity if they are to interact and productively collaborate with humans or make decisions that are interpretable by humans. If such a capacity were to arise artificially, it could also offer important insights into questions surrounding development of human language and cognition.\nBut if we wish to arrive at formation of communication from first principles, it must form out of necessity. The approaches that learn to plausibly imitate language from examples of human language, while tremendously useful, do not learn why language exists. Such supervised approaches can capture structural and statistical relationships in language, but they do not capture its functional aspects, or that language happens for purposes of successful coordination between humans. Evaluating success of such imitationbased approaches on the basis of linguistic plausibility also presents challenges of ambiguity and requirement of human involvement.\nCopyright c\u00a9 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nRecently there has been a surge of renewed interest in the pragmatic aspects of language use and it is also the focus of our work. We adopt a view of (Gauthier and Mordatch 2016) that an agent possesses an understanding of language when it can use language (along with other tools such as non-verbal communication or physical acts) to accomplish goals in its environment. This leads to evaluation criteria that can be measured precisely and without human involvement.\nIn this paper, we propose a physically-situated multiagent learning environment and learning methods that bring about emergence of a basic compositional language. This language is represented as streams of abstract discrete symbols uttered by agents over time, but nonetheless has a coherent structure that possesses a defined vocabulary and syntax. The agents utter communication symbols alongside performing actions in the physical environment to cooperatively accomplish goals defined by a joint reward function shared between all agents. There are no pre-designed meanings associated with the uttered symbols - the agents form concepts relevant to the task and environment and assign arbitrary symbols to communicate them.\nThere are similarly no explicit language usage goals, such as making correct utterances, and no explicit roles agents are assigned, such as speaker or listener, or explicit turntaking dialogue structure as in traditional language games. There may be an arbitrary number of agents in a population communicating at the same time and part of the difficulty is learning to refer specific agents. A population of agents is situated as moving particles in a continuous two-dimensional environment, possessing properties such as color and shape. The goals of the population are based on non-linguistic objectives, such as moving to a location and language arises from the need to coordinate on those goals. We do not rely on any supervision such as human demonstrations or text corpora.\nSimilar to recent work,we formulate the discovery the action and communication protocols for our agents jointly as a reinforcement learning problem. Agents perform physical actions and communication utterances according to an identical policy that is instantiated for all agents and fully determines the action and communication protocols. The policies are based on neural network models with an architecture composed of dynamically-instantiated recurrent modules. This allows decentralized execution with a variable\nar X\niv :1\n70 3.\n04 90\n8v 2\n[ cs\n.A I]\n2 4\nJu l 2\n01 8\nnumber of agents and communication streams. The joint dynamics of all agents and environment, including discrete communication streams are fully-differentiable, the agents\u2019 policy is trained end-to-end with backpropagation through time.\nThe languages formed exhibit interpretable compositional structure that in general assigns symbols to separately refer to environment landmarks, action verbs, and agents. However, environment variation leads to a number of specialized languages, omitting words that are clear from context. For example, when there is only one type of action to take or one landmark to go to, words for those concepts do not form in the language. Considerations of the physical environment also have an impact on language structure. For example, a symbol denoting go action is typically uttered first because the listener can start moving before even hearing the destination. This effect only arises when linguistic and physical behaviors are treated jointly and not in isolation.\nThe presence of a physical environment also allows for alternative strategies aside from language use to accomplish goals. A visual sensory modality provides an alternative medium for communication and we observe emergence of non-verbal communication such as pointing and guiding when language communication is unavailable. When even non-verbal communication is unavailable, strategies such as direct pushing may be employed to succeed at the task. It is important to us to build an environment with a diverse set of capabilities which language use develops alongside with.\nBy compositionality we mean the combination of multiple words to create meaning, as opposed to holistic languages that have a unique word for every possible meaning (Kirby 2001). Our work offers insights into why such compositional structure emerges. In part, we find it to emerge when we explicitly encourage active vocabulary sizes to be small through a soft penalty. This is consistent with analysis in evolutionary linguistics (Nowak, Plotkin, and Jansen 2000) that finds composition to emerge only when number of concepts to be expressed becomes greater than a factor of agent\u2019s symbol vocabulary capacity. Another important component leading to composition is training on a variety of tasks and environment configurations simultaneously. Training on cases where most information is clear from context (such as when there is only one landmark) leads to formation of atomic concepts that are reused compositionally in more complicated cases."
        },
        {
            "heading": "Related Work",
            "text": "Recent years have seen substantial progress in practical natural language applications such as machine translation (Sutskever, Vinyals, and Le 2014; Bahdanau, Cho, and Bengio 2014), sentiment analysis (Socher et al. 2013), document summarization (Durrett, Berg-Kirkpatrick, and Klein 2016), and domain-specific dialogue (Dhingra et al. 2016). Much of this success is a result of intelligently designed statistical models trained on large static datasets. However, such approaches do not produce an understanding of language that can lead to productive cooperation with humans.\nAn interest in pragmatic view of language understanding has been longstanding (Austin 1962; Grice 1975) and\nhas recently argued for in (Gauthier and Mordatch 2016; Lake et al. 2016; Lazaridou, Pham, and Baroni 2016). Pragmatic language use has been proposed in the context of twoplayer reference games (Golland, Liang, and Klein 2010; Vogel et al. 2014; Andreas and Klein 2016) focusing on the task of identifying object references through a learned language. (Winograd 1973; Wang, Liang, and Manning 2016) ground language in a physical environment and focusing on language interaction with humans for completion of tasks in the physical environment. In such a pragmatic setting, language use for communication of spatial concepts has received particular attention in (Steels 1995; Ullman, Xu, and Goodman 2016).\nAside from producing agents that can interact with humans through language, research in pragmatic language understanding can be informative to the fields of linguistics and cognitive science. Of particular interest in these fields has been the question of how syntax and compositional structure in language emerged, and why it is largely unique to human languages (Kirby 1999; Nowak, Plotkin, and Jansen 2000; Steels 2005). Models such as Rational Speech Acts (Frank and Goodman 2012) and Iterated Learning (Kirby, Griffiths, and Smith 2014) have been popular in cognitive science and evolutionary linguistics, but such approaches tend to rely on pre-specified procedures or models that limit their generality.\nThe recent work that is most similar to ours is the application of reinforcement learning approaches towards the purposes of learning a communication protocol, as exemplified by (Bratman et al. 2010; Foerster et al. 2016; Sukhbaatar, Szlam, and Fergus 2016; Lazaridou, Peysakhovich, and Baroni 2016)."
        },
        {
            "heading": "Problem Formulation",
            "text": "The setting we are considering is a cooperative partially observable Markov game (Littman 1994), which is a multiagent extension of a Markov decision process. A Markov game for N agents is defined by set of states S describing the possible configurations of all agents, a set of actions A1, ...,AN and a set of observations O1, ...,ON for each agent. Initial states are determined by a distribution \u03c1 : S 7\u2192 [0, 1]. State transitions are determined by a function T : S \u00d7A1 \u00d7 ...\u00d7AN 7\u2192 S. For each agent i, rewards are given by function ri : S \u00d7 Ai 7\u2192 R, observations are given by function oi : S 7\u2192 Oi. To choose actions, each agent i uses a stochastic policy \u03c0i : Oi \u00d7Ai 7\u2192 [0, 1].\nIn this work, we assume all agents have identical action and observation spaces, and all agents act according to the same policy \u03c0 and receive a shared reward. We consider a finite horizon setting, with episode length T . In a cooperative setting, the problem is to find a policy that maximizes the expected shared return for all agents, which can be solved as a joint minimization problem:\nmax \u03c0 R(\u03c0), where R(\u03c0) = E [ T\u2211 t=0 N\u2211 i=0 r(sti,a t i) ]\n.. ..\n.. .. l\nl ..\n..\n\u03c0\n\u03c0\n.. ..\n.. .. .. .. .. \u03b8"
        },
        {
            "heading": "Grounded Communication Environment",
            "text": "As argued in the introduction, grounding multi-agent communication in a physical environment is crucial for interesting communication behaviors to emerge. In this work, we consider a physically-simulated two-dimensional environment in continuous space and discrete time. This environment consists ofN agents andM landmarks. Both agent and landmark entities inhabit a physical location in space p and posses descriptive physical characteristics, such as color and shape type. In addition, agents can direct their gaze to a location v.Agents can act to move in the environment and direct their gaze, but may also be affected by physical interactions with other agents. We denote the physical state of an entity (including descriptive characteristics) by x and describe its precise details and transition dynamics in the Appendix.\nIn addition to performing physical actions, agents utter verbal communication symbols c at every timestep. These utterances are discrete elements of an abstract symbol vocabulary C of size K. We do not assign any significance or meaning to these symbols. They are treated as abstract categorical variables that are emitted by each agent and observed by all other agents. It is up to agents at training time to assign meaning to these symbols. As shown in Section , these symbols become assigned to interpretable concepts. Agents may also choose not to utter anything at a given timestep, and there is a cost to making an utterance, loosely representing the metabolic effort of vocalization. We denote a vector representing one-hot encoding of symbol c with boldface c.\nEach agent has internal goals specified by vector g that are private and not observed by other agents. These goals are grounded in the physical environment and include tasks such as moving to or gazing at a location. These goals may involve other agents (requiring the other agent to move to a location, for example) but are not observed by them and thus necessitate coordination and communication between agents. Verbal utterances are one tool which the agents can use to cooperatively accomplish all goals, but we also observe emergent use of non-verbal signals and altogether noncommunicative strategies.\nTo aid in accomplishing goals, each agent has internal recurrent memory bank m that is also private and not observed by other agents. This memory bank has no pre-designed behavior and it is up to the agents to learn to utilize it appropriately.\nThe full state of the environment is given by s =[ x1,...,(N+M) c1,...,N m1,...,N g1,...,N ] \u2208 S. Each agent observes physical states of all entities in the environment, verbal utterances of all agents, and its own private memory and goal vector. The observation for agent i is oi(s) =[ ix1,...,(N+M) c1,...,N mi gi ] . Where ixj is the observation of entity j\u2019s physical state in agent i\u2019s reference frame (see Appendix for details). More intricate observation models are possible, such as physical observations solely from pixels or verbal observations from a single input channel. These models would require agents learning to perform visual processing and source separation, which are orthogonal to this work. Despite the dimensionality of observations varying with the number of physical entities and communication streams, our policy architecture as described in Section allows a single policy parameterization across these variations.\n.. ..\n.. .. .. .. \u03c0 \u03c0 .. .. .. .. .. .. .. \u03b8\nFigure 2: The transition dynamics of N agents from time t \u2212 1 to t. Dashed lines indicate one-to-one dependencies between agents and solid lines indicate all-to-all dependencies."
        },
        {
            "heading": "Policy Learning with Backpropagation",
            "text": "Each agent acts by sampling actions from a stochastic policy \u03c0, which is identical for all agents and defined by parameters \u03b8. There are several common options for finding optimal policy parameters. The model-free framework of Qlearning can be used to find the optimal state-action value function, and employ a policy that acts greedily to according to the value function. Unfortunately, Q function dimensionality scales quadratically with communication vocabulary size, which can quickly become intractably large. Alternatively it is possible to directly learn a policy function using\nmodel-free policy gradient methods, which use sampling to estimate the gradient of policy return dRd\u03b8 . The gradient estimates from these methods can exhibit very high variance and credit assignment becomes an especially difficult problem in the presence of sequential communication actions.\nInstead of using model-free reinforcement learning methods, we build an end-to-end differentiable model of all agent and environment state dynamics over time and calculate dRd\u03b8 with backpropagation. At every optimization iteration, we sample a new batch of 1024 random environment instantiations and backpropagate their dynamics through time to calculate the total return gradient. Figure 2 shows the dependency chain between two timesteps. A similar approach was employed by (Foerster et al. 2016; Sukhbaatar, Szlam, and Fergus 2016) to compute gradients for communication actions, although the latter still employed model-free methods for physical action computation.\nThe physical state dynamics, including discontinuous contact events can be made differentiable with smoothing. However, communication actions require emission of discrete symbols, which present difficulties for backpropagation."
        },
        {
            "heading": "Discrete Communication and Gumbel-Softmax Estimator",
            "text": "In order to use categorical communication emissions c in our setting, it must be possible to differentiate through them. There has been a wealth of work in machine learning on differentiable models with discrete variables, but we found recent approach in (Jang, Gu, and Poole 2016; Maddison, Mnih, and Teh 2016) to be particularly effective in our setting. The approach proposes a Gumbel-Softmax distribution, which is a continuous relaxation of a discrete categorical distribution. Given K-categorical distribution parameters p, a differentiable K-dimensional one-hot encoding sampleG from the Gumbel-Softmax distribution can be calculated as:\nG(logp)k = exp((logpk + \u03b5)/\u03c4)\u2211K j=0 exp((logpj + \u03b5)/\u03c4)\nWhere \u03b5 are i.i.d. samples from Gumbel(0, 1) distribution, \u03b5 = \u2212log(\u2212log(u)), u \u223c U [0, 1] and \u03c4 is a softmax temperature parameter. We did not find it necessary to anneal the temperature and set it to 1 in all our experiments for training and sample directly from the categorical distribution at test time. To emit a communication symbol, our policy is trained to directly output logp \u2208 RK , which is transformed to a symbol emission sample c \u223c G(logp). The resulting gradient can be estimated as dcd\u03b8 = dG dp dp d\u03b8 ."
        },
        {
            "heading": "Policy Architecture",
            "text": "The policy class we consider in this work are stochastic neural networks. The policy outputs samples of an agent\u2019s physical actions u, communication symbol utterance c, and internal memory updates \u2206m. The policy must consolidate multiple incoming communication symbol streams emitted by other agents, as well as incoming observations of physical entities. Importantly, the number of agents (and thus the\na\nnumber of communication streams) and number of physical entities can vary between environment instantiations. To support this, the policy instantiates a collection of identical processing modules for each communication stream and each observed physical entity. Each processing module is a fully-connected multi-layer perceptron. The weights between all communication processing and physical observation modules are shared. The outputs of individual processing modules are pooled with a softmax operation into feature vectors\u03c6c and\u03c6x for communication and physical observation streams, respectively. Such weight sharing and pooling makes it possible to apply the same policy parameters to any number of communication and physical observations.\nThe pooled features and agent\u2019s private goal vector are passed to the final processing module that outputs distribution parameters [ \u03c8u \u03c8c ] from which action samples are generated as u = \u03c8u + \u03b5 and c \u223c G(\u03c8c), where \u03b5 is a zero-mean Gaussian noise.\nUnlike communication games where agents only emit a single utterance, our agents continually emit a stream of symbols over time. Thus processing modules that read and write communication utterance streams benefit greatly from recurrent memory that can capture meaning of a stream over time. To this end, we augment each communication processing and output module with an independent internal memory state m, and each module outputs memory state updates \u2206m. In this work we use simple additive memory updates mt = tanh(mt\u22121 + \u2206mt\u22121 + \u03b5) for simplicity and interpretability, but other memory architectures such LSTMs can be used. We build all fully-connected modules with 256 hidden units and 2 layers each in all our experiments, using exponential-linear units and dropout with a rate of 0.1 between all hidden layers. Size is feature vectors \u03c6 is 256 and size of each memory module is 32. The overall policy architecture is shown in Figure 3."
        },
        {
            "heading": "Auxiliary Prediction Reward",
            "text": "To help policy training avoid local minima in more complex environments, we found it helpful to include auxiliary goal prediction tasks, similar to recent work in reinforcement learning (Dosovitskiy and Koltun 2016; Silver et al. 2016). In agent i\u2019s policy, each communication processing module j additionally outputs a prediction g\u0302i,j of agent j\u2019s goals. We do not use g\u0302 as an input in calculating actions. It is only used for the purposes of auxiliary prediction task. At the end of the episode, we add a reward for predicting other agent\u2019s goals, which in turn encourages communication utterances that convey the agent\u2019s goals clearly to other agents. Across all agents this reward has the form:\nrg = \u2212 \u2211\n{i,j|i 6=j}\n\u2016g\u0302Ti,j \u2212 gTj \u20162"
        },
        {
            "heading": "Compositionality and Vocabulary Size",
            "text": "What leads to compositional syntax formation? One known constructive hypothesis requires modeling the process of language transmission and acquisition from one generation of agents to the next iteratively as in (Kirby, Griffiths, and Smith 2014). In such iterated learning setting, compositionality emerges due to poverty of stimulus - one generation will only observe a limited number of symbol utterances from the previous generation and must infer meaning of unseen symbols. This approach requires modeling language acquisition between agents, but when implemented with predesigned rules was shown over multiple iterations between generations to lead to formation of a compositional vocabulary.\nAlternatively, (Nowak, Plotkin, and Jansen 2000) observed that emergence of compositionality requires the number of concepts describable by a language to be above a factor of vocabulary size. In our preliminary environments the number of concepts to communicate is still fairly small and is within the capacity of a non-compositional language. We use a maximum vocabulary size K = 20 in all our experiments. We tested a smaller maximum vocabulary size, but found that policy optimization became stuck in a poor local minima where concepts became conflated. Instead, we propose to use a large vocabulary size limit but use a soft penalty function to prevent the formation of unnecessarily large vocabularies. This allows the intermediate stages of policy optimization to explore large vocabularies, but then converge on an appropriate active vocabulary size. As shown in Figure 6, this is indeed what happens.\nHow do we penalize large vocabulary sizes? (Nowak, Plotkin, and Jansen 2000) proposed a word population dynamics model that defines reproductive ratios of words to be proportional to their frequency, making already popular words more likely to survive. Inspired by these rich-getricher dynamics, we model the communication symbols as being generated from a Dirichlet Process (Teh 2011). Each communication symbol has a probability of being symbol ck as\np(ck) = nk\n\u03b1+ n\u2212 1\nWhere nk is the number of times symbol ck has been uttered and n is the total number of symbols uttered. These counts are accumulated over agents, timesteps, and batch entries. \u03b1 is a Dirichlet Process hyperparameter corresponding to the probability of observing an out-of-vocabulary word. The resulting reward across all agents is the log-likelihood of all communication utterances to independently have been generated by a Dirichlet Process:\nrc = \u2211 i,t,k 1[cti = ck]logp(ck)\nMaximizing this reward leads to consolidation of symbols and the formation of compositionality. This approach is similar to encouraging code population sparsity in autoencoders (Ng 2011), which was shown to give rise to compositional representations for images."
        },
        {
            "heading": "Experiments",
            "text": "We experimentally investigate how variation in goals, environment configuration, and agents physical capabilities lead to different communication strategies. In this work, we consider three types of actions an agent needs to perform: go to location, look at location, and do nothing. Goal for agent i consists of an action to perform, a location to perform it on r\u0304, and an agent r that should perform that action. These goal properties are accumulated into goal description vector g. These goals are private to each agent, but may involve other agents. For example, agent i may want agent r to go to location r\u0304. This goal is not observed by agent r, and requires communication between agents i and r. The goals are assigned to agents such that no agent receives conflicting goals. We do however show generalization in the presence of conflicting goals in Section .\nAgents can only communicate in discrete symbols and have individual reference frames without a shared global positioning reference (see Appendix), so cannot directly send goal position vector. What makes the task possible is that we place goal locations r\u0304 on landmark locations of which are observed by all agents (in their invidiaul reference frames). The strategy then is for agent i to unambiguously communicate landmark reference to agent r. Importantly, we do not provide explicit association between goal positions and landmark reference. It is up to the agents to learn to associate a position vector with a set of landmark properties and communicate them with discrete symbols.\nIn the results that follow, agents do not observe other agents. This disallows capacity for non-verbal communication, necessitating the use of language. In section we report what happens when agents are able to observe each other and capacity for non-verbal communication is available.\nDespite training with continuous relaxation of the categorical distribution, we observe very similar reward performance at test time. No communication is provided as a baseline (again, non-verbal communication is not possible). The no-communication strategy is for all agents go towards the centroid of all landmarks."
        },
        {
            "heading": "Syntactic Structure",
            "text": "We observe a compositional syntactic structure emerging in the stream of symbol uttered by agents. When trained on environments with only two agents, but multiple landmarks and actions, we observe symbols forming for each of the landmark colors and each of the action types. A typical conversation and physical agent configuration is shown in first row of Figure 4 and is as follows: Green Agent: GOTO, GREEN, ... Blue Agent: GOTO, BLUE, ...\nThe labels for abstract symbols are chosen by us purely for interpretability and visualization and carry no meaning for training. While there is recent work on interpreting continuous machine languages (Andreas, Dragan, and Klein 2017), the discrete nature and small size of our symbol vocabulary makes it possible to manually labels to the symbols. See results in supplementary video for consistency of the vocabulary usage.\nPhysical environment considerations play a part in the syntactic structure. The action type verb GOTO is uttered first because actions take time to accomplish in the grounded\nenvironment. When the agent receives GOTO symbol it starts moving toward the centroid of all the landmarks (to be equidistant from all of them) and then moves towards the specific landmark when it receives its color identity.\nWhen the environment configuration can contain more than three agents, agents need to form symbols for referring to each other. Three new symbols form to refer to agent colors that are separate in meaning from landmark colors. The typical conversations are shown in second and third rows of Figure 4. Red Agent: GOTO, RED, BLUE-AGENT, ... Green Agent: ..., ..., ..., ... Blue Agent: RED-AGENT, GREEN, LOOKAT, ...\nAgents may not omit any utterances when they are the subject of their private goal, in which case they have access to that information and have no need to announce it. In this language, there is no set ordering to word utterances. Each symbol contributes to sentence meaning independently, similar to case marking grammatical strategies used in many human languages (Beuls and Steels 2013).\nThe agents largely settle on using a consistent set of symbols for each meaning, due to vocabulary size penalties and that discourage synonyms. We show the aggregate streams of communication utterances in Figure 5.\nIn simplified environment configurations when there is only one landmark or one type of action to take, no symbols are formed to refer to those concepts because they are clear from context.\nSymbol Vocabulary Usage We find word activation counts to settle on the appropriate compositional word counts. That early during training large vocabulary sizes are being taken advantage of to explore the space of communication possibilities before settling on the appropriate effective vocabulary sizes as shown in Figure 6. In this figure, 1x1x3 case refers to environment with two agents and a single action, which requires only communicating one of three landmark identities. 1x2x3 contains two types of actions, and 3x3x3 case contains three agents that require explicit referencing."
        },
        {
            "heading": "Generalization to Unseen Configurations",
            "text": "One of the advantages of decentralised execution policies is that trained agents can be placed into arbitrarily-sized groups and still function reasonably. When there are additional agents in the environment with the same color identity, all agents of the same color will perform the same task if they are being referred to. Additionally, when agents of a\nparticular color are asked to perform two conflicting tasks (such as being asked go to two different landmarks by two different agents), they will perform the average of the conflicting goals assigned to them. Such cases occur despite never having been seen during training.\nDue to the modularized observation architecture, the number of landmarks in the environment can also vary between training and execution. The agents perform sensible behaviors with different numbers of landmarks, despite not being trained in such environments. For example, when there are distractor landmarks of novel colors, the agents never go towards them. When there are multiple landmarks of the same color, the agent communicating the goal still utters landmark color (because the goal is the position of one of the landmarks). However, the agents receiving the landmark color utterance go towards the centroid of all landmark of the same color, showing a very sensible generalization strategy. An example of such case is shown in fourth row of Figure 4."
        },
        {
            "heading": "Non-verbal Communication and Other Strategies",
            "text": "The presence of a physical environment also allows for alternative strategies aside from language use to accomplish goals. In this set of experiments we enable agents to observe other agents\u2019 position and gaze location, and in turn disable communication capability via symbol utterances. When agents can observe each other\u2019s gaze, a pointing strategy forms where the agent can communicate a landmark location by gazing in its direction, which the recipient correctly interprets and moves towards. When gazes of other agents cannot be observed, we see behavior of goal sender agent moving towards the location assigned to goal recipient agent (despite receiving no explicit reward for doing so), in order to guide the goal recipient to that location. Lastly, when neither visual not verbal observation is available on part of the goal recipient, we observe the behavior of goal sender directly pushing the recipient to the target location. Examples of such strategies are shown in Figure 7 and supplementary video. It is important to us to build an environment with a diverse set of\ncapabilities which language use develops alongside with."
        },
        {
            "heading": "Conclusion",
            "text": "We have presented a multi-agent environment and learning methods that brings about emergence of an abstract compositional language from grounded experience. This abstract language is formed without any exposure to human language use. We investigated how variation in environment configuration and physical capabilities of agents affect the communication strategies that arise.\nIn the future, we would like experiment with larger number of actions that necessitate more complex syntax and larger vocabularies. We would also like integrate exposure to human language to form communication strategies that are compatible with human use."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank OpenAI team for helpful comments and fruitful discussions. This work was funded in part by ONR PECASE N000141612723."
        },
        {
            "heading": "Appendix: Physical State and Dynamics",
            "text": "The physical state of the agent is specified by x = [ p p\u0307 v d ] where p\u0307 is the velocity of p. d \u2208 R3 is the color associted with the agent. Landmarks have similar state, but without gaze and velocity components. The physical state transition dynamics for a single agent i are given by:\nxti = [ p p\u0307 v ]t i = [ p + p\u0307\u2206t \u03b3p\u0307 + (up + f(x1, ...,xN ))\u2206t uv ]t\u22121 i\nWhere f(x1, ...,xN ) are the physical interaction forces (such as collision) between all agents in the environment and any obstacles, \u2206t is the simulation timestep (we use 0.1), and (1 \u2212 \u03b3) is a damping coefficient (we use 0.5). The action space of the agent is a = [ up uv c ]. The observation of any location pj in reference frame of agent i is ipj = Ri(pj \u2212 pi), where Ri is the random rotation matrix of agent i. Giving each agent a private random orientation prevents identifying landmarks in a shared coordinate frame (using words such as top-most or left-most)."
        }
    ],
    "title": "Emergence of Grounded Compositional Language in Multi-Agent Populations",
    "year": 2018
}