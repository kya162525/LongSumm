{
    "abstractText": "We present a framework to systematically analyze convolutional neural networks (CNNs) used in classification of cars in autonomous vehicles. Our analysis procedure comprises an image generator that produces synthetic pictures by sampling in a lower dimension image modification subspace and a suite of visualization tools. The image generator produces images which can be used to test the CNN and hence expose its vulnerabilities. The presented framework can be used to extract insights of the CNN classifier, compare across classification models, or generate training and validation datasets.",
    "authors": [
        {
            "affiliations": [],
            "name": "Tommaso Dreossi"
        },
        {
            "affiliations": [],
            "name": "Shromona Ghosh"
        },
        {
            "affiliations": [],
            "name": "Alberto Sangiovanni-Vincentelli"
        },
        {
            "affiliations": [],
            "name": "Sanjit A. Seshia"
        }
    ],
    "id": "SP:5e59e60ea7284223c482209dd6005ef99320b007",
    "references": [
        {
            "authors": [
                "Aly",
                "Mohamed"
            ],
            "title": "Real time detection of lane markers in urban streets",
            "venue": "In Intelligent Vehicles Symposium,",
            "year": 2008
        },
        {
            "authors": [
                "Dougherty",
                "Mark"
            ],
            "title": "A review of neural networks applied to transport",
            "venue": "Transportation Research Part C: Emerging Technologies,",
            "year": 1995
        },
        {
            "authors": [
                "Dreossi",
                "Tommaso",
                "Donz\u00e9",
                "Alexandre",
                "Seshia",
                "Sanjit A"
            ],
            "title": "Compositional falsification of cyber-physical systems with machine learning components",
            "venue": "In NASA Formal Methods Symposium,",
            "year": 2017
        },
        {
            "authors": [
                "Geiger",
                "Andreas",
                "Lenz",
                "Philip",
                "Stiller",
                "Christoph",
                "Urtasun",
                "Raquel"
            ],
            "title": "Vision meets robotics: The kitti dataset",
            "venue": "International Journal of Robotics Research, IJRR,",
            "year": 2013
        },
        {
            "authors": [
                "Goodfellow",
                "Ian",
                "Pouget-Abadie",
                "Jean",
                "Mirza",
                "Mehdi",
                "Xu",
                "Bing",
                "Warde-Farley",
                "David",
                "Ozair",
                "Sherjil",
                "Courville",
                "Aaron",
                "Bengio",
                "Yoshua"
            ],
            "title": "Generative adversarial nets",
            "venue": "In Advances in neural information processing systems,",
            "year": 2014
        },
        {
            "authors": [
                "Halton",
                "John H"
            ],
            "title": "On the efficiency of certain quasi-random sequences of points in evaluating multi-dimensional integrals",
            "venue": "Numerische Mathematik,",
            "year": 1960
        },
        {
            "authors": [
                "Huang",
                "Xiaowei",
                "Kwiatkowska",
                "Marta",
                "Wang",
                "Sen",
                "Wu",
                "Min"
            ],
            "title": "Safety verification of deep neural networks",
            "venue": "arXiv preprint arXiv:1610.06940,",
            "year": 2016
        },
        {
            "authors": [
                "Katz",
                "Guy",
                "Barrett",
                "Clark",
                "Dill",
                "David",
                "Julian",
                "Kyle",
                "Kochenderfer",
                "Mykel"
            ],
            "title": "Reluplex: An efficient smt solver for verifying deep neural networks",
            "venue": "arXiv preprint arXiv:1702.01135,",
            "year": 2017
        },
        {
            "authors": [
                "Kong",
                "Hui",
                "Audibert",
                "Jean-Yves",
                "Ponce",
                "Jean"
            ],
            "title": "Vanishing point detection for road detection",
            "venue": "In Computer Vision and Pattern Recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Niederreiter",
                "Harald"
            ],
            "title": "Low-discrepancy and low- sequences",
            "venue": "Journal of number theory,",
            "year": 1988
        },
        {
            "authors": [
                "Papernot",
                "Nicolas",
                "McDaniel",
                "Patrick",
                "Jha",
                "Somesh",
                "Fredrikson",
                "Matt",
                "Celik",
                "Z Berkay",
                "Swami",
                "Ananthram"
            ],
            "title": "The limitations of deep learning in adversarial settings",
            "venue": "In Security and Privacy (EuroS&P),",
            "year": 2016
        },
        {
            "authors": [
                "Redmon",
                "Joseph",
                "Divvala",
                "Santosh",
                "Girshick",
                "Ross",
                "Farhadi",
                "Ali"
            ],
            "title": "You only look once: Unified, real-time object detection",
            "venue": "In Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Sobol",
                "Ilya M"
            ],
            "title": "Uniformly distributed sequences with an additional uniform property",
            "venue": "USSR Computational Mathematics and Mathematical Physics,",
            "year": 1976
        },
        {
            "authors": [
                "Szegedy",
                "Christian",
                "Zaremba",
                "Wojciech",
                "Sutskever",
                "Ilya",
                "Bruna",
                "Joan",
                "Erhan",
                "Dumitru",
                "Goodfellow",
                "Ian",
                "Fergus",
                "Rob"
            ],
            "title": "Intriguing properties of neural networks",
            "venue": "arXiv preprint arXiv:1312.6199,",
            "year": 2013
        },
        {
            "authors": [
                "Teichmann",
                "Marvin",
                "Weber",
                "Michael",
                "Z\u00f6llner",
                "J. Marius",
                "Cipolla",
                "Roberto",
                "Urtasun",
                "Raquel"
            ],
            "title": "Multinet: Real-time joint semantic reasoning for autonomous driving",
            "venue": "Computing Research Repository, CoRR,",
            "year": 2016
        },
        {
            "authors": [
                "Wu",
                "Bichen",
                "Iandola",
                "Forrest",
                "Jin",
                "Peter H",
                "Keutzer",
                "Kurt"
            ],
            "title": "Squeezedet: Unified, small, low power fully convolutional neural networks for real-time object detection for autonomous driving",
            "year": 2016
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Convolutional neural networks (CNN) are powerful models that have recently achieved the state of the art in object classification and detection tasks. It is no surprise that they are used extensively in large scale Cyber-Physical Systems (CPS). For CPS used in safety critical purposes, verifying CNN models is of utmost importance (Dreossi et al., 2017). An emerging domain where CNNs have found application is autonomous driving where object detectors are used to identify cars, pedestrians, or road signs (Dougherty, 1995; Bojarski et al., 2016).\nCNNs are usually composed of extensively parallel nonlinear layers that allow the networks to learn highly nonlinear functions. While CNNs are able to achieve high accuracy for object detection, their analysis has proved to be extremely difficult. Proving their correctness, i.e., to show that a CNN always correctly detects a particular object, has become practically impossible. One approach to address this problem analyzes the robustness of CNNs with respect to perturbations. Using optimization-based techniques (Szegedy et al., 2013; Papernot et al., 2016) or gen-\n*Equal contribution 1University of California, Berkeley, USA. Correspondence to: Tommaso Dreossi <dreossi@berkeley.edu>, Shromona Ghosh <shromona.ghosh@berkeley.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nerative NNs (Goodfellow et al., 2014), it is possible to find minimal adversarial modifications that can cause a CNN to misclassify an altered picture. Another approach inspired by formal methods aims at formally proving the correctness of neural networks by using, e.g., Linear Programming or SMT solvers (Huang et al., 2016; Katz et al., 2017). Unfortunately, these verification techniques usually impose restrictions on treated CNNs and suffer from scalability issues.\nIn this work, we present a framework to systematically test CNNs by generating synthetic datasets. In contrast to the adversarial generation techniques, we aim at generating realistic pictures rather than introducing perturbations into preexisting ones. In this paper, we focus on self-driving applications, precisely on CNNs used for detection of cars. However, the presented techniques are general enough for application to other domains.\nOur framework consists of three main modules: an image generator, a collection of sampling methods, and a suite of visualization tools. The image generator renders realistic images of road scenarios. The images are obtained by arranging basic objects (e.g., road backgrounds, cars) and by tuning image parameters (e.g., brightness, contrast, saturation). By preserving the aspect ratios of the objects, we generate more realistic images. All possible configurations of the objects and image parameters define a modification space whose elements map to a subset of the CNN feature space (in our case, road scenarios). The goal of the sampling methods is to provide modification points to the image generator that produces pictures used to extract information from the CNN. We provide different sampling techniques, depending on the user needs. In particular, we focus on samplings methods that cover the modification space evenly and active optimization-based methods to generate images that are misclassified by the analyzed CNN. Finally, the visualization tools are used to display the gathered information. Our tool can display the sampled modifications against metrics of interest such as the probability associated with the predicted bounding boxes (the box containing the car) or the intersection over union (IOU) used to measure the accuracy of the prediction box.\nThe contributions provided by our framework are twofold:\nar X\niv :1\n70 8.\n03 30\n9v 2\n[ cs\n.C V\n] 1\n1 A\nug 2\n01 7\n\u2022 Analysis of Neural Network Classifiers. The systematic analysis is useful to obtain insights of the considered CNN classifier, such as the identification of blind spots or corner cases. Our targeted testing can also be used to compare across CNN models;\n\u2022 Dataset Generator. Our picture generator can generate large data sets for which the diversity of the pictures can be controlled by the user. This overcomes a lack of training data, one of the limiting problems in training of CNNs. Also, a target synthesized dataset can be used as a benchmark for a specific domain of application.\nWe present a systematic methodology for finding failure cases of CNN classifiers. This is a first attempt towards verifying machine learnt components in complex systems.\nThe paper is organized as follows: Sec. 2 describes the analysis framework and defines the picture generator, sampling methods, and visualization tools; Sec. 3 to implementation details and experimental evaluations."
        },
        {
            "heading": "2. CNN Analyzer",
            "text": ""
        },
        {
            "heading": "2.1. Overview",
            "text": "We begin by introducing some basic notions and by giving an overview of our CNN analysis framework.\nLet f : X \u2192 Y be a CNN that assigns to every feature vector x \u2208 X a label y \u2208 Y , where X and Y are a feature and a label space, respectively. In our case x can be a picture of a car and y is the CNN prediction representing information such as the detected object class, the prediction confidence, or the object position.\nOur analysis technique (Alg. 1) consists in a loop where at each step an image modification configuration m is sampled, an image x is generated using the modification m, and a prediction y is returned by the analyzed CNN. Intuitively, m describes the configuration of the synthetic picture x to be generated. y is then the prediction of the CNN on this generated image x. A modification m can specify, for instance, the x and y coordinates of a car in a picture as well as the brightness or contrast of the image to be generated. At each loop iteration, the information m,x,y are stored in the data structure D that is later used to inspect and visualize the CNN behavior. The loop is repeated until a condition on D is met. Some examples of halting conditions can be the discovery of a misclassified picture, a maximum number of generated images, or the achievement of coverage threshold on the modification space.\nThe key steps of this algorithm are the picture generation (i.e., how an image is rendered from a modification choice) and how modifications are sampled (i.e., how to chose a\nAlgorithm 1 Analyze CNN function CNNanalysis\nrepeat m\u2190 sample(M ) x\u2190 generateImage(m) y\u2190 f(x) D.add(m,x,y) until condition(D) visualize(D)\nend function\nmodification in such a way to achieve the analysis goal). In the following, we define image modifications and show how synthetic pictures are generated (Sec. 2.2). Next, we introduce some sampling methods (Sec. 2.3), and finally some visualization tools (Sec 2.4)."
        },
        {
            "heading": "2.2. Image Generation",
            "text": "Let X\u0303 \u2286 X be a subset of the feature space of f : X \u2192 Y . A generation function \u03b3 : M \u2192 X\u0303 is a function that maps every modification m \u2208M to a feature \u03b3(m) \u2208 X\u0303 .\nModification functions can be used to compactly represent a subset of the feature space. For instance, modifications of a given picture, such as displacement of a car and brightness, can be seen as the dimensions of a 2-D modification space. Low-dimensional modification spaces allow us to analyze CNNs on compact domains as opposite to intractable feature spaces. Let us clarify these concepts with the following example where a set of pictures is abstracted into a 3-D box.\nLet X be the set of 1242\u00d7 375 RBG pictures (Kitti image resolution (Geiger et al., 2013)). Since we are interested in the automotive context, we consider the subset X\u0303 \u2282 X of pictures of cars in different positions on a particular background with different color contrasts. In these settings, we can define, for instance, the generation function \u03b3 that maps M = [0, 1]3 to X\u0303 , where the dimensions ofM characterize the x, y positions of the car and the image contrast, respectively. For instance, \u03b3(0, 0, 0) places the car on the left close to the observer with high contrast, \u03b3(1, 0, 0) shifts the car to the right, or \u03b3(1, 1, 1) sees the car on the right, far from the observer, with low contrast.\nFig. 1 shows some images of X\u0303 disposed accordingly to their location in the modification space M . When moving on the x-axis of M , the car shifts horizontally; a change on the y-axis affects the position of the car in depth; the z-axis affects the contrast of the picture. This simple example shows how the considered feature space can be abstracted in a 3-D modification space in which every point corresponds to a particular image.\nIn this examples, we chose the extreme positions of the car (i.e., maximum and minimum x and y position of the car) on the sidelines of the road and the image vanishing point. Both the sidelines and the vanishing point can be automatically detected (Aly, 2008; Kong et al., 2009). The vanishing point is useful to determine the vanishing lines necessary to resize and place the car when altering its position in the y modification dimension. For instance, the car is placed and shrunk towards the vanishing point as the y coordinate of its modification element gets close to 1 (see Fig. 2)."
        },
        {
            "heading": "2.3. Sampling Methods",
            "text": "We now consider some methods to efficiently sample the modification space. Good sampling techniques should provide a high coverage of the abstract space and identify samples whose concretizations lead to misclassifying images.\nLow-discrepancy Sequences A low-discrepancy (or quasi-random) sequence is a sequence of n-tuples that fills an n-D space more uniformly than uncorrelated random points. Low-discrepancy sequences are useful to cover boxes by reducing gaps and clustering of points.\nLet U = [0, 1]n be a n-D box, J \u2286 U be a sub-box, and X \u2282 U be a set of m points. The discrepancy D(J,X) of J is the difference between the proportion of points in J compared to U and the volume of J compared to U :\nD(J,X) = |#(J)/m\u2212 vol(J)| (1)\nwhere #(J) is the number of points of X in J and vol(J) is the volume of J . The star-discrepancy D\u2217(X) is the\nworst case distribution of X:\nD\u2217(X) = max J D(J,X) (2)\nLow-discrepancy sequences generate sets of points that minimize the star-discrepancy. Some examples of lowdiscrepancy sequences are the Van der Corput, Halton (Halton, 1960), or Sobol (Sobol, 1976) sequences. In our experiments, we used the Halton and lattice-based (Niederreiter, 1988) sequences. These sampling methods ensure an optimal coverage of the abstract space and allows us to identify clusters of misclassified pictures as well as isolated corner cases otherwise difficult to spot with a uniform sampling technique.\nActive Learning At every step, given a sample, we generate images which are presented as input to the neural network under test. This becomes an expensive process when the number of samples necessary for the covering the input space is large. We propose using active learning to minimize the number of images generated and only sample points which have a high probability of being a counter example.\nWe model the function from the sample space U = [0, 1]n to the score (output) of the CNN as a Gaussian Process (GP). GPs are a popular choice for nonparametric regression in machine learning, where the goal is to find an approximation of a nonlinear map p(u) : U \u2192 R from an input sample u \u2208 U to the score produced by the neural network. The main assumption is that the values of p, associated with the different sample inputs, are random variables and have a joint Gaussian distribution. This distribution is specified by a mean function, which is assumed to be zero without loss of generality, and a covariance function k(u, u), called kernel function.\nThe GP framework can be used to predict the score p(u) at an arbitrary sample u \u2208 U based on a set of t past observations yt = [p\u0303(u1), . . . , p\u0303(ut)]T at samples Ut = {u1, . . . , ut} without generating the image for u. The observations of the function values p\u0303(ut) = p(ut) + wt are corrupted by Gaussian noise wt \u223c N(0, \u03c32). Conditioned on these observations, the mean and variance of the prediction at u are given by:\n\u00b5t(u) = kt(u)(Kt + It\u03c32)\u22121yt \u03c31t (u) = k(u, u)\u2212 kt(u)(Kt + It\u03c32)\u22121kTt (u) (3)\nwhere the vector kt(u) = [k(u, u1), . . . , k(u, ut)] contains the covariances between the new input, u, and the past data points in Ut, the covariance matrix, Kt \u2208 Rt\u00d7t, has entries [Kt](i, j) = k(ui, uj) for i, j \u2208 {1, . . . , t}, and the identity matrix is denoted by It \u2208 Rt\u00d7t.\nGiven a GP, any Bayesian optimization algorithm is designed to find the global optimum of an unknown func-\ntion within few evaluations on the real system. Since we search for counterexamples, i.e, samples where the score returned by the neural network is low, we use GP-Lower Confidence Bound (GP-LCB) as our objective function. Since the optimal sample u\u2217t is not known a priori, the optimal strategy has to balance learning about the location of the most falsifying sample (exploration), and selecting a sample that is known to lead to low scores (exploitation). We formulate the objective function as, ut = argminu\u2208U\u00b5t\u22121(u) \u2212 \u03b2 1/2 t \u03c3t\u22121(u) where \u03b2t is a constant which determines the confidence bound."
        },
        {
            "heading": "2.4. Visualization",
            "text": "We now show how the gathered information can be visualized and interpreted.\nIn our data analysis, we consider two factors: the confidence score and the Intersection Over Union (IOU) that is a metric used to measure the accuracy of detections. IOU is defined as the area of overlap over the area of the union of the predicted and ground-truth bounding boxes. Our visualization tool associates the center of the car of the generated images to the confidence score and IOU returned by the treated CNN. Fig. 3 depicts some examples where the x and y are the center coordinates of the car, z is the IOU, and the color represents the CNN confidence score. We also offer the possibility to superimpose the experimental data on the background used to render the pictures (see Fig. 4). In this case, the IOU is represented by the dimension of the marker. This representation helps us to identify particular regions of interest on the road. In the next section, we will see how these data can be interpreted."
        },
        {
            "heading": "3. Implementation and Evaluation",
            "text": "We implemented the presented framework in a tool available at https://github.com/shromonag/ FalsifyNN. The tool comes with a library composed by a dozen of road backgrounds and car models, and it interfaces to the CNNs SqueezeDet (Wu et al., 2016), Kit-\ntiBox (Teichmann et al., 2016), and Yolo (Redmon et al., 2016). Both the image library and CNN interfaces can be personalized by the user.\nAs an illustrative case study, we considered a countryside background and a Honda Civic and we generated 1k synthetic images using our rendering techniques (Sec. 2.2) and the Halton sampling sequence (Sec. 2.3). We used the generated pictures to analyze SqueezeDet (Wu et al., 2016), a CNN for object detection for autonomous driving, and Yolo (Redmon et al., 2016), a multipurpose CNN for realtime detection.\nFig. 3 displays the center of the car in the generated pictures associated with the confidence score and IOU returned by both SqueezeDet and Yolo. Fig. 4 superimposes the heat maps of Fig. 3 on the used background.\nThere are several interesting insights that emerge from the graphs obtained(for this combination of background and car model). SqueezeDet has, in general, a high confidence and IOU, but has a blind spot for cars in the middle of the road on the right(see the cluster of blue points in Fig. 3(b) and 4(a)). Yolo\u2019s confidence and IOU decrease with the car distance (see Fig. 3(b)). We were able to detect a blind area in Yolo, corresponding to cars on the far left (see blue points in Fig. 3(b)).\nNote how our analysis can be used to visually compare the two CNNs by graphically highlighting their differences in terms of detections, confidence scores, and IOUs. A comprehensive analysis and comparison of these CNNs should involve images generated by combinations of different cars and backgrounds. However, this experiment already shows the benefits in using the presented framework and highlights the quantity and quality of information that can be extracted from a CNN even with a simple study."
        },
        {
            "heading": "Acknowledgement",
            "text": "The authors acknowledge Forrest Iandola and Kurt Keutzer for giving the presentation of this work at Reliable Machine Learning in the Wild - ICML 2017 Workshop."
        }
    ],
    "title": "Systematic Testing of Convolutional Neural Networks for Autonomous Driving",
    "year": 2017
}