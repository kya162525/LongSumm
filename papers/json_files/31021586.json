{
    "abstractText": "Nonnegative matrix factorization (NMF) has become a widely used tool for the analysis of high-dimensional data as it automatically extracts sparse and meaningful features from a set of nonnegative data vectors. We first illustrate this property of NMF on three applications, in image processing, text mining and hyperspectral imaging \u2013this is the why. Then we address the problem of solving NMF, which is NP-hard in general. We review some standard NMF algorithms, and also present a recent subclass of NMF problems, referred to as near-separable NMF, that can be solved efficiently (that is, in polynomial time), even in the presence of noise \u2013this is the how. Finally, we briefly describe some problems in mathematics and computer science closely related to NMF via the nonnegative rank.",
    "authors": [
        {
            "affiliations": [],
            "name": "Nicolas Gillis"
        }
    ],
    "id": "SP:671a0cb774b72741aa6ba366d3177412c2a46fc7",
    "references": [
        {
            "authors": [
                "A. Ambikapathi",
                "T.H. Chan",
                "C.Y. Chi",
                "K. Keizer"
            ],
            "title": "Hyperspectral data geometry based estimation of number of endmembers using p-norm based pure pixel identification",
            "venue": "IEEE Trans. on Geoscience and Remote Sensing 51(5),",
            "year": 2013
        },
        {
            "authors": [
                "U. Ara\u00fajo",
                "B. Saldanha",
                "R. Galv\u00e3o",
                "T. Yoneyama",
                "H. Chame",
                "V. Visani"
            ],
            "title": "The successive projections algorithm for variable selection in spectroscopic multicomponent analysis",
            "venue": "Chemometrics and Intelligent Laboratory Systems 57(2),",
            "year": 2001
        },
        {
            "authors": [
                "S. Arora",
                "R. Ge",
                "Y. Halpern",
                "D. Mimno",
                "A. Moitra",
                "D. Sontag",
                "Y. Wu",
                "M. Zhu"
            ],
            "title": "A practical algorithm for topic modeling with provable guarantees",
            "venue": "In: Int. Conf. on Machine Learning (ICML \u201913),",
            "year": 2013
        },
        {
            "authors": [
                "S. Arora",
                "R. Ge",
                "R. Kannan",
                "A. Moitra"
            ],
            "title": "Computing a nonnegative matrix factorization \u2013 provably",
            "venue": "Proc. of the 44th Symp. on Theory of Computing (STOC",
            "year": 2012
        },
        {
            "authors": [
                "R. Badeau",
                "N. Bertin",
                "E. Vincent"
            ],
            "title": "Stability analysis of multiplicative update algorithms and application to nonnegative matrix factorization",
            "venue": "IEEE Trans. on Neural Networks",
            "year": 2010
        },
        {
            "authors": [
                "M. Berry",
                "M. Browne",
                "A. Langville",
                "V. Pauca",
                "R. Plemmons"
            ],
            "title": "Algorithms and Applications for Approximate Nonnegative Matrix Factorization",
            "venue": "Computational Statistics & Data Analysis",
            "year": 2007
        },
        {
            "authors": [
                "J. Bioucas-Dias",
                "J. Nascimento"
            ],
            "title": "Estimation of signal subspace on hyperspectral data",
            "venue": "Remote Sensing, p. 59820L. International Society for Optics and Photonics",
            "year": 2005
        },
        {
            "authors": [
                "J. Bioucas-Dias",
                "A. Plaza",
                "N. Dobigeon",
                "M. Parente",
                "Q. Du",
                "P. Gader",
                "J. Chanussot"
            ],
            "title": "Hyperspectral unmixing overview: Geometrical, statistical, and sparse regression-based approaches",
            "venue": "IEEE J. of Selected Topics in Applied Earth Observations and Remote Sensing",
            "year": 2012
        },
        {
            "authors": [
                "V. Bittorf",
                "B. Recht",
                "E. R\u00e9",
                "J. Tropp"
            ],
            "title": "Factoring nonnegative matrices with linear programs",
            "venue": "Advances in Neural Information Processing Systems (NIPS",
            "year": 2012
        },
        {
            "authors": [
                "D. Blei"
            ],
            "title": "Probabilistic topic models",
            "venue": "Communications of the ACM 55(4),",
            "year": 2012
        },
        {
            "authors": [
                "C. Boutsidis",
                "E. Gallopoulos"
            ],
            "title": "SVD based initialization: A head start for nonnegative matrix factorization",
            "venue": "Pattern Recognition",
            "year": 2008
        },
        {
            "authors": [
                "C. Boutsidis",
                "M. Mahoney",
                "P. Drineas"
            ],
            "title": "An improved approximation algorithm for the column subset selection problem",
            "venue": "Proc. of the 20th Annual ACM-SIAM Symp. on Discrete Algorithms (SODA",
            "year": 2009
        },
        {
            "authors": [
                "G. Braun",
                "S. Fiorini",
                "S. Pokutta",
                "D. Steurer"
            ],
            "title": "Approximation limits of linear programs (beyond hierarchies)",
            "venue": "Proc. of the 53rd Annual IEEE Symp. on Foundations of Computer Science (FOCS\u2019",
            "year": 2012
        },
        {
            "authors": [
                "R. Bro"
            ],
            "title": "Multi-way analysis in the food industry: Models, algorithms, and applications",
            "venue": "Ph.D. thesis, University of Copenhagen",
            "year": 1998
        },
        {
            "authors": [
                "P. Businger",
                "G. Golub"
            ],
            "title": "Linear least squares solutions by householder transformations",
            "venue": "Numerische Mathematik",
            "year": 1965
        },
        {
            "authors": [
                "D. Cai",
                "X. He",
                "J. Han",
                "T. Huang"
            ],
            "title": "Graph regularized nonnegative matrix factorization for data representation",
            "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence 33(8),",
            "year": 2011
        },
        {
            "authors": [
                "E. Carlini",
                "F. Rapallo"
            ],
            "title": "Probability matrices, non-negative rank, and parameterization of mixture models",
            "venue": "Linear Algebra and its Applications 433,",
            "year": 2010
        },
        {
            "authors": [
                "G. Casalino",
                "N. Del Buono",
                "C. Mencar"
            ],
            "title": "Subtractive clustering for seeding non-negative matrix factorizations",
            "venue": "Information Sciences",
            "year": 2013
        },
        {
            "authors": [
                "A. \u00c7ivril",
                "M. Magdon-Ismail"
            ],
            "title": "On selecting a maximum volume sub-matrix of a matrix and related problems",
            "venue": "Theoretical Computer Science 410(47-49),",
            "year": 2009
        },
        {
            "authors": [
                "A. \u00c7ivril",
                "M. Magdon-Ismail"
            ],
            "title": "Exponential inapproximability of selecting a maximum volume sub-matrix",
            "venue": "Algorithmica 65(1),",
            "year": 2013
        },
        {
            "authors": [
                "T.H. Chan",
                "W.K. Ma",
                "A. Ambikapathi",
                "C.Y. Chi"
            ],
            "title": "A simplex volume maximization framework for hyperspectral endmember extraction",
            "venue": "IEEE Trans. on Geoscience and Remote Sensing 49(11),",
            "year": 2011
        },
        {
            "authors": [
                "T.H. Chan",
                "W.K. Ma",
                "C.Y. Chi",
                "Y. Wang"
            ],
            "title": "A convex analysis framework for blind separation of nonnegative sources",
            "venue": "IEEE Trans. on Signal Processing 56(10),",
            "year": 2008
        },
        {
            "authors": [
                "E. Chi",
                "T. Kolda"
            ],
            "title": "On tensors, sparsity, and nonnegative factorizations",
            "venue": "SIAM J. on Matrix Analysis and Applications 33(4),",
            "year": 2012
        },
        {
            "authors": [
                "S. Choi"
            ],
            "title": "Algorithms for orthogonal nonnegative matrix factorization",
            "venue": "Proc. of the Int. Joint Conf. on Neural Networks, pp",
            "year": 2008
        },
        {
            "authors": [
                "A. Cichocki",
                "A. Phan"
            ],
            "title": "Fast local algorithms for large scale Nonnegative Matrix and Tensor Factorizations",
            "venue": "IEICE Trans. on Fundamentals of Electronics Vol. E92-A",
            "year": 2009
        },
        {
            "authors": [
                "A. Cichocki",
                "R. Zdunek",
                "S.I. Amari"
            ],
            "title": "Non-negative Matrix Factorization with Quasi-Newton Optimization",
            "venue": "Lecture Notes in Artificial Intelligence, Springer,",
            "year": 2006
        },
        {
            "authors": [
                "A. Cichocki",
                "R. Zdunek",
                "S.I. Amari"
            ],
            "title": "Hierarchical ALS Algorithms for Nonnegative Matrix and 3D Tensor Factorization",
            "venue": "Lecture Notes in Computer Science,",
            "year": 2007
        },
        {
            "authors": [
                "A. Cichocki",
                "R. Zdunek",
                "A. Phan",
                "S.I. Amari"
            ],
            "title": "Nonnegative Matrix and Tensor Factorizations: Applications to Exploratory Multi-way Data Analysis and Blind Source Separation",
            "year": 2009
        },
        {
            "authors": [
                "P. Comon"
            ],
            "title": "Independent component analysis, A new concept",
            "venue": "Signal Processing 36,",
            "year": 1994
        },
        {
            "authors": [
                "M. Conforti",
                "G. Cornu\u00e9jols",
                "G. Zambelli"
            ],
            "title": "Extended formulations in combinatorial optimization",
            "venue": "4OR: A Quarterly Journal of Operations Research 10(1),",
            "year": 2010
        },
        {
            "authors": [
                "G. Das",
                "D. Joseph"
            ],
            "title": "The Complexity of Minimum Convex Nested Polyhedra",
            "venue": "Proc. of the 2nd Canadian Conf. on Computational Geometry,",
            "year": 1990
        },
        {
            "authors": [
                "A. d\u2019Aspremont",
                "L. El Ghaoui",
                "M. Jordan",
                "G. Lanckriet"
            ],
            "title": "A Direct Formulation for Sparse PCA Using Semidefinite Programming",
            "venue": "SIAM Review 49(3),",
            "year": 2007
        },
        {
            "authors": [
                "M. Daube-Witherspoon",
                "G. Muehllehner"
            ],
            "title": "An iterative image space reconstruction algorithm suitable for volume ECT",
            "venue": "IEEE Trans. on Medical Imaging",
            "year": 1986
        },
        {
            "authors": [
                "K. Devarajan"
            ],
            "title": "Nonnegative Matrix Factorization: An Analytical and Interpretive Tool in Computational Biology",
            "venue": "PLoS Computational Biology 4(7),",
            "year": 2008
        },
        {
            "authors": [
                "C. Ding",
                "X. He",
                "H. Simon"
            ],
            "title": "On the Equivalence of Nonnegative Matrix Factorization and Spectral Clustering",
            "venue": "SIAM Int. Conf. Data Mining (SDM\u201905),",
            "year": 2005
        },
        {
            "authors": [
                "C. Ding",
                "T. Li",
                "M. Jordan"
            ],
            "title": "Convex and semi-nonnegative matrix factorizations",
            "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence",
            "year": 2010
        },
        {
            "authors": [
                "C. Ding",
                "T. Li",
                "W. Peng"
            ],
            "title": "On the equivalence between non-negative matrix factorization and probabilistic latent semantic indexing",
            "venue": "Computational Statistics & Data Analysis 52(8),",
            "year": 2008
        },
        {
            "authors": [
                "C. Ding",
                "T. Li",
                "W. Peng",
                "H. Park"
            ],
            "title": "Orthogonal nonnegative matrix t-factorizations for clustering",
            "venue": "Proc. of the 12th ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining, pp",
            "year": 2006
        },
        {
            "authors": [
                "W. Ding",
                "M. Rohban",
                "P. Ishwar",
                "V. Saligrama"
            ],
            "title": "Topic discovery through data dependent and random projections",
            "venue": "In: Int. Conf. on Machine Learning (ICML \u201913),",
            "year": 2013
        },
        {
            "authors": [
                "E. Elhamifar",
                "G. Sapiro",
                "R. Vidal"
            ],
            "title": "See all by looking at a few: Sparse modeling for finding representative objects",
            "venue": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR",
            "year": 2012
        },
        {
            "authors": [
                "E. Esser",
                "M. Moller",
                "S. Osher",
                "G. Sapiro",
                "J. Xin"
            ],
            "title": "A convex model for nonnegative matrix factorization and dimensionality reduction on physical space",
            "venue": "IEEE Trans. on Image Processing 21(7),",
            "year": 2012
        },
        {
            "authors": [
                "C. F\u00e9votte",
                "N. Bertin",
                "J.L. Durrieu"
            ],
            "title": "Nonnegative matrix factorization with the Itakura-Saito divergence: With application to music analysis",
            "venue": "Neural Computation",
            "year": 2009
        },
        {
            "authors": [
                "S. Fiorini",
                "V. Kaibel",
                "K. Pashkovich",
                "D. Theis"
            ],
            "title": "Combinatorial bounds on nonnegative rank and extended formulations",
            "venue": "Discrete Mathematics 313(1),",
            "year": 2013
        },
        {
            "authors": [
                "X. Fu",
                "W.K. Ma",
                "T.H. Chan",
                "J. Bioucas-Dias",
                "M.D. Iordache"
            ],
            "title": "Greedy algorithms for pure pixels identification in hyperspectral unmixing: A multiple-measurement vector viewpoint",
            "venue": "Proc. of 21st European Signal Processing Conf. (EUSIPCO",
            "year": 2013
        },
        {
            "authors": [
                "E. Gaussier",
                "C. Goutte"
            ],
            "title": "Relation between PLSA and NMF and implications",
            "venue": "Proc. of the 28th Annual Int. ACM SIGIR Conf. on Research and Development in Information Retrieval,",
            "year": 2005
        },
        {
            "authors": [
                "R. Ge"
            ],
            "title": "Provable algorithms for machine learning problems",
            "venue": "Ph.D. thesis,",
            "year": 2013
        },
        {
            "authors": [
                "N. Gillis"
            ],
            "title": "Nonnegative matrix factorization: Complexity, algorithms and applications",
            "venue": "Ph.D. thesis, Universite\u0301 catholique de Louvain",
            "year": 2011
        },
        {
            "authors": [
                "N. Gillis"
            ],
            "title": "Sparse and unique nonnegative matrix factorization through data preprocessing",
            "venue": "Journal of Machine Learning Research 13(Nov),",
            "year": 2012
        },
        {
            "authors": [
                "N. Gillis"
            ],
            "title": "Robustness analysis of Hottopixx, a linear programming model for factoring nonnegative matrices",
            "venue": "SIAM J. on Matrix Analysis and Applications 34(3),",
            "year": 2013
        },
        {
            "authors": [
                "N. Gillis"
            ],
            "title": "Successive nonnegative projection algorithm for robust nonnegative blind source separation",
            "year": 2013
        },
        {
            "authors": [
                "N. Gillis",
                "F. Glineur"
            ],
            "title": "Using underapproximations for sparse nonnegative matrix factorization",
            "venue": "Pattern Recognition",
            "year": 2010
        },
        {
            "authors": [
                "N. Gillis",
                "F. Glineur"
            ],
            "title": "Accelerated multiplicative updates and hierarchical ALS algorithms for nonnegative matrix factorization",
            "venue": "Neural Computation",
            "year": 2012
        },
        {
            "authors": [
                "N. Gillis",
                "F. Glineur"
            ],
            "title": "On the geometric interpretation of the nonnegative rank",
            "venue": "Linear Algebra and its Applications 437(11),",
            "year": 2012
        },
        {
            "authors": [
                "N. Gillis",
                "R. Luce"
            ],
            "title": "Robust near-separable nonnegative matrix factorization using linear optimization",
            "venue": "Journal of Machine Learning Research",
            "year": 2014
        },
        {
            "authors": [
                "N. Gillis",
                "S. Vavasis"
            ],
            "title": "Fast and robust recursive algorithms for separable nonnegative matrix factorization",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "year": 2013
        },
        {
            "authors": [
                "N. Gillis",
                "S. Vavasis"
            ],
            "title": "Semidefinite programming based preconditioning for more robust near-separable nonnegative matrix factorization",
            "year": 2013
        },
        {
            "authors": [
                "G. Golub",
                "C. Van Loan"
            ],
            "title": "Matrix Computation, 3rd Edition",
            "year": 1996
        },
        {
            "authors": [
                "J. Gouveia",
                "P. Parrilo",
                "R. Thomas"
            ],
            "title": "Approximate cone factorizations and lifts of polytopes",
            "year": 2013
        },
        {
            "authors": [
                "L. Grippo",
                "M. Sciandrone"
            ],
            "title": "On the convergence of the block nonlinear Gauss-Seidel method under convex constraints",
            "venue": "Operations Research Letters",
            "year": 2000
        },
        {
            "authors": [
                "N. Guan",
                "D. Tao",
                "Z. Luo",
                "B. Yuan"
            ],
            "title": "NeNMF: an optimal gradient method for nonnegative matrix factorization",
            "venue": "IEEE Trans. on Signal Processing 60(6),",
            "year": 2012
        },
        {
            "authors": [
                "D. Guillamet",
                "J. Vitri\u00e0"
            ],
            "title": "Non-negative matrix factorization for face recognition",
            "venue": "Lecture Notes in Artificial Intelligence,",
            "year": 2002
        },
        {
            "authors": [
                "J. Han",
                "L. Han",
                "M. Neumann",
                "U. Prasad"
            ],
            "title": "On the rate of convergence of the image space reconstruction algorithm. Operators and Matrices",
            "year": 2009
        },
        {
            "authors": [
                "N.D. Ho"
            ],
            "title": "Nonnegative matrix factorization - algorithms and applications",
            "venue": "Ph.D. thesis, Universite\u0301 catholique de Louvain",
            "year": 2008
        },
        {
            "authors": [
                "P. Hoyer"
            ],
            "title": "Nonnegative matrix factorization with sparseness constraints",
            "venue": "Journal of Machine Learning Research",
            "year": 2004
        },
        {
            "authors": [
                "C.J. Hsieh",
                "I. Dhillon"
            ],
            "title": "Fast coordinate descent methods with variable selection for non-negative matrix factorization",
            "venue": "Proc. of the 17th ACM SIGKDD int. conf. on Knowledge discovery and data mining,",
            "year": 2011
        },
        {
            "authors": [
                "K. Huang",
                "N. Sidiropoulos",
                "A. Swami"
            ],
            "title": "Non-negative matrix factorization revisited: Uniqueness and algorithm for symmetric decomposition",
            "venue": "IEEE Trans. on Signal Processing 62(1),",
            "year": 2014
        },
        {
            "authors": [
                "M.D. Iordache",
                "J. Bioucas-Dias",
                "A. Plaza"
            ],
            "title": "Total variation spatial regularization for sparse hyperspectral unmixing",
            "venue": "IEEE Trans. on Geoscience and Remote Sensing 50(11),",
            "year": 2012
        },
        {
            "authors": [
                "S. Jia",
                "Y. Qian"
            ],
            "title": "Constrained nonnegative matrix factorization for hyperspectral unmixing",
            "venue": "IEEE Trans. on Geoscience and Remote Sensing 47(1),",
            "year": 2009
        },
        {
            "authors": [
                "V. Kaibel"
            ],
            "title": "Extended Formulations in Combinatorial Optimization",
            "venue": "Optima 85,",
            "year": 2011
        },
        {
            "authors": [
                "B. Kanagal",
                "V. Sindhwani"
            ],
            "title": "Rank selection in low-rank matrix approximations",
            "venue": "Advances in Neural Information Processing Systems (NIPS",
            "year": 2010
        },
        {
            "authors": [
                "Q. Ke",
                "T. Kanade"
            ],
            "title": "Robust L1 norm factorization in the presence of outliers and missing data by alternative convex programming",
            "venue": "IEEE Conf. on Computer Vision and Pattern Recognition",
            "year": 2005
        },
        {
            "authors": [
                "H. Kim",
                "H. Park"
            ],
            "title": "Sparse non-negative matrix factorizations via alternating non-negativity-constrained least squares for microarray data analysis",
            "venue": "Bioinformatics 23(12),",
            "year": 2007
        },
        {
            "authors": [
                "H. Kim",
                "H. Park"
            ],
            "title": "Non-negative Matrix Factorization Based on Alternating Non-negativity Constrained Least Squares and Active Set Method",
            "venue": "SIAM J. on Matrix Analysis and Applications 30(2),",
            "year": 2008
        },
        {
            "authors": [
                "J. Kim",
                "Y. He",
                "H. Park"
            ],
            "title": "Algorithms for nonnegative matrix and tensor factorizations: A unified view based on block coordinate descent framework",
            "venue": "Journal of Global Optimization",
            "year": 2013
        },
        {
            "authors": [
                "J. Kim",
                "H. Park"
            ],
            "title": "Fast nonnegative matrix factorization: An active-set-like method and comparisons",
            "venue": "SIAM J. on Scientific Computing",
            "year": 2011
        },
        {
            "authors": [
                "A. Kumar",
                "V. Sindhwani"
            ],
            "title": "Near-separable non-negative matrix factorization with l1- and Bregman loss functions",
            "year": 2013
        },
        {
            "authors": [
                "A. Kumar",
                "V. Sindhwani",
                "P. Kambadur"
            ],
            "title": "Fast conical hull algorithms for near-separable non-negative matrix factorization",
            "venue": "In: Int. Conf. on Machine Learning (ICML \u201913),",
            "year": 2013
        },
        {
            "authors": [
                "C. Lawson",
                "R. Hanson"
            ],
            "title": "Solving Least Squares Problems",
            "year": 1974
        },
        {
            "authors": [
                "D. Lee",
                "H. Seung"
            ],
            "title": "Learning the Parts of Objects by Nonnegative Matrix Factorization",
            "venue": "Nature 401,",
            "year": 1999
        },
        {
            "authors": [
                "D. Lee",
                "H. Seung"
            ],
            "title": "Algorithms for Non-negative Matrix Factorization",
            "venue": "In Advances in Neural Information Processing (NIPS \u201901)",
            "year": 2001
        },
        {
            "authors": [
                "T. Lee",
                "A. Shraibman"
            ],
            "title": "Lower bounds in communication complexity",
            "venue": "Now Publishers Inc",
            "year": 2009
        },
        {
            "authors": [
                "A. Lef\u00e8vre"
            ],
            "title": "Dictionary learning methods for single-channel source separations",
            "venue": "Ph.D. thesis, Ecole Normale Supe\u0301rieure de Cachan",
            "year": 2012
        },
        {
            "authors": [
                "L. Li",
                "G. Lebanon",
                "H. Park"
            ],
            "title": "Fast Bregman divergence NMF using Taylor expansion and coordinate descent",
            "venue": "Proc. of the 18th ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining,",
            "year": 2012
        },
        {
            "authors": [
                "L. Li",
                "Y.J. Zhang"
            ],
            "title": "FastNMF: highly efficient monotonic fixed-point nonnegative matrix factorization algorithm with good applicability",
            "venue": "J. Electron. Imaging Vol",
            "year": 2009
        },
        {
            "authors": [
                "T. Li",
                "Y. Zhang",
                "V. Sindhwani"
            ],
            "title": "A non-negative matrix tri-factorization approach to sentiment classification with lexical prior knowledge",
            "venue": "Association of Computational Lingustics,",
            "year": 2009
        },
        {
            "authors": [
                "Y. Li",
                "D. Sima",
                "S. Van Cauter",
                "A. Croitor Sava",
                "U. Himmelreich",
                "Y. Pi",
                "S. Van Huffel"
            ],
            "title": "Hierarchical non-negative matrix factorization (hNMF): a tissue pattern differentiation method for glioblastoma multiforme diagnosis using MRSI",
            "venue": "NMR in Biomedicine",
            "year": 2013
        },
        {
            "authors": [
                "C.J. Lin"
            ],
            "title": "On the convergence of multiplicative update algorithms for nonnegative matrix factorization",
            "venue": "IEEE Trans. on Neural Networks",
            "year": 2007
        },
        {
            "authors": [
                "C.J. Lin"
            ],
            "title": "Projected Gradient Methods for Nonnegative Matrix Factorization",
            "venue": "Neural Computation",
            "year": 2007
        },
        {
            "authors": [
                "J. Liu",
                "P. Wonka",
                "J. Ye"
            ],
            "title": "Sparse non-negative tensor factorization using columnwise coordinate descent",
            "venue": "Pattern Recognition",
            "year": 2012
        },
        {
            "authors": [
                "W.K. Ma",
                "J. Bioucas-Dias",
                "T.H. Chan",
                "N. Gillis",
                "P. Gader",
                "A. Plaza",
                "A. Ambikapathi",
                "C.Y. Chi"
            ],
            "title": "A Signal Processing Perspective on Hyperspectral Unmixing",
            "venue": "IEEE Signal Processing Magazine 31(1),",
            "year": 2014
        },
        {
            "authors": [
                "M. Mahoney",
                "P. Drineas"
            ],
            "title": "CUR matrix decompositions for improved data analysis",
            "venue": "Proc. of the National Academy of Sciences",
            "year": 2009
        },
        {
            "authors": [
                "P. Melville",
                "V. Sindhwani"
            ],
            "title": "Recommender systems. Encyclopedia of machine learning",
            "year": 2010
        },
        {
            "authors": [
                "L. Miao",
                "H. Qi"
            ],
            "title": "Endmember extraction from highly mixed data using minimum volume constrained nonnegative matrix factorization",
            "venue": "IEEE Trans. on Geoscience and Remote Sensing 45(3),",
            "year": 2007
        },
        {
            "authors": [
                "A. Moitra"
            ],
            "title": "An almost optimal algorithm for computing nonnegative rank",
            "venue": "Proc. of the 24th Annual ACM-SIAM Symp. on Discrete Algorithms (SODA",
            "year": 2013
        },
        {
            "authors": [
                "W. Naanaa",
                "J.M. Nuzillard"
            ],
            "title": "Blind source separation of positive and partially correlated data",
            "venue": "Signal Processing 85(9),",
            "year": 2005
        },
        {
            "authors": [
                "J. Nascimento",
                "J. Bioucas-Dias"
            ],
            "title": "Vertex component analysis: a fast algorithm to unmix hyperspectral data",
            "venue": "IEEE Trans. on Geoscience and Remote Sensing 43(4),",
            "year": 2005
        },
        {
            "authors": [
                "P. Paatero",
                "U. Tapper"
            ],
            "title": "Positive matrix factorization: a non-negative factor model with optimal utilization of error estimates of data",
            "venue": "values. Environmetrics",
            "year": 1994
        },
        {
            "authors": [
                "J. Rapin",
                "J. Bobin",
                "A. Larue",
                "J.L. Starck"
            ],
            "title": "Sparse and non-negative BSS for noisy data",
            "venue": "IEEE Trans. on Signal Processing 61(22),",
            "year": 2013
        },
        {
            "authors": [
                "H. Ren",
                "C.I. Chang"
            ],
            "title": "Automatic spectral target recognition in hyperspectral imagery",
            "venue": "IEEE Trans. on Aerospace and Electronic Systems",
            "year": 2003
        },
        {
            "authors": [
                "R. Sandler",
                "M. Lindenbaum"
            ],
            "title": "Nonnegative matrix factorization with earth mover\u2019s distance metric",
            "venue": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR",
            "year": 2009
        },
        {
            "authors": [
                "F. Shahnaz",
                "M. Berry",
                "V. Pauca",
                "R. Plemmons"
            ],
            "title": "Document clustering using nonnegative matrix factorization",
            "venue": "Information Processing and Management",
            "year": 2006
        },
        {
            "authors": [
                "P. Smaragdis",
                "C. F\u00e9votte",
                "G. Mysore",
                "N. Mohammadiha",
                "M. Hoffman"
            ],
            "title": "A Unied View of Static and Dynamic Source Separation Using Non-Negative Factorizations",
            "venue": "IEEE Signal Processing Magazine",
            "year": 2014
        },
        {
            "authors": [
                "N. Takahashi",
                "R. Hibi"
            ],
            "title": "Global convergence of modified multiplicative updates for nonnegative matrix factorization",
            "venue": "Computational Optimization and Applications",
            "year": 2013
        },
        {
            "authors": [
                "V. Tan",
                "C. F\u00e9votte"
            ],
            "title": "Automatic relevance determination in nonnegative matrix factorization. In: Signal Processing with Adaptive Sparse Structured Representations (SPARS",
            "year": 2009
        },
        {
            "authors": [
                "S. Vavasis"
            ],
            "title": "On the complexity of nonnegative matrix factorization",
            "venue": "SIAM J. on Optimization",
            "year": 2009
        },
        {
            "authors": [
                "F. Wang",
                "T. Li",
                "X. Wang",
                "S. Zhu",
                "C. Ding"
            ],
            "title": "Community discovery using nonnegative matrix factorization",
            "venue": "Data Min. Knowl. Disc. 22(3),",
            "year": 2011
        },
        {
            "authors": [
                "S. Wild",
                "J. Curry",
                "A. Dougherty"
            ],
            "title": "Improving non-negative matrix factorizations through structured initialization",
            "venue": "Pattern Recognition",
            "year": 2004
        },
        {
            "authors": [
                "M. Winter"
            ],
            "title": "N-FINDR: an algorithm for fast autonomous spectral end-member determination in hyperspectral data",
            "venue": "Proc. SPIE Conf. on Imaging Spectrometry V, pp",
            "year": 1999
        },
        {
            "authors": [
                "Y. Xue",
                "C. Tong",
                "Y. Chen",
                "W.S. Chen"
            ],
            "title": "Clustering-based initialization for non-negative matrix factorization",
            "venue": "Applied Mathematics and Computation",
            "year": 2008
        },
        {
            "authors": [
                "Z. Yang",
                "E. Oja"
            ],
            "title": "Linear and nonlinear projective nonnegative matrix factorization",
            "venue": "IEEE Trans. on Neural Networks",
            "year": 2010
        },
        {
            "authors": [
                "M. Yannakakis"
            ],
            "title": "Expressing Combinatorial Optimization Problems by Linear Programs",
            "venue": "Journal of Computer and System Sciences",
            "year": 1991
        },
        {
            "authors": [
                "R. Zdunek"
            ],
            "title": "Initialization of nonnegative matrix factorization with vertices of convex polytope",
            "venue": "Artificial Intelligence and Soft Computing, Lecture Notes in Computer Science,",
            "year": 2012
        },
        {
            "authors": [
                "S. Zhong",
                "J. Ghosh"
            ],
            "title": "Generative model-based document clustering: a comparative study",
            "venue": "Knowledge and Information Systems",
            "year": 2005
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :1\n40 1.\n52 26\nv2 [\nst at\n.M L\n] 7\nM ar\nKeywords. Nonnegative matrix factorization, applications, algorithms."
        },
        {
            "heading": "1 Introduction",
            "text": "Linear dimensionality reduction (LDR) techniques are a key tool in data analysis, and are widely used for example for compression, visualization, feature selection and noise filtering. Given a set of data points xj \u2208 R\np for 1 \u2264 j \u2264 n and a dimension r < min(p, n), LDR amounts to computing a set of r basis elements wk \u2208 R\np for 1 \u2264 k \u2264 r such that the linear space spanned by the wk\u2019s approximates the data points as closely as possible, that is, such that we have for all j\nxj \u2248\nr\u2211\nk=1\nwkhj(k), for some weights hj \u2208 R r. (1)\nIn other words, the p-dimensional data points are represented in a r-dimensional linear subspace spanned by the basis elements wk\u2019s and whose coordinates are given by the vectors hj \u2019s. LDR is equivalent to low-rank matrix approximation: in fact, constructing\n\u2022 the matrix X \u2208 Rp\u00d7n such that each column is a data point, that is, X(:, j) = xj for 1 \u2264 j \u2264 n,\n\u2022 the matrix W \u2208 Rp\u00d7r such that each column is a basis element, that is, W (:, k) = wk for 1 \u2264 k \u2264 r, and\n\u2022 the matrix H \u2208 Rr\u00d7n such that each column of H gives the coordinates of a data point X(:, j) in the basis W , that is, H(:, j) = hj for 1 \u2264 j \u2264 n,\nthe above LDR model (1) is equivalent to X \u2248WH, that is, to approximate the data matrix X with a low-rank matrix WH.\nA first key aspect of LDR is the choice of the measure to assess the quality of the approximation. It should be chosen depending on the noise model. The most widely used measure is the Frobenius norm of the error, that is, ||X \u2212WH||2F = \u2211 i,j(X\u2212WH) 2 ij . The reason for the popularity of the Frobenius norm is two-fold. First, it implicitly assumes the noise N present in the matrix X = WH +N to be Gaussian, which is reasonable in many practical situations (see also the introduction of Section 3). Second, an optimal approximation can be computed efficiently through the truncated singular value decomposition (SVD); see [57] and the references therein. Note that the SVD is equivalent to principal component analysis (PCA) after mean centering of the data points (that is, after shifting all data points so that their mean is on the origin).\nA second key aspect of LDR is the assumption on the structure of the factors W and H. The truncated SVD and PCA do not make any assumption on W and H. For example, assuming independence of the columns of W leads to independent component analysis (ICA) [29], or assuming sparsity of W (and/or H) leads to sparse low-rank matrix decompositions, such as sparse PCA [32]. Nonnegative matrix factorization (NMF) is an LDR where both the basis elements wk\u2019s and the weights hj \u2019s are assumed to be component-wise nonnegative. Hence NMF aims at decomposing a given nonnegative data matrix X as X \u2248 WH where W \u2265 0 and H \u2265 0 (meaning that W and H are component-wise nonnegative). NMF was first introduced in 1994 by Paatero and Tapper [97] and gathered more and more interest after an article by Lee and Seung [79] in 1999.\nIn this paper, we explain why NMF has been so popular in different data mining applications, and how one can compute NMF\u2019s. The aim of this paper is not to give a comprehensive overview of all NMF applications and algorithms \u2013and we apologize for not mentioning many relevant contributions\u2013 but rather to serve as an introduction to NMF, describing three applications and several standard algorithms."
        },
        {
            "heading": "2 The Why \u2013 NMF Generates Sparse and Meaningful Features",
            "text": "The reason why NMF has become so popular is because of its ability to automatically extract sparse and easily interpretable factors. In this section, we illustrate this property of NMF through three applications, in image processing, text mining and hyperspectral imaging. Other applications include air emission control [97], computational biology [34], blind source separation [22], single-channel source separation [82], clustering [35], music analysis [42], collaborative filtering [92], and community detection [106]."
        },
        {
            "heading": "2.1 Image Processing \u2013 Facial Feature Extraction",
            "text": "Let each column of the data matrix X \u2208 Rp\u00d7n+ be a vectorized gray-level image of a face, with the (i, j)th entry of matrix X being the intensity of the ith pixel in the jth face. NMF generates two factors (W,H) so that each image X(:, j) is approximated using a linear combination of the columns of W ; see Equation (1), and Figure 1 for an illustration. Since W is nonnegative, the columns of W can be interpreted as images (that is, vectors of pixel intensities) which we refer to as the basis images. As the weights in the linear combinations are nonnegative (H \u2265 0), these basis images can only be summed up to reconstruct each original image. Moreover, the large number of images in the data set must be reconstructed approximately with only a few basis images (in fact, r is in general much smaller than n), hence the latter should be localized features (hence sparse) found simultaneously in several\nimages. In the case of facial images, the basis images are features such as eyes, noses, mustaches, and lips (see Figure 1) while the columns of H indicate which feature is present in which image (see also [79, 61]).\nA potential application of NMF is in face recognition. It has for example been observed that NMF is more robust to occlusion than PCA (which generates dense factors): in fact, if a new occluded face (e.g., with sun glasses) has to be mapped into the NMF basis, the non-occluded parts (e.g., the mustache or the lips) can still be well approximated [61]."
        },
        {
            "heading": "2.2 Text Mining \u2013 Topic Recovery and Document Classification",
            "text": "Let each column of the nonnegative data matrix X correspond to a document and each row to a word. The (i, j)th entry of the matrix X could for example be equal to the number of times the ith word appears in the jth document in which case each column of X is the vector of word counts of a document; in practice, more sophisticated constructions are used, e.g., the term frequency - inverse document frequency (tf-idf). This is the so-called bag-of-words model: each document is associated with a set of words with different weights, while the ordering of the words in the documents is not taken into account (see, e.g., the survey [10] for a discussion). Note that such a matrix X is in general rather sparse as most documents only use a small subset of the dictionary. Given such a matrix X and a factorization rank r, NMF generates two factors (W,H) such that, for all 1 \u2264 j \u2264 n, we have\nX(:, j) \ufe38 \ufe37\ufe37 \ufe38\njth document\n\u2248 r\u2211\nk=1\nW (:, k) \ufe38 \ufe37\ufe37 \ufe38 kth topic\nH(k, j) \ufe38 \ufe37\ufe37 \ufe38\nimportance of kth topic\nin jth document\n, with W \u2265 0 and H \u2265 0.\nThis decomposition can be interpreted as follows (see, also, e.g., [79, 101, 3]):\n\u2022 Because W is nonnegative, each column of W can be interpreted as a document, that is, as a bag of words.\n\u2022 Because the weights in the linear combinations are nonnegative (H \u2265 0), one can only take the union of the sets of words defined by the columns of W to reconstruct all the original documents.\n\u2022 Moreover, because the number of documents in the data set is much larger than the number of basis elements (that is, the number of columns of W ), the latter should be set of words found simultaneously in several documents. Hence the basis elements can be interpreted as topics, that is, set of words found simultaneously in different documents, while the weights in the linear combinations (that is, the matrix H) assign the documents to the different topics, that is, identify which document discusses which topic.\nTherefore, given a set of documents, NMF identifies topics and simultaneously classifies the documents among these different topics. Note that NMF is closely related to existing topic models, in particular probabilistic latent semantic analysis and indexing (PLSA and PLSI) [45, 37]."
        },
        {
            "heading": "2.3 Hyperspectral Unmixing \u2013 Identify Endmembers and Classify Pixels",
            "text": "Let the columns of the nonnegative data matrix X be the spectral signatures of the pixels in a scene being imaged. The spectral signature of a pixel is the fraction of incident light being reflected by that pixel at different wavelengths, and is therefore nonnegative. For a hyperspectral image, there are usually between 100 and 200 wavelength-indexed bands, observed in much broader spectrum than the visible light. This allows for more accurate analysis of the scene under study.\nGiven a hyperspectral image (see Figure 2 for an illustration), the goal of blind hyperspectral unmixing (blind HU) is two-fold:\n1. Identify the constitutive materials present in the image; for example, it could be grass, roads, or metallic surfaces. These are referred to as the endmembers.\n2. Classify the pixels, that is, identify which pixel contains which endmember and in which proportion. (In fact, pixels are in general mixture of several endmembers, due for example to low spatial resolution or mixed materials.)\nThe simplest and most popular model used to address this problem is the linear mixing model. It assumes that the spectral signature of a pixel results from the linear combination of the spectral signature of the endmembers it contains. The weights in the linear combination correspond to the abundances of these endmembers in that pixel. For example, if a pixel contains 30% of grass and 70% of road surface, then, under the linear mixing model, its spectral signature will be 0.3 times the spectral signature of the grass plus 0.7 times the spectral signature of the road surface. This is exactly the NMF model: the spectral signatures of the endmembers are the basis elements, that is, the columns of W , while the abundances of the endmembers in each pixel are the weights, that is, the columns of H. Note that the factorization rank r corresponds to the number of endmembers in the hyperspectral image. Figure 2 illustrates such a decomposition.\nTherefore, given a hyperspectral image, NMF is able to compute the spectral signatures of the endmembers and simultaneously the abundance of each endmember in each pixel. We refer the reader to [8, 90] for recent surveys on blind HU techniques."
        },
        {
            "heading": "3 The How \u2013 Some Algorithms",
            "text": "We have seen in the previous section that NMF is a useful LDR technique for nonnegative data. The question is now: can we compute such factorizations? In this paper, we focus on the following optimization problem\nmin W\u2208Rp\u00d7r ,H\u2208Rr\u00d7n\n||X \u2212WH||2F such that W \u2265 0 and H \u2265 0. (2)\nHence we implicitly assume Gaussian noise on the data; see Introduction. Although this NMF model is arguably the most popular, it is not always reasonable to assume Gaussian noise for nonnegative data, especially for sparse matrices such as document data sets; see the discussion in [23]. In fact, many other objective functions are used in practice, e.g., the (generalized) Kullback-Leibler divergence for text mining [23], the Itakura-Saito distance for music analysis [42], the \u21131 norm to improve robustness against outliers [71], and the earth mover\u2019s distance for computer vision tasks [100]. Other NMF models are motivated by statistical considerations; we refer the reader to the recent survey [102].\nThere are many issues when using NMF in practice. In particular,\n\u2022 NMF is NP-hard. Unfortunately, as opposed to the unconstrained problem which can be solved efficiently using the SVD, NMF is NP-hard in general [105]. Hence, in practice, most\nalgorithms are applications of standard nonlinear optimization methods and may only be guaranteed to converge to stationary points; see Section 3.1. However, these heuristics have been proved to be successful in many applications. More recently, Arora et al. [4] described a subclass of nonnegative matrices for which NMF can be solved efficiently. These are the near-separable matrices which will be addressed in Section 3.2. Note that Arora et al. [4] also described an algorithmic approach for exact NMF1 requiring O ( (pn)2 rr2 ) operations \u2013later improved to O ( (pn)r 2) by Moitra [94]\u2013 hence polynomial in the dimensions p and n for r fixed. Although r is usually small in practice, this approach cannot be used to solving real-world problems because of its high computational cost (in contrast, most heuristic NMF algorithms run in O(pnr) operations; see Section 3.1).\n\u2022 NMF is ill-posed. Given an NMF (W,H) of X, there usually exist equivalent NMF\u2019s (W \u2032,H \u2032) with W \u2032H \u2032 = WH. In particular, any matrix Q satisfying WQ \u2265 0 and Q\u22121H \u2265 0 generates such an equivalent factorization. The matrix Q can always be chosen as the permutation of a diagonal matrix with positive diagonal elements (that is, as a monomial matrix) and this amounts to the scaling and permutation of the rank-one factors W (:, k)H(k, :) for 1 \u2264 k \u2264 r; this is not an issue in practice. The issue is when there exist non-monomial matrices Q satisfying the above conditions. In that case, such equivalent factorizations generate different interpretations: for example, in text mining, they would lead to different topics and classifications; see the discussion in [48]. Here is a simple example\n\n 0 1 1 1 1 0 1 1 1 1 0 1\n\n =\n\n 0 1 1 1 0 1 1 1 0\n\n\n\n 1 0 0 0.5 0 1 0 0.5 0 0 1 0.5\n\n =\n\n 1 0 0 0 1 0 0 0 1\n\n\n\n 0 1 1 1 1 0 1 1 1 1 0 1\n\n .\nWe refer the reader to [66] and the references therein for recent results on non-uniqueness of NMF.\nIn practice, this issue is tackled using other priors on the factors W and H and adding proper regularization terms in the objective function. The most popular prior is sparsity which can be tackled with projections [64] or with \u21131-norm penalty [72, 48]. For example, in blind HU (Section 2.3), the abundance maps (that is, the rows of matrix H) are usually very sparse (most pixels contain only a few endmembers) and applying plain NMF (2) usually gives poor results for these data sets. Other priors for blind HU include piece-wise smoothness of the spectral signatures or spatial coherence (neighboring pixels are more likely to contain the same materials) which are usually tackled with TV-like regularizations (that is, \u21131 norm of the difference between neighboring values to preserve the edges in the image); see, e.g., [68, 67], and the references therein. Note that the design and algorithmic implementation of refined NMF models for various applications is a very active area of research, e.g., graph regularized NMF [16], orthogonal NMF [24], tri-NMF [38, 85], semi and convex NMF [36], projective NMF [110], minimum volume NMF [93], and hierarchical NMF [86], to cite only a few.\n\u2022 Choice of r. The choice of the factorization rank r, that is, the problem of order model selection, is usually rather tricky. Several popular approaches are: trial and error (that is, try different values of r and pick the one performing best for the application at hand), estimation using the SVD (that is, look at the decay of the singular values of the input data matrix), and\n1Exact NMF refers to the NMF problem where an exact factorization is sought: X = WH with W \u2265 0 and H \u2265 0.\nthe use of experts insights (e.g., in blind HU, experts might have a good guess for the number of endmembers present in a scene); see also [7, 104, 70] and the references therein.\nIn this section, we focus on the first issue. In Section 3.1, we present several standard algorithms for the general problem (2). In Section 3.2, we describe the near-separable NMF problem and several recent algorithms."
        },
        {
            "heading": "3.1 Standard NMF Algorithms",
            "text": "Almost all NMF algorithms designed for (2) use a two-block coordinate descent scheme (exact or inexact; see below), that is, they optimize alternatively over one of the two factors, W or H, while keeping the other fixed. The reason is that the subproblem in one factor is convex. More precisely, it is a nonnegative least squares problem (NNLS): for example, for H fixed, we have to solve minW\u22650 ||X\u2212 WH||2F . Note that this problem has a particular structure as it can be decomposed into p independent NNLS in r variables since\n||X \u2212WH||2F =\np \u2211\ni=1\n||Xi: \u2212Wi:H|| 2 2 =\np \u2211\ni=1\nWi: ( HHT ) W Ti: \u2212 2Wi: ( HXTi: ) + ||Xi:|| 2 2. (3)\nMany algorithms exist to solve the NNLS problem, and NMF algorithms based on two-block coordinate descent differ by which NNLS algorithm is used; see also, e.g., the discussion in [74]. It is interesting to notice that the problem is symmetric in W and H since ||X\u2212WH||2F = ||X\nT \u2212HTW T ||2F . Therefore, we can focus on the update of only one factor and, in fact, most NMF algorithms use the same update for W and H, and therefore adhere to the framework described in Algorithm CD.\nAlgorithm CD Two-Block Coordinate Descent \u2013 Framework of Most NMF Algorithms Input: Input nonnegative matrix X \u2208 Rp\u00d7n+ and factorization rank r. Output: (W,H) \u2265 0: A rank-r NMF of X \u2248WH.\n1: Generate some initial matrices W (0) \u2265 0 and H(0) \u2265 0; see Section 3.1.8. 2: for t = 1, 2, . . . \u2020 do 3: W (t) = update ( X,H(t\u22121),W (t\u22121) ) . 4: H(t) T = update ( XT ,W (t) T ,H(t\u22121) T ) ."
        },
        {
            "heading": "5: end for",
            "text": "\u2020See Section 3.1.7 for stopping criteria.\nThe update in steps 3 and 4 of Algorithm CD usually guarantees the objective function to decrease. In this section, we describe the most widely used updates, that is, we describe several standard and widely used NMF algorithms, and compare them in Section 3.1.6. But first we address an important tool to designing NMF algorithms: the optimality conditions. To simplify notations, we will drop the iteration index t."
        },
        {
            "heading": "3.1.1 First-Order Optimality Conditions",
            "text": "Given X, let us denote F (W,H) = 12 ||X \u2212WH|| 2 F . The first-order optimality conditions for (2) are\nW \u2265 0, \u2207WF = WHH T \u2212XHT \u2265 0, W \u25e6 \u2207WF = 0, (4) H \u2265 0, \u2207HF = W TWH \u2212W TX \u2265 0, H \u25e6 \u2207HF = 0,\nwhere \u25e6 is the component-wise product of two matrices. Any (W,H) satisfying these conditions is a stationary point of (2).\nIt is interesting to observe that these conditions give a more formal explanation of why NMF naturally generates sparse solutions [51]: in fact, any stationary point of (2) is expected to have zero entries because of the conditions W \u25e6\u2207WF = 0 and H \u25e6\u2207HF = 0, that is, the conditions that for all i, k either Wik is equal to zero or the partial derivative of F with respect to Wik is, and similarly for H."
        },
        {
            "heading": "3.1.2 Multiplicative Updates",
            "text": "Given X, W and H, the multiplicative updates (MU) modify W as follows\nW \u2190 W \u25e6\n[ XHT ]\n[WHHT ] (5)\nwhere [ ][ ] denotes the component-wise division between two matrices. The MU were first developed in [33] for solving NNLS problems, and later rediscovered and used for NMF in [80]. The MU are based on the majorization-minimization framework. In fact, (5) is the global minimizer of a quadratic function majorizing F , that is, a function that is larger than F everywhere and is equal to F at the current iterate [33, 80]. Hence minimizing that function guarantees F to decrease and therefore leads to an algorithm for which F monotonically decreases. The MU can also be interpreted as a rescaled gradient method: in fact,\nW \u25e6\n[ XHT ]\n[WHHT ] = W \u2212\n[W ]\n[WHHT ] \u25e6 \u2207WF. (6)\nAnother more intuitive interpretation is as follows: we have that [ XHT ]\nik\n[WHHT ]ik \u2265 1 \u21d0\u21d2 (\u2207WF )ik \u2264 0.\nTherefore, in order to satisfy (4), for each entry of W , the MU either (i) increase it if its partial derivative is negative, (ii) decrease it if its partial derivative is positive, or (iii) leave it unchanged if its partial derivative is equal to zero.\nIf an entry of W is equal to zero, the MU cannot modify it hence it may occur that an entry of W is equal to zero while its partial derivative is negative which would not satisfy (4). Therefore, the MU are not guaranteed to converge to a stationary point2. There are several ways to fix this issue, e.g., rewriting the MU as a rescaled gradient descent method \u2013see Equation (6): only entries in the same row interact\u2013 and modifying the step length [87], or using a small positive lower bound for the entries of W and H [52, 103]; see also [5]. A simpler and nice way to guarantee convergence of the MU to a stationary point is proposed in [23]: use the original updates (5) while reinitializing zero entries of W to a small positive constant when their partial derivatives become negative.\nThe MU became extremely popular mainly because (i) they are simple to implement3, (ii) they scale well and are applicable to sparse matrices4, and (iii) they were proposed in the paper of Lee\n2If the initial matrices are chosen positive, some entries can first converge to zero while their partial derivative eventually becomes negative or zero (when strict complementarity is not met) which is numerically unstable; see [52] for some numerical experiments.\n3For example, in Matlab: W = W.*(X*H\u2019)./(W*(H*H\u2019)). 4When computing the denominator WHHT in the MU, it is crucial to compute HHT first in order to have the lowest\ncomputational cost, and make the MU scalable for sparse matrices; see, e.g., footnote 3.\nand Seung [79] which launched the research on NMF. However, the MU converge relatively slowly; see, e.g., [62] for a theoretical analysis, and Section 3.1.6 for some numerical experiments. Note that the original MU only update W once before updating H. They can be significantly accelerated using a more effective alternation strategy [52]: the idea is to update W several times before updating H because the products HHT and XHT do not need to be recomputed."
        },
        {
            "heading": "3.1.3 Alternating Least Squares",
            "text": "The alternating least squares method (ALS) first computes the optimal solution of the unconstrained least squares problem minW ||X\u2212WH||F and then project the solution onto the nonnegative orthant:\nW \u2190 max ( argminZ\u2208Rp\u00d7r ||X \u2212 ZH||F , 0 ) ,\nwhere the max is taken component-wise. The method has the advantage to be relatively cheap, and easy to implement5. ALS usually does not converge: the objective function of (2) might oscillate under the ALS updates (especially for dense input matrices X; see Section 3.1.6). It is interesting to notice that, because of the projection, the solution generated by ALS is not scaled properly. In fact, the error can be reduced (sometimes drastically) by multiplying the current solution WH by the constant\n\u03b1\u2217 = argmin\u03b1\u22650 ||X \u2212 \u03b1WH||F = \u3008X,WH\u3009 \u3008WH,WH\u3009 = \u3008XHT ,W \u3009 \u3008W TW,HHT \u3009 . (7)\nAlthough it is in general not recommended to use ALS because of the convergence issues, ALS can be rather powerful for initialization purposes (that is, perform a few steps of ALS and then switch to another NMF algorithm), especially for sparse matrices [28]."
        },
        {
            "heading": "3.1.4 Alternating Nonnegative Least Squares",
            "text": "Alternating nonnegative least squares (ANLS) is a class of methods where the subproblems in W and H are solved exactly, that is, the update for W is given by\nW \u2190 argminW\u22650 ||X \u2212WH||F .\nMany methods can be used to solve the NNLS argminW\u22650 ||X \u2212WH||F , and dedicated active-set methods have shown to perform very well in practice6; see [72, 73, 75]. Other methods are based for example on projected gradients [88], Quasi-Newton [26], or fast gradient methods [60]. ANLS is guaranteed to converge to a stationary point [59]. Since each iteration of ANLS computes an optimal solution of the NNLS subproblem, each iteration of ANLS decreases the error the most among NMF algorithms following the framework described in Algorithm CD. However, each iteration is computationally more expensive, and more difficult to implement.\nNote that, because usually the initial guess WH is a poor approximation of X, it does not make much sense to solve the NNLS subproblems exactly at the first steps of Algorithm CD, and therefore it might be profitable to use ANLS rather in a refinement step of a cheaper NMF algorithm (such as the MU or ALS).\n5For example, in Matlab: W = max(0,(X*H\u2019)/(H*H\u2019)). 6In particular, the Matlab function lsqnonneg implements an active-set method from [78]."
        },
        {
            "heading": "3.1.5 Hierarchical Alternating Least Squares",
            "text": "Hierarchical alternating least squares (HALS) solves the NNLS subproblem using an exact coordinate descent method, updating one column of W at a time. The optimal solutions of the corresponding subproblems can be written in closed form. In fact, the entries of a column of W do not interact \u2013see Equation (3)\u2013 hence the corresponding problem can be decoupled into p quadratic problems with a single nonnegative variable. HALS updates W as follows. For \u2113 = 1, 2, . . . , r:\nW (:, \u2113)\u2190 argminW (:,\u2113)\u22650\n\u2225 \u2225 \u2225X \u2212 \u2211\nk 6=\u2113 W (:, k)H(k, :) \u2212W (:, \u2113)H(\u2113, :)\n\u2225 \u2225 \u2225 F\n\u2190 max\n(\n0, XH(\u2113, :)T \u2212\n\u2211 k 6=\u2113W (:, k) ( H(k, :)H(\u2113, :)T )\n||H(\u2113, :)||22\n)\n.\nHALS has been rediscovered several times, originally in [27] (see also [25]), then as the rank-one residue iteration (RRI) in [63], as FastNMF in [84], and also in [89]. Actually, HALS was first described in Rasmus Bro\u2019s thesis [14, pp.161-170] (although it was not investigated thoroughly):\n. . . to solve for a column vector w of W it is only necessary to solve the unconstrained problem and subsequently set negative values to zero. Though the algorithm for imposing non-negativity is thus simple and may be advantageous in some situations, it is not pursued here. Since it optimizes a smaller subset of parameters than the other approaches it may be unstable in difficult situations.\nHALS was observed to converge much faster than the MU (see [47, p.131] for a theoretical explanation, and Section 3.1.6 for a comparison) while having almost the same computational cost; see [52] for a detailed account of the flops needed per iteration. Moreover, HALS is, under some mild assumptions, guaranteed to converge to a stationary point; see the discussion in [52]. Note that one should be particularly careful when initializing HALS otherwise the algorithm could set some columns of W to zero initially (e.g., if WH is badly scaled with WH \u226b X) hence it is recommended to initially scale (W,H) according to (7); see the discussion in [47, p.72].\nIn the original HALS, each column of W is updated only once before updating H. However, as for the MU, it can be sped up by updating W several times before updating H [52], or selecting the entries of W to update following a Gauss-Southwell-type rule [65]. HALS can also be generalized to other cost functions using Taylor expansion [83]."
        },
        {
            "heading": "3.1.6 Comparison",
            "text": "Figure 3 displays the evolution of the objective function of (2) for the algorithms described in the previous section: on the left, the dense CBCL data set (see also Figure 1), and, on the right, the sparse Classic document data set. As anticipated in the description of the different algorithms in the previous sections, we observe that:\n\u2022 The MU converge rather slowly.\n\u2022 ALS oscillates for the dense matrix (CBCL data set) and performs quite poorly while, for the sparse matrix (Classic data set), it converges initially very fast but then stabilizes and cannot compute a solution with small objective function value.\n\u2022 ANLS performs rather well for the dense matrix and is the second best after HALS. However, it performs rather poorly for the sparse matrix.\n\u2022 HALS performs the best as it generates the best solutions within the allotted time.\nFor other comparisons of NMF algorithms and more numerical experiments, we refer the reader to the book [28], the theses [63, 47], the survey [6], and the references therein.\nFurther research on NMF includes the design of more efficient algorithms, in particular for regularized problems; see, e.g., [98] for a recent example of imposing sparsity in a more robust and stable way. We conclude this section with some comments about stopping criteria and initializations of NMF algorithms."
        },
        {
            "heading": "3.1.7 Stopping Criterion",
            "text": "There are several approaches for the stopping criterion of NMF algorithms, as in usual non-linear optimization schemes, e.g., based on the evolution of the objective function, on the optimality conditions (4), or on the difference between consecutive iterates. These criteria are typically combined with either a maximum number of iterations or a time limit to ensure termination; see, e.g., the discussion in [47]. In this section, we would like to point out an issue which is sometimes overlooked in the literature when using the optimality conditions to assess the convergence of NMF algorithms. A criterion based on the optimality conditions is for example C(W,H) = CW (W ) + CH(H) where\nCW (W ) = ||min(W, 0)||F \ufe38 \ufe37\ufe37 \ufe38\nW\u22650\n+ ||min(\u2207WF, 0)||F \ufe38 \ufe37\ufe37 \ufe38\n\u2207WF\u22650\n+ ||W \u25e6 \u2207WF ||F \ufe38 \ufe37\ufe37 \ufe38\nW\u25e6\u2207WF=0\n, (8)\nand similarly CH(H) for H, so that C(W,H) = 0 \u21d0\u21d2 (W,H) is a stationary point of (2). There are several problems to using C(W,H) (and other similar variants) as a stopping criterion and for comparing the convergence of different algorithms:\n\u2022 It is sensitive to scaling. For \u03b1 > 0 and \u03b1 6= 1, we will have in general that\nCW (W ) + CH(H) = C(W,H) 6= C(\u03b1W,\u03b1 \u22121H).\nsince the first two terms in (8) are sensitive to scaling. For example, if one solves the subproblem in W exactly and obtains CW (W ) = 0 (this will be the case for ANLS; see Section 3.1.4), then \u2207HF can be made arbitrarily small by multiplying W by a small constant and dividing H by the same constant (while, if H \u2265 0, it will not influence the first term which is equal to zero). This issue can be handled with proper normalization, e.g., imposing ||W (:, k)||2 = ||H(k, :)||2 for all k; see [63].\n\u2022 The value of C(W,H) after the update of W can be very different from the value after an update of H (in particular, if the scaling is bad or if |m\u2212n| \u226b 0). Therefore, one should be very careful when using this type of criterion to compare ANLS-type methods with other algorithms such as the MU or HALS as the evolution of C(W,H) can be misleading (in fact, an algorithm that monotonically decreases the objective function, such as the MU or HALS, is not guaranteed to monotonically decrease C(W,H).) A potential fix would be to scale the columns of W and the rows of H so that CW (W ) after the update of H and CH(H) after the update of W have the same order of magnitude."
        },
        {
            "heading": "3.1.8 Initialization",
            "text": "A simple way to initialize W and H is to generate them randomly (e.g., generating all entries uniformly at random in the interval [0,1]). Several more sophisticated initialization strategies have been developed in order to have better initial estimates in the hope to (i) obtain a good factorization with fewer iterations, and (ii) converge to a better stationary point. However, most initialization strategies come with no theoretical guarantee (e.g., a bound on the distance of the initial point to optimality) which can be explained in part by the complexity of the problem (in fact, NMF is NP-hard in general \u2013see the introduction of this section). This could be an interesting direction for further research. We list some initialization strategies here, they are based on\n\u2022 Clustering techniques. Use the centroids computed with some clustering method, e.g., with kmeans or spherical k-means, to initialize the columns of W , and initialize H as a proper scaling of the cluster indicator matrix (that is, Hkj 6= 0 \u21d0\u21d2 X(:, j) belongs to the kth cluster) [107, 109]; see also [18] and the references therein for some recent results.\n\u2022 The SVD. Let \u2211r\nk=1 ukv T k be the best rank-r approximation of X (which can be computed,\ne.g., using the SVD; see Introduction). Each rank-one factor ukv T k might contain positive and negative entries (except for the first one, by the Perron-Frobenius theorem7). However, denoting [x]+ = max(x, 0), we have\nukv T k = [uk]+[v T k ]+ + [\u2212uk]+[\u2212v T k ]+ \u2212 [\u2212uk]+[v T k ]+ \u2212 [uk]+[\u2212v T k ]+,\n7Actually, the first factor could contain negative entries if the input matrix is reducible and its first two singular values are equal to one another; see, e.g., [47, p.16].\nand the first two rank-one factors in this decomposition are nonnegative. Boutsidis et al. [11] proposed to replace each rank-one factor in\n\u2211r k=1 ukv T k with either [uk]+[v T k ]+ or [\u2212uk]+[\u2212v T k ]+,\nselecting the one with larger norm and scaling it properly.\n\u2022 Column subset selection. It is possible to initialize the columns of W using data points, that is, initialize W = X(:,K) for some set K with cardinality r; see [21, 112] and Section 3.2.\nIn practice, one may use several initializations, and keep the best solution obtained; see, e.g., the discussion in [28]."
        },
        {
            "heading": "3.2 Near-Separable NMF",
            "text": "A matrix X is r-separable if there exists an index set K of cardinality r such that\nX = X(:,K)H for some H \u2265 0.\nIn other words, there exists a subset of r columns of X which generates a convex cone containing all columns of X. Hence, given a separable matrix, the goal of separable NMF is to identify the subset of columns K that allows to reconstruct all columns of X (in fact, given X(:,K), H can be computed by solving a convex optimization program; see Section 3.1). The separability assumption makes sense in several applications: for example,\n\u2022 In text mining (see Section 2.2), separability of the word-by-document matrix requires that for each topic, there exists a document only on that topic. Note that we can also assume separability of the transpose of X (that is, of the document-by-word matrix), i.e., for each topic there exists one word used only by that topic (referred to as an \u2018anchor\u2019 word). In fact, the latter is considered a more reasonable assumption in practice; see [77, 3, 39] and also the thesis [46] for more details.\n\u2022 In hyperspectral unmixing (see Section 2.3), separability of the wavelength-by-pixel matrix requires that for each endmember there exists a pixel containing only that endmember. This is the so-called pure-pixel assumption, and makes sense for relatively high spatial resolution hyperspectral images; see [8, 90] and the references therein.\nSeparability has also been used successfully in blind source separation [95, 22], video summarization and image classification [40], and foreground-background separation in computer vision [76]. Note that for facial feature extraction described in Section 2.1, separability does not make sense since we cannot expect features to be present in the data set.\nIt is important to points out that separable NMF is closely related to several problems, including\n\u2022 Column subset selection which is a long-standing problem in numerical linear algebra (see [12] and the references therein).\n\u2022 Pure-pixel search in hyperspectral unmixing which has been addressed long before NMF was introduced; see [90] for a historical note.\n\u2022 The problem of identifying a few important data points in a data set (see [40] and the references therein).\n\u2022 Convex NMF [36], and the CUR decomposition [91].\nTherefore, it is difficult to pinpoint the roots of separable NMF and a comprehensive overview of all methods related to separable NMF is out of the scope of this paper. However, to the best of our knowledge, it is only very recently that provably efficient algorithms for separable NMF have been proposed. This new direction of research was launched by a paper by Arora et al. [4] which shows that NMF of separable matrices can be computed efficiently (that is, in polynomial time), even in the presence of noise (the error can be bounded in terms of the noise level; see below). We focus in this section on these provably efficient algorithms for separable NMF.\nIn the noiseless case, separable NMF reduces to identifying the extreme rays of the cone spanned by the columns of X. If the columns of the input matrix X are normalized so that their entries sum to one, that is, X(:, j) \u2190 ||X(:, j)||\u221211 X(:, j) for all j (and discarding the zero columns of X), then the problem reduces to identifying the vertices of the convex hull of the columns of X. In fact, since the entries of each column of X sum to one and X = X(:,K)H , the entries of each column of H must also sum to one: as X and H are nonnegative, we have for all j\n1 = ||X(:, j)||1 = ||X(:,K)H(:, j)||1\n= \u2211\nk\n||X(:,K(k))||1H(k, j) = \u2211\nk\nH(k, j) = ||H(:, j)||1.\nTherefore, the columns of X are convex combinations (that is, linear combinations with nonnegative weights summing to one) of the columns of X(:,K).\nIn the presence of noise, the problem is referred to as near-separable NMF, and can be formulated as follows\n(Near-Separable NMF) Given a noisy r-separable matrix X\u0303 = X+N with X = W [Ir,H \u2032]\u03a0 where W and H \u2032 are nonnegative matrices, \u03a0 is a permutation matrix and N is the noise, find a set K of r indices such that X\u0303(:,K) \u2248W .\nIn the following, we describe some algorithms for near-separable NMF; they are classified in two categories: algorithms based on self-dictionary and sparse regression (Section 3.2.1) and geometric algorithms (Section 3.2.2)."
        },
        {
            "heading": "3.2.1 Self-Dictionary and Sparse Regression Framework",
            "text": "In the noiseless case, separable NMF can be formulated as follows\nmin Y \u2208Rn\u00d7n ||Y ||row,0 such that X = XY and Y \u2265 0, (9)\nwhere ||Y ||row,0 is the number of non-zero rows of Y . In fact, if all the entries of a row of Y are equal to zero, then the corresponding column of X is not needed to reconstruct the other columns of X. Therefore, minimizing the number of rows of Y different from zero is equivalent to minimizing the number of columns of X used to reconstruct all the other columns of X, which solves the separable NMF problem. In particular, given an optimal solution Y \u2217 of (9) and denoting K = {i|Y \u2217(i, :) 6= 0}, we have X = WY \u2217(K, :) where W = X(:,K).\nIn the presence of noise, the constraints X = XY are usually reformulated as ||X \u2212XY || \u2264 \u03b4 for some \u03b4 > 0 or put as a penalty \u03bb||X \u2212 XY || in the objective function for some penalty parameter \u03bb > 0. In [40, 41], ||Y ||row,0 is replaced using \u21131-norm type relaxation:\n||Y ||q,1 = \u2211\nj\n||Y (i, :)||q ,\nwhere q > 1 so that ||Y ||q,1 is convex and (9) becomes a convex optimization problem. Note that this idea is closely related to compressive sensing where \u21131-norm relaxation is used to find the sparsest solution to an underdetermined linear system. This relaxation is exact given that the matrix involved in the linear system satisfies some incoherence properties. In separable NMF, the columns and rows of matrix X are usually highly correlated hence it is not clear how to extend the results from the compressive sensing literature to this separable NMF model; see, e.g., the discussion in [90].\nA potential problem in using convex relaxations of (9) is that it cannot distinguish duplicates of the columns of W . In fact, if a column of W is present twice in the data matrix X, the corresponding rows of Y can both be non-zero hence both columns of W can potentially be extracted (this is because of the convexity and the symmetry of the objective function) \u2013in [40], k-means is used as a pre-processing in order to remove duplicates. Moreover, although this model was successfully used to solve real-world problems, no robustness results were developed so far so it is not clear how this model behaves in the presence of noise (only asymptotic results were proved, that is, when the noise level goes to zero and when no duplicates are present [40]).\nA rather different approach to enforce row sparsity was suggested in [9], and later improved in [54]. Row sparsity of Y is enforced by (i) minimizing a weighted sum of the diagonal entries of Y hence enforcing diag(Y ) to be sparse (in fact, this is nothing but a weighted \u21131 norm since Y is nonnegative), and (ii) imposing all entries in a row of Y to be smaller than the corresponding diagonal entry of Y (we assume here that the columns of X are normalized). The second condition implies that if diag(Y ) is sparse then Y is row sparse. The corresponding near-separable NMF model is:\nmin Y \u2208Rn\u00d7n\npT diag(Y ) such that ||X \u2212XY ||1 \u2264 \u03b4 and 0 \u2264 Yij \u2264 Yii \u2264 1, (10)\nfor some positive vector p \u2208 Rn with distinct entries (this breaks the symmetry so that the model can distinguish duplicates). This model has been shown to be robust: defining the parameter8 \u03b1 as\n\u03b1(W ) = min 1\u2264j\u2264r min x\u2208Rr\u22121\n+\n||W (:, j) \u2212W (:,Jj)x||1, where Jj = {1, 2, . . . , r}\\{j},\nand for a near-separable matrix X\u0303 = W [Ir,H \u2032]\u03a0 + N (see above) with \u01eb = maxj ||N(:, j)||1 \u2264 O ( \u03b12(W )\nr\n)\n, the model (10) can be used to identify the columns of W with \u21131 error proportional to\nO (\nr\u01eb \u03b1(W )\n) , that is, the identified index set K satisfies maxj mink\u2208K ||X\u0303(:, k) \u2212W (:, j)||1 \u2264 O ( r\u01eb \u03b1(W ) ) ;\nsee [54, Th.7] for more details.\nFinally, a drawback of the approaches based on self-dictionary and sparse regression is that they are computationally expensive as they require to tackle optimization problems with n2 variables."
        },
        {
            "heading": "3.2.2 Geometric Algorithms",
            "text": "Another class of near-separable algorithms are based on geometric insights and in particular on the fact that the columns of W are the vertices of the convex hull of the normalized columns of X. The first geometric algorithms can be found in the remote sensing literature (they are referred to as endmember extraction algorithms or pure-pixel identification algorithms), see [90] for a historical note; and [8] for a comprehensive survey. Because of the large body of literature, we do not aim at surveying\n8The larger the parameter \u03b1 is, the less sensitive the data to noise. For example, it can be easily checked that \u01eb = maxj ||N(:, j)||1 <\n\u03b1 2 is a necessary condition to being able to distinguish the columns of W [49].\nall algorithms but rather focus on a single algorithm which is particularly simple while being rather effective in practice: the successive projection algorithm (SPA). Moreover, the ideas behind SPA are at the heart of many geometric-based near-separable NMF algorithms (see below).\nSPA looks for the vertices of the convex hull of the columns of the input data matrix X and works as follows: at each step, it selects the column of X with maximum \u21132 norm and then updates X by projecting each column onto the orthogonal complement of the extracted column; see Algorithm SPA. SPA is extremely fast as it can be implemented in 2pnr + O(pr2) operations, using the formula ||(I \u2212 uuT )v||22 = ||v|| 2 2 \u2212 (u\nT v)2, for any u, v \u2208 Rm with ||u||2 = 1 [55]. Moreover, if r is unknown, it can be estimated using the norm of the residual R.\nAlgorithm SPA Successive Projection Algorithm [2] Input: Near-separable matrix X\u0303 = W [Ir,H \u2032]\u03a0+N where W is full rank, H \u2032 \u2265 0, the entries of each\ncolumn of H \u2032 sum to at most one, \u03a0 is a permutation and N is the noise, and the number r of columns of W .\nOutput: Set of r indices K such that X\u0303(:,K) \u2248W (up to permutation).\n1: Let R = X\u0303 , K = {}. 2: for k = 1 : r do 3: p = argmaxj ||R:j||2. 4: R = ( I \u2212 R:pR T :p\n||R:p||22\n)\nR.\n5: K = K \u222a {p}."
        },
        {
            "heading": "6: end for",
            "text": "Let us prove the correctness of SPA in the noiseless case using induction, and assuming W is full rank (this is a necessary and sufficient condition) and assuming the entries of each column of H \u2032 sum to at most one (this can be achieved through normalization; see above). At the first step, SPA identifies a column of W because the \u21132 norm can only be maximized at a vertex of the convex hull of a set of points; see Figure 4 for an illustration. In fact, for all 1 \u2264 j \u2264 n,\n||X(:, j)||2 = ||WH(:, j)||2 \u2264\nr\u2211\nk=1\nH(k, j)||W (:, k)||2 \u2264 max 1\u2264k\u2264r ||W (:, k)||2.\nThe first inequality follows from the triangle inequality, and the second since H(k, j) \u2265 0 and \u2211\nk H(k, j) \u2264 1. Moreover, by strong convexity of the \u21132 norm and the full rank assumption on W , the first inequality is strict unless H(:, k) is a column of the identity matrix, that is, unless X(:, j) = W (:, k) for some k. For the induction step, assume without loss of generality that SPA has extracted the first \u2113 columns of W , and let W\u2113 = W (:, 1:\u2113) and P \u22a5 W\u2113 be the projection onto the orthogonal complement of the columns of W\u2113 so that P \u22a5 W\u2113 W\u2113 = 0. We have, for all 1 \u2264 j \u2264 n,\n||P\u22a5W\u2113X(:, j)||2 = ||P \u22a5 W\u2113 WH(:, j)||2 \u2264\nr\u2211\nk=1\nH(k, j)||P\u22a5W\u2113W (:, k)||2 \u2264 max\u2113+1\u2264k\u2264r ||P\u22a5W\u2113W (:, k)||2,\nwhere P\u22a5W\u2113W (:, k) 6= 0 for \u2113 + 1 \u2264 k \u2264 r since W is full rank. Hence, using the same reasoning as above, SPA will identify a column of W not extracted yet, which concludes the proof.\nMoreover, SPA is robust to noise: given a near-separable matrix X\u0303 = W [Ir,H \u2032]\u03a0 + N with W\nfull rank, H \u2032 nonnegative with ||H \u2032(:, j)||1 \u2264 1 \u2200j, and \u01eb = maxj ||N(:, j)||2 \u2264 O (\n\u03c3min(W )\u221a r\u03ba2(W )\n)\n, SPA\nidentifies the columns of W up to \u21132 error proportional to O ( \u01eb \u03ba2(W ) ) , where \u03ba(W ) = \u03c3max(W )\n\u03c3min(W ) [55,\nTh.3]. These bounds can be improved using post-processing (see below) which reduces the error to O (\u01eb \u03ba(W )) [3], or preconditioning which significantly increases the upper bound on the noise level, to \u01eb \u2264 O ( \u03c3min(W )\nr \u221a r\n)\n, and reduces the error to O (\u01eb \u03ba(W )) [56].\nIt is interesting to note that SPA has been developed and used for rather different purposes in various fields:\n\u2022 Numerical linear algebra. SPA is closely related to the modified Gram-Schmidt algorithm with column pivoting, used for example to solve linear least squares problems [15].\n\u2022 Chemistry (and in particular spectroscopy). SPA is used for variable selection in spectroscopic multicomponent analysis; in fact, the name SPA comes from [2].\n\u2022 Hyperspectral imaging. SPA is closely related to several endmember extraction algorithms; in particular N-FINDR [108] and its variants, the automatic target generation process (ATGP) [99], and the successive volume maximization algorithm (SVMAX) [21]; see the discussion in [90] for more details. The motivation behind all these approaches is to identify an index set K that maximizes the volume of the convex hull of the columns of X(:,K). Note that most endmember extraction algorithms use an LDR (such as the SVD) as a pre-processing step for noise filtering, and SPA can be combined with an LDR to improve performance.\n\u2022 Text mining. Arora et al. [3] proposed FastAnchorWords whose differences with SPA are that (i) the projection is made onto the affine hull of the columns extracted so far (instead of the linear span), and (ii) the index set extracted is refined using the following post-processing step: let K be the extracted index set by SPA, for each k \u2208 K:\n\u2013 Compute the projection R of X into the orthogonal complement of X(:,K\\{k}).\n\u2013 Replace k with the index corresponding to the column of R with maximum \u21132 norm.\n\u2022 Theoretical computer science. SPA was proved to be a good heuristic to identify a subset of columns of a given matrix whose convex hull has maximum volume [19, 20] (in the sense that no polynomial-time algorithm can achieve better performance up to some logarithmic factors).\n\u2022 Sparse regression with self-dictionary. SPA is closely related to orthogonal matching pursuit and can be interpreted as a greedy method to solve the sparse regression problem with selfdictionary (9); see [44] and the references therein.\nMoreover, there exist many geometric algorithms which are variants of SPA, e.g., vertex component analysis (VCA) using linear functions instead of the \u21132-norm [96], \u2113p-norm based pure pixel algorithm (TRI-P) using p-norms [1], FastSepNMF using strongly convex functions [55], the successive nonnegative projection algorithm (SNPA) [50] and the fast conical hull algorithm (XRAY) [77] using nonnegativity constraints for the projection step.\nFurther research on near-separable NMF includes the design of faster and/or provably more robust algorithms. In particular, there does not seem to exist an algorithm guaranteed to be robust for any matrix W such that \u03b1(W ) > 0 and running in O(n) operations."
        },
        {
            "heading": "4 Connections with Problems in Mathematics and Computer Sci-",
            "text": "ence\nIn this section, we briefly mention several connections of NMF with problems outside data mining and machine learning. The minimum r such that an exact NMF of a nonnegative matrix X exists is the nonnegative rank of X, denoted rank+(X). More precisely, given X \u2208 R p\u00d7n + , rank+(X) is the minimum r such that there exist W \u2208 Rp\u00d7r+ and H \u2208 R r\u00d7n + with X = WH. The nonnegative rank has tight connections with several problems in mathematics and computer science:\n\u2022 Graph Theory. Let G(X) = (V1 \u222a V2, E) be the bipartite graph induced by X (that is, (i, j) \u2208 E \u21d0\u21d2 Xij 6= 0). The minimum biclique cover bc(G(X)) of G(X) is the minimum number of complete bipartite subgraphs needed to cover G(X). It can be checked easily that for any (W,H) \u2265 0 such that X = WH =\n\u2211r k=1W:kHk:, we have\nG(X) = \u222ark=1G(W:kHk:),\nwhere G(W:kHk:) are complete bipartite subgraphs hence bc(G(W:kHk:)) = 1 \u2200k. Therefore,\nbc(G(X)) \u2264 rank+(X).\nThis lower bound on the nonnegative rank is referred to as the rectangle covering bound [43].\n\u2022 Extended Formulations. Given a polytope P , an extended formulation (or lift, or extension) is a higher dimensional polyhedron Q and a linear projection \u03c0 such that \u03c0(Q) = P . When the polytope P has exponentially many facets, finding extended formulations of polynomial size is of great importance since it allows to solve linear programs (LP) over P in polynomial time. It turns out that the minimum number of facets of an extended formulation Q of a polytope P is equal to the nonnegative rank of its slack matrix [111], defined as X(i, j) = aTi vj \u2212 bi where vj is the jth vertex of P and {x \u2208 Rn | aTi x\u2212 bi \u2265 0} its ith facet with ai \u2208 R\nn and bi \u2208 R, that is, X is a facet-by-vertex matrix and X(i, j) is the slack of the jth vertex with respect to ith facet; see the surveys [30, 69] and the references therein. These ideas can be generalized to approximate extended formulations, directly related to approximate factorizations (hence NMF) [13, 58].\n\u2022 Probability. Let X(k) \u2208 {1, . . . , p} and Y (k) \u2208 {1, . . . , n} be two independent variables for each 1 \u2264 k \u2264 r, and P (k) be the joint distribution with\nP (k) ij = P\n( X(k) = i, Y (k) = j ) = P ( X(k) = i ) P ( Y (k) = j ) .\nEach distribution P (k) corresponds to a nonnegative rank-one matrix. Let us define the mixture P of these k independent distributions as follows:\n\u2013 Choose the distribution P (k) with probability \u03b1k, where \u2211r k=1 \u03b1k = 1. \u2013 Draw X and Y from the distribution P (k).\nWe have that P = \u2211r\nk=1 \u03b1kP (k) is the sum of r rank-one nonnegative matrices. In practice, only\nP is observed and computing its nonnegative rank and a corresponding factorization amounts to explaining the distribution P with as few independent variables as possible; see [17] and the references therein.\n\u2022 Communication Complexity. In its simplest variant, communication complexity addresses the following problem: Alice and Bob have to compute the following function\nf : {0, 1}m \u00d7 {0, 1}n 7\u2192 {0, 1} : (x, y) 7\u2192 f(x, y).\nAlice only knows x and Bob y, and the aim is to minimize the number of bits exchanged between Alice and Bob to compute f exactly. Nondeterministic communication complexity (NCC) is a variant where Bob and Alice first receive a message before starting their communication; see [81, Ch.3] and the references therein for more details. The communication matrix X \u2208 {0, 1}2 n\u00d72m is equal to the function f for all possible combinations of inputs. Yannakakis [111] showed that the NCC for computing f is upper bounded by the logarithm of the nonnegative rank of the communication matrix (this result is closely related to the rectangle covering bound described above: in fact, \u2308log(bc(G(X))\u2309 equals to the NCC of f).\n\u2022 Computational Geometry. Computing the nonnegative rank is closely related to the problem of finding a polytope with minimum number of vertices nested between two given polytopes [53]. This is a well-known problem is computational geometry, referred to as the nested polytopes problem; see [31] and the references therein."
        },
        {
            "heading": "5 Conclusion",
            "text": "NMF is an easily interpretable linear dimensionality reduction technique for nonnegative data. It is a rather versatile technique with many applications, and brings together a broad range of researchers. In the context of \u2018Big Data\u2019 science, which becomes an increasingly important topic, we believe NMF has a bright future; see Figure 5 for an illustration of the number of publications related to NMF since the publication of the Lee and Seung paper [79]."
        },
        {
            "heading": "Acknowledgment",
            "text": "The author would like to thank Rafal Zdunek, Wing-Kin Ma, Marc Pirlot and the Editors of the book \u2018Regularization, Optimization, Kernels, and Support Vector Machines\u2019 for insightful comments which helped improve the paper.\n1998 2000 2002 2004 2006 2008 2010 2012 2014\n0\n500\n1000\n1500\n2000\n2500\nGoogle Scholar Scopus"
        }
    ],
    "title": "The Why and How of Nonnegative Matrix Factorization",
    "year": 2014
}