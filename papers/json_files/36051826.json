{
    "abstractText": "Existing cache and main memory compression techniques compress data in small fixed-size blocks, typically cache lines. Moreover, they use simple compression algorithms that focus on exploiting redundancy within a block. These techniques work well for scientific programs that are dominated by arrays. However, they are ineffective on object-based programs because objects do not fall neatly into fixed-size blocks and have a more irregular layout. We present the first compressed memory hierarchy designed for object-based applications. We observe that (i) objects, not cache lines, are the natural unit of compression for these programs, as they traverse and operate on object pointers; and (ii) though redundancy within each object is limited, redundancy across objects of the same type is plentiful. We exploit these insights through Zippads, an object-based compressed memory hierarchy, and COCO, a cross-object-compression algorithm. Building on a recent object-based memory hierarchy, Zippads transparently compresses variable-sized objects and stores them compactly. As a result, Zippads consistently outperforms a state-of-theart compressed memory hierarchy: on a mix of arrayand object-dominated workloads, Zippads achieves 1.63\u00d7 higher compression ratio and improves performance by 17%. CCSConcepts \u00b7Computer systems organization\u2192Processors and memory architectures.",
    "authors": [
        {
            "affiliations": [],
            "name": "Po-An Tsai"
        },
        {
            "affiliations": [],
            "name": "Daniel Sanchez"
        }
    ],
    "id": "SP:62a619e0ebcf5b81a1f053b3cd5d34c04ad6d224",
    "references": [
        {
            "authors": [
                "Alaa R Alameldeen",
                "David A Wood"
            ],
            "title": "Adaptive cache compression for high-performance processors",
            "venue": "In Proc. ISCA-31",
            "year": 2004
        },
        {
            "authors": [
                "Alaa R Alameldeen",
                "David AWood"
            ],
            "title": "Frequent pattern compression: A significance-based compression scheme for L2 caches",
            "venue": "Technical Report 1500",
            "year": 2004
        },
        {
            "authors": [
                "Bowen Alpern",
                "Steve Augart",
                "Stephen M Blackburn",
                "Maria Butrico",
                "AnthonyCocchi",
                "Perry Cheng",
                "Julian Dolby",
                "Stephen Fink",
                "DavidGrove",
                "Michael Hind"
            ],
            "title": "The Jikes research virtual machine project: Building an open-source research community",
            "venue": "IBM Systems Journal 44,",
            "year": 2005
        },
        {
            "authors": [
                "Angelos Arelakis",
                "Fredrik Dahlgren",
                "Per Stenstrom"
            ],
            "title": "HyComp: A hybrid cache compression method for selection of data-type-specific compression methods",
            "venue": "In Proc. MICRO-48",
            "year": 2015
        },
        {
            "authors": [
                "Angelos Arelakis",
                "Per Stenstrom"
            ],
            "title": "SC2: A statistical compression cache scheme",
            "venue": "In Proc. ISCA-41",
            "year": 2014
        },
        {
            "authors": [
                "Nathan Beckmann",
                "Daniel Sanchez"
            ],
            "title": "Bridging Theory and Practice in Cache Replacement",
            "venue": "Technical Report MIT-CSAIL-TR-2015- 034. Massachusetts Institute of Technology",
            "year": 2015
        },
        {
            "authors": [
                "Nathan Beckmann",
                "Daniel Sanchez"
            ],
            "title": "Maximizing Cache Performance Under Uncertainty",
            "venue": "In Proc. HPCA-23",
            "year": 2017
        },
        {
            "authors": [
                "Stephen M Blackburn",
                "Robin Garner",
                "Chris Hoffmann",
                "Asjad M Khang",
                "Kathryn S McKinley",
                "Rotem Bentzur",
                "Amer Diwan",
                "Daniel Feinberg",
                "Daniel Frampton",
                "Samuel Z Guyer",
                "Martin Hirzel",
                "Antony Hosking",
                "Maria Jump",
                "Han Lee",
                "J Eliot B Moss",
                "Aashish Phansalkar",
                "Darko Stefanovic",
                "Thomas VanDrunen",
                "Daniel von Dincklage",
                "Ben Wiedermann"
            ],
            "title": "The DaCapo benchmarks: Java benchmarking development and analysis",
            "venue": "In Proc. OOPSLA",
            "year": 2006
        },
        {
            "authors": [
                "Stephen M Blackburn",
                "Kathryn S McKinley"
            ],
            "title": "Immix: A markregion garbage collector with space efficiency, fast collection, and mutator performance",
            "venue": "In Proc. PLDI",
            "year": 2008
        },
        {
            "authors": [
                "Hans-J Boehm"
            ],
            "title": "An artificial garbage collection benchmark. http://hboehm.info/gc/gc_bench.html, archived at https://perma.cc/ Y4BY-7RN4",
            "year": 2002
        },
        {
            "authors": [
                "Guangyu Chen",
                "Mahmut Kandemir",
                "Mary J Irwin"
            ],
            "title": "Exploiting frequent field values in Java objects for reducing heap memory requirements. In VEE",
            "year": 2005
        },
        {
            "authors": [
                "Guangyu Chen",
                "M Kandemir",
                "Narayanan Vijaykrishnan",
                "Mary Jane Irwin",
                "Bernd Mathiske",
                "Mario Wolczko"
            ],
            "title": "Heap compression for memory-constrained Java environments",
            "venue": "In Proc. OOPSLA",
            "year": 2003
        },
        {
            "authors": [
                "Xi Chen",
                "Lei Yang",
                "Robert P Dick",
                "Li Shang",
                "Haris Lekatsas"
            ],
            "title": "C-Pack: A high-performance microprocessor cache compression algorithm. IEEE transactions on very large scale integration (VLSI) systems",
            "year": 2010
        },
        {
            "authors": [
                "David Cheriton",
                "Amin Firoozshahian",
                "Alex Solomatnikov",
                "John P Stevenson",
                "Omid Azizi"
            ],
            "title": "HICAMP: Architectural support for efficient concurrency-safe shared structured data access",
            "venue": "In Proc. ASPLOS-XVII",
            "year": 2012
        },
        {
            "authors": [
                "Esha Choukse",
                "Mattan Erez",
                "Alaa Alameldeen"
            ],
            "title": "Compresso: Pragmatic main memory compression",
            "venue": "In Proc. MICRO-51",
            "year": 2018
        },
        {
            "authors": [
                "Esha Choukse",
                "Mattan Erez",
                "Alaa Alameldeen"
            ],
            "title": "Compress- Points: An evaluation methodology for compressed memory systems",
            "venue": "Computer Architecture Letters 17,",
            "year": 2018
        },
        {
            "authors": [
                "Magnus Ekman",
                "Per Stenstrom"
            ],
            "title": "A robust main-memory compression scheme",
            "venue": "In Proc. ISCA-32",
            "year": 2005
        },
        {
            "authors": [
                "Jason Evans"
            ],
            "title": "jemalloc http://jemalloc.net",
            "year": 2005
        },
        {
            "authors": [
                "Yee Ling Gan"
            ],
            "title": "Redesigning the memory hierarchy for memory-safe programming languages",
            "venue": "Master\u2019s thesis. Massachusetts Institute of Technology",
            "year": 2018
        },
        {
            "authors": [
                "Sanjay Ghemawat",
                "Paul Menage"
            ],
            "title": "TCMalloc: Thread-Caching Malloc http://goog-perftools.sourceforge.net/doc/tcmalloc.html",
            "year": 2005
        },
        {
            "authors": [
                "Erik G Hallnor",
                "Steven K Reinhardt"
            ],
            "title": "A unified compressed memory hierarchy",
            "venue": "In Proc. HPCA-11",
            "year": 2005
        },
        {
            "authors": [
                "Jungrae Kim",
                "Michael Sullivan",
                "Esha Choukse",
                "Mattan Erez"
            ],
            "title": "Bit-plane compression: Transforming data for better compression in many-core architectures",
            "venue": "In Proc. ISCA-43",
            "year": 2016
        },
        {
            "authors": [
                "Seikwon Kim",
                "Seonyoung Lee",
                "Taehoon Kim",
                "Jaehyuk Huh"
            ],
            "title": "Transparent dual memory compression architecture",
            "venue": "In Proc. PACT-26",
            "year": 2017
        },
        {
            "authors": [
                "Jan Kotek"
            ],
            "title": "JDBM: A simple transactional persistent engine for Java. http://jdbm.sourceforge.net",
            "year": 2012
        },
        {
            "authors": [
                "Chi-Keung Luk",
                "Robert Cohn",
                "Robert Muth",
                "Harish Patil",
                "Artur Klauser",
                "Geoff Lowney",
                "StevenWallace",
                "Vijay Janapa Reddi",
                "andKimHazelwood"
            ],
            "title": "Pin: Building customized program analysis tools with dynamic instrumentation",
            "venue": "In Proc. PLDI",
            "year": 2005
        },
        {
            "authors": [
                "Biswabandan Panda",
                "Andr\u00e9 Seznec"
            ],
            "title": "Dictionary sharing: An efficient cache compression scheme for compressed caches",
            "venue": "In Proc. MICRO-49",
            "year": 2016
        },
        {
            "authors": [
                "Gennady Pekhimenko",
                "Evgeny Bolotin",
                "Nandita Vijaykumar",
                "Onur Mutlu",
                "Todd C Mowry",
                "Stephen W Keckler"
            ],
            "title": "A case for toggle-aware compression for GPU systems",
            "venue": "In Proc. HPCA-22",
            "year": 2016
        },
        {
            "authors": [
                "Gennady Pekhimenko",
                "Tyler Huberty",
                "Rui Cai",
                "Onur Mutlu",
                "Phillip B Gibbons",
                "Michael A Kozuch",
                "Todd C Mowry"
            ],
            "title": "Exploiting compressed block size as an indicator of future reuse",
            "venue": "In Proc. HPCA-21",
            "year": 2015
        },
        {
            "authors": [
                "Gennady Pekhimenko",
                "Vivek Seshadri",
                "Yoongu Kim",
                "Hongyi Xin",
                "Onur Mutlu",
                "Phillip B Gibbons",
                "Michael A Kozuch",
                "Todd C Mowry"
            ],
            "title": "Linearly compressed pages: A low-complexity, low-latency main memory compression framework",
            "venue": "Technical Report SAFARI 2012-002",
            "year": 2012
        },
        {
            "authors": [
                "Gennady Pekhimenko",
                "Vivek Seshadri",
                "Yoongu Kim",
                "Hongyi Xin",
                "Onur Mutlu",
                "Phillip B Gibbons",
                "Michael A Kozuch",
                "Todd C Mowry"
            ],
            "title": "Linearly compressed pages: a low-complexity, low-latency main memory compression framework",
            "venue": "In Proc. MICRO-46",
            "year": 2013
        },
        {
            "authors": [
                "Gennady Pekhimenko",
                "Vivek Seshadri",
                "Onur Mutlu",
                "Phillip B Gibbons",
                "Michael A Kozuch",
                "Todd C Mowry"
            ],
            "title": "Base-delta-immediate compression: Practical data compression for on-chip caches",
            "venue": "In Proc. PACT-21",
            "year": 2012
        },
        {
            "authors": [
                "Moinuddin K. Qureshi",
                "David Thompson",
                "Yale N. Patt"
            ],
            "title": "The V-Way cache: Demand based associativity via global replacement",
            "venue": "In Proc. ISCA-32",
            "year": 2005
        },
        {
            "authors": [
                "Andrey Rodchenko",
                "Christos Kotselidis",
                "Andy Nisbet",
                "Antoniu Pop",
                "Mikel Luj\u00e1n"
            ],
            "title": "MaxSim: A simulation platform for managed applications",
            "venue": "In Proc. ISPASS",
            "year": 2017
        },
        {
            "authors": [
                "Daniel Sanchez",
                "Christos Kozyrakis"
            ],
            "title": "ZSim: Fast and accurate microarchitectural simulation of thousand-core systems",
            "venue": "In Proc. ISCA-",
            "year": 2013
        },
        {
            "authors": [
                "Somayeh Sardashti",
                "Angelos Arelakis",
                "Per Stenstr\u00f6m",
                "David A Wood"
            ],
            "title": "A primer on compression in the memory hierarchy",
            "venue": "Synthesis Lectures on Computer Architecture 10,",
            "year": 2015
        },
        {
            "authors": [
                "Somayeh Sardashti",
                "Andr\u00e9 Seznec",
                "David A Wood"
            ],
            "title": "Skewed compressed caches",
            "venue": "In Proc. MICRO-47",
            "year": 2014
        },
        {
            "authors": [
                "Somayeh Sardashti",
                "David A Wood"
            ],
            "title": "Decoupled compressed cache: Exploiting spatial locality for energy-optimized compressed caching",
            "venue": "In Proc. MICRO-46",
            "year": 2013
        },
        {
            "authors": [
                "Jennifer B Sartor",
                "Stephen M Blackburn",
                "Daniel Frampton",
                "Martin Hirzel",
                "Kathryn S McKinley"
            ],
            "title": "Z-rays: divide arrays and conquer speed and flexibility",
            "venue": "In Proc. PLDI",
            "year": 2010
        },
        {
            "authors": [
                "Jennifer B Sartor",
                "Wim Heirman",
                "Stephen M Blackburn",
                "Lieven Eeckhout",
                "Kathryn S McKinley"
            ],
            "title": "Cooperative cache scrubbing",
            "venue": "In Proc. PACT-23",
            "year": 2014
        },
        {
            "authors": [
                "Jennifer B Sartor",
                "Martin Hirzel",
                "Kathryn S McKinley"
            ],
            "title": "No bit left behind: The limits of heap data compression",
            "venue": "In Proc. ISMM",
            "year": 2008
        },
        {
            "authors": [
                "Ali Shafiee",
                "Meysam Taassori",
                "Rajeev Balasubramonian",
                "Al Davis"
            ],
            "title": "MemZip: Exploring unconventional benefits from memory compression",
            "venue": "In Proc. HPCA-20",
            "year": 2014
        },
        {
            "authors": [
                "Dimitrios Skarlatos",
                "Nam Sung Kim",
                "Josep Torrellas"
            ],
            "title": "Page- Forge: A near-memory content-aware page-merging architecture",
            "venue": "In Proc. MICRO-50",
            "year": 2017
        },
        {
            "authors": [
                "Sun Microsystems"
            ],
            "title": "Memory management in the Java HotSpot virtual machine. http://www.oracle.com/technetwork/java/javase/ memorymanagement-whitepaper-150215.pdf",
            "year": 2006
        },
        {
            "authors": [
                "Yingying Tian",
                "Samira M Khan",
                "Daniel A Jim\u00e9nez",
                "Gabriel H Loh"
            ],
            "title": "Last-level cache deduplication",
            "venue": "In Proc. ICS\u201916",
            "year": 2016
        },
        {
            "authors": [
                "R Brett Tremaine",
                "Peter A Franaszek",
                "John T Robinson",
                "Charles O Schulz",
                "T Basil Smith",
                "Michael E Wazlowski",
                "P Maurice Bland"
            ],
            "title": "IBM memory expansion technology (MXT)",
            "venue": "IBM Journal of Research and Development",
            "year": 2001
        },
        {
            "authors": [
                "Po-An Tsai",
                "Yee Ling Gan",
                "Daniel Sanchez"
            ],
            "title": "Rethinking the memory hierarchy for modern languages",
            "venue": "In Proc. MICRO-51",
            "year": 2018
        },
        {
            "authors": [
                "Stephen Tu",
                "Wenting Zheng",
                "Eddie Kohler",
                "Barbara Liskov",
                "Samuel Madden"
            ],
            "title": "Speedy transactions in multicore in-memory databases",
            "venue": "In Proc. SOSP-24",
            "year": 2013
        },
        {
            "authors": [
                "Christian Wimmer",
                "Michael Haupt",
                "Michael L Van De Vanter",
                "Mick Jordan",
                "Laurent Dayn\u00e8s",
                "Douglas Simon"
            ],
            "title": "Maxine: An approachable virtual machine for, and in",
            "venue": "Java. ACM Transactions on Architecture and Code Optimization (TACO) 9,",
            "year": 2013
        },
        {
            "authors": [
                "Clifford Wolf"
            ],
            "title": "Yosys Open Synthesis Suite. http://www.clifford. at/yosys",
            "year": 2012
        },
        {
            "authors": [
                "Yi Zhao",
                "Jin Shi",
                "Kai Zheng",
                "Haichuan Wang",
                "Haibo Lin",
                "Ling Shao"
            ],
            "title": "Allocation wall: A limiting factor of Java applications on emerging multi-core platforms",
            "venue": "In Proc. OOPSLA",
            "year": 2009
        }
    ],
    "sections": [
        {
            "text": "Existing cache and main memory compression techniques compress data in small fixed-size blocks, typically cache lines. Moreover, they use simple compression algorithms that focus on exploiting redundancy within a block. These techniques work well for scientific programs that are dominated by arrays. However, they are ineffective on object-based programs because objects do not fall neatly into fixed-size blocks and have a more irregular layout.\nWe present the first compressed memory hierarchy designed for object-based applications. We observe that (i) objects, not cache lines, are the natural unit of compression for these programs, as they traverse and operate on object pointers; and (ii) though redundancy within each object is limited, redundancy across objects of the same type is plentiful. We exploit these insights through Zippads, an object-based compressed memory hierarchy, and COCO, a cross-object-compression algorithm. Building on a recent object-based memory hierarchy, Zippads transparently compresses variable-sized objects and stores them compactly. As a result, Zippads consistently outperforms a state-of-theart compressed memory hierarchy: on a mix of array- and object-dominated workloads, Zippads achieves 1.63\u00d7 higher compression ratio and improves performance by 17%.\nCCSConcepts \u00b7Computer systems organization\u2192Processors and memory architectures.\nKeywords cache; memory; object-based; compression.\nACM Reference Format: Po-An Tsai and Daniel Sanchez. 2019. Compress Objects, Not Cache Lines: An Object-Based CompressedMemory Hierarchy. In Proceedings of 2019 Architectural Support for Programming Languages and Operating Systems (ASPLOS\u201919).ACM, New York, NY, USA, 14 pages. http://dx.doi.org/10.1145/3297858.3304006\nASPLOS\u201919, April 13\u015b17, 2019, Providence, RI, USA \u00a9 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. This is the author\u2019s version of the work. It is posted here for your personal use. Not for redistribution. The definitive Version of Record was published in Proceedings of 2019 Architectural Support for Programming Languages and Operating Systems (ASPLOS\u201919), http://dx.doi.org/10.1145/3297858.3304006."
        },
        {
            "heading": "1 Introduction",
            "text": "Compression has become an attractive technique to improve the performance and efficiency of modern memory hierarchies. Ideally, compressing data at a level of the memory hierarchy (e.g., main memory or the last-level cache) brings two key benefits. First, it increases the effective capacity of that level (e.g., reducing page faults or cache misses). Second, it reduces bandwidth demand to that level, as each access fetches a smaller amount of compressed data. Because accesses to off-chip memory or large on-chip caches are slow and expensive, the benefits of compression justify its overheads. Therefore, prior work has proposed compressed main memory [17, 25, 34, 51] and cache [1, 22, 30, 41, 42] architectures, as well as specialized compression algorithms [4, 5, 24, 35].\nUnfortunately, hardware memory hierarchy compression techniques must contend with a key challenge: supporting random, fine-grained memory accesses. Whereas classic compression techniques work best on large blocks of data, many programs access small amounts of data (words or cache lines) at a time, so compressing data in large chunks would be inefficient. The need for random accesses introduces three interrelated problems. First, it limits memory hierarchy compression techniques to use small blocks, on the order of a cache line (64\u015b128 B). Second, because the startup latency of decompressing and compressing a block cannot be amortized across large blocks, it limits these techniques to use simple compression algorithms optimized for latency rather than throughput. Third, compressed blocks have variable size, requiring an extra level of indirection to translate uncompressed addresses into compressed blocks. Depending on the implementation, this extra level of indirection either requires significant metadata (e.g., extra cache or TLB state) or causes internal fragmentation. These problems limit compression ratio and thus the overall value of the techniques.\nPrior work has addressed these issues within the context of cache hierarchies, and thus focuses on compressing cache lines. For example, compression algorithms like BDI [35] and BPC [24] achieve low latency by exploiting redundancy within words of a cache line, and compressed main-memory organizations like LCP [34] achieve low access latency at the expense of lower compression ratios by forcing most cache lines in a page to have the same compressed size. These approaches work well on array-based applications, such as scientific workloads, where most data follows a regular layout and uses homogeneous data types.\nBy contrast, existing compressed hierarchy techniques are ineffective on object-based applications, i.e., those where most data is stored in objects. These applications do not have a regular memory layout: each object has fields of different types and compressibilities; objects of different types are interleaved in memory; and objects are not aligned to the fixed-size cache lines that compression techniques workwith. For these reasons, the evaluations of these techniques show small improvements on object-heavy applications.\nWe present the first object-based compressed memory hierarchy. We leverage two key insights. First, we observe that objects, not cache lines, are the natural unit of compression for object-based programs. Objects are small, typically on the order of a cache line. And object-based applications follow a disciplined access pattern: they always access data within an object and dereference object pointers to access other objects. Therefore, compressing variable-size objects instead of fixed-size cache lines and pointing directly to compressed objects can avoid the extra level of indirection and layout issues of existing compressed main memories. Second, we observe that there is significant redundancy (i.e., commonality or value locality [40]) across objects of the same type. Exploiting this redundancy, which current algorithms do not leverage, can enable high compression ratios.\nWe present two contributions that leverage the above\ninsights to compress object-based applications effectively:\n\u00b7 Zippads is a novel compressed object-based memory hi-\nerarchy. Zippads transfers objects (rather than cache lines) across levels and transparently compresses them when appropriate. Unlike prior designs, Zippads does not add a level of translation between compressed and uncompressed addresses. Instead, Zippads directly rewrites pointers to objects as it moves objects across hierarchy levels. To achieve this, Zippads builds on Hotpads [19, 52], a recent objectbased memory hierarchy. Though they are not our focus, Zippads also works well on array-based applications. \u00b7 Cross-object-compression (COCO) is a novel compres-\nsion algorithm that exploits redundancy across objects cheaply. COCO chooses a small number of base objects, and stores only the bytes that differ from an object\u2019s base. While Zippads can use other compression algorithms (e.g., BDI), using COCO increases compression ratio substantially.\nWe evaluate these techniques in simulation and prototype COCO in RTL. Our evaluation shows that, across a mix of array-based and object-based workloads, these techniques substantially outperform a combination of a state-of-theart compressed last-level cache and compressed main memory [33]: Zippads alone increases compression ratio by 1.37\u00d7 on average and by up to 1.54\u00d7, and Zippads+COCO increases compression ratio by 1.63\u00d7 on average and by up to 2\u00d7. As a result, Zippads+COCO reduces main memory traffic by 56% and improves performance by 17% on average, while incurring only 3.2% storage overhead."
        },
        {
            "heading": "2 Background and motivation",
            "text": "We first review related work in compressed memory hierarchies, then illustrate the challenges and opportunities of compressed hierarchies in object-based programs."
        },
        {
            "heading": "2.1 Related work in compressed hierarchies",
            "text": "Much prior work has focused on compressed memory hierarchies to reduce data movement. While compression is too onerous to be used in small private caches, it is sensible to implement in main memory and the large last-level cache (LLC). Prior techniques thus can be broadly classified into three domains: (i) compression algorithms, (ii) compressed memory architectures, and (iii) compressed cache architectures.\nCompression algorithms aim to reduce the number of bits required to represent a data chunk (e.g., a cache line). Since decompression latency adds delay to the critical path of a memory access, unlike general compression algorithms, memory hierarchy compression favors simpler algorithms that achieve low decompression latency and area overhead, even if they achieve a lower compression ratio. Moreover, since programs issue fine-grained memory accesses, prior hardware-based compression techniques focus on compressing cache lines, matching the natural data transfer granularity of the LLC and main memory.\nFrequent pattern compression (FPC) [2] recognizes repeated patterns or small-value integers and uses a static encoding to compress every 32-bit data chunk in a cache line. Base-Delta-Immediate (BDI) [35] observes that values in a cache line usually have a small dynamic range, so BDI compresses a cache line by representing it with a base value and per-word deltas. SC2 [5] uses Huffman coding to compress cache lines, and recomputes the dictionary infrequently, leveraging the observation that frequent values change rarely. Because recomputing the dictionary requires recompressing all the data, SC2 is suitable for caches but less attractive for main memory. FP-H [4] is tailored to compress floatingpoint values. HyComp [4] combines multiple compression algorithms in a single system and dynamically selects the appropriate algorithm. Bit-Plane Compression (BPC) [24] targets homogeneous arrays in GPGPUs and improves compression ratio over BDI by compressing the deltas better.\nThese techniques add few cycles to each memory access. However, they usually exploit redundancy within a single block, a very fine-grained size. They work well for arraybased programs with homogeneous data types. But as we will see later, object-based programs have lower redundancy across nearby words, making these techniques less effective.\nCompressedmainmemory architectures are faced with one key challenge: choosing a memory layout that adds little latency while enabling good compression ratios.\nMXT [51] compresses 1 KB data chunks with a heavyweight compression algorithm. While it achieves a high compression ratio, its decompression latency is very high (64\ncycles). To locate the compressed data, MXT adds a TLB-like translation table to translate chunk addresses, which adds even more latency and requires significant state.\nRobust Memory Compression (RMC) [17] and Linearly Compressed Pages (LCP) [34] trade off lower compression ratios for lower latency overheads. They compress smaller (64-byte) cache lines and leverage the virtual memory system, which they modify to translate from uncompressed virtual pages to compressed physical pages. To keep translation mechanisms simple, each physical page is restricted to be power-of-two-sized. This incurs internal fragmentation, which reduces the compression ratio (e.g., a page that compresses to slightly more than 1KB must use a 2 KB frame).\nRMC and LCP differ in the layout of compressed pages. RMC compresses each cache line to one of four possible sizes. Each page table entry is extended to track the sizes of all the lines (64\u00d72=128 bits). To compute the address of a compressed cache line, the system must add up the sizes of all preceding cache lines, a non-trivial computation [34].\nBy contrast, LCP requires cache lines to compress to the same size. This makes the compressed cache line address trivial to compute in the common case (a simple shift). LCP stores the non-fitting cache lines uncompressed after all the compressed blocks and chooses the page\u2019s compression ratio to minimize the final compressed size.\nOther prior work builds on these architectures and improves over them. For example, DMC [25] combines LCP and MXT by applying LCP to hot pages and MXT to cold pages. Compresso [15] introduces techniques to reduce metadata accesses, limit overflows, and improve spatial locality.\nCompressed cache architectures havemore flexibility than compressed main memory, as their associative structure offers more design choices than main memory\u2019s directly addressed layout. The key challenge in compressed caches is tracking compressed lines with small tag array overheads and high data array utilization. These architectures typically perform serial tag and data array accesses, and require extra tag entries (usually 2\u00d7, about 6% area overhead) to track more compressed cache lines than uncompressed caches.\nVSC [1] divides the cache into sets like in a conventional cache, but lets each set store a variable number of variablysized, compressed cache lines. Each tag has a pointer to identify the line\u2019s location within the set. Cache architectures with decoupled tag and data stores, such as the indirectindexed cache [22] and V-Way cache [37], use a longer forward pointer and can store compressed cache lines anywhere in the data array, reducing fragmentation. Meanwhile, DCC [42], SCC [41], and DISH [30] leverage decoupled sector caches to track multiple compressed lines per sector.\nThese compressed caches still compress each cache line individually; prior work that exploits redundancy across lines incurs large overheads [29] and is practical only on throughput-oriented processors with high latency tolerance."
        },
        {
            "heading": "2.2 Opportunities for object-based programs",
            "text": "Array-dominated applications, common in scientific computing, provide many opportunities for compression because nearby words share data types and are likely to have similar byte patterns. Prior techniques like BDI [35] and FP-H [4] are effective on these programs. However, many applications are dominated by objects, which have a more irregular layout: nearby words correspond to different fields and are unlikely to have similar values. Prior compression techniques are less effective on object-heavy applications.\nObject-heavy applications are common. Fig. 1 shows the footprint composition of eight Java benchmarks. Only the first two benchmarks, which implement common scientific kernels (fft and spmv), are array-dominated. The remaining six benchmarks, which include databases, graph analytics, and a key-value store, have at least 40% and up to 75% of the heap footprint allocated to objects. More than 90% of those objects are small (< 128 B [8]). Therefore, prior algorithms that leverage similarities among nearbywords are unlikely to compress well on these applications because a large portion of their footprint is small objects, not homogeneous arrays.\nFortunately, object-based applications provide new opportunities for compressed memory hierarchies. First, objectbased applications perform memory accesses within objects and always follow pointers to other objects. Therefore, objects, and not cache lines, are the right compression unit. Second, though nearby words have low redundancy, similarities exist across objects of the same type.\nWe now illustrate these two insights with a simple B-tree Java microbenchmark, BTree. Fig. 2 shows the three main object types in BTree: Node, Entry[], and Entry, and their in-memory layouts. We show the layout in Maxine [54], the Java Virtual Machine (JVM) we use in our evaluation, but other JVMs like HotSpot [49] and Jikes [3] use a similar layout. Red arrows denote pointers (references) across objects. Fig. 3a shows the memory layout of a 4-entry B-tree node.\nFig. 3b shows an example of applying LCP to BTree. LCP uses hybrid BDI+FPC compression (see Sec. 7.1 for details). In this example, each 64B line is compressed into a fixed-size 32B chunk. A few blocks can be compressed beyond 32B, but the remaining space (shown hatched) is left unused due to LCP\u2019s design. The total size of this compressed page is then rounded up to the closest power-of-2 page size, which often\nNode\nEntry[]\nEntry[0]\nEntry[1]\nEntry[2]\nEntry[3]\n0x00\n0x100\n0x40\n0x80\n0xC0\n(a) No compression\n0x00\n0x40\n0x80\n0x60\n0x20\nSignificant\nunused space\ndue to\nperformance-\noriented layout\n(b) LCP\nNode\u2019 Entry[]\u2019\nEntry[0]\u2019 Entry[1]\u2019 Entry[2]\u2019 Entry[3]\u2019\n(c) Object-based\ncompression\n(d) Cross-object\ncompression\nEntry[]\u2019\nEntry[0]\u2019 \u2206Entry[1]\n\u2206Entry[2] \u2206Entry[3]\nNode\u2019\n4K page\n2K page\nCompact layout;\npointers to compressed\nobjects directly\n\u2026 \u2026\n\u2026\nFigure 3. Different compression techniques applied to BTree.\ncauses more unused space, limiting efficiency. For BTree, LCP achieves only 10% compression ratio (Table 1), because in addition to layout inefficiencies, the compression algorithm cannot compress many of the objects effectively.\nFig. 3c shows the compressed memory layout if we compress objects instead of cache lines. Compressed objects are stored contiguously, with no space left unused. Moreover, each pointer (red arrow) directly points to the compressed object. This approach removes the need to translate between compressed and uncompressed address spaces, and it is safe because programs may access objects only through pointers. This disciplined access pattern removes the fragmentation caused by the tradeoff between compression ratio and fast address calculation in prior work. With this compression technique, we can achieve a compression ratio of 1.56\u00d7.\nMoreover, there is still an opportunity to improve compression ratio in object-oriented programs. We find that objects of the same type usually have similar contents. For example, many of the bytes in Entry[0] and Entry[1] are identical. Therefore, it is better to compress across objects instead of across nearby words in a cache line.\nFig. 3d shows an example of cross-object compression. Instead of storing all Entry objects, we store one base object (Entry[0]). For other Entry objects, we only store the bytes that differ (\u2206Entry) between those objects and the base object. This further reduces footprint over Fig. 3c, achieving an even higher compression ratio of 1.95\u00d7.\nHowever, to realize these insights, hardware needs to access data at object granularity and must have control over pointers between objects, as we explain next."
        },
        {
            "heading": "3 Baseline system: Hotpads",
            "text": "Zippads and COCO compress objects instead of cache lines. Thus, they are better suited to an object-based memory hierarchy than a conventional cache hierarchy. We implement them on top of Hotpads, a recent object-based hierarchy. We now describe the principles and relevant features of Hotpads; please see prior work [19, 52] for details.\nModern languages like Java and Go adopt an object-based memory model and hide the memory layout from the programmer. This prevents many classes of errors and enables automatic memory management. Hotpads extends the same insight to the memory hierarchy: It hides the memory layout from software and dispenses with the conventional flat address space interface. Instead, Hotpads adopts an objectbased interface. Hotpads leverages this interface to efficiently transfer and manage variable-sized objects instead of fixedsize cache lines. Hotpads also provides hardware support for memory allocation, unifies hierarchical garbage collection and data placement, and avoids most associative lookups.\nHotpads is not specific to particular languages, and it is not just a way to accelerate garbage collection or other managedlanguage features. Rather, Hotpads achieves a more efficient memory system by leveraging the principles behind garbage collection and matching them to the structure of the memory hierarchy. As a result, Hotpads can also accelerate applications in low-level unmanaged languages. These applications can use Hotpads\u2019s object-based model selectively, as Hotpads includes a legacy mode to support a flat address space. As we will see in Sec. 7.5, Hotpads and Zippads outperform high-performance allocators on two C/C++ benchmarks."
        },
        {
            "heading": "3.1 Hotpads overview",
            "text": "Hotpads is a hardware-managed hierarchy of scratchpad-like memories called pads. Pads are designed to store variablesized objects efficiently. Hotpads transfers objects across pad levels implicitly, in response to memory accesses. Fig. 5 shows an example Hotpads hierarchy with three levels of pads, but Hotpads can use any number of levels.\nFig. 6 shows the structure of a pad. Most space is devoted to the data array, which is managed as a circular buffer. The\ndata array has a contiguous block of allocated objects followed by a block of free space. Hotpads uses simple bump pointer allocation: fetched or newly allocated objects are placed at the end of the allocated region. Pads have two auxiliary structures: (i) the canonical tag (c-tag) array, which is a decoupled tag store that a fraction of the accesses use to find a resident copy of an object; and (ii) the metadata array, which tracks information of objects stored in the data array.\nUnlike caches, pads have separate addresses from memory and can act as the backing store of some objects. This enables an efficient object flow: objects are first allocated in the L1 pad and move up the hierarchy as they become cold and are evicted. Short-lived objects are garbage-collected before they reach main memory, which greatly reduces memory traffic and footprint. An object\u2019s canonical level is the largest hierarchy level an object it has reached. The canonical level acts as the object\u2019s backing store."
        },
        {
            "heading": "3.2 Hotpads example",
            "text": "Fig. 4 illustrates the main features of Hotpads through a simple example showing a single-core systemwith two levels of pads (we use a single-core setup for simplicity, but Hotpads supports multi-core systems [52]). This example uses only one type of object, ListNode, with two members, an integer value and a pointer to another ListNode.\n0 shows the initial state of the system: the core\u2019s register file holds a pointer to object A in the L2 pad, and A points to objectB in main memory. The L1 and L2 pads also hold other objects (shown in solid orange) that are not relevant here.\nIn this example, A\u2019s canonical level (i.e., its backing store) is the L2: A does not exist in main memory and does not have a main memory address. B\u2019s canonical level is main memory.\n1 shows the state of the system after the program accesses A\u2019s value. A is copied into the L1 pad, taking some free space at the end of the allocated region. Then, the pointer in register r1 is rewritten to point to this L1 copy. This way, subsequent dereferences of r1 access the L1 copy directly.\nPrograms can also access objects by dereferencing pointers to them. 2 shows the state of the system after the core dereferences A\u2019s pointer to B. B is copied into the L1 pad, and A\u2019s pointer to B is rewritten to point to its L1 copy.\nSince programs may have multiple pointers to the same object, pads must have a way to find copies of objects from higher levels. This is the role of the c-tag array, which, for each object copy, stores the object\u2019s canonical address (i.e., its address at its canonical level). For example, when A is copied into the L1 pad, the c-tag array inserts a translation from A\u2019s L2 address to the copy\u2019s L1 address. Thanks to pointer rewriting, only pointers to higher levels need to check the c-tag array. This eliminates most associative lookups.\n3 shows the state of the system after the program creates a new object C. C is allocated directly in the L1 pad\u2019s free space and requires no backing storage in main memory.\nWhen a pad runs out of free space, it triggers a collectioneviction (CE) process to free up space. In 4 the L1 pad has filled up, so the pad starts a CE to free L1 space. Similarly to garbage collection (GC), a CE walks the data array to detect live vs. dead objects. In addition to GC, a CE evicts live but non-recently accessed objects to the next-level pad. In this example, C is dead (i.e., unreferenced) and a new object D is referenced from A, and thus live. Note that A\u2019s L1 copy has been modified, so the L2 data is now stale. Only B has been accessed recently in the L1.\n5 shows the state after the L1 CE. First, C has been collected. Second, A and D have been evicted to the L2 pad. Since A was originally copied from the L2 pad, the modified\ncopy is written back to A\u2019s L2 location. By contrast, D is moved up to the L2 pad and thus has a new canonical address, an L2 address. Third, B has been kept in the L1 and moved to the start of the array.\nAs in compacting GC, during CEs, live objects are compacted into a contiguous region to simplify free space management. Moreover, CEs also update pointers in the system (register file, pointers in pads) to point to the new location. For example, both a register (r3) and the pointer in A are updated to D\u2019s new canonical address.\nThis always-moving-up object flow is critical for Zippads, as objects start their lifetime uncompressed andmove to compressed levels only when they become cold and are evicted. This move changes the object\u2019s original address and requires updating all the pointers to the object. Zippads leverages this to point directly to the compressed object, avoiding uncompressed-to-compressed address translation (Sec. 4.1)."
        },
        {
            "heading": "3.3 Hotpads implementation details",
            "text": "Next, we discuss the implementation details of Hotpads that are relevant to Zippads and COCO.\nISA: Hotpads modifies the ISA to support an object-based memory model. These ISA changes are transparent to application programmers, but require runtime and compiler modifications. Table 2 shows a subset of the ISA to support object accesses and to convey pointer information to Hotpads. Pointers may be dereferenced or compared, but their raw contents cannot be accessed. This lets Hotpads control their contents. Hotpads uses base+offset addressing modes, where the base register always holds an object pointer. The offset can be an immediate (base + displacement mode) or a register (base + index mode). Data load/store instructions are used for non-pointer data ( 1 ); pointer load/store instructions are used to access pointer data ( 2 ); and the alloc instruction allocates a new object ( 3 ).\nPointer format: Hotpads controls andmanipulates pointers within pads. Fig. 7 shows the format of Hotpads pointers. This format is a microarchitectural choice, as pointers are opaque to software. The lower 48 bits contain the object\u2019s address and always point to the first word of the object. The\nSize Address (48b)"
        },
        {
            "heading": "47 04863 50",
            "text": "Figure 7. Hotpads pointer format.\nupper 14 bits contain the object\u2019s size (in words), and the other 2 bits store metadata that is not relevant to Zippads. All objects are word-aligned. Storing the object\u2019s size in the pointer simplifies reading objects: fetching sizewords from the starting address yields the entire object. Zippads extends this pointer format to store compression metadata.\nCollection-evictions: CEs occur entirely in hardware, and are much faster than software GC because pads are small. To make CEs efficient, Hotpads enforces an important invariant: Objects at a particular level may only point to objects at the same or higher levels. In this way, CEs at smaller pads (e.g., L1) will not involve larger pads (L2, L3) because those pads have no pointers to the L1 pad. This makes CE cost proportional to pad size.\nCEs enable Hotpads\u2019s object flow: evicting an object from its original canonical level to the next level requires updating all the pointers to the object (e.g., D from 4 to 5 ). This would be impractically expensive to do for a single object, requiring a scan of the evicting pad and all smaller ones. But CEs amortize this scan across all evicted objects, making updating pointers cheap. Zippads thus piggybacks on CEs to point to compressed objects directly (Fig. 3c)."
        },
        {
            "heading": "4 Zippads: An object-based compressed memory hierarchy",
            "text": "Zippads leverages Hotpads to (i) manipulate and compress objects rather than cache lines, and (ii) avoid the extra level of indirection in conventional compressed main memories by pointing directly to compressed objects. Zippads is agnostic to the compression algorithm used, and can use conventional algorithms like BDI or FPC, but works best with the COCO compression algorithm. We first describe Zippads, then explain COCO in the next section.\nFig. 8 shows an example Zippads hierarchy. The last-level pad andmainmemory are compressed,while the core-private L1 and L2 pads are not. Other Zippads hierarchies are possible, e.g., there could be multiple levels of compressed pads, or only main memory could be compressed. Once a level uses compression, it is sensible for larger levels to remain compressed, though Zippads does not require this. Zippads transfers compressed objects directly between compressed levels. To simplify our explanation, we first assume objects are always small (< 128 B). We discuss how Zippads handles larger objects in Sec. 4.3."
        },
        {
            "heading": "4.1 Compressing objects",
            "text": "Zippads aims to store compressed objects compactly, with no unused space between them to maximize compression ratio. Objects move from uncompressed to compressed storage for two reasons: newly moved objects and dirty writebacks.\nCase 1. Newly moved objects: As explained in Sec. 3, Hotpads performs in-hierarchy memory management: objects start their lifetime at the L1 pad, and are moved into larger pads and main memory when they have not been recently used. Hotpads leverages this object flow to minimize the impact of dead objects, collecting them as soon as possible. Zippads further leverages this to facilitate compression: objects start their lifetime uncompressed, and when they become not-recently used, they are evicted into the last-level pad and compressed there. This is a key difference from conventional hierarchies, where objects are mapped to a main memory address to begin with, forcing the problem of translating from uncompressed to compressed addresses.\nNewly moved objects are the easiest case to handle: Zippads simply compresses the object and then stores it at the beginning of the available space (with bump-pointer allocation). Fig. 9 illustrates this process. This leaves no space between compressed objects (however, compressed objects are still word-aligned and may have a few unused bytes). The object\u2019s new canonical address is now in compressed memory, and all pointers to the object are updated to this new canonical address, as explained in Sec. 3.2.\nCase 2. Dirty writebacks: An object can reach to a compressed level, then be fetched into the L1 and modified. This object is then eventually written back to the compressed level. This dirty writeback is more complex than the initial move, because the object\u2019s compressed size may change. And since other objects in this level may have pointers to this object, Zippads cannot simply move it.\nFig. 10 shows how Zippads handles dirty writebacks. If the compressed object\u2019s new size is the same or smaller than its old size, the object is stored in the old location. If the new size is smaller, this wastes some space, which is left unused.\nHowever, if the compressed object\u2019s new size is larger than its old size, the object cannot be stored at its old location. We call this an overflow. Zippads allocates a new location for the compressed object (using bump-pointer allocation as usual). Because other pointers to the old location may still exist, Zippads turns the old location into a forwarding thunk: it stores the new pointer in the first word of the object. Further accesses to the old location follow the forwarding thunk to find the object. As we will see, overflows are rare (Sec. 7.6).\nPeriodic compaction: Althoughdirtywritebacks that change the size of an object are rare, they introduce storage inefficiencies, either leaving some unused space or causing forwarding thunks. However, these inefficiencies are temporary: in compressed pads, the collection-eviction (CE) process compacts\nCase 2: Dirty writeback If new size <= old size If new size > old size\nall the live objects into a contiguous region. Zippads modifies the compaction step of CEs to handle recompressed objects: it eliminates both the unused space at the end of smaller recompressed objects and the forwarding thunks caused by overflows. Like Hotpads, Zippads performs mainmemory garbage collection in software. Zippads enhances the garbage collection algorithm to work on compressed objects and to perform these compaction optimizations."
        },
        {
            "heading": "4.2 Encoding compression information in pointers",
            "text": "Most compressed cache architectures employ a common optimization: they use the cache tag to encode information about the compressed cache line needed to perform accesses and decompression, such as the compressed size or the type of compression algorithm it uses. Leveraging the tags is more efficient than encoding this information in the data array itself, as the cache immediately knows how much data to access and what decompression algorithm to use.\nZippads cannot use this optimization, most importantly because main memory has no cache tags, but also because not all pad objects may have a canonical tag. Instead, Zippads encodes compression information directly in pointers. This is possible because pointers are opaque to software and we can change their format without changing the ISA. This approach yields all the benefits of encoding information in tags because all accesses start from a pointer.\nFig. 11 shows Zippads\u2019s pointer format. First, the size field now encodes the object\u2019s compressed size. Second, Zippads devotes a few extra bits to store algorithm-specific compression information (e.g., the compression format used). This way, when transferring objects between levels, hardware knows how much data to fetch and which decompression algorithm to use. Moreover, this encoding enables using different compression schemes for different objects.\nAlgorithm-specific compression information slightly reduces the address width. This is acceptable because Zippads uses word (i.e., 8-byte) addresses. For example, when Zippads"
        },
        {
            "heading": "48 48-X 063 50",
            "text": "uses the BDI algorithm, which requires 4 bits per pointer, 44-bit word addresses allow almost the same address space as conventional 48-bit byte addresses in x86-64."
        },
        {
            "heading": "4.3 Compressing large objects",
            "text": "So far, we have assumed that all objects are small (<128B). However, programs can allocate larger objects. In Hotpads, large objects and arrays are fetched as subobjects in 64B chunks to avoid overfetching (e.g., if only one element is used in a 1KB array). Zippads also needs to handle large objects; otherwise, decompressing a large object for just one element would incur a very high latency.\nZippads thus breaks large objects to smaller subobjects and compresses them individually. This way, when the core only accesses part of a large object, Zippads need not decompress the whole object. Specifically, when allocating a large object (>128B in our implementation), Zippads does not reserve the full capacity for it. Instead, it first allocates an array of pointers, which we call the index array. Each pointer in the index array points to a 64B subobject. All pointers are initially null, and the space for a subobject is allocated when an access to a particular subobject occurs (i.e., allocate-on-access).\nFig. 12 shows an example of allocating and accessing a large object. At 1 , the program allocates a 256B object, so Zippads allocates an index array with 4 elements. At 2 , the core accesses the element at a 72-byte offset, which belongs to the second subobject. Zippads then allocates a subobject and modifies the pointer in the index array. At 3 and 4 , the core updates the value and writes it back to the location. Zippads first accesses the index array to find the subobject pointer and traverses this pointer to update the field.\nThe index array is a microarchitecture optimization invisible to software. Subobjects are also compressed when their canonical addresses change. Pointers in the index array are updated as normal objects in Hotpads. Compressing at subobject granularity also avoids moving large objects for overflows. One drawback is that the footprint of large objects increases by 18 , but this is a small overhead compared to the benefit of low decompression latency and a more compact layout. Evaluation results show that Zippads offsets this overhead (Sec. 7.2)."
        },
        {
            "heading": "5 COCO: Cross-object-compression algorithm",
            "text": "Zippads works with any compression algorithm. But there is limited redundancy within each object, so existing algorithms like BDI yield limited benefits. We thus propose\nCOCO (Cross-Object COmpression), a new compression algorithm that exploits redundancy across objects. COCO achieves high compression ratios and is cheap to implement."
        },
        {
            "heading": "5.1 COCO compression format",
            "text": "COCO is a differential compression algorithm: it compresses an object by storing only the bytes that differ from a different base object. Specifically, the compressed object format has three elements: 1. The base object id, an integer (32 bits in our implementa-\ntion) that uniquely identifies the base object.\n2. A diff bitmapwith one bit per byte of the object. The ith bit\nis set iff the object\u2019s ith byte differs from the base object. 3. A string of byte diffs containing the bytes that are different\nfrom the base object\u2019s. Fig. 13 shows an example COCO-compressed object. The uncompressed object has the same values in the first and second words, a 2-byte difference in the third, and a 4-byte difference in the fourth. Therefore, the compressed object stores only the six differing bytes in addition to the header."
        },
        {
            "heading": "5.2 Compression and decompression circuits",
            "text": "COCO compression/decompression circuits are simple to implement and only require narrow comparators and multiplexers. Our implementations compress/decompress one word (8 bytes) per cycle. This provides sufficient throughput for our purposes. The compression circuit compares the base object and the uncompressed object word by word. Each cycle, it produces one byte of the diff bitmap and a chunk of delta bytes. The decompression circuit takes the base object, bitmap, and delta bytes as inputs and produces one word of the decompressed object per cycle.\nWe have written the RTL for these circuits and synthesized them at 45nm using yosys [55] and the FreePDK45 standard cell library [23]. The compression circuit requires an area equivalent to 810 NAND2 gates at a 2.8 GHz frequency. The decompression circuit requires an area equivalent to 592 NAND2 gates at a 3.4 GHz frequency. These frequencies are much higher than typical uncore frequencies (1\u015b2GHz), and a more recent fabrication process would yield faster circuits.\nFinally, COCO area overheads are much smaller than prior techniques, such as BPC (68K NAND2 gates [24]) or C-pack (40K NAND2 gates [13]). This result shows that COCO is practical and simple to implement in hardware."
        },
        {
            "heading": "5.3 Building the base object collection",
            "text": "COCO allocates extra space in main memory to store base objects. We empirically find that statically assigning one base object per type id works well: same-type objects have the same layout and often share many values. This approach also makes compression faster: instead of trying multiple base objects to decide which base object to use, COCO simply selects the base object using the object type id (the first word of the uncompressed object).\nWe find that COCO is largely insensitive to the choice of base object, so our implementation simply uses the first object of each type that it sees (i.e., the first object that is evicted to a compressed level) as the base object. It may be beneficial to dynamically update the base object or to have multiple base objects per type; we leave this to future work."
        },
        {
            "heading": "5.4 Caching base objects",
            "text": "Compressing and decompressing objects require fast access to the base object. If COCO had to access the base object from the last-level pad or main memory on each decompression, this would significantly increase decompression latency and bandwidth consumption.\nInstead, we serve base objects from a small and fast base object cache, 8 KB in our implementation. This cache stores the most frequently used base objects, and is indexed with the base object id.\nWe find that a small cache suffices because the popularity of object types is highly skewed. Fig. 14 shows the distribution of accesses over themost popular object types for four selected apps. Tens of object type ids account for most of the accesses.\nThis overhead is similar to prior dictionary-based compression algorithms, such as SC2 or FP-H (4 KB dictionary [4, 5]). COCO only needs to fetch the base object content when it misses in the base object cache."
        },
        {
            "heading": "6 Integrating Zippads and COCO",
            "text": "We have so far discussed Zippads independently from the compression algorithm, and COCO independently from Zippads. As discussed earlier, objects and arrays have different types of redundancy, and Zippads should compress both objects and arrays well. Therefore, Zippads uses different compression algorithms for each: COCO for objects, and a conventional hybrid algorithm, BDI+FPC, for arrays. To this end, our Zippads+COCO implementation adds 4 bits of compression metadata to each pointer, as shown in Table 3. The first bit denotes whether the data is an array compressed with BDI+FPC, and the remaining 3 bits are used by BDI+FPC compression. If it is not a compressed array, the second bit\nindicates whether it is an object compressed with COCO. If it is also not a COCO-compressed object, the third bit indicates whether it is an object or an array, which is required for Zippads+COCO to compress data during CEs.\nWe also extend the ISA to distinguish objects and arrays at allocation time: Zippads adds a new alloc_array instruction, as shown in Table 4, used to allocate arrays, and leaves the original alloc to allocate objects. Both instructions are identical, except that alloc_array sets the array bit in the new pointer, whereas alloc does not.\nArrays use hybrid BDI+FPC compression, in the same style as HyComp [4]. We use 3 bits in the pointer to select the right decompression algorithm, and replace one choice in the original BDI encoding (Base2-\u22061) to represent FPC compression.\nNon-COCOZippads variant: To better understand the benefits of Zippads and COCO, we also evaluate Zippads-BF, a variant of Zippads that does not use COCO. Zippads-BF instead uses hybrid BDI+FPC for all objects and arrays. As we will see, Zippads-BF outperforms existing compressed hierarchies due to its more compact layout."
        },
        {
            "heading": "6.1 Discussion",
            "text": "Although we have integrated COCO with Zippads in this work, in principle COCO should be usable with other compressed architectures. However, these architectures should somehow convey object boundaries to COCO, which would require runtime and hardware changes. For example, one solution could be to use type-segregated object pools, where each region of memory is dedicated to storing objects of a particular type, instead of bump-pointer allocation; however, this makes object allocation slower, may hurt spatial locality, and requires significant metadata to map pools to object types. Another approach could be to align all objects to cache line boundaries. This could achieve good performance on the compressed cache and memory, but excessive padding would use uncompressed caches (L1 and L2) poorly.\nIn the end, because retrofitting an object-based compression algorithm into a cache-based hierarchy faces significant hurdles and Zippads already demonstrates significant improvements over prior techniques even without COCO, we choose to not implement COCO outside of Zippads."
        },
        {
            "heading": "7 Evaluation",
            "text": "We evaluate Zippads and COCO on a mix of array-based and object-based workloads. We evaluate on Java workloads because Java is a memory-safe language that aligns well with our baseline system, Hotpads. To show that Zippads is not specific to Java or memory-safe languages, we also evaluate on two C/C++ benchmarks in Sec. 7.5."
        },
        {
            "heading": "7.1 Methodology",
            "text": "We evaluate Zippads using MaxSim [38], a simulation platform that combines ZSim [39], a Pin-based [27] simulator, and Maxine [54], a 64-bit metacircular research JVM."
        },
        {
            "heading": "7.1.1 Hardware",
            "text": "We compare the following techniques:\nUncompressed: Our baseline uses a three-level cache hierarchy without compression, with parameters shown in Table 5.\nCompressed memory hierarchy (CMH): We implement a state-of-the-art compressed memory hierarchy that compresses both the LLC and main memory. We use HyCompstyle hybrid BDI+FPC compression (Sec. 6). The compressed LLC uses the VSC [1] design with 2\u00d7 tag array entries and uses the CAMP compression-aware replacement policy [32]. The compressed main memory uses LCP [34], which we idealize by assuming a perfect metadata cache that always hits.\nHotpads: We configure Hotpads as in prior work [19, 52], with three levels of uncompressed pads.\nZippads: Zippads uses a compressed last-level pad with 2\u00d7 the canonical tag array entries, similar to the VSC LLC in the compressed memory hierarchy design. We also use an 8KB base object cache to store frequently-used base objects. Zippads uses COCO for objects and BDI+FPC for arrays; we also evaluate a variant of Zippads, Zippads-BF, that always uses BDI+FPC for objects and arrays, like the CMH design.\nCache scrubbing: Modern languages like Java incur memory overheads due to garbage collection [56]. Therefore, we also implement cooperative cache scrubbing [44], which adds instructions to zero and scrub (i.e., undirty) cache lines and use them in the JVM for both the uncompressed and compressed cache hierarchies to reduce memory traffic due to object allocation and recycling. Scrubbing does not improve compression ratio, but it improves the performance of garbage-collected languages with simple mechanisms."
        },
        {
            "heading": "7.1.2 Software",
            "text": "JVM: Our cache-based baseline uses the Maxine JVM with a tuned, stop-the-world generational GC with a 64MB young heap. Hotpads and Zippads use a modified JIT compiler that follows their new ISA.\nWorkloads: We study eight Java workloads from different domains. We select workloads with heap sizes larger than 100MB, so that they exercise main memory. We use two scientific workloads, fft and spmv from the Scimark2 [36] suite; two database workloads, h2 from the Dacapo [8] suite1 and SPECjbb2005 [48]; two graph processing workloads, PageRank and Coloring from JgraphT [28], a popular Java graph library; GuavaCache, a key-value store from Google Core Libraries for Java [21], and BTree, the example we saw in Sec. 2, from the JDBM [26] database.\nTable 6 describes their input sets. We fast-forward JVM initialization and warm up the JIT compiler like prior work [8] by running the same workload multiple times before starting simulation. We run all workloads to completion to avoid sampling bias in compression ratio [16]. For each workload, we first find the smallest heap size that does not crash, and use 2\u00d7 that size. This is standard methodology [9, 44].\nIn addition to Java workloads, we also study two C/C++ workloads. GCBench [10] is a C benchmark that creates, traverses, and destroys binary trees. GCBench\u2019s default input incurs only a 16MB active memory footprint, so we scale the input to incur a 512MB active memory footprint, which stresses main memory. Silo [53] is a C++ in-memory OLTP\n1We evaluate Java workloads with large footprints (>100MB min heap size); h2 is the only such one from DaCapo.\ndatabase, configured to run the TPC-C benchmark. Both the baseline and CMH use high-performance allocators (tcmalloc [20] for GCBench and jemalloc [18] for Silo). For Hotpads and Zippads, we modify these workloads to use the Hotpads and Zippads ISAs to allocate and access objects.\nMetrics: We report the average compression ratio, sampled every 100M cycles, of different schemes. We also report total memory traffic (in bytes) and performance (inverse of execution time). All metrics are normalized to the baseline system without compression."
        },
        {
            "heading": "7.2 Zippads improves compression ratio",
            "text": "Fig. 15 shows the compression ratio of the five memory hierarchies we compare. Each group of bars shows results for a different application. Compressed hierarchies have their bars hatched; uncompressed hierarchies are shown unhatched.\nCMH compresses memory footprint effectively for arraydominated scientific workloads fft and spmv, achieving compression ratios of 1.67 and 1.53, respectively. Zippads also compresses these two workloads well and achieves slightly higher compression ratios than CMH, 1.97 and 1.79, because it better compresses non-heap data (e.g., code and JVM state), which is not array-based. There is no difference between Zippads-BF and Zippads because these two workloads are dominated by arrays, which Zippads always compresses with hybrid BDI+FPC.\nThe otherworkloads are object-dominated, and differences across techniques are larger. CMH only compresses around 10% of the total footprint and has compression ratios between 1.06 to 1.27. By contrast, Zippads achieves high compression ratios. The difference in compression ratio between CMH and Zippads correlates well with the ratio of object footprint shown in Fig. 1. For example, guavacache has the highest ratio for objects in the main memory footprint, and the difference between CMH and Zippads is also the highest: Zippads compresses 2\u00d7 better than CMH. Meanwhile, specjbb has the lowest ratio of objects (around 40%) in the heap footprint, so the difference in compression ratio between CMH and Zippads is also the lowest among these workloads.\nThere is also a large difference between Zippads-BF and Zippads in these applications. Owing to its more compact layout, Zippads-BF achieves compression ratios of 1.56\u015b1.78 in these applications, significantly higher than CMH, despite\nonly using BDI+FPC. By contrast, Zippads uses COCO for objects, which increases compression ratios to 1.82\u015b2.24.\nOn average, CMH achieves a compression ratio of only 1.24, while Zippads-BF and Zippads achieve ratios of 1.70 and 2.01, i.e., 1.37\u00d7 and 1.63\u00d7 better than CMH. These results show that compressing objects rather than cache lines and adopting an object-specific compression algorithm are both important contributions to the effectiveness of Zippads."
        },
        {
            "heading": "7.3 Zippads reduces main memory traffic",
            "text": "Fig. 16 shows the memory traffic of all schemes, measured in total bytes read and written, normalized to Uncompressed. In addition to schemes we saw above, Fig. 16 also reports the Scrubbing variants of Uncompressed and CMH (CMH+S).\nWe find that CMH reduces main memory traffic for many applications for two main reasons. First, the compressed LLC has a higher effective capacity, and thus higher cache hit rate. This helps cache-capacity sensitive applications, in particular h2, specjbb, and btree. Second, the compressed main memory (LCP) lets the system fetch consecutive cache lines in a single 64-byte burst, which helps applications with high spatial locality. This is the case for spmv, coloring, and pagerank. On average, CMH reduces main memory traffic by 15% over Uncompressed.\nScrubbing helps the allocation-heavy database workloads, reducing their traffic by 60%. But Scrubbing is not as effective for others, especially for scientific workloads that only allocate once. On average, Scrubbing reduces main memory traffic by 15%. CMH and Scrubbing (CMH+S) yield additive benefits since they are orthogonal techniques. CMH+S saves 30% of main memory traffic on average.\nHotpads does not reduce traffic for scientific workloads, but it saves memory traffic significantly for object-based workloads because of its object-friendly features: object-gra-\nnularity data movement, in-pad allocation, and hardwarebased in-pad garbage collection. These features especially help h2, specjbb, and guavacache. On average, Hotpads reduces main memory traffic by 66%.\nZippads improves over Hotpads by adding the benefits of high compression ratios. In workloads where CMH saves significant traffic, such as spmv, h2, and btree, Zippads also yields significant benefits over Hotpads. Like Hotpads, Zippads also benefits guavacache significantly, whereas other techniques yield little benefit. On average, Zippads reduces main memory traffic by 2\u00d7 over the baseline, by 56% over CMH+S, and by 22% over Hotpads."
        },
        {
            "heading": "7.4 Zippads improves system performance",
            "text": "Fig. 17 shows the end-to-end performance of the different memory hierarchies. The performance improvement of different schemes correlates well with their memory traffic reduction. For example, CMH reduces the memory traffic for spmv, and it also improves performance by 12%. Scrubbing reduces memory traffic the most for database workloads and thus improves performance the most for them.\nHotpads\u2019s object-level operation and in-pad memory management provide large benefits across all object-based programs. Zippads again adds the benefits of memory compression over Hotpads, helping spmv, h2, and btree.\nOn average, CMH improves performance by 5%, Scrubbing by 6%, CMH+S by 11%, and Hotpads by 24%. Zippads improves performance over Uncompressed by 30%. This represents a 5% improvement over Hotpads and a 17% improvement over CMH+S. Overall, these results show that Zippads incurs small compression overheads, so compression consistently improves performance."
        },
        {
            "heading": "7.5 Zippads is effective on C/C++ benchmarks",
            "text": "Zippads is not specific to Java workloads, and also helps object-based programs in unmanaged languages. To show this, Fig. 18 compares the compression ratio, normalizedmain memory traffic, and performance of the different schemes on two object-heavy C/C++ workloads. Small objects occupy over 95% of the memory footprint in these workloads, so trends are similar to those of object-heavy Java workloads.\nFirst, CMH achieves negligible memory footprint reductions for these workloads. By contrast, Zippads-BF achieves high compression ratios for these workloads, 1.61 and 1.23, thanks to its compact layout; and Zippads achieves even higher compression ratios, 2.01 and 1.70, thanks to COCO.\nSecond, all compression techniques reduce main memory traffic. CMH\u2019s reduced memory traffic stem from accesses to freshly allocated pages. These pages are zeroed and thus compress well. Thus, CMH reduces main memory traffic by 47% even though it does not compress the data produced by these workloads. Hotpads reduces main memory traffic by 57%, and Zippads-BF and Zippads reduce traffic further, by 85% and 2.2\u00d7, by effectively compressing main memory.\nUncomp. CMH Hotpads Zippads-BF Zippads\n0.5\n1.0\n1.5\n2.0\n2.5\nC o\nm p\nre s s io\nn R\na ti o\ngcbench silo\n(a) Compression ratio.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nM e\nm o\nry T\nra ff\nic (\nB )\ngcbench silo\n(b) Memory traffic.\n0.9\n1.0\n1.1\n1.2\n1.3\n1.4\n1.5\n1.6\nS p e e d u p\ngcbench silo\n2.7x2.7x2.7x\n(c) Performance.\nFigure 18. Results for C/C++ benchmarks.\nFinally, CMH achieves a 20% speedup over the uncompressed baseline in GCbench due to its reduced memory traffic. Hotpads and Zippads are both 2.7\u00d7 faster than the baseline. These speedups stem fromHotpads features, including in-hierarchy allocation and collection-evictions. Zippads preserves Hotpads\u2019s high performance while achieving a high compression ratio. Silo is neither memory-intensive nor allocation-intensive, so memory hierarchy compression has modest performance benefits: CMH only improves performance by 4%, Zippads-BF by 6%, and Zippads by 10%."
        },
        {
            "heading": "7.6 Zippads analysis",
            "text": "Base object cache misses: Fig. 19 shows the number of misses per kilo-cycle (MPKC) in the COCO base object cache. Most workloads have fewer than 0.001 MPKC. The database workloads have slightly more misses, but are still around 0.01 MPKC. These misses reduce performance by less than 0.1%. Moreover, a 16KB base object cache does not help much. We thus conclude that a small base object cache is effective.\nOverflow frequency: Fig. 20 shows the rate of overflows due to dirty writebacks, in overflows per thousand cycles. Overflows are rare across all workloads we evaluate. They happen most frequently in guavacache, but still at a low frequency of 0.4 overflows per Kcycle. For other workloads, overflows happen less than 0.01 times per Kcycle.\nHardware overhead analysis: Table 7 shows the total storage per last-level cache or pad of different schemes. Hotpads adds 6.5% storage over the baseline for the pad metadata and canonical tag entries. CMH adds 12.7% over the baseline due to 2\u00d7 tags, encoding bits in tag entries, and a 32KB metadata\ncache. Similarly, Zippads adds 9.2% over Hotpads due to doubling the number of canonical tags (to track more objects) and the 8KB base object cache. This is only 3.2% extra storage over the CMH LLC and 16.3% over the baseline. Overall, this shows similar on-chip storage requirements as prior compressed caches. These overheads are uniformly offset by the high compression ratios that Zippads achieves. Moreover, most overheads stem from the larger tag array, which can be removed if only compressed main memory (2x smaller memory footprint) is needed."
        },
        {
            "heading": "8 Additional related work",
            "text": "Prior work on software techniques to reduce the memory footprint of managed languages has also considered objectlevel compression. They compress objects by removing zeros [12, 45] or frequent field values [11]. COCO is inspired by these techniques. However, prior software techniques must be simple and must be used selectively to limit overheads. By contrast, COCO can be used to compress all objects.\nDiscontiguous array designs [12, 43] divide arrays into indexed chunks to avoid fragmentation. Zippads\u2019s subobject compression shares the same motivation.\nPrior work in hardware deduplication [14, 47, 50] also seeks to reduce memory footprint. However, deduplication techniques work well only when applications have coarsegrained redundancy, with large chunks of identical data. COCO can be seen as a byte-level deduplication technique.\nSome prior work in compression has also considered bandwidth usage and link utilization. MemZip [46] places compressed cache lines and theirmetadata for address translation next to each other to reduce bandwidth usage. Toggle-aware compression [31] considers the extra dynamic energy consumed by on-chip and off-chip links due to the more frequent toggling caused by compression. Zippads can be combined with these techniques (e.g., data bus inversion) to further reduce the energy consumed by links."
        },
        {
            "heading": "9 Conclusion",
            "text": "Conventional compressed hierarchies focus on compressing cache lines, which limits compression efficiency and adds substantial overheads. In this paper, we leverage two insights about object-based programs to improve compressed hierarchies. First, these programs perform object-level accesses, so objects are the right unit of compression. Second, there is significant redundancy across objects. Using these insights,\nwe propose Zippads, the first object-based compressed memory hierarchy, and COCO, a new, cross-object-compression algorithm. Zippads+COCO improves compression ratio over a combination of state-of-the-art techniques by up to 2\u00d7 and by 1.63\u00d7 on average. It also reduces memory traffic by 56% and improves performance by 17%."
        },
        {
            "heading": "Acknowledgments",
            "text": "We sincerely thank Maleen Abeydeera, Joel Emer, Mark Jeffrey, Anurag Mukkara, Suvinay Subramanian, Victor Ying, and the anonymous reviewers for their feedback. We thank Nathan Beckmann for sharing his compressed cache and CAMP implementation [6, 7], and Gennady Pekhimenko for sharing his BDI and FPC implementations. This work was supported in part by NSF grant CAREER-1452994 and by a grant from the Qatar Computing Research Institute."
        }
    ],
    "title": "Compress Objects, Not Cache Lines:An Object-Based Compressed Memory Hierarchy",
    "year": 2019
}