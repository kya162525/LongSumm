{
    "abstractText": "Although neural machine translation with the encoder-decoder framework has achieved great success recently, it still suffers drawbacks of forgetting distant information, which is an inherent disadvantage of recurrent neural network structure, and disregarding relationship between source words during encoding step. Whereas in practice, the former information and relationship are often useful in current step. We target on solving these problems and thus introduce relation networks to learn better representations of the source. The relation networks are able to facilitate memorization capability of recurrent neural network via associating source words with each other, this would also help retain their relationships. Then the source representations and all the relations are fed into the attention component together while decoding, with the main encoderdecoder framework unchanged. Experiments on several datasets show that our method can improve the translation performance significantly over the conventional encoder-decoder model and even outperform the approach involving supervised syntactic knowledge.",
    "authors": [
        {
            "affiliations": [],
            "name": "Wen Zhang"
        },
        {
            "affiliations": [],
            "name": "Jiawei Hu"
        },
        {
            "affiliations": [],
            "name": "Yang Feng"
        },
        {
            "affiliations": [],
            "name": "Qun Liu"
        }
    ],
    "id": "SP:d6a94546f2d140868422879b611cdd3ed7936157",
    "references": [
        {
            "authors": [
                "Roee Aharoni",
                "Yoav Goldberg."
            ],
            "title": "Towards string-to-tree neural machine translation",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 132\u2013140, Vancouver, Canada, July. Association for Computational Linguistics.",
            "year": 2017
        },
        {
            "authors": [
                "Dzmitry Bahdanau",
                "Kyunghyun Cho",
                "Yoshua Bengio."
            ],
            "title": "Neural machine translation by jointly learning to align and translate",
            "venue": "ICLR 2015.",
            "year": 2015
        },
        {
            "authors": [
                "Joost Bastings",
                "Ivan Titov",
                "Wilker Aziz",
                "Diego Marcheggiani",
                "Khalil Simaan."
            ],
            "title": "Graph convolutional encoders for syntax-aware neural machine translation",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1957\u20131967, Copenhagen, Denmark, September. Association for Computational Linguistics.",
            "year": 2017
        },
        {
            "authors": [
                "Kyunghyun Cho",
                "Bart van Merrienboer",
                "Caglar Gulcehre",
                "Dzmitry Bahdanau",
                "Fethi Bougares",
                "Holger Schwenk",
                "Yoshua Bengio."
            ],
            "title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation",
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724\u20131734, Doha, Qatar, October. Association for Computational Linguistics.",
            "year": 2014
        },
        {
            "authors": [
                "Michael Collins",
                "Philipp Koehn",
                "Ivona Kucerova."
            ],
            "title": "Clause restructuring for statistical machine translation",
            "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL\u201905), pages 531\u2013540, Ann Arbor, Michigan, June. Association for Computational Linguistics.",
            "year": 2005
        },
        {
            "authors": [
                "Jonas Gehring",
                "Michael Auli",
                "David Grangier",
                "Yann Dauphin."
            ],
            "title": "A convolutional encoder model for neural machine translation",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 123\u2013135, Vancouver, Canada, July. Association for Computational Linguistics.",
            "year": 2017
        },
        {
            "authors": [
                "Jonas Gehring",
                "Michael Auli",
                "David Grangier",
                "Denis Yarats",
                "Yann N. Dauphin."
            ],
            "title": "Convolutional sequence to sequence learning",
            "venue": "Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1243\u20131252, International Convention Centre, Sydney, Australia, 06\u201311 Aug. PMLR.",
            "year": 2017
        },
        {
            "authors": [
                "Sepp Hochreiter",
                "J\u00fcrgen Schmidhuber."
            ],
            "title": "Long short-term memory",
            "venue": "Neural Computation, 9(8):1735\u2013 1780.",
            "year": 1997
        },
        {
            "authors": [
                "G. Huang",
                "Z. Liu",
                "L. v. d. Maaten",
                "K.Q. Weinberger."
            ],
            "title": "Densely connected convolutional networks",
            "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2261\u20132269, July.",
            "year": 2017
        },
        {
            "authors": [
                "Sergey Ioffe",
                "Christian Szegedy."
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 448\u2013456, Lille, France, 07\u201309 Jul. PMLR.",
            "year": 2015
        },
        {
            "authors": [
                "Nal Kalchbrenner",
                "Phil Blunsom."
            ],
            "title": "Recurrent continuous translation models",
            "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700\u20131709, Seattle, Washington, USA, October. Association for Computational Linguistics.",
            "year": 2013
        },
        {
            "authors": [
                "Philipp Koehn",
                "Franz Josef Och",
                "Daniel Marcu."
            ],
            "title": "Statistical phrase-based translation",
            "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages 48\u201354. Association for Computational Linguistics.",
            "year": 2003
        },
        {
            "authors": [
                "Junhui Li",
                "Deyi Xiong",
                "Zhaopeng Tu",
                "Muhua Zhu",
                "Min Zhang",
                "Guodong Zhou."
            ],
            "title": "Modeling source syntax for neural machine translation",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 688\u2013697, Vancouver, Canada, July. Association for Computational Linguistics.",
            "year": 2017
        },
        {
            "authors": [
                "Tal Linzen",
                "Emmanuel Dupoux",
                "Yoav Goldberg."
            ],
            "title": "Assessing the ability of lstms to learn syntax-sensitive dependencies",
            "venue": "Transactions of the Association for Computational Linguistics, 4:521\u2013535.",
            "year": 2016
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "Wei-Jing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.",
            "year": 2002
        },
        {
            "authors": [
                "Adam Santoro",
                "David Raposo",
                "David G Barrett",
                "Mateusz Malinowski",
                "Razvan Pascanu",
                "Peter Battaglia",
                "Tim Lillicrap."
            ],
            "title": "A simple neural network module for relational reasoning",
            "venue": "I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 4974\u20134983. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "M. Schuster",
                "K.K. Paliwal."
            ],
            "title": "Bidirectional recurrent neural networks",
            "venue": "IEEE Transactions on Signal Processing, 45(11):2673\u20132681, Nov.",
            "year": 1997
        },
        {
            "authors": [
                "Rico Sennrich",
                "Barry Haddow."
            ],
            "title": "Linguistic input features improve neural machine translation",
            "venue": "Proceedings of the First Conference on Machine Translation, pages 83\u201391, Berlin, Germany, August. Association for Computational Linguistics.",
            "year": 2016
        },
        {
            "authors": [
                "Rico Sennrich",
                "Barry Haddow",
                "Alexandra Birch."
            ],
            "title": "Neural machine translation of rare words with subword units",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u20131725, Berlin, Germany, August. Association for Computational Linguistics.",
            "year": 2016
        },
        {
            "authors": [
                "Xing Shi",
                "Inkit Padhi",
                "Kevin Knight."
            ],
            "title": "Does string-based neural mt learn source syntax? In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1526\u20131534, Austin, Texas, November",
            "venue": "Association for Computational Linguistics.",
            "year": 2016
        },
        {
            "authors": [
                "Jinsong Su",
                "Shan Wu",
                "Deyi Xiong",
                "Yaojie Lu",
                "Xianpei Han",
                "Biao Zhang."
            ],
            "title": "Variational recurrent neural machine translation",
            "venue": "arXiv preprint arXiv:1801.05119.",
            "year": 2018
        },
        {
            "authors": [
                "Ilya Sutskever",
                "Oriol Vinyals",
                "Quoc V Le."
            ],
            "title": "Sequence to sequence learning with neural networks",
            "venue": "Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 3104\u20133112. Curran Associates, Inc.",
            "year": 2014
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 6000\u20136010. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "Matthew D Zeiler."
            ],
            "title": "Adadelta: an adaptive learning rate method",
            "venue": "arXiv preprint arXiv:1212.5701.",
            "year": 2012
        },
        {
            "authors": [
                "Franz Josef Och."
            ],
            "title": "Minimum error rate training in statistical machine translation",
            "venue": "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160\u2013167, Sapporo, Japan, July. Association for Computational Linguistics.",
            "year": 2003
        },
        {
            "authors": [
                "Yang Liu",
                "Maosong Sun."
            ],
            "title": "Contrastive unsupervised word alignment with non-local features",
            "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI\u201915, pages 2295\u20132301. AAAI Press.",
            "year": 2015
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In recent years, Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has achieved great success in some language pairs, rivalling the state-ofthe-art Statistical Machine Translation (SMT). The Recurrent Neural Network (RNN) encoder-decoder architecture is widely used framework for NMT, the principle behind which is that: encoding the meaning of the input bidirectionally into a concept space via RNNs and decoding into target words with RNNs based on this encoding (Sutskever et al., 2014; Bahdanau et al., 2015). This means that encoding principle leads to a deeper understanding and learning of the translation rules, and hence better translation than conventional SMT that considers only surface forms, e.g., words and phrases.\nThe RNNs with gating, such as Gated Recurrent Unit (GRU) (Cho et al., 2014) or Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), are designed to memorize useful history information and meanwhile forget irrelative information. Together with attention technique which makes the decoding process only focus on the most related source words, the RNN encoder-decoder framework is expected to be able to handle long sequences and consider the globally related information. However, the practical situation is that RNNs tend to forget old history information, especially the far older one. Sometimes the older information is indispensable for generating proper translation, e.g., for the source sentence \u201ctake the heavy box away\u201d, when translating \u201caway\u201d, \u201ctake\u201d should be considered together. In addition, it has been proven that using phrases rather than words in SMT (Koehn et al., 2003) brings performance improvement, while in NMT the attention is only modeled in the unit of words. In the same sense, improvement is expected if attention is operated on more words rather than one.\nMoreover, NMT produces the representation for the source by running through the source words sequentially with a bidirectional RNN (Schuster and Paliwal, 1997), so it only employs word order information and ignores the relation between words. Although some researchers have demonstrated that\n\u2217Corresponding author. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://\ncreativecommons.org/licenses/by/4.0/\nar X\niv :1\n80 5.\n11 15\n4v 2\n[ cs\n.C L\n] 9\nS ep\n2 01\n8\nNMT is able to capture certain syntactic phenomena (e.g. subject-verb agreement) without external syntactic information (Linzen et al., 2016; Shi et al., 2016), there are some other works which has shown their superior performance by modeling word relationship explicitly. However, these works usually need to introduce external syntactic knowledge or connect words according to their relations in the syntactic structure (Sennrich et al., 2016; Bastings et al., 2017; Aharoni and Goldberg, 2017; Li et al., 2017).\nIn this paper, we present a method to refine the NMT based on the above two points. The main idea is to learn relationship between the source word pairs. Corresponding to the first point, our method employs Convolutional Neural Networks (CNNs) to collect local information around one word and relates each word with its neighbors, which ensures the subsequent operations are performed in the unit of multiple words. As for the second point, Relation Network (RN) (Santoro et al., 2017) is introduced to establish pairwise relationship between words, meanwhile, there\u2019s no need to attain external input of syntactic knowledge. In this way, our model can memorize all words ahead and behind via additional connection between words no matter how distant they are. In the RNs, the representations of the source words produced by RNNs are taken as objects and the relationships between them are reasoned.\nSpecifically, our method introduces a RN component between the encoder and the attention layer in the RNN encoder-decoder framework (Sutskever et al., 2014; Bahdanau et al., 2015). The RN component is composed of three layers: first, the CNN layer slides window along the output of the encoder to capture information among multiple words around one word, then the graph propagation layer constructs a fully connected graph with the information of one window as one node and transfers messages along the edges, so that each node can collect the information from all other nodes, and last the multi-layer perceptron layer transforms the information of each node to the form which is suitable for the attention component to use. We performed experiments on several datasets and got significant improvements over vanilla NMT and SMT systems. Besides, our model significantly outperforms two other models, which introduced latent variables to capture the implicit semantics and employed explicitly external syntactic knowledge respectively."
        },
        {
            "heading": "2 Background",
            "text": "As the main idea of our method is to introduce relation networks into the attention-based NMT (Bahdanau et al., 2015) to learn word relationship and keep all source words in memory, in this section we will briefly describe the baseline model \u2013 the attention-based NMT first and the technique used in this paper \u2013 relation networks."
        },
        {
            "heading": "2.1 Attention-based NMT",
            "text": "The attention-based NMT follows the encoder-decoder framework, with an additional attention module. It works on the assumption that the source sentence and the target translation share a common continuous\nspace. It first encodes the source sentence into a continuous space and then performs decoding based on this space, meanwhile, employing attention to indicate the relevance of each source word to the current translation. Figure 1 shows the architecture of the attention-based NMT (Bahdanau et al., 2015), which is composed of three components: the encoder, the attention layer and the decoder.\nThe Encoder The encoder uses a pair of GRUs to run through source words bidirectionally to get two sequences of hidden states, which are concatenated to produce corresponding hidden state for the i-th source word\n\u2212\u2192 h i = \u2212\u2212\u2212\u2192 GRU ( xi, \u2212\u2192 h i\u22121 ) ;\n\u2190\u2212 h i = \u2190\u2212\u2212\u2212 GRU ( xi, \u2190\u2212 h i+1 ) ; hi = [\u2212\u2192 h i; \u2190\u2212 h i ] (1)\nThe Attention Layer The attention layer aims to extract the source information (called attention) which is highly related to the generation of the current target word. To get the attention of the j-th decoding step, the correlation degree between current target word yj and hi is first evaluated as\neij = v T a tanh(Wasj\u22121 +Uahi) (2)\nThen, for the j-th decoding step, the correlation degree is normalized over the whole source sequence, all source hidden states are added weightedly according to the normalized correlation degree to obtain the attention aj\n\u03b1ij = exp (eij)\u2211ls\ni\u2032=1 exp ( ei\u2032j ) ; aj =\n\u2211ls i=1 \u03b1ijhi (3)\nThe Decoder The decoder first employs a variant of GRU to roll the target information according to previous target word yj\u22121, previous hidden state sj\u22121 and the attention aj . The details are described in Bahdanau et al. (2015). The current target hidden state sj is calculated by\nsj = g(yj\u22121, sj\u22121,aj) (4)\nAfter that, the decoder gives a probability distribution over all the words in the target vocabulary and selects the target word with the highest probability as the output of the current step\np(yj |y<j ,x) \u221d exp(f(sj , yj\u22121,aj) \u00b7Wv) (5)\nwhere f stands for a linear transformation and Wv is a weight matrix."
        },
        {
            "heading": "2.2 Relation Networks",
            "text": "A relation network (RN) is a neural network with a structure integrated for relational reasoning. The RN is designed to constrain the functional form of a neural network so that it can capture the core common properties of relational reasoning. Hence its capability of computing relations is inherent without needing to be learned specially.\nFormally, given a set of input \u201cobjects\u201d denoted as O = {o1, o2, \u00b7 \u00b7 \u00b7 , on}, RN can be formed as a composition function of objects (Santoro et al., 2017), represented as\nRN(O) = f\u03c6 (\u2211 i,j g\u03b8 (oi, oj) ) (6)\nwhere oi is the i-th object, and f\u03c6 and g\u03b8 are functions used to calculate relations. Multi-layer perceptrons are often used for f\u03c6 and g\u03b8, as their parameters are learnable synaptic weights, making RNs end-to-end differentiable. Here the role of g\u03b8 is to infer how two objects are related, or whether they are related, and hence the output of g\u03b8 can be treated as \u201crelations\u201d."
        },
        {
            "heading": "3 NMT with Relation Networks",
            "text": "In this paper, we introduce a Relation Network Layer (denoted as RNL) on the basis of the attentionbased NMT (Bahdanau et al., 2015) and frame it between the encoder and the attention layer. The RNL first employs CNNs to collect information in the unit of multi-words rather than one single word, then takes the outputs of CNNs as objects and makes them fully connected to build a graph propagation layer and associate with each other, finally transforms the acquired representations with word relations via MLP into the form suitable for the attention layer to use. Next, the outputs of the RNL are directly fed into the attention layer, so the RNL can still fit the encoder-decoder framework well. The architecture of our RNL is shown in Figure 2. Briefly, the RNL is composed of three components: the CNN layer, the Graph Propagation (GP) layer and the Multi-layer Perceptron (MLP) layer.\nThe CNN Layer CNNs are used to collect local information around one word. In this way, not only the information of a single word but their neighbors are considered. The number of neighbors to be considered depends on the kernel width k but can also vary by stacking several convolution layers, e.g., stacking 2 convolution layers with the kernel width k = 3 can collect information from 5 words at the same time.\nIn the CNN layer, the input is the hidden states produced by the bidirectional GRUs (Bi-GRUs), denoted as h = {h1, ...,hi, ...,hls}, so each source word is represented by its hidden state. A filter is applied to convolute over a window of k words to get the convolutional representation. Given the i-th source word and its hidden state hi \u2208 Rd, the hidden states covered by the window with the width of k are concatenated and then are fed to the filter where we denote the concatenated vector as hki =[ hi\u2212b(k\u22121)/2c; \u00b7 \u00b7 \u00b7 ;hi; \u00b7 \u00b7 \u00b7 ;hi+b(k\u22121)/2c ] . For the first and last b(k \u2212 1)/2c words of a sentence, the hidden state hi with i < 1 or i > ls are set to zeros (padding). Then the filtering process mentioned above can be formed as\nci = f ( Wcnnh k i + bcnn ) (7)\nWcnn \u2208 Rk\u00d7d is the convolution weights and bcnn is the bias, where the two together define a linear operation. f is the leaky RELU with the coefficient 0.1 to control the angle of the negative slope. In the RNL, leaky RELU is used as all the nonlinear activation functions. The output of the CNN layer is c = {c1, ..., ci, ..., cls}.\nThe GP Layer The GP layer is used to learn the relationships between source words. It adopts the outputs of the CNN layer c = {c1, c2, ..., cls} as input and formulates the relationships between them into a graph. Here ci can be thought as the object mentioned in Section 2.2. In this graph, each input ci is taken as a node and has edges connected to all other nodes. Then information flows along the edges and each node receives messages from all its direct neighbors. We call this process graph propagation.\nAfter graph propagation process, another sequence of vectors {r1, r2, ..., rls} is produced. The generation of ri can be decomposed into three steps:\n\u2022 Each input vector ci in c is concatenated with all vectors in c (including itself) to get a set of vectors Ci = {ci1, \u00b7 \u00b7 \u00b7 cij , \u00b7 \u00b7 \u00b7 , cils} where cij = [ci; cj ].\n\u2022 Each cij is converted into vector rij by a 4-hidden-layers MLP. The conversion with 1-hidden-layer MLP can be represented as\nrij = f (Wgpcij + bgp) (8)\n\u2022 Average over all the outputs above to get the final representation for the i-th source word\nri = 1\nls\n\u2211ls j=1 rij (9)\nThe MLP Layer There are several nonlinear transformations which map the inputs into different vector spaces in the GP layer. In order to reduce computation complexity, the output features size of the nonlinear transformations is set to small. Hence we use another MLP layer to map the feature back into the original space, usually the same as that of hi to have more powerful representation. The final state oi for the i-th source word after the entire RN layer can be got by another 2-hidden-layers MLP, 1-hidden-layer MLP can be written as\noi = f(Wmlpri + bmlp) (10)\nResidual Stacking technique is used in our method. Concretely, we stack multiple layers inside the encoder and meanwhile apply residual connection for two adjacent layers. Assume hlin and h l out are the input and the output of the l-th layer, respectively, then residual connection is conducted to get the final output of the l-th layer in the following two steps. First, the input and the output of the l-th layer are added together:\nhl = hlin + h l out (11)\nNext, dense concatenation (Huang et al., 2017) is employed to receives features from all previous layers and the final output of the l-th layer is produced by\nhldc = Wdc\n[ h1; h2; \u00b7 \u00b7 \u00b7 ; hl ] + bdc (12)\nwhere weight matrix Wdc and bias bdc are adjusted to map the dense-concatenated vectors into the same feature space as the input. Then hldc is fed to the next layer which means h l+1 in = h l dc."
        },
        {
            "heading": "4 Related Work",
            "text": "Many researchers have worked on learning the relationships of the source words to improve translation performance. One line is to refine source presentations by adding relationships between source words or between source and target words, with the main architecture remaining the RNN encoder-decoder framework. Sennrich et al. (2016) enriched source representations with POS tags, dependency labels and other linguistic features. Bastings et al. (2017) employed graph convolutional networks to model relations of words in dependency trees for the source embeddings to include these relations. These two models both require extra supervised syntax input while our method does not need external knowledge and learn the relationship by its own.\nAnother line is to change the structure of the neural network. Gehring et al. (2017a) and Gehring et al. (2017b) proposed to substitute the conventional RNN encoder with the CNN encoder in order to train faster. They employed stacked CNNs to capture relationships between source words which can be calculated simultaneously, not like RNNs, the computation of which is constrained by temporal dependencies. The attention scores are also computed based on the output of the CNNs and the decoder is still the RNN-based decoder. Vaswani et al. (2017) is another work to eschew the recurrence. It instead\nrelied entirely on the attention mechanism to draw the global dependencies between input and output. Su et al. (2018) introduced latent random variables into the decoder of NMT and generated these variables recurrently to capture the global semantic contexts and model strong and complex dependencies among target words at different timesteps.\nOur method still follows the RNN encoder-decoder framework, giving full play to the advantages of RNNs, which transfers information through words bidirectionally. In addition, we also employs RNs in our method to connect the source words explicitly, further captures relationships between source words without any external knowledge injection, which enables the model to learn the relationships itself and facilitates easy application."
        },
        {
            "heading": "5 Experiments",
            "text": "In the experiment section, we first compare our system with two baseline systems on a Chinese-English (Zh-En) dataset and the WMT17 English-German (En-De) dataset, then compare our method with a related approach on the WMT16 En-De dataset. Finally, we give some analyses about our method in different aspects."
        },
        {
            "heading": "5.1 Data Preparation",
            "text": "We performed experiments on three datasets:\nNIST The training data consisted of 1.25M Zh-En parallel sentence pairs with 25M Chinese tokens and 27M English tokens1. We used NIST 2002 test dataset (878 sentences) as the validation set, and another four NIST test datasets as the test datasets: NIST 2003 (MT03), NIST 2004 (MT04), NIST 2005 (MT05) and NIST 2006 (MT06), which contain 919, 1788, 1082 and 1357 sentences respectively.\nWMT17 The training data was composed of 5.6M En-De preprocessed parallel sentence pairs 2 with 141M English tokens and 194M German tokens. The test dataset of newstest2014 (3003 sentences) was used as the validation set and the following test datasets were used as the test datasets: newstest2015 (2169 sentences), newstest2016 (2999 sentences) and newstest2017 (3004 sentences). Besides, 8k merging operations were performed to learn byte-pair encodings (BPE) (Sennrich et al., 2016) on the target side of the parallel training data.\nWMT16 We conducted experiments on WMT16 dataset, the same dataset as the work of Bastings et al. (2017) for comparison. We kept the same settings as those in Bastings et al. (2017): The original dataset consists of 4500966 sentence pairs, with 4173550 left after filtering pairs which contains more than 50 tokens on either side after tokenization. newstest2015 and newstest2016 were used as the validation set and test dataset, respectively. 16k BPE merging operations were conducted on the target side of the bilingual training data.\nFor WMT16 dataset, case-sensitive 4-gram BLEU score (Papineni et al., 2002) was reported by using the multi-bleu.pl script. The results on the other two datasets were evaluated with case-insensitive 4-gram BLEU score."
        },
        {
            "heading": "5.2 Systems",
            "text": "Results of five systems on different datasets were reported:\nRNNsearch We implemented the attention-based NMT of Bahdanau et al. (2015) by PyTorch framework3 with the following settings: the length of the sentences on both sides was limited up to 50 tokens with 30K vocabulary, and the source and target word embedding sizes were both set to 512, the size of all hidden units in both encoder and decoder RNNs was also set to 512, and all parameters were initialized by using uniform distribution over [\u22120.1, 0.1]. The mini-batch stochastic gradient descent (SGD) algorithm was employed to train the model with batch size of 80. In addition, the learning rate was adjusted by Adadelta optimizer (Zeiler, 2012) with \u03c1 = 0.95 and = 1e-6. Dropout was applied on the output layer with dropout rate of 0.5. The beam size was set to 10.\n1We chose LDC2002E18, LDC2003E07, LDC2003E14, Hansard\u2019s portion of LDC2004T07, LDC2004T08 and LDC2005T06 from the LDC corpora. There were 1.11M sentence pairs left after filtering.\n2http://data.statmt.org/wmt17/translation-task/preprocessed 3http://pytorch.org\nSystems test16 BiRNN+GCN 23.9\nRNMT 25.4\nTable 3: Performance comparison with the related work on the WMT16 En-De dataset.\nRNNsearch? This system is an improved version of RNNsearch where the decoder employs a conditional GRU layer with attention module, consisting of two GRUs and an attention module for each step4. Specifically, Equation 4 is substituted with the following two equations:\ns\u0303j = GRU1(yj\u22121, sj\u22121); sj = GRU2(aj , s\u0303j) (13)\nBesides, for the calculation of attention in Equation 2, sj\u22121 is replaced with s\u0303j\u22121. The other components of the system keep the same as RNNsearch. We used the same settings for RNNsearch and RNNsearch?.\nVRNMT A novel Variational Recurrent NMT (VRNMT) model, proposed by Su et al. (2018), captures more semantic context and complex dependencies among target words by generating latent random variables recurrently in the NMT decoder.\nBiRNN+GCN This is the model presented by Bastings et al. (2017). They incorporated dependency syntactic structure into the bidirectional RNN (BiRNN) encoder of NMT and modeled the relation among the source words by using graph convolutional networks (GCNs).\nRNMT Our system was implemented by embedding the RNLs into the Bi-GRUs of RNNsearch?. The overall structure used alternatively stacked GRUs and RNLs, in which the two GRU layers are in opposite direction. Inside the RNL, the GP layer employed a 4-hidden-layers MLP (shown in Equation 8) and the MLP layer contained 2 hidden layers (as in Equation 10). For the Zh-En translation task, two convolution layers with kernel width of 1 and 3 were stacked, the output channel sizes of CNN were 128 and 256 respectively, followed by batch normalization (BN) (Ioffe and Szegedy, 2015) with learnable parameters, and MLP contained 256 units. For the En-De translation task, only one convolution layer was used with kernel width of 3, the output channel size was 96, 128 was adopted as the hidden size of MLP. All of the other settings were the same with those of RNNsearch?."
        },
        {
            "heading": "5.3 Performance Comparison",
            "text": "We compared our system RNMT with the two baseline systems RNNsearch and RNNsearch? both on the NIST Zh-En and the WMT17 En-De translation tasks. As RNMT was implemented on the basis of RNNsearch?, in the strict sense, RNNsearch? is the baseline. From the results shown in Table 1, we can see that RNMT significantly improves translation quality on all test datasets and outperforms RNNsearch? by 1.48 BLEU points averagely on the Zh-En dataset. Besides, comparison between our model to VRNMT shows that proposed simple model stably produces better performance on all test datasets and outperforms VRNMT 1.04 BLEU score on average.\n4https://github.com/nyu-dl/dl4mt-tutorial/blob/master/docs/cgru.pdf\nOn the WMT17 En-De dataset, as shown in Table 2, RNMT shows superiority on three test datasets stably, and averagely achieves the gains of 1.7 BLEU points over RNNsearch?, with only 4.1M parameters more. Given the above results, we can conclude that RN can indeed learn the relationships between the source words and these relationships are useful and bring improvement on the translation performance.\nWe also compared our method with the work of Bastings et al. (2017) which requires the injection of external syntactic knowledge, to see whether the relationships produced by RNs can lead to better translation than the syntax from supervised learning. The results in Table 3 show that our system can achieve an improvement of 1.5 BLEU scores. We believe that the relationships of the source words derived from RNs do not necessarily conform to human cognition, but it can be simultaneously tailored with the other parts of the translation system. In this way, RNs can generate the relationships more suitable for the NMT."
        },
        {
            "heading": "5.4 Impact of Input Length",
            "text": "One motivation of adding RNs is that RNNs tend to forget the distant history which RNs memorize it by explicitly introducing relations between pairs of words. Therefore, we assume that our method suppose to bring greater improvement on relative long sentences, which contains more distant history information than shorter ones that usually forgotten by RNNs. Based on this sense, we split the source sentences in the MT03 test dataset into different bins according to their length and evaluated BLEU scores of the translations from RNNsearch? and RNMT on the different bins, respectively.\nThe results are shown in Figure 3. In the bins holding sentences no longer than 50, the BLEU scores of the two systems are close to each other. When the sentence length surpasses 50, RNMT shows its superiority over RNNsearch?. As the sentence length grows, the difference becomes increasingly large. This verifies the deduce that our method can not only memorize history information but capture the relationship between words, both of which are beneficial to translate long sentences.\n5.5 Word Alignment\nIn this section, we will verify the translation performance of our model from another perspective. Intuitively, the better translation should have better alignment to the source sentence, so we evaluated the quality of the alignments derived from the attention module of the NMT using Alignment Error Rate (AER) (Och, 2003). We did this experiment on the artificially aligned dataset from Liu and Sun (2015) which contains 900 Zh-En sentence pairs. The alignments were got in this way for both RNNsearch?\nsystem and our system. When one target word was generated, we retained the alignment link with the highest probability \u03b1ij in Equation 3.\nThe comparison results are shown in Table 4. It illustrates that our system RNMT can produce better translations than the baseline RNNsearch?, a difference of 1.72 BLEU points. Besides, the AER score is 1.1 points lower than the baseline model. Note that the smaller the AER score, the better the alignment quality.\nAlong with the translation results, we also produce the word alignment matrix based on each target word\u2019s attention probability distribution over the whole source sentence. Two source sentences are randomly sampled from websites, both comparisons between baseline alignment and improved alignment generated by RNNsearch? and RNMT are shown in Figure 4.\nFor the first example, from the view of source side, it is obviously unreasonable that the Chinese word yi is contributed to generate three discontinuous English words the, is and for, grammatical knowledge show that the word yi should be only aligned to the English word for, just like the result of our model. Besides, on the target\u2019s ground, if one Chinese word is translated into an English phrase, all words in the phrase should be aligned to the Chinese word, RNNsearch? model improperly aligns new and is to some other irrelevant words besides the correct one. When generating word is, almost the whole source sentence should be considered, our model gets more centralized alignment for it.\nIn the second case, unlike the baseline model, our model produces correct translation jazz music for jueshi yinyue and alignment. the together with origin is aligned to the source word fayuan, while RNNsearch? mistakenly aligns the to two source words almost with equal probability."
        },
        {
            "heading": "5.6 Translation Examples",
            "text": "As shown in Table 5, we give two example translations generated from baseline model and proposed model. Comparing the translation results between two systems, we can observe that RNNsearch? often miss some information of the source sentence, especially for the long sentence. Both of the sentences are complex sentences with long dependent adversative relation, for the first example, the baseline model forgets the information of the long distance clause about women jinnian yizhi \u00b7 \u00b7 \u00b7 toumingdu and ignores to translate the second clause. It similarly happens that, when producing the target text for the second sample, RNNsearch? loses the information after chengnuo dongaohui and fails to capture the latter clause with adversative relation. In addition, another phenomenon observed is that the longer the source sentence is, it is easier to ignore important information for RNNsearch?. However, as can be seen from the boldfaced sections marked in results generated with RNMT, proposed model with CNN could captures more source information successfully.\nSpecifically, RNNs are skilled in modeling the order information of a sequence, while CNNs mainly focus on local features around some specific word. Both of them are weak to capture the long-distance dependency information, However, facts prove that proposed relation layer succeeds in alleviating the deficiencies of the two by integrating CNNs with bidirectional RNNs subtly."
        },
        {
            "heading": "6 Conclusion",
            "text": "As RNNs are not good at remembering the old history and cannot consider word relationship either, sometimes conventional NMT cannot get enough source information and hence emphasizes too much on the fluency of the target. As a result, it suffers from meaning-drift and generates \u201cinaccurate\u201d translation. Even so, NMT can still benefit from the recurrence of RNNs. In this paper, we propose to incorporate RNLs into the attentional NMT. The RNs employs CNNs to collect information around one word and explicitly connect each word with all the other words. In this way, it provides the opportunities for NMT to capture relationship between source words and hence leads to a better source representation. Our method can get better translation on the NIST Zh-En dataset and the WMT En-De dataset and can even outperform the system with supervised syntactic knowledge."
        },
        {
            "heading": "Acknowledgements",
            "text": "We highly appreciate the anonymous reviewers for their precious comments. This work was supported in part by National Natural Science Foundation of China (Nos. 61472428 and 61662077)."
        }
    ],
    "title": "Refining Source Representations with Relation Networks for Neural Machine Translation",
    "year": 2018
}