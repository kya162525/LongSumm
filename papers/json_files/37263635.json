{
    "abstractText": "ion \u03bbx.A In the last case \u03bbx. is a binder and free occurrences of x in A become bound . A term in which all variables are bound is said to be closed otherwise it is open. The motivating idea is that closed term represent functions. The intended meaning of AB is the application of function A to argument B while \u03bbx.A is the function which for input x returns A. Terms which are the same except for renaming of bound variables are not distinguished, thus \u03bbx.x and \u03bby.y are the same, identity function. 3In his monograph Church defines two slightly differing calculi called \u03bbI and \u03bbK, of these \u03bbK is now regarded as canonical and is what we sketch here.",
    "authors": [
        {
            "affiliations": [],
            "name": "David Turner"
        }
    ],
    "id": "SP:dbdbf8ad5b4550d80a91c2a391965cbfa0958a51",
    "references": [
        {
            "authors": [
                "R.L. Constable"
            ],
            "title": "On computable numbers",
            "venue": "Journal of Mathematics,",
            "year": 1936
        },
        {
            "authors": [
                "Prentice Hall",
                "1986. Robert L. Constable",
                "Scott F. Smith"
            ],
            "title": "Computational Foundations of Basic Recursive Function Theory",
            "venue": "Proceedings 3rd IEEE Symposium on Logic in Computer Science,",
            "year": 1988
        },
        {
            "authors": [
                "T. Coquand",
                "G. Huet"
            ],
            "title": "The Calculus of Constructions",
            "venue": "Information and Computation,",
            "year": 1988
        },
        {
            "authors": [
                "N.D. Goodman",
                "J. Myhill"
            ],
            "title": "Choice Implies Excluded Middle\u201d, Zeit",
            "venue": "alectica,",
            "year": 1978
        },
        {
            "authors": [
                "Andrew Hodges"
            ],
            "title": "Did Church and Turing have a thesis about machines?\u201d, this collection",
            "venue": "J. Hughes \u201cThe Design and Implementation of Programming Languages\u201d,",
            "year": 1983
        },
        {
            "authors": [
                "W. Howard"
            ],
            "title": "The Formulae as Types Notion of Construction",
            "venue": "Essays on Combinatory Logic, Lambda Calculus and Formalism,",
            "year": 1984
        },
        {
            "authors": [
                "F. Major",
                "M. Turcotte"
            ],
            "title": "The Combination of Symbolic and Numerical Computation for Three-Dimensional Modelling of RNA",
            "venue": "P. Martin-Lo\u0308f \u201cAn Intuitionist Theory of Types - Predicative Part\u201d, in Logic Colloquium",
            "year": 1991
        },
        {
            "authors": [
                "R. Milner"
            ],
            "title": "A Theory of Type Polymorphism in Programming",
            "venue": "Journal of Computer and System Sciences,",
            "year": 1978
        },
        {
            "authors": [
                "S.L. Peyton Jones"
            ],
            "title": "Functional Programming Languages and Computer Architecture, Copenhangen",
            "venue": "Journal of Functional Programming ,",
            "year": 1993
        },
        {
            "authors": [
                "S.L. Peyton Jones"
            ],
            "title": "Haskell 98 language and libraries: the Revised Report",
            "venue": "Journal of Functional Programming",
            "year": 2003
        },
        {
            "authors": [
                "Christopher Strachey"
            ],
            "title": "Fundamental Concepts in Programming Languages\u201d, originally notes for an International Summer School on computer programming, Copenhagen, August 1967, published in Higher-Order and Symbolic Computation, Vol 13, Issue 1/2, April 2000 \u2014 this entire issue is dedicated in memory of Strachey",
            "year": 2000
        },
        {
            "authors": [
                "Dana Scott",
                "Christopher Strachey"
            ],
            "title": "Toward a mathematical semantics for computer languages\u201d, Oxford University Programming Research Group Technical Monograph PRG-6",
            "venue": "Paul Taylor \u201cAbstract Stone Duality\u201d, privately circulated,",
            "year": 1971
        },
        {
            "authors": [
                "M. A"
            ],
            "title": "Turing \u201cOn computable numbers with an application to the Entscheidungsproblem",
            "venue": "Proceedings London Mathematical Society,",
            "year": 1937
        },
        {
            "authors": [
                "D.A. Turner"
            ],
            "title": "A New Implementation Technique for Applicative Languages",
            "venue": "Software-Practice and Experience,",
            "year": 1979
        }
    ],
    "sections": [
        {
            "heading": "To appear in \u201cChurch\u2019s Thesis after 70 Years\u201d ed. A. Olszewski, Logos Verlag, Berlin, 2006. 1",
            "text": ""
        },
        {
            "heading": "Church\u2019s Thesis",
            "text": "and"
        },
        {
            "heading": "Functional Programming",
            "text": ""
        },
        {
            "heading": "David Turner Middlesex University, UK",
            "text": "The earliest statement of Church\u2019s Thesis, from Church (1936) p356 is\nWe now define the notion, already discussed, of an effectively calculable function of positive integers by identifying it with the notion of a recursive function of positive integers (or of a lambdadefinable function of positive integers).\nThe phrase in parentheses refers to the apparatus which Church had developed to investigate this and other problems in the foundations of mathematics: the calculus of lambda conversion. Both the Thesis and the lambda calculus have been of seminal influence on the development of Computing Science. The main subject of this article is the lambda calculus but I will begin with a brief sketch of the emergence of the Thesis.\nThe epistemological status of Church\u2019s Thesis is not immediately clear from the above quotation and remains a matter of debate, as is explored in other papers of this volume. My own view, which I will state but not elaborate here, is that the thesis is empirical because it relies for its significance on a claim about what can be calculated by mechanisms. This becomes clearer in Church\u2019s restatement of the thesis the following year, after he had seen Turing\u2019s paper, see below. For a fuller discussion see Hodges (this volume).\nThree definitions of the effectively computable functions of the natural numbers (non-negative integers, hereafter N), developed nearly contemporaneously in the early to mid 1930\u2019s, turned out to be equivalent. Church (1936, quoted above) showed that his own theory of lambda definable functions yielded the same functions on Nk \u2192 N as the recursive functions of Herbrand and Go\u0308del [Herbrand 1932, Go\u0308del 1934]. This was proved independently by Kleene (1936).\nA few months later Turing (1936) introduced his concept of logical computing machine (LCM) - a finite automaton with an unbounded tape divided into squares, on which it could move left or right and read or write symbols from a finite alphabet, in accordance with a specified state transition table. A central\nresult of the paper is the existence of a universal LCM, which can emulate the behaviour of any LCM whose description is written on its tape. In an appendix Turing shows that the numeric functions computable by his machines coincide with the lambda-definable ones.\nIn his review of Turing\u2019s paper, Church (1937) writes\nthere is involved here the equivalence of three different notions: computability by a Turing machine, general recursiveness . . . and lambdadefinability . . . The first has the advantage of making the identification with effectiveness in the ordinary sense evident immediately . . . The second and third have the advantage of suitability for embodiment in a system of symbolic logic.\nThe Turing machine led, about a decade later, to the Turing/von-Neumann computer - a realization in electronics of Turing\u2019s universal machine, with the important optimization that (an initial portion of) the tape is replaced by a random access store. The concept of a programming language didn\u2019t yet exist in 1936, but the second and third notions were eventually to provide the basis of what we now know as functional programming."
        },
        {
            "heading": "2 The Halting Theorem",
            "text": "All three notions of computability involve partiality in an essential way. General recursion schemas yield the partial recursive functions, which may for some values of their arguments fail to produce a result. We will write their type as Nk \u2192 N . We have N = N \u222a {\u22a5} where the undefined value \u22a5 represents non-termination1. The recursive functions are the subset that are everywhere defined. That this subset is not recursively enumerable is shown by a use of Cantor\u2019s diagonalization argument2. Since the partial recursive functions are recursively enumerable it follows that the property of being total (for a partial recursive function) is not recursively decidable.\nBy a separate argument it can be shown that the property for a partial recursive function of being defined at a specified value of its input vector is also not in general recursively decidable. Similarly, Turing machines may not halt and lambda-terms may have no normal form; and these properties are not, respectively, Turing-computable or lambda-definable, as is shown in each case by a simple argument involving self-application.\nThus of perhaps equal importance with Church\u2019s Thesis and which emerges from it is the Halting Theorem: given an arbitrary computation whose result is of type N we cannot in general decide if it is \u22a5. What is actually proven, e.g. of the halts predicate on Turing machines, is that it is not Turing-computable\n1The idea of treating non-termination as a peculiar kind of value, \u22a5, is more recent and was not current at the time of Church and Turing\u2019s foundational work.\n2The proof is purely constructive and doesn\u2019t depend on Church\u2019s Thesis: any effective enumeration, h, of computable functions in N \u2192 N is incomplete - it lacks f(n) = h(n)(n)+1.\n(equiv not lambda-definable etc). It is by an appeal to Church\u2019s Thesis that we pass from this to the assertion that halting is not effectively decidable.\nThe three convergent notions (to which others have since been added) identify an apparently unique, effectively enumerable, class of functions of type Nk \u2192 N corresponding to what is computable by finite but unbounded means. Church\u2019s identification of this class with effective calculability amounts to the conjecture that this is the best we can do.\nIn the case of the Turing machine the unbounded element is the tape (it is initially blank, save for a finite segment but provides an unlimited working store). In the case of the lambda calculus it is the fact that there is no limit to the intermediate size to which a term may grow during the course of its attempted reduction to normal form. In the case of recursive functions it is the minimalization operation, which searches for the smallest n N on which a specified recursive function takes the value 0.\nThe Halting Theorem tells us that unboundedness of the kind needed for computational completeness is effectively inseparable from the possibility of nontermination."
        },
        {
            "heading": "3 The Lambda Calculus",
            "text": "Of the various convergent notions of computability Church\u2019s lambda calculus is distinguished by its combination of simplicity with remarkable expressive power.\nThe lambda calculus was conceived as part of a larger theory, including logical operators such as implication, intended as an alternative foundation for mathematics based on functions rather than sets. This gave rise to paradoxes, including a version of the Russell paradox. What remained with the propositional part stripped out is a consistent theory of pure functions, of which the first systematic exposition is Church (1941)3.\nIn the sketch given here we use for variables lower case letters: a, b, c \u00b7 \u00b7 \u00b7x, y, z and as metavariables denoting terms upper case letters: A,B,C \u00b7 \u00b7 \u00b7. The abstract syntax of the lambda calculus has three productions. A term is one of\nvariable e.g. x\napplication AB\nabstraction \u03bbx.A\nIn the last case \u03bbx. is a binder and free occurrences of x in A become bound . A term in which all variables are bound is said to be closed otherwise it is open. The motivating idea is that closed term represent functions. The intended meaning of AB is the application of function A to argument B while \u03bbx.A is the function which for input x returns A. Terms which are the same except for renaming of bound variables are not distinguished, thus \u03bbx.x and \u03bby.y are the same, identity function.\n3In his monograph Church defines two slightly differing calculi called \u03bbI and \u03bbK, of these \u03bbK is now regarded as canonical and is what we sketch here.\nIn writing terms we freely use parentheses to remove ambiguity. We further adopt the conventions that application is left-associative and that the scope of a binder extends as far to the right as possible. For example f g h means (f g)h and \u03bbx.\u03bby.Ba means \u03bbx.(\u03bby.(Ba)).\nThe calculus has only one essential rule, which shows how to substitute an argument into the body of a function:\n(\u03b2) (\u03bbx.A)B \u2192\u03b2 [B/x]A\nHere [B/x]A means substitute B for free occurrences of x in A. The smallest reflexive, symmetric, transitive, substitutive relation on terms including \u2192\u03b2 , written \u21d4, is Church\u2019s notion of \u03bb\u2212conversion. If we omit symmetry from the definition we get an oriented relation, written \u21d2, called reduction.\nAn instance of the left hand side of rule \u03b2 is called a redex . A term containing no redex is said to be in normal form. A term which is convertible to one in normal form is said to be normalizing . There are non-normalizing terms, of which perhaps the simplest is (\u03bbx.xx)(\u03bbx.xx). We have the cyclic\n(\u03bbx.xx)(\u03bbx.xx)\u2192\u03b2 (\u03bbx.xx)(\u03bbx.xx)\nas the only available step. The two most important technical results are\nChurch-Rosser Theorem If A\u21d4 B there is a term C such that A\u21d2 C and B \u21d2 C. An immediate consequence of this is that the normal form of a normalizing term is unique4.\nNormal Order Theorem Stated informally: the normal form of a normalizing term can be found by repeatedly reducing its leftmost redex5.\nTo see the significance of the normal order theorem consider the term\n(\u03bby.z)((\u03bbx.xx)(\u03bbx.xx))\nWe have (\u03bby.z)((\u03bbx.xx)(\u03bbx.xx))\u2192\u03b2 z\nwhich is the normal form. But if we try to reduce the argument ((\u03bbx.xx)(\u03bbx.xx)) to normal form first, we get stuck in an endless loop.\nIn general there are many ways of reducing a term, since it or one of its reducts may contain multiple redexes. The normal order theorem gives a sequential procedure, normal order reduction , which is guaranteed to reach the normal form if there is one. Note that normal order reduction substitutes arguments into function bodies without first reducing any redexes inside the argument, which amounts to lazy evaluation.\n4This means unique up to changes of bound variable, of course. 5In case of nested redexes, leftmost is usually defined as leftmost-outermost, although the\ntheorem will still hold if we take leftmost-innermost.\nA closed term of pure6 \u03bb-calculus is called a combinator . Note that any normalizing closed term of pure \u03bb-calculus must reduce to an abstraction. Some combinators with their conventional names are:\nS = \u03bbx.\u03bby.\u03bbz.xz(yz)\nK = \u03bbx.\u03bby.x\nI = \u03bbx.x\nB = \u03bbx.\u03bby.\u03bbz.x(yz)\nC = \u03bbx.\u03bby.\u03bbz.xzy\nIt is evident that \u03bb-calculus has a rich collection of functions, including functions of higher type, that is whose arguments and/or results are functions, but since (at least closed) terms can denote only functions and never ground objects it remains to show how to represent data such as the natural numbers. Here are the Church numerals\n0 = \u03bba.\u03bbb.b\n1 = \u03bba.\u03bbb.ab\n2 = \u03bba.\u03bbb.a(ab)\n3 = \u03bba.\u03bbb.a(a(ab))\netc. \u00b7 \u00b7 \u00b7\nTo understand this representation for numbers note the effect of applying a Church numeral to function f and object a:\n0fa \u21d4 a 1fa \u21d4 fa 2fa \u21d4 f(fa) 3fa \u21d4 f(f(fa))\nThe numbers are thus represented as iterators. It is now straightforward to define the arithmetic operations, for example\n+ = \u03bbm.\u03bbn.\u03bba.\u03bbb.ma(nab)\n\u00d7 = \u03bbm.\u03bbn.\u03bba.\u03bbb.m(na)b\npredecessor and subtraction are a little trickier, see Church (1941). We also need a way to branch on 0:\nzero = \u03bba.\u03bbb.\u03bbn.n(Kb)a\n6Pure means using only variables and no proper constants, as \u03bb-calculus is presented here.\nWe have\nzero A B N \u21d4 A, N \u21d4 0 \u21d4 B, N \u21d4 n+ 1\nThe master-stroke, which shows every recursive function to be \u03bb-definable is to find a universal fixpoint operator, that is a term Y with the property that for any term F , Y F \u21d4 F (Y F ) There are many such terms, of which the simplest is due to H.B.Curry.\nY = \u03bbf.(\u03bbx.f(xx))(\u03bbx.f(xx))\nThe reader may satisfy himself that we have Y F \u21d4 F (Y F ) as required. The beauty of \u03bb-definability as a theory of computation is that it gives not only \u2014 assuming Church\u2019s Thesis \u2014 all computable functions of type N \u2192 N but also those of higher type of any finite degree, such as (N \u2192 N) \u2192 N , (N \u2192 N)\u2192 (N \u2192 N) and so on.\nMoreover we are not limited to arithmetic. The idea behind the Church numerals is very general and allows any data type \u2014 pairs, lists, trees and so on \u2014 to be represented in a purely functional way. Each datum is encoded as a function that captures its elimination operation, that is the way in which information is extracted from it during computation. It is also possible to represent codata, such as infinite lists, infinitary trees and so on.\nPart of the simplicity of the calculus lies in its considering only functions of a single argument. This is no real restriction since it is a basic result of set theory that for any sets A, B, the function spaces (A \u00d7 B) \u2192 C and A \u2192 (B \u2192 C) are isomorphic. Replacing the first by the second is called Currying7. We have made implicit use of this idea all along, e.g. + is curried addition."
        },
        {
            "heading": "Solvability and non-strictness",
            "text": "A non-normalizing term is by no means necessarily useless. An example is Y , which has no normal form but can produce one when applied to another term. On the other hand (\u03bbx.xx)(\u03bbx.xx) is irredeemable \u2014 there is no term and no sequence of terms to which it can be applied and yield a normal form.\nDefinition: a term T is SOLVABLE if there are terms A1, \u00b7 \u00b7 \u00b7 , Ak for some k \u2265 0 such that TA1 \u00b7 \u00b7 \u00b7Ak is normalizing. Thus Y is solvable because we have for example Y (\u03bbx.\u03bby.y)\u21d4 (\u03bby.y) whereas (\u03bbx.xx)(\u03bbx.xx) is unsolvable.\nAn important result, due to Christopher Wadsworth (1976), is that a term is solvable if and only if it can be reduced to head normal form:\n\u03bbx1 \u00b7 \u00b7 \u00b7\u03bbxn.xkA1 \u00b7 \u00b7 \u00b7Am 7After H.B.Curry, although the idea was first used in Scho\u0308nfinkel (1924).\nthe variable xk is called the head and if the term is closed must be one of the x1 \u00b7 \u00b7 \u00b7xn. If a term is solvable normal order reduction will reduce it to HNF in a finite number of steps8.\nAll unsolvable terms are equally useless, so we can think of them as being equivalent and introduce a special term \u22a5 to represent them. This gives us an extension of \u21d4 for which we will use \u2261. The two fundamental properties of \u22a5, which follow from the definitions of unsolvability and head normal form, are:\n\u22a5 A \u2261 \u22a5 (1) \u03bbx.\u22a5 \u2261 \u22a5 (2)\nIntroducing \u22a5 allows an ordering relation to be defined on terms with \u22a5 as least element and a stronger equivalence relation using limits which is studied in domain theory (see later). We make one further remark here.\nDefinition: a term A is STRICT if\nA \u22a5 \u2261 \u22a5\nand non-strict otherwise. A strict function thus has \u22a5 for a fixpoint and applying Y to it will produce \u22a5. So non-strict functions play an essential role in the theory of \u03bb-definability \u2013 without them we could not use Y to encode recursion."
        },
        {
            "heading": "Combinatory Logic",
            "text": "Closely related to \u03bb-calculus is combinatory logic, originally due to Scho\u0308nfinkel (1924) and subsequently explored by H.B.Curry. This has meagre apparatus indeed \u2014 just application and a small collection of named combinators. These are defined by stating their reduction rule. In the minimal version we have two combinators, defined as follows\nS x y z \u21d2 x z(y z)\nK x y \u21d2 x\nhere x, y, z are metavariables standing for arbitrary terms and are used to state the reduction rules. Combinatory logic terms have no variables and are built using only constants and application:, e.g. K(SKK).\nA central result, perhaps one of the strangest in all of logic, is that every \u03bb-definable function can be written using only S and K. Here is a start"
        },
        {
            "heading": "I = SKK",
            "text": "The proof is by considering application to an arbitrary term. We have\nSKKx\u21d2 Kx(Kx)\u21d2 x\nas required.\n8In the published version of the paper Wadsworth\u2019s result on solvablity and hnf was erroneously attributed to Corrado Bo\u0308hm.\nThe definitive study of combinatory logic and its relationship to lambda calculus is Curry & Feys (1958). There are several algorithms for transcribing \u03bb-terms to combinators and for convenience most of these use besides S, K, additional combinators such as B, C, I etc.\nIt would seem that only a dedicated cryptologist would choose to write other than very small programs directly in combinatory logic. However, Turner (1979a) describes compilation to combinators as an implementation method for a high-level functional programming language. This required finding a translation algorithm, described in Turner (1979b), that produces compact combinator code when translating expressions containing many nested \u03bb-abstractions. The attraction of the method is that combinator reduction rules are much simpler than \u03b2-reduction, each requiring only a few machine instructions, allowing a fast interpreter to be constructed which carries out normal order reduction."
        },
        {
            "heading": "The paradox",
            "text": "It is easy to see why the original versions of \u03bb-calculus and combinatory logic, which included properly logical notions, led to paradoxes. (Curry calls these theories illative.) The untyped theory is too powerful, because of the fixpoint combinator, Y . Suppose N is a term denoting logical negation. We have\nY N \u21d4 N(Y N)\nwhich is the Russell paradox. Even minimal logic, which lacks negation, becomes inconsistent in the presence of Y \u2014 implication is sufficient to generate the paradox, see Barendregt (1984) p575. Because of this Y is sometimes called Curry\u2019s paradoxical combinator ."
        },
        {
            "heading": "Typed \u03bb-calculi",
            "text": "The \u03bb-calculus of Church (1941) is untyped : it allows the promiscuous application of any term to any other, so types arise only in the interpretation of terms. In a typed \u03bb-calculus the rules of term formation embody some theory of types. Only terms which are well-typed according to the theory are permitted. The rules for \u03b2 reduction remain unchanged, as does the Church-Rosser Theorem. Most type systems disallow self-application, as in (\u03bbx.xx), preventing the formation of a fixpoint combinator like Curry\u2019s Y . Typed \u03bb-calculi fall into two main groups depending on what is done about this\n(i) Add an explicit fixpoint construction to the calculus - for example a polymorphic constant Y of type schema (\u03b1 \u2192 \u03b1) \u2192 \u03b1, with reduction rule Y H \u21d2 H(Y H). This allows general recursion at every type and thus retains the computational completeness of untyped \u03bb.\n(ii) In the other kind of typed \u03bb-calculus there is no fixpoint construct and every term is normalizing . This brings into play a fundamental isomorphism between programming and logic: the Propositions-as-Types principle.\nThis gives two apparently very different models of functional programming, which we discuss in the next two sections."
        },
        {
            "heading": "4 Lazy Functional Programming",
            "text": "Imperative programming languages, from the earliest such as FORTRAN and COBOL which emerged in the 1950\u2019s to current \u201dobject-oriented\u201d ones such as C++ and Java have certain features in common. Their basic action is the assignment command, which changes the content of a location in memory and they have an explicit flow of control by which these state changes are ordered. This reflects more or less directly the structure of the Turing/von Neumann computer, as a central processing unit operating on a passive store. Backus (1978) calls them \u201dvon Neumann languages\u201d.\nFunctional9 programming languages offer a radical alternative \u2014 they are descriptive rather than imperative, have no assignment command and no explicit flow of control \u2014 sub-computations are ordered only partially, by data dependency.\nThe claimed merits of functional programming \u2014 in conciseness, mathematical tractability, potential for parallel execution \u2014 have been argued in many places so we will not dwell on them here. Nor will we go into the history of the concept, other than to say that the basic ideas go back over four decades, see in particular the important early papers of McCarthy (1960), Landin (1966) \u2014 and that for a long period functional programming was mainly practised in imperative languages with functional subsets (LISP, Scheme, Standard ML).\nThe disadvantages of functional programming within a language that includes imperative features are two. First, you are not forced to explore the limits of the functional style, since you can escape at will into an imperative idiom. Second, the presence of side effects, exceptions etc., even if they are rarely used , invalidate important theorems on which the benefits of the style rest.\nThe \u03bb-calculus is the most natural candidate for functional programming: it is computationally complete in the sense of Church\u2019s Thesis, it includes functions of higher type and it comes with a theory of \u03bb-conversion that provides a basis for reasoning about program transformation, correctness of evaluation mechanisms and so on. The notation is a little spartan for most tastes but it was shown long ago by Peter Landin that the dish can be sweetened by adding a sprinkling of syntactic sugar10.\n9We here use functional to mean what some call purely functional, an older term for this is applicative, yet another term which includes other mathematically based models, such as logic programming, is declarative.\n10The phrase syntactic sugar is due to Strachey, as are other evocative terms and concepts in programming language theory."
        },
        {
            "heading": "Efficient Normal Order Reduction",
            "text": "The Normal Order Theorem tells us that an implementation of \u03bb-calculus on a sequential machine should use normal order reduction11, otherwise it may fail to find the normal form of a normalizing term. This requires that arguments be substituted unevaluated into function bodies as we noted earlier. In general this will produce multiple copies of the argument, requiring any redexes it contains to be reduced multiple times. For \u03bb-calculus-based functional programming to be a viable technology it is necessary to have an efficient way of handling this.\nA key step was the invention of normal graph reduction, by Wadsworth (1971). In this scheme the term is held as a directed acyclic graph, and the result of \u03b2-reduction is that a single copy of the argument is retained, with the function body containing multiple pointers to it. As a consequence any redexes in the argument are reduced at most once.\nTurner adapted this idea to graph reduction on S,K, I, etc. combinators, allowing a much simpler abstract machine. In Turner\u2019s scheme the graph may be cyclic, permitting a more compact representation of recursion. The reduction rule for the Y combinator, Y H \u21d2 H (Y H), creates a loop in the graph, increasing the amount of sharing. The combinators are a target code for a compiler for compilation from a high level functional language. Initially this was SASL (Turner 1976) and in later incarnations of the system, Miranda.\nWhile using a set of combinators fixed in advance is a good solution if graph reduction is to be carried out by an interpreter, if the final target of compilation is to be native code on conventional hardware it is advantageous to use the \u03bbabstractions present (explicitly or implicitly) in the program source as the combinators whose reduction rules are to be implemented. This requires a sourceto-source transformation called \u03bb-lifting , Hughes (1983), Johnsson (1985). This method was first used in the compiler of LML, a lazy version of the functional subset of ML, written by Lennart Augustsson & Thomas Johnsson at Chalmers University in Sweden, around 1984. Their model for mapping graph reduction onto conventional hardware, the G machine, has since been further refined, leading to the optimized model of Simon Peyton Jones (1992).\nThus over a period of two decades normal order functional languages have been implemented with increasing efficiency."
        },
        {
            "heading": "Miranda",
            "text": "Miranda is a functional language designed by David Turner in 1983-6 and is a sugaring of a typed \u03bb-calculus with a universal fixpoint operator. There are no explicit \u03bb\u2019s \u2014 instead we have function definition by equations and local definitions with where. The insight that one can have \u03bb-calculus without \u03bb goes back to Peter Landin (1966) and his ISWIM notation. Neither is the user required to mark recursive definitions as such - the compiler analyses the call graph and inserts Y where it is required.\n11Except where prior analysis of the program shows it can be avoided, a process known as strictness analysis.\nThe use of normal order reduction (aka lazy evaluation) and non-strict functions has a very pervasive effect. It supports a more mathematical style of programming, in which infinite data structures can be described and used and, which is most important, permits communicating processes and input/output to be programmed in a purely functional manner.\nMiranda is based on the earlier lazy functional language SASL (Turner, 1976) with the addition of the system of polymorphic strong typing of Milner (1978). For an overview of Miranda see Turner (1986).\nMiranda doesn\u2019t use Church numerals for its arithmetic \u2014 modern computers have fast fixed and floating point arithmetic units and it would be perverse not to take advantage of them. Arithmetic operations on unbounded size integers and 64bit floating point numbers are provided as primitives.\nIn place of the second order representation of data used within the pure untyped lambda calculus we have algebraic type definitions. For example\nbool ::= False | True nat ::= Zero | Suc nat tree ::= Leaf nat | Fork tree tree\nIntroducing new data types in this way is in fact better than using second order impredicative definitions for two reasons: you get clearer and more specific type error messages if you misuse them \u2014 and each algebraic type comes with a principle of induction which can be read off from the definition. The analysis of data is by pattern matching, for example\nflatten :: tree -> [nat] flatten (Leaf n) = [n] flatten (Fork x y) = flatten x ++ flatten y\nThe type specification of flatten is optional as the compiler is able to deduce this; ++ is list concatenation.\nThere is a rich vocabulary of standard functions for list processing, map, filter, foldl, foldr, etc. and a notation, called list comprehension that gives concise expression to a useful class of iterations.\nMiranda was widely used for teaching and for about a decade following its initial release by Research Software Ltd in 1985-6 provided a de facto standard for pure functional programming, being taken up by over 200 universities. The fact that it was interpreted rather than compiled limited its use outside education, but several significant industrial projects were successfully undertaken using Miranda, see for example Major et. al. (1991) and Page & Moe (1993).\nHaskell, a successor language designed by a committee, includes many extensions, of which the most important are type classes and monadic input-output. The language remains purely functional, however. For a detailed description see S. L. Peyton Jones (2003). Available implementations of Haskell include, besides an interpreter suitable for educational use, native code compilers. This makes Haskell a viable choice for production use in a range of areas.\nThe fact that people are able to write large programs for serious applications in a language, like Miranda or Haskell, that is essentially a sugaring of \u03bb-calculus is in itself a vindication of Church\u2019s Thesis."
        },
        {
            "heading": "Domain Theory",
            "text": "The mathematical theory which explains programming languages with general recursion is Scott\u2019s domain theory.\nThe typed \u03bb-calculus looks as though it ought to have a set-theoretic model, in which types denote sets and \u03bb-abstractions denote functions. But the fixpoint operator Y is problematic. It is not the case in set theory that every function f A\u2192 A has a fixpoint in A.\nThere is second kind of fixpoint to be explained, at the level of types. We can define recursive algebraic data types, like (we are here using Miranda notation):\nbig ::= Leaf nat | Node (big -> big)\nThis appears to require a set with the property\nBig \u223c= N + (Big \u2192 Big)\nwhich is impossible on cardinality grounds. Dana Scott\u2019s domain theory solves both these problems. A domain is a complete partial order: a set with a least element, \u22a5, representing non-termination, and limits of ascending chains (or more generally of directed sets). The function space A \u2192 B for domains A, B, is defined to contain just the continuous functions from A to B and this is itself a domain. Continuous means preserving limits. The continuous functions are also monotonic (= order preserving). For a complete partial order, D, each monotonic function f D \u2192 D has a least fixed point, \u2294\u221e n=0 f\nn\u22a5. A plain set, like N can be turned into a domain by adding \u22a5, to get N . Further, domain equations, like D \u223c= N + (D \u00d7D), D \u223c= N + (D \u2192 D) and so on, all have solutions. The details can be found in Scott (1976) or Abramsky & Jung (1994). This includes that there is a non-trivial12 domain D\u221e with D\u221e \u223c= D\u221e \u2192 D\u221e providing a semantic model for Church\u2019s untyped \u03bb-calculus.\nDomain theory was originally developed to underpin denotational semantics, Christopher Strachey\u2019s project to formalize semantic descriptions of real programming languages using a typed \u03bb-calculus as the metalanguage (see Strachey, 1967, Strachey & Scott, 1971). Strachey\u2019s semantic equations made frequent use of Y to explain control structures such as loops and also required recursive type equations to account for the domains of the various semantic functions. It was during Scott\u2019s collaboration with Strachey in the period around 1970 that domain theory emerged.\nFunctional programming in non-strict languages like Miranda and Haskell is essentially programming directly in the metalanguage of denotational semantics.\n12The one-point domain, with \u22a5 for its only element, if allowed, would be a trivial solution.\nComputability at higher types, revisited\nDana Scott once remarked that \u03bb-calculus is only an algebra, not a calculus. With domain theory and proofs using limits we get a genuine calculus, allowing many new results.\nStudying a typed functional language with arithmetic, Plotkin (1977) showed that if we consider functions of higher type where we allow inputs as well as outputs to be \u22a5, there are computable functions which are not \u03bb-definable. Using domain B where B = {True, False}, two examples are:\nOr B \u2192 B \u2192 B where Or x y is True if either x or y is True\nExists (N \u2192 B)\u2192 B where Exists f is True when \u2203i N.f i = True\nThis complete or parallel Or must interleave two computations, since either of its inputs may be \u22a5. Exists is a multi-way generalization.\nWhat we get from untyped \u03bb-calculus, or a typed calculus withN and general recursion, are the sequential functions. To get all computable partial functions at every type we must add primitives expressing interleaving or concurrency. In fact just the two above are sufficient.\nThis becomes important for programming with exact real numbers, an active area of research. Martin Escardo (1996) shows that a \u03bb-calculus with a small number of primitives including Exists can express every computable function of analysis, including those of higher type, e.g. differentiation and integration."
        },
        {
            "heading": "5 Strong Functional Programming",
            "text": "There is an extended family of typed \u03bb-calculi, all without Y or any other method of expressing general recursion, in which every term is normalizing. The family includes\nsimply typed \u03bb-calculus \u2014 this is a family in itself\nGirard\u2019s system F (1971), also known as the second order \u03bb-calculus (we consider here the Church-style or explicitly typed version)\nCoquand & Huet\u2019s calculus of constructions (1988)\nMartin-Lo\u0308f\u2019s intuitionist theory of types (1973)\nIn a change of convention we will use upper case letters A,B,C \u00b7 \u00b7 \u00b7 for types and lower case letters a, b, c \u00b7 \u00b7 \u00b7 for terms, reserving x, y, z, for \u03bb-calculus variables (this somewhat makeshift convention will be adequate for a short discussion).\nIn addition to the usual conversion and reduction relations, \u21d4,\u21d2, these theories have a judgement of well-typing , written a : A which says that term a has type A (which may or may not be unique).\nAll the theories share the following properties:\nChurch-Rosser If a\u21d4 b there is a term c such that a\u21d2 c and b\u21d2 c.\nDecidability of well-typing This what is meant by saying that a programming language or formalism is strongly typed (aka staticly typed).\nStrongly normalizing Every well-typed term is normalizing and every reduction sequence terminates in a normal form.\nUniqueness of normal forms Immediate from Church-Rosser.\nDecidability of \u21d4 on well-typed terms From the two previous properties \u2014 reduce both sides to normal form and see if they are equal.\nNote that decidability of the well typing judgment, a : A, is not the same as type inference. The latter means that given an a we can find an A with a : A, or determine that there isn\u2019t one. The simply typed \u03bb-calculus has type inference (in fact with most general types) but none of the stronger theories do.\nThe first two properties in the list are shared with other well-behaved typed functional calculi, including those with general recursion. So the distinguishing property here is strong normalization. Programming in a language of this kind has important differences from the more familiar kind of functional programming. For want of any hitherto agreed name, we can call it strong functional programming13.\nAn obvious difference is that all evaluations terminate14, so we do not have to worry about \u22a5. It is clear that such a language cannot be computationally complete \u2014 there will be always-terminating computable functions it cannot express (and one of these will be the interpreter for the language itself). It should not be inferred that a strongly normalizing language must therefore be computationally weak. Even simple typed lambda calculus, equipped with N as a base type and primitive recursion, can express every recursive function of arithmetic whose totality is provable in first order number theory (a result due to Go\u0308del, 1958). A proposed elementary functional programming system along these lines, but including codata as well as data, is discussed in Turner (2004).\nA less obvious but most striking consequence of strongly normalization is a new and unexpected interface between \u03bb-calculus and logic. We show how this works by considering the simplest calculus of this class."
        },
        {
            "heading": "Propositions-as-Types",
            "text": "The simply typed \u03bb-calculus (STLC) has for its types the closure under \u2192 of a set of base types, which we will leave unspecified. As before we use A,B,C \u00b7 \u00b7 \u00b7 as variables ranging over types. We can associate with each closed term a type schema, for example\n\u03bbx.x : A\u2192 A 13Another possible term is \u201ctotal functional programming\u201d, although this has the disadvantage of encouraging the unfortunate term \u201ctotal function\u201d (redundant because it is part of the definition function that it is everywhere defined on its domain).\n14This seems to rule out indefinitely proceeding processes, such as an operating system, but we can include these by allowing codata and corecursion, see eg Turner (2004).\nThe function \u03bbx.x has many types but they are all instances of A \u2192 A, which is its most general type.\nA congruence first noticed by Curry in the 1950\u2019s is that the types of closed terms in STLC correspond to tautologies of intuitionist propositional logic, if we read \u2192 as implication, e.g. A \u2192 A is a tautology. The correspondence is exact, for example A \u2192 B is not a tautology and neither can we make any closed term of this type. Further, the most general types of the combinators s = \u03bbx.\u03bby.\u03bbz.xz(yz) and k = \u03bbx.\u03bby.x are\ns : ((A\u2192 (B \u2192 C))\u2192 ((A\u2192 B)\u2192 (A\u2192 C))\nk : A\u2192 (B \u2192 A)\nand these formulae are the two standard axioms for the intuitionist theory of implication: every other tautology in \u2192 can be derived from them by modus ponens. What is going on here?\nLet us look at the rules for forming well-typed terms of simply typed \u03bb.\n(x : A) c : A\u2192 B b : B a : A\n\u03bbx.b : A\u2192 B c a : B\nOn the left15 we have the rule for abstraction, on the right that for application. If we look only at the types and ignore the terms, these are the introduction and elimination rules for implication in a natural deduction system. So naturally, the formulae we can derive using these rules are all and only the tautologies of the intuitionist theory of implication16.\nIn the logical reading, the terms on the left of the colons provide witnessing information \u2013 they record how the formula on the right was proved. The judgement a : A thus has two readings \u2014 that term a has type A, but also that proof-object or witness a proves proposition A.\nThe correspondence readily extends to the other connectives of propositional logic by adding some more type constructors to SLTC besides \u2192. The type of pairs, cartesian product, A \u00d7 B, corresponds to the conjunction A \u2227 B. The disjoint union type, A \u2295 B, corresponds to the disjunction A \u2228 B. The empty type corresponds to the absurd (or False) proposition, which has no proof.\nThis Curry-Howard isomorphism between types and propositions is jointly attributed to Curry (1958) and to W. Howard (1969), who showed how it extended to all the connectives of intuitionist logic including the quantifiers. It is at the same time an isomorphism between terminating programs and constructive (or intuitionistic) proofs.\n15The left hand rule says that if from assumption x : A we can derive b : B then we can derive what is under the line.\n16The classical theory of implication includes additional tautologies dependant on the law of the excluded middle \u2014 the leading example is ((A\u2192 B) \u2192 A) \u2192 A, Pierce\u2019s law."
        },
        {
            "heading": "The Constructive Theory of Types",
            "text": "Per Martin-Lo\u0308f (1973) formalizes a proposed foundational language for constructive mathematics based on the isomorphism. The Intuitionist (or Constructive) Theory of Types is at one and the same time a higher order logic and a theory of types, providing for constructive mathematics what for classical mathematics is done by set theory. It provides a unified notation in which to write functions, types, propositions and proofs.\nUnlike the constructive set theory of Myhill (1975), Martin-Lo\u0308f type theory includes a principle of choice (not as an axiom, it is provable within the theory). It seems that the source of the non-constructivities of set theory is not the choice principle, which for Martin-Lo\u0308f is constructively valid, but the axiom of separation, a principle which is noticeably absent from type theory17 18.\nConstructive type theory is both a theory of constructive mathematics and a strongly typed functional programming language. Verifying the validity of proofs is the same process as type checking. Martin-Lof (1982) writes\nI do not think that the search for high level programming languages that are more and more satisfactory from a logical point of view can stop short of anything but a language in which all of constructive mathematics can be expressed.\nThere exist by now a number of different versions of the theory, including several computer-based implementations, of which perhaps the longest established is NuPRL (Constable et al. 1986).\nAn alternative impredicative theory, also based on the Curry-Howard isomorphism, is Coquand and Huet\u2019s Calculus of Constructions (1988) which provides the basis for the COQ proof system developed at INRIA."
        },
        {
            "heading": "6 Type Theory with Partial Types",
            "text": "Being strongly normalizing, constructive type theory cannot be computationally complete. Moreover we might like to reason about partial functions and general recursion using this powerful logic. Is it possible to somehow unify type theory with a constructive version of Dana Scott\u2019s domain theory?\nIn his PhD thesis Scott F. Smith (1989) investigated adding partial types to the type theory of NuPRL. The idea can be sketched briefly as follows. For each ordinary type T there is a partial type T of T -computations, whose elements include those of T and a divergent element, \u22a5. For partial types (only) there is a fixpoint operator, fix : (T \u2192 T ) \u2192 T . This allows the definition of general recursive functions.\n17Note that Goodman & Myhill\u2019s (1978) proof that Choice implies Excluded Middle makes use of an instance of the Axiom of Separation. The title should be Choice + Separation implies Excluded Middle.\n18The frequent proposals to \u201cimprove\u201d CTT by adding a subtyping constructor should therefore be viewed with suspicion.\nThe constructive account of partial types is significantly different from the classical account given by domain theory. For example we cannot assert\n\u2200x : T . x T \u2228 x = \u22a5\nbecause constructively this implies an effective solution to the halting problem for T . A number of intriguing theorems emerge. Certain non-computability results can be established absolutely , that is independently of Church\u2019s Thesis, see Constable & Smith (1988)19. Further, the logic of the host type theory is altered so that it is no longer compatible with classical logic \u2014 some instances of the law of the excluded middle, of the form \u2200x.P (x)\u2228\u00acP (x) can be disproved.\nTo recapture domain theory requires something more than T and fix, namely a second order fixpoint operator, FIX, that solves recursive equations in partial types. As far as the present author is aware, noone has yet shown how to do this within the logic of type theory. This would unify the two theories of functional programming. Among other benefits it would allow us to give within type theory a constructive account of the denotational semantics of recursive programming languages.\nAlmost certainly relevant here is Paul Taylor\u2019s Abstract Stone Duality (2002), a computational approach to topology. The simplest partial type is Sierpinski space, \u03a3, which has only one point other than \u22a5. This plays a special role in Taylor\u2019s theory: the open sets of a space X are the functions in X \u2192 \u03a3 and can be written as \u03bb-terms. ASD embraces both traditional spaces like the reals and Scott domains (topologically these are non-Hausdorff spaces)."
        },
        {
            "heading": "CONCLUSION",
            "text": "Church\u2019s Thesis played a founding role in computing theory by providing a single notion of effective computability. Without this foundation we might have been stuck with a plethora of notions of computability depending on computer architecture, programming language etc.: we might have Motorola-computable versus Intel-computable, Java-computable versus C-computable and so on.\nThe \u03bb-calculus, which Church developed during the period of convergence from which the Thesis emerged, has influenced almost every aspect of the development of programming and programming languages. It is the basis of functional programming, which after a long infancy is entering adulthood as a practical alternative to traditional ad-hoc imperative programming languages. Many important ideas in mainstream programming languages \u2014 recursion, procedures as parameters, linked lists and trees, garbage collectors \u2014 came by cross fertilization from functional programming. Moreover the main schools of both\n19The paper misleadingly claims that among these is the Halting Theorem, which would be remarkable. What is in fact proved is the extensional halting theorem, which is already provable in domain theory, trivially from monotonicity. The real Halting Theorem is intensional , in that the halting function whose existence is to be disproved is allowed access to the internal structure of the term, by being given its Go\u0308del number.\noperational and denotational semantics are \u03bb-calculus based and amount to using functional programming to explain other programming systems.\nThe original project from whose wreckage by paradox \u03bb-calculus survived, to unify logic with an account of computable functions, appears to have been reborn in unexpected form, via the propositions-as-types paradigm. Further exciting developments undoubtedly lie ahead and ideas from Church\u2019s \u03bb-calculus will continue to be central to them.\n[last revised 18.04.2021]"
        }
    ],
    "title": "Church\u2019s Thesis and Functional Programming",
    "year": 2021
}