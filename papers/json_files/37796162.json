{
    "abstractText": "Super resolution is the problem of artificially enlarging a low resolution photograph to recover a plausible high resolution version. In the regime of high magnification factors, the problem is dramatically underspecified and many plausible, high resolution images may match a given low resolution image. In particular, traditional super resolution techniques fail in this regime due to the multimodality of the problem and strong prior information that must be imposed on image synthesis to produce plausible high resolution images. In this work we propose a new probabilistic deep network architecture, a pixel recursive super resolution model, that is an extension of PixelCNNs to address this problem. We demonstrate that this model produces a diversity of plausible high resolution images at large magnification factors. Furthermore, in human evaluation studies we demonstrate how previous methods fail to fool human observers. However, high resolution images sampled from this probabilistic deep network do fool a naive human observer a significant fraction of the time.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ryan Dahl"
        },
        {
            "affiliations": [],
            "name": "Mohammad Norouzi"
        },
        {
            "affiliations": [],
            "name": "Jonathon Shlens"
        }
    ],
    "id": "SP:b55bacb173019f6a6d72d916a0b27803d1820dbc",
    "references": [
        {
            "authors": [
                "M. Abadi",
                "A. Agarwal",
                "P. Barham",
                "E. Brevdo",
                "Z. Chen",
                "C. Citro",
                "G.S. Corrado",
                "A. Davis",
                "J. Dean",
                "M. Devin",
                "S. Ghemawat",
                "I. Goodfellow",
                "A. Harp",
                "G. Irving",
                "M. Isard",
                "Y. Jia",
                "R. Jozefowicz",
                "L. Kaiser",
                "M. Kudlur",
                "J. Levenberg",
                "D. Man\u00e9",
                "R. Monga",
                "S. Moore",
                "D. Murray",
                "C. Olah",
                "M. Schuster",
                "J. Shlens",
                "B. Steiner",
                "I. Sutskever",
                "K. Talwar",
                "P. Tucker",
                "V. Vanhoucke",
                "V. Vasudevan",
                "F. Vi\u00e9gas",
                "O. Vinyals",
                "P. Warden",
                "M. Wattenberg",
                "M. Wicke",
                "Y. Yu"
            ],
            "title": "and X",
            "venue": "Zheng. Tensor- Flow: Large-scale machine learning on heterogeneous systems",
            "year": 2015
        },
        {
            "authors": [
                "M. Aharon",
                "M. Elad",
                "A. Bruckstein"
            ],
            "title": "Svdd: An algorithm for designing overcomplete dictionaries for sparse representation",
            "venue": "Trans. Sig. Proc.,",
            "year": 2006
        },
        {
            "authors": [
                "S.R. Bowman",
                "L. Vilnis",
                "O. Vinyals",
                "A.M. Dai",
                "R. J\u00f3zefowicz",
                "S. Bengio"
            ],
            "title": "Generating sentences from a continuous space",
            "venue": "CoRR, abs/1511.06349",
            "year": 2015
        },
        {
            "authors": [
                "J. Bruna",
                "P. Sprechmann",
                "Y. LeCun"
            ],
            "title": "Super-resolution with deep convolutional sufficient statistics",
            "venue": "CoRR, abs/1511.05666",
            "year": 2015
        },
        {
            "authors": [
                "E.L. Denton",
                "S. Chintala",
                "A. Szlam",
                "R. Fergus"
            ],
            "title": "Deep generative image models using a laplacian pyramid of adversarial networks",
            "venue": "NIPS",
            "year": 2015
        },
        {
            "authors": [
                "A. Deshpande",
                "J. Lu",
                "M. Yeh",
                "D.A. Forsyth"
            ],
            "title": "Learning diverse image colorization",
            "venue": "CoRR, abs/1612.01958",
            "year": 2016
        },
        {
            "authors": [
                "C. Dong",
                "C.C. Loy",
                "K. He",
                "X. Tang"
            ],
            "title": "Image superresolution using deep convolutional networks",
            "venue": "CoRR, abs/1501.00092",
            "year": 2015
        },
        {
            "authors": [
                "R. Fattal"
            ],
            "title": "Image upsampling via imposed edge statistics",
            "venue": "ACM Trans. Graph.,",
            "year": 2007
        },
        {
            "authors": [
                "W.T. Freeman",
                "T.R. Jones",
                "E.C. Pasztor"
            ],
            "title": "Examplebased super-resolution",
            "venue": "IEEE Computer graphics and Applications",
            "year": 2002
        },
        {
            "authors": [
                "W.T. Freeman",
                "E.C. Pasztor"
            ],
            "title": "Markov networks for superresolution",
            "venue": "CISS",
            "year": 2000
        },
        {
            "authors": [
                "D. Garcia"
            ],
            "title": "srez: Adversarial super resolution",
            "venue": "https:// github.com/david-gpu/srez",
            "year": 2016
        },
        {
            "authors": [
                "L.A. Gatys",
                "A.S. Ecker",
                "M. Bethge"
            ],
            "title": "A neural algorithm of artistic style",
            "venue": "CoRR, abs/1508.06576",
            "year": 2015
        },
        {
            "authors": [
                "I. Goodfellow",
                "J. Pouget-Abadie",
                "M. Mirza",
                "B. Xu",
                "D. Warde-Farley",
                "S. Ozair",
                "A. Courville"
            ],
            "title": "and Y",
            "venue": "Bengio. Generative adversarial nets",
            "year": 2014
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "CVPR",
            "year": 2015
        },
        {
            "authors": [
                "H. Hou",
                "H. Andrews"
            ],
            "title": "Cubic splines for image interpolation and digital filtering",
            "venue": "Acoustics, Speech and Signal Processing, IEEE Transactions on,",
            "year": 2003
        },
        {
            "authors": [
                "J. Huang",
                "D. Mumford"
            ],
            "title": "Statistics of natural images and models",
            "venue": "Computer Vision and Pattern Recognition, 1999. IEEE Computer Society Conference on., volume 1. IEEE",
            "year": 1999
        },
        {
            "authors": [
                "J.-B. Huang",
                "A. Singh",
                "N. Ahuja"
            ],
            "title": "Single image superresolution from transformed self-exemplars",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition)",
            "year": 2015
        },
        {
            "authors": [
                "J. Johnson",
                "A. Alahi",
                "F. Li"
            ],
            "title": "Perceptual losses for real-time style transfer and super-resolution",
            "venue": "CoRR, abs/1603.08155",
            "year": 2016
        },
        {
            "authors": [
                "C. Kaae S\u00f8nderby",
                "J. Caballero",
                "L. Theis",
                "W. Shi",
                "F. Husz\u00e1r"
            ],
            "title": "Amortised MAP Inference for Image Superresolution",
            "venue": "ArXiv e-prints,",
            "year": 2016
        },
        {
            "authors": [
                "A. Karpathy",
                "G. Toderici",
                "S. Shetty",
                "T. Leung",
                "R. Sukthankar",
                "L. Fei-Fei"
            ],
            "title": "Large-scale video classification with convolutional neural networks",
            "venue": "CVPR",
            "year": 2014
        },
        {
            "authors": [
                "J. Kim",
                "J.K. Lee",
                "K.M. Lee"
            ],
            "title": "Accurate image superresolution using very deep convolutional networks",
            "venue": "CoRR, abs/1511.04587",
            "year": 2015
        },
        {
            "authors": [
                "K.I. Kim",
                "Y. Kwon"
            ],
            "title": "Single-image super-resolution using sparse regression and natural image prior",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(6):1127\u2013 1133",
            "year": 2010
        },
        {
            "authors": [
                "D. Kundur",
                "D. Hatzinakos"
            ],
            "title": "Blind image deconvolution",
            "venue": "IEEE signal processing magazine, 13(3):43\u201364",
            "year": 1996
        },
        {
            "authors": [
                "H. Larochelle",
                "I. Murray"
            ],
            "title": "The neural autoregressive distribution estimator",
            "venue": "The Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, volume 15 of JMLR: W&CP, pages 29\u201337",
            "year": 2011
        },
        {
            "authors": [
                "K.R. Laughery",
                "R.H. Fowler"
            ],
            "title": "Sketch artist and identi-kit procedures for recalling faces",
            "venue": "Journal of Applied Psychology, 65(3):307",
            "year": 1980
        },
        {
            "authors": [
                "C. Ledig",
                "L. Theis",
                "F. Huszar",
                "J. Caballero",
                "A. Aitken",
                "A. Tejani",
                "J. Totz",
                "Z. Wang",
                "W. Shi"
            ],
            "title": "Photo-realistic single image super-resolution using a generative adversarial network",
            "venue": "arXiv:1609.04802",
            "year": 2016
        },
        {
            "authors": [
                "Z. Liu",
                "P. Luo",
                "X. Wang",
                "X. Tang"
            ],
            "title": "Deep learning face attributes in the wild",
            "venue": "Proceedings of International Conference on Computer Vision (ICCV)",
            "year": 2015
        },
        {
            "authors": [
                "K. Ma",
                "Q. Wu",
                "Z. Wang",
                "Z. Duanmu",
                "H. Yong",
                "H. Li",
                "L. Zhang"
            ],
            "title": "Group mad competition - a new methodology to compare objective image quality models",
            "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "L. Metz",
                "B. Poole",
                "D. Pfau",
                "J. Sohl-Dickstein"
            ],
            "title": "Unrolled generative adversarial networks",
            "venue": "CoRR, abs/1611.02163",
            "year": 2016
        },
        {
            "authors": [
                "M. Mirza",
                "S. Osindero"
            ],
            "title": "Conditional generative adversarial nets",
            "venue": "CoRR, abs/1411.1784",
            "year": 2014
        },
        {
            "authors": [
                "K. Nasrollahi",
                "T.B. Moeslund"
            ],
            "title": "Super-resolution: A comprehensive survey",
            "venue": "Mach. Vision Appl.,",
            "year": 2014
        },
        {
            "authors": [
                "A. Odena",
                "V. Dumoulin",
                "C. Olah"
            ],
            "title": "Deconvolution and checkerboard artifacts",
            "venue": "Distill",
            "year": 2016
        },
        {
            "authors": [
                "A. Radford",
                "L. Metz",
                "S. Chintala"
            ],
            "title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
            "venue": "CoRR, abs/1511.06434",
            "year": 2015
        },
        {
            "authors": [
                "Y. Romano",
                "J. Isidoro",
                "P. Milanfar"
            ],
            "title": "RAISR: rapid and accurate image super resolution",
            "venue": "CoRR, abs/1606.01299",
            "year": 2016
        },
        {
            "authors": [
                "S. Roth",
                "M.J. Black"
            ],
            "title": "Fields of experts: A framework for learning image priors",
            "venue": "CVPR",
            "year": 2005
        },
        {
            "authors": [
                "T. Salimans",
                "A. Karpathy",
                "X. Chen",
                "D.P. Kingma",
                "Y. Bulatov"
            ],
            "title": "Pixelcnn++: A pixelcnn implementation with discretized logistic mixture likelihood and other modifications. under review at ICLR 2017",
            "year": 2017
        },
        {
            "authors": [
                "A. Saxena",
                "S.H. Chung",
                "A.Y. Ng"
            ],
            "title": "Learning depth from single monocular images",
            "venue": "In NIPS 18. MIT Press",
            "year": 2005
        },
        {
            "authors": [
                "I.V. Serban",
                "A. Sordoni",
                "R. Lowe",
                "L. Charlin",
                "J. Pineau",
                "A.C. Courville",
                "Y. Bengio"
            ],
            "title": "A hierarchical latent variable encoder-decoder model for generating dialogues",
            "venue": "CoRR, abs/1605.06069",
            "year": 2016
        },
        {
            "authors": [
                "Q. Shan",
                "Z. Li",
                "J. Jia",
                "C.-K. Tang"
            ],
            "title": "Fast image/video upsampling",
            "venue": "ACM Transactions on Graphics (TOG), 27(5):153",
            "year": 2008
        },
        {
            "authors": [
                "C.K. S\u00f8 nderby",
                "T. Raiko",
                "L. Maal\u00f8 e",
                "S. r. K. S\u00f8 nderby",
                "O. Winther"
            ],
            "title": "Ladder variational autoencoders",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2016
        },
        {
            "authors": [
                "J. Sun",
                "Z. Xu",
                "H.-Y. Shum"
            ],
            "title": "Image super-resolution using gradient profile prior",
            "venue": "Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, pages 1\u20138. IEEE",
            "year": 2008
        },
        {
            "authors": [
                "B. Uria",
                "I. Murray",
                "H. Larochelle"
            ],
            "title": "Rnade: The realvalued neural autoregressive density-estimator",
            "venue": "C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 2175\u20132183. Curran Associates, Inc.",
            "year": 2013
        },
        {
            "authors": [
                "A. van den Oord",
                "N. Kalchbrenner",
                "K. Kavukcuoglu"
            ],
            "title": "Pixel recurrent neural networks",
            "venue": "ICML, 2016",
            "year": 2016
        },
        {
            "authors": [
                "A. van den Oord",
                "N. Kalchbrenner",
                "O. Vinyals",
                "L. Espeholt",
                "A. Graves",
                "K. Kavukcuoglu"
            ],
            "title": "Conditional image generation with pixelcnn decoders",
            "venue": "NIPS, 2016",
            "year": 2016
        },
        {
            "authors": [
                "Z. Wang",
                "A.C. Bovik",
                "H.R. Sheikh",
                "E.P. Simoncelli"
            ],
            "title": "Image quality assessment: from error visibility to structural similarity",
            "venue": "IEEE transactions on image processing, 13(4):600\u2013612",
            "year": 2004
        },
        {
            "authors": [
                "Z. Wang",
                "E.P. Simoncelli",
                "A.C. Bovik"
            ],
            "title": "Multiscale structural similarity for image quality assessment",
            "venue": "Signals, Systems and Computers, 2004. Conference Record of the Thirty-Seventh Asilomar Conference on, volume 2, pages 1398\u20131402. Ieee",
            "year": 2004
        },
        {
            "authors": [
                "C.Y. Yang",
                "S. Liu",
                "M.H. Yang"
            ],
            "title": "Structured face hallucination",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2013
        },
        {
            "authors": [
                "F. Yu",
                "Y. Zhang",
                "S. Song",
                "A. Seff",
                "J. Xiao"
            ],
            "title": "Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop",
            "venue": "arXiv preprint arXiv:1506.03365",
            "year": 2015
        },
        {
            "authors": [
                "X. Yu",
                "F. Porikli"
            ],
            "title": "Ultra-Resolving Face Images by Discriminative Generative Networks",
            "venue": "pages 318\u2013333. Springer International Publishing, Cham",
            "year": 2016
        },
        {
            "authors": [
                "R. Zhang",
                "P. Isola",
                "A.A. Efros"
            ],
            "title": "Colorful image colorization",
            "venue": "ECCV",
            "year": 2016
        },
        {
            "authors": [
                "D. Zoran",
                "Y. Weiss"
            ],
            "title": "From learning models of natural image patches to whole image restoration",
            "venue": "Proceedings of the 2011 International Conference on Computer Vision, ICCV \u201911, pages 479\u2013486, Washington, DC, USA",
            "year": 2011
        },
        {
            "authors": [
                "D. Zoran",
                "Y. Weiss"
            ],
            "title": "From learning models of natural image patches to whole image restoration",
            "venue": "CVPR",
            "year": 2011
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "The problem of super resolution entails artificially enlarging a low resolution photograph to recover a corresponding plausible image with higher resolution [31]. When a small magnification is desired (e.g., 2\u00d7), super resolution techniques achieve satisfactory results [41, 8, 16, 39, 22] by building statistical prior models of images [35, 2, 51] that capture low-level characteristics of natural images.\nThis paper studies super resolution with particularly small inputs and large magnification ratios, where the amount of information available to accurately construct a high resolution image is very limited (Figure 1, left column). Thus, the problem is underspecified and many plausible, high resolution images may match a given low resolution input image. Building improved models for state-ofthe-art in super resolution in the high magnification regime\n\u2217Work done as a member of the Google Brain Residency program (g.co/brainresidency).\nis significant for improving the state-of-art in super resolution, and more generally for building better conditional generative models of images [44, 33, 30, 43].\nAs the magnification ratio increases, a super resolution model need not only account for textures, edges, and other low-level statistics [16, 39, 22], but must increasingly account for complex variations of objects, viewpoints, illumination, and occlusions. At increasing levels of magnification, the details do not exist in the source image anymore, and the predictive challenge shifts from recovering details\n1\nar X\niv :1\n70 2.\n00 78\n3v 2\n[ cs\n.C V\n] 2\n2 M\nar 2\n01 7\n(e.g., deconvolution [23]) to synthesizing plausible novel details de novo [33, 44].\nConsider a low resolution image of a face in Figure 1, left column. In such 8\u00d78 pixel images the fine spatial details of the hair and the skin are missing and cannot be faithfully restored with interpolation techniques [15]. However, by incorporating prior knowledge of faces and their typical variations, a sketch artist might be able to imagine and draw believable details using specialized software packages [25].\nIn this paper, we show how a fully probabilistic model that is trained end-to-end using a log-likelihood objective can play the role of such an artist by synthesizing 32\u00d732 face images depicted in Figure 1, middle column. We find that drawing multiple samples from this model produces high resolution images that exhibit multi-modality, resembling the diversity of images that plausibly correspond to a low resolution image. In human evaluation studies we demonstrate that naive human observers can easily distinguish real images from the outputs of sophisticated super resolution models using deep networks and mean squared error (MSE) objectives [21]. However, samples drawn from our probabilistic model are able fool a human observer up to 27.9% of the time \u2013 compared to a chance rate of 50%.\nIn summary, the main contributions of the paper include:\n\u2022 Characterization of the underspecified super resolution problem in terms of multi-modal prediction.\n\u2022 Proposal of a new probabilistic model tailored to the super resolution problem, which produces diverse, plausible non-blurry high resolution samples.\n\u2022 Proposal of a new loss term for conditional probabilistic models with powerful autoregressive decoders to avoid the conditioning signal to be ignored.\n\u2022 Human evaluation demonstrating that traditional metrics in super resolution (e.g., pSNR and SSIM) fail to capture sample quality in the regime of underspecified super resolution.\nWe proceed by describing related work, followed by explaining how the multi-modal problem is not addressed using traditional objectives. Then, we propose a new probabilistic model building on top of ResNet [14] and PixelCNN [43]. The paper highlights the diversity of high resolution samples generated by the model and demonstrates the quality of the samples through human evaluation studies."
        },
        {
            "heading": "2. Related work",
            "text": "Super resolution has a long history in computer vision [31]. Methods relying on interpolation [15] are easy to implement and widely used, however these methods suffer from a lack of expressivity since linear models cannot express complex dependencies between the inputs and outputs. In practice, such methods often fail to adequately pre-\ndict high frequency details leading to blurry high resolution outputs.\nEnhancing linear methods with rich image priors such as sparsity [2] or Gaussian mixtures [51] have substantially improved the quality of the methods; likewise, leveraging low-level image statistics such as edge gradients improves predictions [47, 41, 8, 16, 39, 22]. Much work has been done on algorithms that search a database of patches and combine them to create plausible high frequency details in zoomed images [9, 17]. Recent patch-based work has focused on improving basic interpolation methods by building a dictionary of pre-learned filters on images and selecting the appropriate patches by an efficient hashing mechanism [34]. Such dictionary methods have improved the inference speed while being comparable to state-of-the-art.\nAnother approach for super resolution is to abandon inference speed requirements and focus on constructing the high resolution images at increasingly higher magnification factors. Convolutional neural networks (CNNs) represent an approach to the problem that avoids explicit dictionary construction, but rather implicitly extracts multiple layers of abstractions by learning layers of filter kernels. Dong et al. [7] employed a three layer CNN with MSE loss. Kim et al. [21] improved accuracy by increasing the depth to 20 layers and learning only the residuals between the high resolution image and an interpolated low resolution image. Most recently, SRResNet [26] uses many ResNet blocks to achieve state of the art pSNR and SSIM on standard super resolution benchmarks\u2013we employ a similar design for our conditional network and catchall regression baseline.\nInstead of using a per-pixel loss, Johnson et al.[18] use Euclidean distance between activations of a pre-trained CNN for model\u2019s predictions vs. ground truth images. Using this so-called preceptual loss, they train feed-forward networks for super resolution and style transfer. Bruna et al. [4] also use perceptual loss to train a super resolution network, but inference is done via gradient propagation to the low-res input (e.g., [12]).\nAnother promising direction has been to employ an adversarial loss for training a network. A super-resolution network is trained in opposition to a secondary network that attempts to discriminate whether or not a synthesized high resolution image is real or fake. Networks trained with traditional Lp losses (e.g. [21, 7]) suffer from blurry images, where as networks employing an adversarial loss predict compelling, high frequency detail [26, 49]. S\u00f8nderby et al. [19] employed networks trained with adversarial losses but constrained the network to learn affine transformations that ensures the model only generate images that downscale back to the low resolution inputs. S\u00f8nderby et al. [19] also explore a masked autoregressive model but without the gated layers and using a mixture of gaussians instead of a multinomial distribution. Denton et al. [5] use a multi-scale\nadversarial network for image synthesis that is amenable for super-resolutions tasks.\nAlthough generative adversarial networks (GANs) [13] provide a promising direction, such networks suffer from several drawbacks: first, training an adversarial network is unstable [33] and many methods are being developed to increase the robustness of training [29]. Second, GANs suffer from a common failure case of mode collapse [29] where by the resulting model produces samples that do not capture the diversity of samples available in the training data. Finally, tracking the performance of adversarial networks is challenging because it is difficult to associate a probabilistic interpretation to their results. These points motivate approaching the problem with a distinct approach to permit covering of the full diversity of the training dataset.\nPixelRNN and PixelCNN [43, 44] are probabilistic generative models that impose an order on image pixels in order to represent them as a long sequence. The probability of subsequent pixels is conditioned on previously observed pixels. One variant of PixelCNN [44] obtained state-of-theart predictive ability in terms of log-likelihood on academic benchmarks such as CIFAR-10 and MNIST. Since PixelCNN uses log-likelihood for training, the model is penalized if negligible probability is assigned to any of the training examples. By contrast, adversarial networks only learn enough to fool a non-stationary discriminator. This latter point suggests that a PixelCNN might be able to predict a large diversity of high resolution images that might be associated with a given low resolution image. Further, using log-likelihood as the training objective allows for hyper parameter search to find models within a model family by simply comparing their log probabilities on a validation set."
        },
        {
            "heading": "3. Probabilistic super resolution",
            "text": "We aim to learn a probabilistic super resolution model that discerns the statistical dependencies between a high resolution image and a corresponding low resolution image. Let x and y denote a low resolution and a high resolution image, and let y\u2217 represent a ground-truth high resolution image. In order to learn a parametric model of p\u03b8(y | x), we exploit a large dataset of pairs of low resolution inputs and ground-truth high resolution outputs, denotedD \u2261 {(x(i),y\u2217(i))}Ni=1. One can easily collect such a large dataset by starting from some high resolution images and lowering the resolution as much as needed. To optimize the parameters \u03b8 of the conditional distribution p, we maximize a conditional log-likelihood objective defined as,\nO(\u03b8 | D) = \u2211\n(x,y\u2217)\u2208D\nlog p(y\u2217 | x) . (1)\nThe key problem discussed in this paper is the exact form of p(y | x) that enables efficient learning and inference,\nwhile generating realistic non-blurry outputs. We first discuss pixel-independent models that assume that each output pixel is generated with an independent stochastic process given the input. We elaborate why these techniques result in sub-optimal blurry super resolution results. Then, we describe our pixel recursive super resolution model that generates output pixels one at a time to enable modeling the statistical dependencies between the output pixels, resulting in sharp synthesized images given very low resolution inputs."
        },
        {
            "heading": "3.1. Pixel independent super resolution",
            "text": "The simplest form of a probabilistic super resolution model assumes that the output pixels are conditionally independent given the inputs. As such, the conditional distribution of p(y | x) factors into a product of independent pixel predictions. Suppose an RGB output y has M pixels each with three color channels, i.e., y \u2208 R3M . Then,\nlog p(y | x) = 3M\u2211 i=1 log p(yi | x) . (2)\nTwo general forms of pixel prediction models have been explored in the literature: Gaussian and multinomial distributions to model continuous and discrete pixel values respectively. In the Gaussian case,\nlog p(yi | x) = \u2212 1\n2\u03c32 \u2016yi \u2212 Ci(x)\u201622 \u2212 log\n\u221a 2\u03c32\u03c0 , (3)\nwhere Ci(x) denotes the ith element of a non-linear transformation of x via a convolutional neural network. Accordingly, Ci(x) is the estimated mean for the ith output pixel yi, and \u03c32 denotes the variance. Often the variance is not learned, in which case maximizing the conditional log-likelihood of (1) reduces to minimizing the MSE between yi and Ci(x) across the pixels and channels throughout the dataset. Super resolution models based on MSE regression fall within this family of pixel independent models [7, 21, 26]. Implicitly, the outputs of a neural network parameterize a set of Gaussians with fixed variance. It is easy to verify that the joint distribution p(y | x) is unimodal as it forms an isotropic multi-variate Gaussian.\nAlternatively, one could discrete the output dimensions into K possible values (e.g., K = 256), and use a multinomial distribution as the predictive model for each pixel [50], where yi \u2208 {1, . . . ,K}. The pixel prediction model based on a multinomial softmax operator is represented as,\np(yi = k | x) = exp{Cik(x)}\u2211K v=1 exp{Civ(x)} , (4)\nwhere a network with a set of softmax weights, {wjk}3,Kj=1,k=1, for each value per color channel is used to induce Cik(x). Even though p(yi | x) in (4) can express\nmultimodal distributions, the conditional dependency between the pixels cannot be captured, i.e., the model cannot choose between drawing an edge at one position vs. another since that requires coordination between the samples."
        },
        {
            "heading": "3.2. Synthetic multimodal task",
            "text": "To demonstrate how pixel independent models fail at conditional image modeling, we create a synthetic dataset that explicitly requires multimodal prediction. For many dense image predictions tasks, e.g. super resolution [31], colorization [50, 6], and depth estimation [37], models that are able to predict a single mode are heavily preferred over models that blend modes together. For example, in the task of colorization selecting a strong red or green for an apple is better than selecting a brown-toned color that reflects the smeared average of all of the apple colors observed in the training set.\nWe construct a simple multimodal MNIST corners dataset to demonstrate the challenge of this problem. MNIST corners is constructed by randomly placing an MNIST digit in either the top-left or bottom-right corner\n(Figure 2, top). Several networks are trained to predict individual samples from this dataset to demonstrate the unique challenge of this simple example.\nThe challenge behind this toy example is for a network to exclusively predict an individual digit in a corner of an image. Training a moderate-sized 10-layer convolutional neural network (\u223c 100K parameters) with an L2 objective (i.e. MSE regression) results in blurry image samples in which the two modes are blended together (Figure 2, L2 regression). That is, never in the dataset does an example image contain a digit in both corners, yet this model incorrectly predicts a blend of such samples. Replacing the loss with a discrete, per-pixel cross-entropy produces sharper images but likewise fails to stochastically predict a digit in a corner of the image (Figure 2, cross-entropy)."
        },
        {
            "heading": "4. Pixel recursive super resolution",
            "text": "The lack of conditional independence between predicted pixels is a significant failure mode for the previous probabilistic objectives in the synthetic example (Equations 3 and 4). One approach to this problem is to define the conditional distribution of the output pixels jointly as a multivariate Gaussian mixture [52] or an undirected graphical model [10]. Both of these conditional distributions require constructing a statistical dependency between output pixels for which inference may be computationally expensive.\nA second approach is to factorize the joint distribution using the chain rule by imposing an order on image pixels,\nlog p(y | x) = M\u2211 i=1 log p(yi | x,y<i) , (5)\nwhere the generation of each output dimension is conditioned on the input and previous output pixels [24, 42]. We denote the conditioning1 up to pixel i by y<i where {y1, . . . ,yi\u22121}. The benefits of this approach are that the exact form of the conditional dependencies is flexible and the inference is straightforward.\nPixelCNN is a stochastic model that provides an explicit model for log p(yi |x,y<i) as a gated, hierarchical chain of cleverly masked convolutions [43, 44, 36]. The goal of PixelCNN is to capture multi-modality and capture pixel correlations in an image. Indeed, training a PixelCNN on the MNIST corners dataset successfully captures the bimodality of the problem and produces sample in which digits reside exclusively in a single corner (Figure 2, PixelCNN). Importantly, the model never predicts both digits simultaneously.\n1Note that in color images one must impose an order on both spatial locations as well as color channels. In a color image the conditioning is based on the the input and previously outputted pixels at previous spatial locations as well as pixels at the same spatial location.\nApplying the PixelCNN to a super-resolution problem is a straightforward application that requires modifying the architecture to supply a conditioning on a low resolution version of the image. In early experiments we found the auto-regressive distribution of the model largely ignore the conditioning of the low resolution image. This phenomenon referred to as \u201coptimization challenges\u201d has been readily documented in the context of sequential autoencoder models [3] (see also [38, 40] for more discussion).\nTo address this issue we modify the architecture of PixelCNN to more explicitly depend on the conditioning of a low resolution image. In particular, we propose a late fusion model [20] that factors the problem into auto-regressive and conditioning components (Figure 3). The auto-regressive portion of the model, termed a prior network captures the serial dependencies of the pixels while the conditioning component, termed a conditioning network captures the global structure of the low resolution image. Specifically, we formulate the prior network to be a PixelCNN and the conditioning network to be a deep convolutional network employed previously for super resolution [26].\nGiven an input x \u2208 RL, let Ai(x) : RL \u2192 RK denote a conditioning network predicting a vector of logit values corresponding to the K possible values that the ith output pixel can take. Similarly, let Bi(y<i) : Ri\u22121 \u2192 RK denote a prior network predicting a vector of logit values for the ith output pixel. Our probabilistic model predicts a distribution over the ith output pixel by simply adding the two sets of\nlogits and applying a softmax operator on them,\np(yi | x,y<i) = softmax(Ai(x) +Bi(y<i)) . (6)\nTo optimize the parameters of A and B jointly, we perform stochastic gradient ascent to maximize the conditional log likelihood in (1). That is, we optimize a cross-entropy loss between the model\u2019s predictions in (6) and discrete ground truth labels y\u2217i \u2208 {1, . . . ,K},\nO1 = \u2211\n(x,y\u2217)\u2208D M\u2211 i=1 ( 1[y\u2217i ] T (Ai(x) +Bi(y \u2217 <i))\n\u2212 lse(Ai(x) +Bi(y\u2217<i)) ) , (7)\nwhere lse(\u00b7) is the log-sum-exp operator corresponding to the log of the denominator of a softmax, and 1[k] denotes a K-dimensional one-hot indicator vector with its kth dimension set to 1.\nOur preliminary experiments indicate that models trained with (7) tend to ignore the conditioning network as the statistical correlation between a pixel and previous high resolution pixels is stronger than its correlation with low resolution inputs. To mitigate this issue, we include an additional loss in our objective to enforce the conditioning network to be optimized. This additional loss measures the cross-entropy between the conditioning network\u2019s predictions via softmax(Ai(x)) and ground truth labels. The total loss that is optimized in our experiments is a sum of two cross-entropy losses formulated as,\nO2 = \u2211\n(x,y\u2217)\u2208D M\u2211 i=1 ( 1[y\u2217i ] T (2Ai(x) +Bi(y \u2217 <i))\n\u2212 lse(Ai(x) +Bi(y\u2217<i))\u2212 lse(Ai(x)) ) . (8)\nOnce the network is trained, sampling from the model is straightforward. Using (6), starting at i = 1, first we sample a high resolution pixel. Then, we proceed pixel by pixel, feeding in the previously sampled pixel values back into the network, and draw new high resolution pixels. The three channels of each pixel are generated sequentially in turn.\nWe additionally consider greedy decoding, where one always selects the pixel value with the largest probability and sampling from a tempered softmax, where the concentration of a distribution p is adjusted by using a temperature parameter \u03c4 > 0,\np\u03c4 = p(1/\u03c4)\n\u2016p(1/\u03c4)\u20161 .\nTo control the concentration of our sampling distribution p(yi | x,y<i), it suffices to divide the logits from A and B by a parameter \u03c4 . Note that as \u03c4 goes towards 0, the distribution converges to the mode."
        },
        {
            "heading": "4.1. Implementation details",
            "text": "We summarize the network architecture for the pixel recursive super resolution model. The conditioning architecture is similar in design to SRResNet [26]. The conditioning network is a feed-forward convolutional neural network that takes a low resolution image through a series of 18\u221230 ResNet blocks [14] and transposed convolution layers [32]. The last layer uses a 1\u00d71 convolution to increase the number of channels to predict a multinomial distribution over 256 possible color channel values for each sub-pixel. The prior network architecture consists of 20 gated PixelCNN blocks with 32 channels at each layer [44]. The final layer of the super-resolution network is a softmax operation over the sum of the activations from the conditioning and prior networks. The model is built by using TensorFlow [1] and trained across 8 GPUs with synchronous SGD updates. For training details and a complete list of architecture parameters, please see Appendix A."
        },
        {
            "heading": "5. Experiments",
            "text": "We assess the effectiveness of the proposed pixel recursive super resolution method on two datasets containing centrally cropped faces (CelebA [27]) and bedroom images (LSUN Bedrooms [48]). In both datasets we resize the images to 8\u00d78 and 32\u00d732 pixels with bicubic interpolation to provide the input x and output y for training and evaluation.\nWe compare our technique against three baselines including (1) Nearest N.; a nearest neighbor search baseline inspired by previous work on example-based super resolution [9], (2) ResNet L2; a deep neural network using Resnet blocks trained with MSE objective, and (3) GAN; a GAN based super resolution model implemented by [11] similar to [49]. We exclude the results of the GANbaseline on bedrooms dataset as they are not competitive, and the model was developed specifically for faces.\nThe Nearest N. baseline computes y for a sample x by searching the training set D = {(x(i),y\u2217(i))}Ni=1 for the nearest example indexed by i\u2217 = argmini\u2016x(i) \u2212 x\u201622, and returns the high resolution counterpart y\u2217(i\n\u2217). The Nearest N. baseline is a representative result of exemplar based super resolution approaches, and helps us test whether the model performs a naive lookup from the training dataset.\nThe ResNet L2 baseline employs a design similar to SRResNet [26] that reports state-of-the-art in terms of image similarity metrics2. Most significantly, we alter the network to compute the residuals with respect to a bicubic interpolation of the input [21]. The L2 regression provides a com-\n2 Note that the regression architecture is nearly identical to the conditioning network in Section 4.1. The slight change is to force the network to predict bounded values in RGB space. To enforce this behavior, the top layer is outputs three channels instead of one and employ a tanh(\u00b7) instead of a ReLU(\u00b7) nonlinearity.\nparison to a state-of-the-art convolutional network that performs a unimodal pixel independent prediction.\nThe GAN super resolution baseline [11] exploits a conditional GAN architecture, and combines an adversarial loss with a consistency loss, which encourages the lowresolution version of predicted y to be close to x as measures by L1. There is a weighting between the two losses specified by [11] as 0.9 for the consistency and 0.1 for the adversarial loss, and we keep them the same in our face experiments."
        },
        {
            "heading": "5.1. Super resolution samples",
            "text": "High resolution samples generated by the pixel recursive super resolution capture the rich structure of the dataset and appear perceptually plausible (Figure 1 and 4; Appendix B and C). Sampling from the super resolution model multiple times results in different high resolution images for a given low resolution image (Figure 5; Appendix B and C). Qualitatively, the samples from the model identify many plausible high resolution images with distinct qualitative features that correspond to a given lower resolution image. Note that the differences between samples for the faces dataset are far less drastic than seen in our synthetic dataset, where failure to cleanly predict modes indicated complete failure.\nThe quality of samples is sensitive to the temperature (Figure 6, right columns). Greedy decoding (\u03c4 = 0) results in poor quality samples that are overly smooth and contain horizontal and vertical line artifacts. Samples from the default temperature (\u03c4 = 1.0) are perceptually more plausible,\nalthough they tend to contain undesired high frequency content. Tuning the temperature (\u03c4 ) between 0.9 and 0.8 proves beneficial for improving the quality of the samples."
        },
        {
            "heading": "5.2. Quantitative evaluation of image similarity",
            "text": "Many methods exist for quantifying image similarity that attempt to measure human perception judgements of similarity [45, 46, 28]. We quantified the prediction accuracy of our model compared to ground truth using pSNR and MS-SSIM (Table 1). We found that our own visual assessment of the predicted image quality did not correspond to these image similarities metrics. For instance, bicubic interpolation achieved relatively high metrics even though the samples appeared quite poor. This result matches recent observations that suggest that pSNR and SSIM provide poor\njudgements of super resolution quality when new details are synthesized [26, 18]. In addition, Figure 6 highlights how the perceptual quality of model samples do not necessarily correspond to negative log likelihood (NLL). Smaller NLL means the model has assigned that image a larger probability mass. The greedy, bicubic, and regression faces are preferred by the model despite exhibiting worse perceptual quality.\nWe next measured how well the high resolution samples corresponded to the low resolution input by measuring the consistency. The consistency is quantified as L2 distance between the low-resolution input image and a bicubic downsampled version of the high resolution estimate. Lower consistencies indicate superior correspondence with the low-resolution image. Note that this is an explicit objective the GAN [11]. The pixel recursive model achieved consistencies on par with the L2 regression model and bicubic interpolation indicating that even though the model was producing diverse samples, the samples were largely constrained by the low-resolution image. Most importantly, the pixel recursive model achieved superior consistencies then the GAN [11] even though the model does not explicitly optimize for this criterion.3\nThe consistency measure additionally provided an important control experiment to determine if the pixel recursive model were just naively copying the nearest training sample. If the pixel recursive model were just copying the nearest training sample, then the consistency of the Nearest N. model would be equivalent to the pixel recursive model. We instead find that the pixel recursive model has superior consistency values indicating that the model is not just naively copying the closest training examples."
        },
        {
            "heading": "5.3. Perceptual evaluation with humans",
            "text": "Given that automated quantitative measures did not match our perceptual judgements, we conducted a human study to assess the effectiveness of the super resolution algorithm. In particular, we performed a forced choice experiment on crowd-sourced workers in order to determine how plausible a given high resolution sample is from each model. Following [50], each worker was presented a true image and a corresponding prediction from a model, and asked \u201cWhich image, would you guess, is from a camera?\u201d. We performed this study across 283 workers on Amazon Mechanical Turk and statistics were accrued across 40 unique workers for each super resolution algorithm.4\n3Note that one may improve the consistency of the GAN by increasing its weight in the objective. Increasing the weight for the consistency term will likely lead to decreased perceptual quality in the images but improved consistency. Regardless, the images generated by the pixel recursive model are superior in both consistency and perceptual quality as judged humans for a range of temperatures.\n4Specifically, each worker was given one second to make a forced choice decision. Workers began a session with 10 practice questions dur-\ning which they received feedback. The practice pairs were not counted in the results. After the practice pairs, each worker was shown 45 additional pairs. A subset of the pairs were simple, golden questions designed to constantly check if the worker was paying attention. Data from workers that\nTable 1 reports the percentage of samples for a given algorithm that a human incorrectly believed to be a real image. Note that a perfect algorithm would fool a human at rate of 50%. The L2 regression model fooled humans 2- 4% of the time and the GAN [11] fooled humans 8.5% of the time. The pixel recursive model fooled humans 11.0% and 27.9% of the time for faces and bedrooms, respectively \u2013 significantly above the state-of-the-art regression model. Importantly, we found that the selection of the sampling temperature \u03c4 greatly influenced the quality of the samples and in turn the fraction of time that humans were fooled. Nevertheless the pixel recursive model outperformed the strongest baseline model, the GAN, across all temperatures. A ranked list of the best and worst fooling examples is reproduced in Appendix D along with the fool rates."
        },
        {
            "heading": "6. Conclusion",
            "text": "We advocate research on super resolution with high magnification ratios, where the problem is dramatically underspecified as high frequency details are missing. Any model that produces non-blurry super resolution outputs must make sensible predictions of the missing content to operate in such a heavily multimodal regime. We present a fully probabilistic method that tackles super resolution with small inputs, demonstrating that even 8\u00d78 images can be enlarged to sharp 32\u00d732 images. Our technique outperforms several strong baselines including the ones optimizing a re-\nanswered golden questions incorrectly were thrown out.\ngression objective or an adversarial loss. We perform human evaluation studies showing that samples from the pixel recursive model look more plausible to humans, and more generally, common metrics like pSNR and SSIM do not correlate with human judgment when the magnification ratio is large."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank Aa\u0308ron van den Oord, Sander Dieleman, and the Google Brain team for insightful comments and discussions."
        },
        {
            "heading": "A. Hyperparameters for pixel recursive super resolution model.",
            "text": ""
        },
        {
            "heading": "B. Samples from models trained on LSUN bedrooms",
            "text": "Input Bicubic ResNet L2 \u03c4 = 1.0 \u03c4 = 0.9 \u03c4 = 0.8 Truth Nearest N.\nInput Bicubic ResNet L2 \u03c4 = 1.0 \u03c4 = 0.9 \u03c4 = 0.8 Truth Nearest N.\nInput Bicubic ResNet L2 \u03c4 = 1.0 \u03c4 = 0.9 \u03c4 = 0.8 Truth Nearest N.\nInput Bicubic ResNet L2 \u03c4 = 1.0 \u03c4 = 0.9 \u03c4 = 0.8 Truth Nearest N.\nInput Bicubic ResNet L2 \u03c4 = 1.0 \u03c4 = 0.9 \u03c4 = 0.8 Truth Nearest N."
        },
        {
            "heading": "C. Samples from models trained on CelebA faces",
            "text": "Input Bicubic ResNet L2 \u03c4 = 1.0 \u03c4 = 0.9 \u03c4 = 0.8 Truth Nearest N. GAN [11]\nInput Bicubic ResNet L2 \u03c4 = 1.0 \u03c4 = 0.9 \u03c4 = 0.8 Truth Nearest N. GAN [11]\nInput Bicubic ResNet L2 \u03c4 = 1.0 \u03c4 = 0.9 \u03c4 = 0.8 Truth Nearest N. GAN [11]\nInput Bicubic ResNet L2 \u03c4 = 1.0 \u03c4 = 0.9 \u03c4 = 0.8 Truth Nearest N. GAN [11]\nInput Bicubic ResNet L2 \u03c4 = 1.0 \u03c4 = 0.9 \u03c4 = 0.8 Truth Nearest N. GAN [11]"
        },
        {
            "heading": "D. Samples images that performed best and worst in human ratings.",
            "text": "The best and worst rated images in the human study. The fractions below the images denote how many times a person choose that image over the ground truth.\nOurs Ground Truth Ours Ground Truth\n23/40 = 57% 34/40 = 85%\n17/40 = 42% 30/40 = 75%\n16/40 = 40% 26/40 = 65%\n1/40 = 2% 3/40 = 7%\n1/40 = 2% 3/40 = 7%\n1/40 = 2% 4/40 = 1%"
        }
    ],
    "title": "Pixel Recursive Super Resolution",
    "year": 2017
}