{
    "abstractText": "The robust and efficient recognition of visual relations in images is a hallmark of biological vision. We argue that, despite recent progress in visual recognition, modern machine vision algorithms are severely limited in their ability to learn visual relations. Through controlled experiments, we demonstrate that visual-relation problems strain convolutional neural networks (CNNs). The networks eventually break altogether when rote memorization becomes impossible, as when intraclass variability exceeds network capacity. Motivated by the comparable success of biological vision, we argue that feedback mechanisms including attention and perceptual grouping may be the key computational components underlying abstract visual reasoning.",
    "authors": [
        {
            "affiliations": [],
            "name": "Matthew Ricci"
        },
        {
            "affiliations": [],
            "name": "Junkyung Kim"
        },
        {
            "affiliations": [],
            "name": "Thomas Serre"
        }
    ],
    "id": "SP:242e5f97c052f2790b0a21d71f0e1d92c487abd1",
    "references": [
        {
            "authors": [
                "M. Abadi",
                "P. Barham",
                "J. Chen",
                "Z. Chen",
                "A. Davis",
                "J. Dean",
                "X. . . Zheng"
            ],
            "title": "TensorFlow: A system for large-scale machine learning",
            "venue": "In Proceedings of the 12th USENIX conference on operating systems design and implementation (pp. 265\u2013283)",
            "year": 2016
        },
        {
            "authors": [
                "K. Ellis",
                "A. Solar-lezama",
                "J.B. Tenenbaum"
            ],
            "title": "Unsupervised Learning by Program Synthesis",
            "venue": "Proceedings of the 28th International Conference on Neural Information Processing Systems,",
            "year": 2015
        },
        {
            "authors": [
                "K.K. Evans",
                "A. Treisman"
            ],
            "title": "Perception of objects in natural scenes: is it really attention free",
            "venue": "J. Exp. Psychol. Hum. Percept. Perform.,",
            "year": 2005
        },
        {
            "authors": [
                "F. Fleuret",
                "T. Li",
                "C. Dubout",
                "E.K. Wampler",
                "S. Yantis",
                "D. Geman"
            ],
            "title": "Comparing machines and humans on a visual categorization test",
            "venue": "Proc. Natl. Acad. Sci. U. S. A.,",
            "year": 2011
        },
        {
            "authors": [
                "J.A. Fodor",
                "Z.W. Pylyshyn"
            ],
            "title": "Connectionism and cognitive architecture: A critical analysis",
            "venue": "Cognition,",
            "year": 1988
        },
        {
            "authors": [
                "D. Geman",
                "S. Geman",
                "N. Hallonquist",
                "L. Younes"
            ],
            "title": "Visual Turing test for computer vision systems",
            "venue": "Proc. Natl. Acad. Sci. U. S. A.,",
            "year": 2015
        },
        {
            "authors": [
                "X. Glorot",
                "Y. Bengio"
            ],
            "title": "Understanding the difficulty of training deep feedforward neural networks",
            "venue": "Proceedings of the 13th international conference on artificial intelligence and statistics (Vol",
            "year": 2010
        },
        {
            "authors": [
                "\u00c7. G\u00fcl\u00e7ehre",
                "Y. Bengio"
            ],
            "title": "Knowledge Matters : Importance of Prior Information for Optimization",
            "venue": "Journal of achine Learning Research,",
            "year": 2016
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "feb). Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification",
            "year": 2015
        },
        {
            "authors": [
                "A.O. Holcombe",
                "D. Linares",
                "M. Vaziri-Pashkam"
            ],
            "title": "Perceiving spatial relations via attentional tracking and shifting",
            "venue": "Curr. Biol.,",
            "year": 2011
        },
        {
            "authors": [
                "D.P. Kingma",
                "J.L. Ba"
            ],
            "title": "Adam: a method for stochastic optimization",
            "venue": "In International conference on learning representations",
            "year": 2015
        },
        {
            "authors": [
                "G.D. Logan"
            ],
            "title": "Spatial attention and the apprehension of spatial relations",
            "venue": "Journal of Experimental Psychology: Human Perception and Performance,",
            "year": 1994
        },
        {
            "authors": [
                "C.M. Moore",
                "C.L. Elsinger",
                "A. Lleras"
            ],
            "title": "Visual attention and the apprehension of spatial relations: The case of depth",
            "venue": "J. Exp. Psychol. Hum. Percept. Perform.,",
            "year": 1994
        },
        {
            "authors": [
                "P.R. Roelfsema"
            ],
            "title": "Cortical algorithms for perceptual grouping",
            "venue": "Annu. Rev. Neurosci.,",
            "year": 2006
        },
        {
            "authors": [
                "L.J. Rosielle",
                "B.T. Crabb",
                "E.E. Cooper"
            ],
            "title": "Attentional coding of categorical relations in scene perception: evidence from the flicker paradigm",
            "venue": "Psychon. Bull. Rev.,",
            "year": 2002
        },
        {
            "authors": [
                "A. Santoro",
                "D. Raposo",
                "D.G.T. Barrett",
                "M. Malinowski",
                "R. Pascanu",
                "P. Battaglia",
                "T. Lillicrap"
            ],
            "title": "A simple neural network module for relational reasoning",
            "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems",
            "year": 2017
        },
        {
            "authors": [
                "Serre",
                "May"
            ],
            "title": "Models of visual categorization",
            "venue": "Wiley Interdiscip. Rev. Cogn. Sci.,",
            "year": 2016
        },
        {
            "authors": [
                "I.J.M. van der Ham",
                "M.J.A. Duijndam",
                "M. Raemaekers",
                "R.J.A. van Wezel",
                "A. Oleksiak",
                "A. Postma"
            ],
            "title": "Retinotopic mapping of categorical and coordinate spatial relation processing in early visual cortex",
            "venue": "PLoS One,",
            "year": 2012
        },
        {
            "authors": [
                "A.A. Wright",
                "J.S. Katz"
            ],
            "title": "Mechanisms of same/different concept learning in primates and avians",
            "venue": "Behav. Processes,",
            "year": 2006
        }
    ],
    "sections": [
        {
            "heading": "Introduction",
            "text": "Consider the images in Fig. 1. The image on the left was correctly classified as a flute by a deep convolutional neural network (CNN; He et al., 2015). This is quite a remarkable feat for such a complicated image. After the network was trained on millions of photographs, this and many other images were accurately categorized into one thousand natural object categories, surpassing, for the first time, the accuracy of a human observer on the ImageNet classification challenge.\nNow, consider the image in the middle. On its face, it is quite simple compared to the image on the left. It is just a binary image containing two curves. Further, it has a rather distinguishing property, at least to the human eye: both curves are the same. The relation between the two items in this simple scene is rather intuitive and immediately obvious to a human observer. Yet, the CNN failed to learn this relation even after seeing millions of training examples.\nWhy is it that a CNN can accurately detect the flute while struggling to recognize the simple relation depicted in the middle panel of Fig. 1? That such task is extremely difficult for contemporary computer vision algorithms like CNNs, is known (Fleuret et al., 2011; Gu\u0308lc\u0327ehre & Bengio, 2016; Ellis et al., 2015; Stabinger et al., 2016). However, these results, which often relied on a single architecture, were not entirely conclusive: does the inability of CNNs to solve various visual-relation problems reflect a poor choice of network hyperparameters or rather a systematic failure of the entire class of models? To our knowledge, there has been no systematic exploration of the limits of contemporary machine learning\nTo appear in the Proceedings of the Annual Meeting of the Cognitive Science Society, 2018\n\u2020 These authors contributed equally to this work. \u2217 Brown University, Department of Cognitive, Linguistic and Psychological Sciences. 190 Thayer Street. Providence, RI 029012\nalgorithms on relational reasoning problems. In this study, we will probe the limits of CNNs on visualrelation tasks. In Experiment 1, we perform a systematic performance analysis of CNN architectures on each of the twenty-three synthetic visual reasoning test (SVRT) problems, which reveals a dichotomy of visual-relation tasks: hard same-different problems vs. easy spatial-relation problems. In Experiment 2, we describe a novel, controlled, visualrelation challenge which convincingly shows that CNNs solve same-different tasks via rote memorization. With these experiments, we hope to motivate the computer vision community to reconsider existing visual question answering challenges and turn to cognitive science and neuroscience for inspiration in the design of visual reasoning architectures.\nExperiment 1: SVRT The synthetic visual reasoning test (SVRT) is a collection of twenty-three binary classification problems in which opposing classes differ based on whether their stimuli obey an abstract rule (Fleuret et al., 2011). For example, in problem number 1, positive examples feature two items which are the same up to translation (Fig. 1, middle panel), whereas negative examples do not. In problem 9, positive examples have three items, the largest of which is in between the two smaller ones (Fig. 1, right panel). All stimuli depict simple, closed, black curves on a white background.\nMethods. We tested nine different CNNs of three different depths (2, 4 and 6 convolutional layers) and with three different convolutional filter sizes (2\u00d72, 4\u00d74 and 6\u00d76) in the first layer. This initial receptive field size effectively determines the size of receptive fields throughout the network. The number of filters in the first layer was 6, 12 or 18, respectively, for\nar X\niv :1\n80 2.\n03 39\n0v 3\n[ cs\n.C V\n] 2\n5 M\nay 2\n01 8\neach choice of initial receptive field size. In the other convolutional layers, filter size was fixed at 2\u00d72 with the number of filters doubling every layer. All convolutional layers had strides of 1 and used ReLU activations. Pooling layers were placed after every convolutional layer, with pooling kernels of size 3\u00d73 and strides of 2. On top of the retinotopic layers, all nine CNNs had three fully connected layers with 1,024 hidden units in each layer, followed by a 2-dimensional classification layer. All CNNs were trained on all problems. Network parameters were initialized using Xavier initialization (Glorot & Bengio, 2010) and were trained using the Adaptive Moment Estimation (Adam) optimizer (Kingma & Ba, 2015) with base learning rate of \u03b7 = 10\u22124. All experiments were run using TensorFlow (Abadi et al., 2016).\nWe obtained the accuracy from the best network for each problem individually. Then, we organized the results into a bar plot, sorted the problems by accuracy and colored the bars red or blue according to the SVRT problem descriptions in (Fleuret et al., 2011). Problems whose descriptions had words like \u201csame\u201d or \u201cidentical\u201d were colored red. These Same-Different (SD) problems had items that are congruent up to some transformation (e.g., middle panel, Fig. 1). Spatial-Relation (SR) problems, whose descriptions have phrases like \u201cleft of\u201d, \u201cnext to\u201d or \u201ctouching,\u201d were colored blue (e.g., right panel, Fig. 1).\nResults. The resulting dichotomy across the SVRT problems is striking (Fig. 2). CNNs fare uniformly worse on SD problems than they do on SR problems. Many SR problems were learned satisfactorily, whereas some SD problems (e.g., problems 20 and 7) resulted in accuracy not substantially above chance. From this analysis, it appears as if SD tasks pose a particularly difficult challenge to CNNs. This result matches earlier evidence for a visual-relation dichotomy hypothesized by Stabinger et al. (2016). Additionally, our search revealed that SR problems are equally well-learned across all net-\nwork configurations, with less than 10% difference in final accuracy between the worst case and the best case. On the other hand, larger networks yielded significantly higher accuracy than smaller ones on SD problems, suggesting that SD problems are more capacity-sensitive than SR problems. Experiment 1 corroborates earlier studies (Fleuret et al., 2011; Gu\u0308lc\u0327ehre & Bengio, 2016; Ellis et al., 2015; Santoro et al., 2017) which found that CNNs perform badly on many visualrelation problems and additionally suggests that low performance cannot be simply attributed to a poor choice of hyperparameters. Experiment 2: PSVRT Though useful for surveying many types of relations, the SVRT challenge has two important limitations. First, different problems have different visual structure; e.g., problem 1 requires that an image have two items (Fig. 1, middle), while problem 9 requires that an image have three (Fig. 1, right). Therefore, image features, not abstract relational rules, might make some problems harder than others. Second, the ad hoc procedure used to generate simple, closed curves as items in SVRT prevents quantification of image variability and its effect on task difficulty. As a result, even within a single problem in SVRT, it is unclear whether its difficulty is inherent to the classification rule itself or rather the choice of image generation parameters unrelated to the rule.\nTo address these limitations, we constructed a new visualrelation benchmark consisting of two idealized problems (Fig. 3) from the dichotomy that emerged from Experiment 1: Spatial Relations (SR) and Same-Different (SD). Critically, both problems used exactly the same images, but with different labels. Further, we parameterized the dataset so that we could systematically control the size of scene items, the number of scene items, and the size of the whole image. Items were binary bit patterns placed on a blank background.\nFor each configuration of image parameters, we trained a new instance of a single CNN architecture and measured the\nease with which it fit the data. Our goal was to examine how hard it is for a CNN architecture to learn relations for visually different but conceptually equivalent problems. If CNNs can truly learn the \u201crule\u201d underlying these problems, then one would expect the models to learn all problems with more-orless equal ease. However, if the CNNs only memorize the distinguishing features of the two image classes, then learning should be affected by the variability of the example images in each category. For example, when image size and items size are large, there are simply more possible samples, which might put a strain on the representational capacity of a CNN trying to learn by rote memorization.\nMethods. Our image generator uses three parameters to control image variability: the size (m) of each bit pattern or item, the size (n) of the input image and the number (k) of items in an image. Our parametric construction allows a dissociation between two possible factors that may affect problem difficulty: classification rules vs. image variability. To highlight the parametric nature of the images, we call this new challenge the parametric SVRT or PSVRT.\nThe image generator is designed such that each image can be used to pose both problems by simply labeling it according to different rules (Fig. 3). In SR, an image is classified according to whether scene items are arranged horizontally or vertically as measured by the orientation of the line joining their centers (with a 45\u25e6 threshold). In SD, an image is classified according to whether or not it contains at least two identical items. When k \u2265 3, the SR category label is determined according to whether the average orientation of the displacements between all pairs of items is greater than or equal to 45\u25e6. Each image can be labeled according to either the SR or SD rules, so we can ensure the image distribution is identical between the two problem types.\nWe trained the same CNN repeatedly from scratch over multiple subsets of the data in order to see if learnability depends on the dataset\u2019s image parameters. Training accuracy was sampled at regular intervals and samples were averaged across the length of a training run as well as over multiple trials for each condition, yielding a scalar measure of learnability called \u201cmean area under the learning curve\u201d (mean ALC). ALC is high when accuracy increases earlier and more rapidly throughout the course of training and/or when it converges to a higher final accuracy by the end of training.\nFirst, we found a baseline architecture which could easily learn both same-different and spatial-relation PSVRT problems for one parameter configuration (item size m = 4, image size n = 60 and item number k = 2). Then, for a range of combinations of item size, image size and number of items, we trained an instance of this architecture from scratch.\nThe baseline CNN we used in this experiment had four convolutional layers. The first layer had 8 filters with a 4\u00d74 receptive field size. In the rest of convolutional layers, filter size was fixed at 2\u00d72 with the number of filters in each layer doubling from the immediately preceding layer. All convolutional layers had ReLU activations with strides of 1. Pooling layers were placed after every convolutional layer, with pool-\ning kernels of size 3\u00d73 and strides of 2. On top of retinotopic layers were three fully connected layers with 256 hidden units each, followed by a 2-dimensional classification layer. We initialized all parameters with the Xavier method, optimized the network with Adam with base rate \u03b7 = 10\u22124 and ran all experiments in Tensorflow.\nTo understand the effect of network size on learnability, we also used two control networks in this experiment: (1) a \u201cwide\u201d control that had the same depth as the baseline but twice as many filters in the convolutional layers and four times as many hidden units in the fully connected layers and (2) and a \u201cdeep\u201d control which had twice as many convolutional layers as the baseline, by adding a convolutional layer of filter size 2\u00d72 after each existing convolutional layer. Each extra convolutional layer had the same number of filters as the immediately preceding convolutional layer.\nWe separately varied the three image parameters to examine their effects on learnability. This resulted in three subexperiments (n was varied between 30 and 180 while m and k were fixed at 4 and 2, respectively; m was varied between 3 and 7, while n and k were fixed at 60 and 2, respectively; k was varied between 2 and 6 while n and m were fixed at 60 and 4, respectively). The baseline CNN was trained from scratch in each condition with 20 million training images and a batch size of 50.\nResults. In all cases where learning occurred, training accuracy eventually jumped from chance-level and gradually plateaued. In other cases, accuracy remained at chance throughout a training session and the ALC was 0.5. Within a single condition, the CNN often only learned for a fraction of 10 randomly initialized trials. This led us to use two different quantities for describing a model\u2019s performance: (1) mean ALC obtained from learned trials (in which accuracy crossed 55%) and (2) the number of trials in which the learning event never took place (non-learned). Note that these two quantities are independent, computed from two complementary subsets of 10 trials.\nIn all conditions, we found a strong dichotomy between SD and SR conditions. In SR, across all image parameters and in all trials, the model immediately learned at the start of training and quickly approached 100% accuracy, producing consistently high and flat mean ALC curves (Fig. 4, blue dotted lines). In SD, however, we found that the overall ALC was significantly lower than SR (Fig. 4, red dotted lines). We also identified two ways in which image variability affects learnability. First, among the trials in which learning occurred, the final accuracy achieved by the CNN decreased as image size (n) and number of items (k) increased. This caused ALC to decrease from around 0.95 to 0.8. Second, increasing n also decreased the chance of learning altogether, with more than half of the trials failing to escape chance level when image size was greater than 60 (Fig. 4, gray bars). In contrast, increasing item size never strained CNN performance. Similar to SR, learnability, both in terms of the proportion of successful trials as well as final accuracy, did not change significantly over the range of item sizes.\nThe fact that straining is only observed in SD, and not in SR and that it is only observed along some of the image parameters, n and k, suggests that straining is not simply a direct outcome of an increase in image variability. Using a CNN with more than twice the number of kernels (Fig. 4, purple dotted lines) or with twice as many convolutional lay-\ners (Fig. 4, brown dotted lines) as the control did not qualitatively change the trend observed in the baseline model. Although increasing network size did result in improved learned accuracy in general, it also made learning less likely, yielding more non-learned trials than the baseline CNN.\nWe also rule out the possibility of the loss of spatial acuity from pooling or subsampling operations as a possible cause of straining. Our CNNs achieved the best overall accuracy when image size was smallest. If the loss of spatial acuity was the source of straining, increasing image size should have improved the network\u2019s performance instead of hurting it because items would have tended to be placed farther apart from each other. Moreover, in other experiments (Kim et al., in press), we found that networks with identical spatial acuity exhibited no straining as long as items were segregated into different channels.\nThe weak effects of item size and item number shed light on the computational strategy used by CNNs to solve SD. We hypothesize that CNNs learn \u201csubtraction templates\u201d, filters with one positive region and one negative region (like a Haar or Gabor wavelet), in order to detect the similarity between two image regions. A different subtraction template is required for each relative arrangement of items, since each item must lie in one of the template\u2019s two regions. When identical items lie in these opposing regions, they are subtracted by the synaptic weights. This difference is then used to choose the appropriate same/different label. This strategy does not require memorizing specific items, so increasing item size (and therefore total number of possible items) should not make the task appreciably harder. Further, a single subtraction template can be used even in scenes with more than two items, since images are classified as \u201csame\u201d when they have at least two identical items. So, any straining effect from item number should be negligible as well. Instead, the principal straining effect with this strategy should arise from image size, which exponentially increases the possible number arrangements of items.\nTaken together, these results suggest that, when CNNs learn a PSVRT problem, they are simply building a feature set tailored to the relative positional arrangements of items in a particular data set, instead of learning the abstract \u201crule\u201d per se."
        },
        {
            "heading": "Discussion",
            "text": "Our results indicate that visual-relation problems can quickly exceed the representational capacity of feedforward networks. While learning templates for individual objects appears to be tractable for today\u2019s deep networks, learning templates for arrangements of objects becomes rapidly intractable because of the combinatorial explosion in the requisite number of features to be stored. That stimuli with a combinatorial structure are difficult to represent with feedforward networks has been long acknowledged by cognitive scientists (Fodor & Pylyshyn, 1988).\nCompared to the feedforward networks in this study, biological visual systems excel at detecting relations. Fleuret et al. (2011) found that humans can learn rather complicated visual rules and generalize them to new instances from just a few SVRT training examples. Their participants could learn the rule underlying the hardest SVRT problem for CNNs in our Experiment 1, problem 20, from an average of about 6 examples. Problem 20 is rather complicated, involving two\nshapes such that \u201cone shape can be obtained from the other by reflection around the perpendicular bisector of the line joining their centers.\u201d In contrast, the best performing network for this problem could not get significantly above chance after one million training examples.\nVisual reasoning ability is not just found in humans. Birds and primates can be trained to recognize same-different relations and then transfer this knowledge to novel objects (Wright & Katz, 2006). A striking example of same-different learning in animals comes from Martinho III & Kacelnik (2016) who showed that newborn ducklings can learn the abstract concept of sameness from a single example. In contrast, we have found in follow-up work that state-of-the-art neural networks demonstrated no ability to transfer the concept of same-different to novel objects even after hundreds of thousands of training examples (Kim et al., in press).\nIt is relatively well accepted that, despite the widespread presence of feedback connections in our visual cortex, certain visual recognition tasks, including the detection of natural object categories, are possible in the near absence of cortical feedback \u2013 based primarily on a single feedforward sweep of activity through our visual cortex (Serre, 2016). However, psychophysical evidence suggests that this feedforward sweep is too spatially coarse to localize objects even when they can be recognized (Evans & Treisman, 2005). The implication is that object localization in clutter requires attention (Zhang et al., 2011). It is difficult to imagine how one could recognize a relation between two objects without spatial information. Indeed, converging evidence (Logan, 1994; Moore et al., 1994; Rosielle et al., 2002; Holcombe et al., 2011; Franconeri et al., 2012; van der Ham et al., 2012) suggests that the processing of spatial relations between pairs of objects in a cluttered scene requires attention, even when individual items can be detected pre-attentively.\nIn follow-up work (Kim et al., in press), we argued that perceptual grouping, a mechanism for binding features into discrete objects (Roelfsema, 2006), is another key nonfeedforward process supporting visual relation detection. We found that relational networks (Santoro et al., 2017), CNN extensions that exhaustively attend to all unbound features in a deep layer, are strained just like CNNs and tend to easily overfit. In contrast, we showed that a network which simulates the effects of perceptual grouping by forcing scene items into separate channels can easily learn our PSVRT tasks without straining. This toy network simulates in a feedforward manner the dynamic sequence of attention shifts between perceptually grouped features believed to underlie visual relation detection (Franconeri et al., 2012). These dynamic representations built \u201con-the-fly\u201d circumvent the combinatorial explosion associated with the storage of synaptic templates for all possible relations, helping to prevent the capacity overload associated with feedforward neural networks.\nHumans can easily detect when two objects are the same up to some transformation (Shepard & Metzler, 1971) or when objects exist in a given spatial relation (Fleuret et al., 2011; Franconeri et al., 2012). More generally, humans can effort-\nlessly construct an unbounded set of structured descriptions about their visual world (Geman et al., 2015). Given the vast superiority of humans over modern computers in their ability to detect visual relations, we see the exploration of attentional and grouping mechanisms as an important next step in our computational understanding of visual reasoning. Acknowledgments The authors would like to thank Drs. Drew Linsley and Sven Eberhardt for their advice, along with Dan Shiebler for earlier work. This research was supported by NSF early career award (IIS-1252951) and DARPA young faculty award (YFA N66001-14-1-4037). Additional support was provided by the Center for Computation and Visualization (CCV) at Brown University. This material is based upon work supported by author MR\u2019s National Science Foundation Graduate Research Fellowship under Grant No. 1644760."
        }
    ],
    "title": "Same-different problems strain convolutional neural networks",
    "year": 2018
}