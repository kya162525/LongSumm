{
    "abstractText": "Recent approaches for high accuracy detection and tracking of object categories in video consist of complex multistage solutions that become more cumbersome each year. In this paper we propose a ConvNet architecture that jointly performs detection and tracking, solving the task in a simple and effective way. Our contributions are threefold: (i) we set up a ConvNet architecture for simultaneous detection and tracking, using a multi-task objective for frame-based object detection and across-frame track regression; (ii) we introduce correlation features that represent object co-occurrences across time to aid the ConvNet during tracking; and (iii) we link the frame level detections based on our across-frame tracklets to produce high accuracy detections at the video level. Our ConvNet architecture for spatiotemporal object detection is evaluated on the large-scale ImageNet VID dataset where it achieves state-of-the-art results. Our approach provides better single model performance than the winning method of the last ImageNet challenge while being conceptually much simpler. Finally, we show that by increasing the temporal stride we can dramatically increase the tracker speed. Our code and models are available at http://github.com/feichtenhofer/detect-track",
    "authors": [
        {
            "affiliations": [],
            "name": "Christoph Feichtenhofer"
        },
        {
            "affiliations": [],
            "name": "Axel Pinz"
        },
        {
            "affiliations": [],
            "name": "Andrew Zisserman"
        }
    ],
    "id": "SP:5af1fe5b91d3c68d9804b60eb3ecbc85a6c0ed61",
    "references": [
        {
            "authors": [
                "L. Bertinetto",
                "J. Valmadre",
                "J.F. Henriques",
                "A. Vedaldi",
                "P.H.S. Torr"
            ],
            "title": "Fully-convolutional siamese networks for object tracking",
            "venue": "ECCV VOT Workshop",
            "year": 2016
        },
        {
            "authors": [
                "D.S. Bolme",
                "J.R. Beveridge",
                "B.A. Draper",
                "Y.M. Lui"
            ],
            "title": "Visual object tracking using adaptive correlation filters",
            "venue": "Proc. CVPR",
            "year": 2010
        },
        {
            "authors": [
                "J. Dai",
                "Y. Li",
                "K. He",
                "J. Sun"
            ],
            "title": "R-FCN: Object detection via region-based fully convolutional networks",
            "venue": "NIPS",
            "year": 2016
        },
        {
            "authors": [
                "M. Danelljan",
                "A. Robinson",
                "F.S. Khan",
                "M. Felsberg"
            ],
            "title": "Beyond correlation filters: Learning continuous convolution operators for visual tracking",
            "venue": "Proc. ECCV",
            "year": 2016
        },
        {
            "authors": [
                "A. Dosovitskiy",
                "P. Fischer",
                "E. Ilg",
                "P. Hausser",
                "C. Hazirbas",
                "V. Golkov"
            ],
            "title": "P",
            "venue": "van der Smagt, D. Cremers, and T. Brox. Flownet: Learning optical flow with convolutional networks. In Proc. ICCV",
            "year": 2015
        },
        {
            "authors": [
                "C. Feichtenhofer",
                "A. Pinz",
                "R. Wildes"
            ],
            "title": "Spatiotemporal residual networks for video action recognition",
            "venue": "NIPS",
            "year": 2016
        },
        {
            "authors": [
                "C. Feichtenhofer",
                "A. Pinz",
                "A. Zisserman"
            ],
            "title": "Convolutional two-stream network fusion for video action recognition",
            "venue": "Proc. CVPR",
            "year": 2016
        },
        {
            "authors": [
                "S. Gidaris",
                "N. Komodakis"
            ],
            "title": "Object detection via a multiregion and semantic segmentation-aware cnn model",
            "venue": "Proc. CVPR",
            "year": 2015
        },
        {
            "authors": [
                "R.B. Girshick"
            ],
            "title": "Fast R-CNN",
            "venue": "Proc. ICCV",
            "year": 2015
        },
        {
            "authors": [
                "R.B. Girshick",
                "J. Donahue",
                "T. Darrell",
                "J. Malik"
            ],
            "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
            "venue": "Proc. CVPR",
            "year": 2014
        },
        {
            "authors": [
                "G. Gkioxari",
                "J. Malik"
            ],
            "title": "Finding action tubes",
            "venue": "Proc. CVPR",
            "year": 2015
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proc. CVPR",
            "year": 2016
        },
        {
            "authors": [
                "D. Held",
                "S. Thrun",
                "S. Savarese"
            ],
            "title": "Learning to track at 100 FPS with deep regression networks",
            "venue": "Proc. ECCV",
            "year": 2016
        },
        {
            "authors": [
                "J.F. Henriques",
                "R. Caseiro",
                "P. Martins",
                "J. Batista"
            ],
            "title": "Highspeed tracking with kernelized correlation filters",
            "venue": "IEEE PAMI, 37(3):583\u2013596",
            "year": 2015
        },
        {
            "authors": [
                "A. Joulin",
                "K. Tang",
                "L. Fei-Fei"
            ],
            "title": "Efficient image and video co-localization with frank-wolfe algorithm",
            "venue": "Proc. ECCV",
            "year": 2014
        },
        {
            "authors": [
                "K. Kang",
                "H. Li",
                "T. Xiao",
                "W. Ouyang",
                "J. Yan",
                "X. Liu",
                "X. Wang"
            ],
            "title": "Object detection in videos with tubelet proposal networks",
            "venue": "Proc. CVPR",
            "year": 2017
        },
        {
            "authors": [
                "K. Kang",
                "H. Li",
                "J. Yan",
                "X. Zeng",
                "B. Yang",
                "T. Xiao",
                "C. Zhang",
                "Z. Wang",
                "R. Wang",
                "X. Wang",
                "W. Ouyang"
            ],
            "title": "T-CNN: tubelets with convolutional neural networks for object detection from videos",
            "venue": "arXiv preprint",
            "year": 2016
        },
        {
            "authors": [
                "K. Kang",
                "W. Ouyang",
                "H. Li",
                "X. Wang"
            ],
            "title": "Object detection from video tubelets with convolutional neural networks",
            "venue": "Proc. CVPR",
            "year": 2016
        },
        {
            "authors": [
                "A. Krizhevsky",
                "I. Sutskever",
                "G.E. Hinton"
            ],
            "title": "ImageNet classification with deep convolutional neural networks",
            "venue": "NIPS, pages 1106\u20131114",
            "year": 2012
        },
        {
            "authors": [
                "S. Kwak",
                "M. Cho",
                "I. Laptev",
                "J. Ponce",
                "C. Schmid"
            ],
            "title": "Unsupervised object discovery and tracking in video collections",
            "venue": "Proc. CVPR",
            "year": 2015
        },
        {
            "authors": [
                "Y. LeCun",
                "B. Boser",
                "J.S. Denker",
                "D. Henderson",
                "R.E. Howard",
                "W. Hubbard",
                "L.D. Jackel"
            ],
            "title": "Backpropagation applied to handwritten zip code recognition",
            "venue": "Neural Computation, 1(4):541\u2013551",
            "year": 1989
        },
        {
            "authors": [
                "T.-Y. Lin",
                "M. Maire",
                "S. Belongie",
                "J. Hays",
                "P. Perona",
                "D. Ramanan",
                "P. Doll\u00e1r",
                "C.L. Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "Proc. ECCV",
            "year": 2014
        },
        {
            "authors": [
                "W. Liu",
                "D. Anguelov",
                "D. Erhan",
                "C. Szegedy",
                "S. Reed",
                "C.-Y. Fu",
                "A.C. Berg"
            ],
            "title": "Ssd: Single shot multibox detector",
            "venue": "Proc. ECCV",
            "year": 2016
        },
        {
            "authors": [
                "J. Long",
                "E. Shelhamer",
                "T. Darrell"
            ],
            "title": "Fully convolutional networks for semantic segmentation",
            "venue": "Proc. CVPR",
            "year": 2015
        },
        {
            "authors": [
                "C. Ma",
                "J.-B. Huang",
                "X. Yang",
                "M.-H. Yang"
            ],
            "title": "Hierarchical convolutional features for visual tracking",
            "venue": "Proc. ICCV",
            "year": 2015
        },
        {
            "authors": [
                "H. Nam",
                "B. Han"
            ],
            "title": "Learning multi-domain convolutional neural networks for visual tracking",
            "venue": "Proc. CVPR",
            "year": 2016
        },
        {
            "authors": [
                "X. Peng",
                "C. Schmid"
            ],
            "title": "Multi-region two-stream R-CNN for action detection",
            "venue": "Proc. ECCV",
            "year": 2016
        },
        {
            "authors": [
                "A. Prest",
                "C. Leistner",
                "J. Civera",
                "C. Schmid",
                "V. Ferrari"
            ],
            "title": "Learning object class detectors from weakly annotated video",
            "venue": "Proc. CVPR",
            "year": 2012
        },
        {
            "authors": [
                "E. Real",
                "J. Shlens",
                "S. Mazzocchi",
                "X. Pan",
                "V. Vanhoucke"
            ],
            "title": "YouTube-BoundingBoxes: A Large High-Precision Human- Annotated Data Set for Object Detection in Video",
            "venue": "ArXiv e-prints",
            "year": 2017
        },
        {
            "authors": [
                "J. Redmon",
                "S. Divvala",
                "R. Girshick",
                "A. Farhadi"
            ],
            "title": "You only look once: Unified",
            "venue": "real-time object detection. In Proc. CVPR",
            "year": 2016
        },
        {
            "authors": [
                "S. Ren",
                "K. He",
                "R. Girshick",
                "J. Sun"
            ],
            "title": "Faster R-CNN: Towards real-time object detection with region proposal networks",
            "venue": "IEEE PAMI",
            "year": 2016
        },
        {
            "authors": [
                "O. Russakovsky",
                "J. Deng",
                "H. Su",
                "J. Krause",
                "S. Satheesh",
                "S. Ma",
                "Z. Huang",
                "A. Karpathy",
                "A. Khosla",
                "M. Bernstein",
                "A.C. Berg",
                "L. Fei-Fei"
            ],
            "title": "ImageNet Large Scale Visual Recognition Challenge",
            "venue": "IJCV, 115(3):211\u2013252",
            "year": 2015
        },
        {
            "authors": [
                "S. Saha",
                "G. Singh",
                "M. Sapienza",
                "P.H. Torr",
                "F. Cuzzolin"
            ],
            "title": "Deep learning for detecting multiple space-time action tubes in videos",
            "venue": "Proc. BMVC.",
            "year": 2016
        },
        {
            "authors": [
                "A. Shrivastava",
                "A. Gupta",
                "R. Girshick"
            ],
            "title": "Training regionbased object detectors with online hard example mining",
            "venue": "Proc. CVPR",
            "year": 2016
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Two-stream convolutional networks for action recognition in videos",
            "venue": "NIPS",
            "year": 2014
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "Proc. ICLR",
            "year": 2015
        },
        {
            "authors": [
                "C. Szegedy",
                "S. Ioffe",
                "V. Vanhoucke",
                "A.A. Alemi"
            ],
            "title": "Inception-v4, Inception-ResNet and the impact of residual connections on learning",
            "year": 2017
        },
        {
            "authors": [
                "C. Szegedy",
                "W. Liu",
                "Y. Jia",
                "P. Sermanet",
                "S. Reed",
                "D. Anguelov",
                "D. Erhan",
                "V. Vanhoucke",
                "A. Rabinovich"
            ],
            "title": "Going deeper with convolutions",
            "venue": "Proc. CVPR",
            "year": 2015
        },
        {
            "authors": [
                "L. Wang",
                "W. Ouyang",
                "X. Wang",
                "H. Lu"
            ],
            "title": "Visual tracking with fully convolutional networks",
            "venue": "Proc. ICCV",
            "year": 2015
        },
        {
            "authors": [
                "S. Xie",
                "R. Girshick",
                "P. Doll\u00e1r",
                "Z. Tu",
                "K. He"
            ],
            "title": "Aggregated residual transformations for deep neural networks",
            "venue": "arXiv preprint arXiv:1611.05431",
            "year": 2016
        },
        {
            "authors": [
                "J. Yang",
                "H. Shuai",
                "Z. Yu",
                "R. Fan",
                "Q. Ma",
                "Q. Liu",
                "J. Deng"
            ],
            "title": "ILSVRC2016 object detection from video: Team NUIST",
            "venue": "http://image-net.org/challenges/talks/ 2016/Imagenet%202016%20VID.pptx",
            "year": 2016
        },
        {
            "authors": [
                "X. Zhu",
                "Y. Xiong",
                "J. Dai",
                "L. Yuan",
                "Y. Wei"
            ],
            "title": "Deep feature flow for video recognition",
            "venue": "Proc. CVPR",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "Our contributions are threefold: (i) we set up a ConvNet architecture for simultaneous detection and tracking, using a multi-task objective for frame-based object detection and across-frame track regression; (ii) we introduce correlation features that represent object co-occurrences across time to aid the ConvNet during tracking; and (iii) we link the frame level detections based on our across-frame tracklets to produce high accuracy detections at the video level. Our ConvNet architecture for spatiotemporal object detection is evaluated on the large-scale ImageNet VID dataset where it achieves state-of-the-art results. Our approach provides better single model performance than the winning method of the last ImageNet challenge while being conceptually much simpler. Finally, we show that by increasing the temporal stride we can dramatically increase the tracker speed. Our code and models are available at http://github.com/feichtenhofer/detect-track"
        },
        {
            "heading": "1. Introduction",
            "text": "Object detection in images has received a lot of attention over the last years with tremendous progress mostly due to the emergence of deep Convolutional Networks [12, 19, 21, 36, 38] and their region based descendants [3, 9, 10, 31]. In the case of object detection and tracking in videos, recent approaches have mostly used detection as a first step, followed by post-processing methods such as applying a tracker to propagate detection scores over time. Such variations on the \u2018tracking by detection\u2019 paradigm have seen impressive progress but are dominated\n\u2217Christoph Feichtenhofer is a recipient of a DOC Fellowship of the Austrian Academy of Sciences at the Institute of Electrical Measurement and Measurement Signal Processing, Graz University of Technology.\nby frame-level detection methods. Object detection in video has seen a surge in interest lately, especially since the introduction of the ImageNet [32] video object detection challenge (VID). Different from the ImageNet object detection (DET) challenge, VID shows objects in image sequences and comes with additional challenges of (i) size: the sheer number of frames that video provides (VID has around 1.3M images, compared to around 400K in DET or 100K in COCO [22]), (ii) motion blur: due to rapid camera or object motion, (iii) quality: internet video clips are typically of lower quality than static photos, (iv) partial occlusion: due to change in objects/viewer positioning, and (v) pose: unconventional object-to-camera poses are frequently seen in video. In Fig. 1, we show example images from the VID dataset; for more examples please see1.\nTo solve this challenging task, recent top entries in the ImageNet [32] video detection challenge use exhaustive post-processing on top of frame-level detectors. For example, the winner [17] of ILSVRC\u201915 uses two multi-stage Faster R-CNN [31] detection frameworks, context suppression, multi-scale training/testing, a ConvNet tracker [39], optical-flow based score propagation and model ensembles.\nIn this paper we propose a unified approach to tackle the problem of object detection in realistic video. Our objective is to directly infer a \u2018tracklet\u2019 over multiple frames by simultaneously carrying out detection and tracking with a\n1http://vision.cs.unc.edu/ilsvrc2015/ui/vid\nar X\niv :1\n71 0.\n03 95\n8v 2\n[ cs\n.C V\n] 7\nM ar\n2 01\n8\nConvNet. To achieve this we propose to extend the R-FCN [3] detector with a tracking formulation that is inspired by current correlation and regression based trackers [1, 13, 25]. We train a fully convolutional architecture end-to-end using a detection and tracking based loss and term our approach D&T for joint Detection and Tracking. The input to the network consists of multiple frames which are first passed through a ConvNet trunk (e.g. a ResNet-101 [12]) to produce convolutional features which are shared for the task of detection and tracking. We compute convolutional cross-correlation between the feature responses of adjacent frames to estimate the local displacement at different feature scales. On top of the features, we employ an RoIpooling layer [3] to classify and regress box proposals as well as an RoI-tracking layer that regresses box transformations (translation, scale, aspect ratio) across frames. Finally, to infer long-term tubes of objects across a video we link detections based on our tracklets.\nAn evaluation on the large-scale ImageNet VID dataset shows that our approach is able to achieve better singlemodel performance than the winner of the last ILSVRC\u201916 challenge, despite being conceptually simple and much faster. Moreover, we show that including a tracking loss may improve feature learning for better static object detection, and we also present a very fast version of D&T that works on temporally-strided input frames."
        },
        {
            "heading": "2. Related work",
            "text": "Object detection. Two families of detectors are currently popular: First, region proposal based detectors R-CNN [10], Fast R-CNN [9], Faster R-CNN [31] and R-FCN [3] and second, detectors that directly predict boxes for an image in one step such as YOLO [30] and SSD [23].\nOur approach builds on R-FCN [3] which is a simple and efficient framework for object detection on region proposals with a fully convolutional nature. In terms of accuracy it is competitive with Faster R-CNN [31] which uses a multilayer network that is evaluated per-region (and thus has a cost growing linearly with the number of candidate RoIs). R-FCN reduces the cost for region classification by pushing the region-wise operations to the end of the network with the introduction of a position-sensitive RoI pooling layer which works on convolutional features that encode the spatially subsampled class scores of input RoIs.\nTracking. Tracking is also an extensively studied problem in computer vision with most recent progress devoted to trackers operating on deep ConvNet features. In [26] a ConvNet is fine-tuned at test-time to track a target from the same video via detection and bounding box regression. Training on the examples of a test sequence is slow and also not applicable in the object detection setting. Other methods use pre-trained ConvNet features to track and have\nachieved strong performance either with correlation [1, 25] or regression trackers on heat maps [39] or bounding boxes [13]. The regression tracker in [13] is related to our method. It is based on a Siamese ConvNet that predicts the location in the second image of the object shown in the center of the previous image. Since this tracker predicts a bounding box instead of just the position, it is able to model changes in scale and aspect of the tracked template. The major drawback of this approach is that it only can process a single target template and it also has to rely on significant data augmentation to learn all possible transformations of tracked boxes. The approach in [1] is an example of a correlation tracker and inspires our method. The tracker also uses a fully-convolutional Siamese network that takes as input the tracking template and the search image. The ConvNet features from the last convolutional layer are correlated to find the target position in the response map. One drawback of many correlation trackers [1, 25] is that they only work on single targets and do not account for changes in object scale and aspect ratio.\nVideo object detection. Action detection is also a related problem and has received increased attention recently, mostly with methods building on two-stream ConvNets [35]. In [11] a method is presented that uses a two-stream R-CNN [10] to classify regions and link them across frames based on the action predictions and their spatial overlap. This method has been adopted by [33] and [27] where the R-CNN was replaced by Faster R-CNN with the RPN operating on two streams of appearance and motion information.\nOne area of interest is learning to detect and localize in each frame (e.g. in video co-localization) with only weak supervision. The YouTube Object Dataset [28], has been used for this purpose, e.g. [15, 20].\nSince the object detection from video task has been introduced at the ImageNet challenge, it has drawn significant attention. In [18] tubelet proposals are generated by applying a tracker to frame-based bounding box proposals. The detector scores across the video are re-scored by a 1D CNN model. In their corresponding ILSVRC submission the group [17] added a propagation of scores to nearby frames based on optical flows between frames and suppression of class scores that are not among the top classes in a video. A more recent work [16] introduces a tubelet proposal network that regresses static object proposals over multiple frames, extracts features by applying Faster R-CNN which are finally processed by an encoder-decoder LSTM. In deep feature flow [42] a recognition ConvNet is applied to key frames only and an optical flow ConvNet is used for propagating the deep feature maps via a flow field to the rest of the frames. This approach can increase detection speed by a factor of 5 at a slight accuracy cost. The approach is error-prone due largely to two aspects: First,\npropagation from the key frame to the current frame can be erroneous and, second, the key frames can miss features from current frames. Very recently a new large-scale dataset for video object detection has been introduced [29] with single objects annotations over video sequences."
        },
        {
            "heading": "3. D&T Approach",
            "text": "In this section we first give an overview of the Detect and Track (D&T) approach (Sect. 3.1) that generates tracklets given two (or more) frames as input. We then give the details, starting with the baseline R-FCN detector [3] (Sect. 3.2), and formulating the tracking objective as crossframe bounding box regression (Sect. 3.3); finally, we introduce the correlation features (Sect. 3.4) that aid the network in the tracking process.\nSect. 4 shows how we link across-frame tracklets to tubes over the temporal extent of a video, and Sect. 5 describes how we apply D&T to the ImageNet VID challenge."
        },
        {
            "heading": "3.1. D&T overview",
            "text": "We aim at jointly detecting and tracking (D&T) objects in video. Fig. 2 illustrates our D&T architecture. We build on the R-FCN [3] object detection framework which is fully convolutional up to region classification and regression, and extend it for multi-frame detection and tracking. Given a set of two high-resolution input frames our architecture first computes convolutional feature maps that are shared for the tasks of detection and tracking (e.g. the features of a ResNet-101[12]). An RPN is used to propose candidate regions in each frame based on the objectness likelihood for pre-defined candidate boxes (i.e. \u201canchors\u201d[31]). Based on these regions, RoI pooling is employed to aggregate position-sensitive score and regression maps, produced from intermediate convolutional layers, to classify boxes and refine their coordinates (regression), respectively.\nWe extend this architecture by introducing a regressor that takes the intermediate position-sensitive regression maps from both frames (together with correlation maps, see below) as input to an RoI tracking operation which outputs the box transformation from one frame to the other. The correspondence between frames is thus simply accomplished by pooling features from both frames, at the same proposal region. We train the RoI tracking task by extending the multi-task objective of R-FCN with a tracking loss that regresses object coordinates across frames. Our tracking loss operates on ground truth objects and evaluates a soft L1 norm [9] between coordinates of the predicted track and the ground truth track of an object.\nSuch a tracking formulation can be seen as a multiobject extension of the single target tracker in [13] where a ConvNet is trained to infer an object\u2019s bounding box from features of the two frames. One drawback of such an approach is that it does not exploit translational equivariance\nwhich means that the tracker has to learn all possible translations from training data. Thus such a tracker requires exceptional data augmentation (artificially scaling and shifting boxes) during training [13] .\nA tracking representation that is based on correlation filters [2, 4, 14] can exploit the translational equivariance as correlation is equivariant to translation. Recent correlation trackers [1, 25] typically work on high-level ConvNet features and compute the cross correlation between a tracking template and the search image (or a local region around the tracked position from the previous frame). The resulting correlation map measures the similarity between the template and the search image for all circular shifts along the horizontal and vertical dimension. The displacement of a target object can thus be found by taking the maximum of the correlation response map.\nDifferent from typical correlation trackers that work on single target templates, we aim to track multiple objects simultaneously. We compute correlation maps for all positions in a feature map and let RoI tracking additionally operate on these feature maps for better track regression. Our architecture is able to be trained end-to-end taking as input frames from a video and producing object detections and their tracks. The next sections describe how we structure our architecture for end-to-end learning of object detection and tracklets."
        },
        {
            "heading": "3.2. Object detection and tracking in R-FCN",
            "text": "Our architecture takes frames It \u2208 RH0\u00d7W0\u00d73 at time t and pushes them through a backbone ConvNet (i.e. ResNet101 [12]) to obtain feature maps xtl \u2208 RHl\u00d7Wl\u00d7Dl where Wl, Hl and Dl are the width, height and number of channels of the respective feature map output by layer l. As in R-FCN [3] we reduce the effective stride at the last convolutional layer from 32 pixels to 16 pixels by modifying the conv5 block to have unit spatial stride, and also increase its receptive field by dilated convolutions [24].\nOur overall system builds on the R-FCN [3] object detector which works in two stages: first it extracts candidate regions of interest (RoI) using a Region Proposal Network (RPN) [31]; and, second, it performs region classification into different object categories and background by using a position-sensitive RoI pooling layer [3]. The input to this RoI pooling layer comes from an extra convolutional layer with output xtcls that operates on the last convolutional layer of a ResNet [12]. The layer produces a bank of Dcls = k2(C + 1) position-sensitive score maps which correspond to a k \u00d7 k spatial grid describing relative positions to be used in the RoI pooling operation for each of the C categories and background. Applying the softmax function to the outputs leads to a probability distribution p over C + 1 classes for each RoI. In a second branch R-FCN puts a sibling convolutional layer with output xtreg after the\nlast convolutional layer for bounding box regression, again a position-sensitive RoI pooling operation is performed on this bank of Dcls = 4k2 maps for class-agnostic bounding box prediction of a box b = (bx, by, bw, bh).\nLet us now consider a pair of frames It, It+\u03c4 , sampled at time t and t+\u03c4 , given as input to the network. We introduce an inter-frame bounding box regression layer that performs position sensitive RoI pooling on the concatenation of the bounding box regression features {xtreg,xt+\u03c4reg } to predict the transformation \u2206t+\u03c4 = (\u2206t+\u03c4x ,\u2206 t+\u03c4 y ,\u2206 t+\u03c4 w ,\u2206 t+\u03c4 h ) of the RoIs from t to t + \u03c4 . The correlation features, that are also used by the bounding box regressors, are described in section 3.4. Fig. 3 shows an illustration of this approach."
        },
        {
            "heading": "3.3. Multitask detection and tracking objective",
            "text": "To learn this regressor, we extend the multi-task loss of Fast R-CNN [9], consisting of a combined classification Lcls and regression loss Lreg , with an additional term that scores the tracking across two frames Ltra. For a single iteration and a batch of N RoIs the network predicts softmax probabilities {pi}Ni=1, regression offsets {bi}Ni=1, and cross-frame RoI-tracks {\u2206t+\u03c4i } Ntra i=1 . Our overall objective function is written as:\nL({pi}, {bi}, {\u2206i}) = 1\nN N\u2211 i=1 Lcls(pi,c\u2217)\n+\u03bb 1\nNfg N\u2211 i=1 [c\u2217i > 0]Lreg(bi, b \u2217 i )\n+\u03bb 1\nNtra Ntra\u2211 i=1 Ltra(\u2206 t+\u03c4 i ,\u2206 \u2217,t+\u03c4 i ).\n(1)\nThe ground truth class label of an RoI is defined by c\u2217i and its predicted softmax score is pi,c\u2217 . b\u2217i is the ground truth regression target, and \u2206\u2217,t+\u03c4i is the track regression target. The indicator function [c\u2217i > 0] is 1 for fore-\nground RoIs and 0 for background RoIs (with c\u2217i = 0). Lcls(pi,c\u2217) = \u2212 log(pi,c\u2217) is the cross-entropy loss for box classification, and Lreg & Ltra are bounding box and track regression losses defined as the smooth L1 function in [9]. The tradeoff parameter is set to \u03bb = 1 as in [3, 9]. The assignment of RoIs to ground truth is as follows: a class label c\u2217 and regression targets b\u2217 are assigned if the RoI overlaps with a ground-truth box at least by 0.5 in intersection-overunion (IoU) and the tracking target \u2206\u2217,t+\u03c4 is assigned only to ground truth targets which are appearing in both frames. Thus, the first term of (1) is active for allN boxes in a training batch, the second term is active forNfg foreground RoIs and the last term is active for Ntra ground truth RoIs which have a track correspondence across the two frames.\nFor track regression we use the bounding box regression parametrisation of R-CNN [9, 10, 31]. For a single object we have ground truth box coordinates Bt = (Btx, B t y, B t w, B t h) in frame t, and similarly B\nt+\u03c4 for frame t+ \u03c4 , denoting the horizontal & vertical centre coordinates and its width and height. The tracking regression values for the target \u2206\u2217,t+\u03c4 = {\u2206\u2217,t+\u03c4x ,\u2206\u2217,t+\u03c4y ,\u2206\u2217,t+\u03c4w ,\u2206 \u2217,t+\u03c4 h } are then\n\u2206\u2217,t+\u03c4x = Bt+\u03c4x \u2212Btx\nBtw \u2206\u2217,t+\u03c4y = Bt+\u03c4y \u2212Bty Bth\n(2)\n\u2206\u2217,t+\u03c4w = log( Bt+\u03c4w Btw ) \u2206\u2217,t+\u03c4h = log( Bt+\u03c4h Bth )). (3)"
        },
        {
            "heading": "3.4. Correlation features for object tracking",
            "text": "Different from typical correlation trackers on single target templates, we aim to track multiple objects simultaneously. We compute correlation maps for all positions in a feature map and let RoI pooling operate on these feature maps for track regression. Considering all possible circular shifts in a feature map would lead to large output dimensionality and also produce responses for too large displace-\nments. Therefore, we restrict correlation to a local neighbourhood. This idea was originally used for optical flow estimation in [5], where a correlation layer is introduced to aid a ConvNet in matching feature points between frames. The correlation layer performs point-wise feature comparison of two feature maps xtl ,x t+\u03c4 l\nxt,t+\u03c4corr (i, j, p, q) = \u2329 xtl(i, j),x t+\u03c4 l (i+ p, j + q) \u232a (4)\nwhere \u2212d \u2264 p \u2264 d and \u2212d \u2264 q \u2264 d are offsets to compare features in a square neighbourhood around the locations i, j in the feature map, defined by the maximum displacement, d. Thus the output of the correlation layer is a feature map of size xcorr \u2208 RHl\u00d7Wl\u00d7(2d+1)\u00d7(2d+1). Equation (4) can be seen as a correlation of two feature maps within a local square window defined by d. We compute this local correlation for features at layers conv3, conv4 and conv5 (we use a stride of 2 in i, j to have the same size in the conv3 correlation). We show an illustration of these features for two sample sequences in Fig. 4.\nTo use these features for track-regression, we let RoI pooling operate on these maps by stacking them with the bounding box features in Sect. 3.2 {xt,t+\u03c4corr ,xtreg,xt+\u03c4reg }."
        },
        {
            "heading": "4. Linking tracklets to object tubes",
            "text": "One drawback of high-accuracy object detection is that high-resolution input images have to be processed which\nputs a hard constraint on the number of frames a (deep) architecture can process in one iteration (due to memory limitations in GPU hardware). Therefore, a tradeoff between the number of frames and detection accuracy has to be made. Since video possesses a lot of redundant information and objects typically move smoothly in time we can use our inter-frame tracks to link detections in time and build longterm object tubes. To this end, we adopt an established technique from action localization [11, 27, 33], which is used to to link frame detections in time to tubes.\nConsider the class detections for a frame at time t, Dt,ci = {xti, yti , wti , hti, pti,c}, where D t,c i is a box indexed by i, centred at (xti, y t i) with width w t i and height h t i, and pti,c is the softmax probability for class c. Similarly, we also have tracks T t,t+\u03c4i = {xti, yti , wti , hti;xti + \u2206t+\u03c4x , yti + \u2206t+\u03c4y , w t i+\u2206 t+\u03c4 w , h t i+\u2206 t+\u03c4 h } that describe the transformation of the boxes from frame t to t+\u03c4 . We can now define a class-wise linking score that combines detections and tracks across time\nsc(D t i,c, D t+\u03c4 j,c , T t,t+\u03c4 ) = pti,c + p t+\u03c4 j,c +\u03c8(D t i , Dj , T t,t+\u03c4 ) (5) where the pairwise score is\n\u03c8(Dti,c, D t+\u03c4 j,c , T t,t+\u03c4 ) =\n{ 1, if Dti , D t+\u03c4 j \u2208 T t,t+\u03c4 ,\n0, otherwise. (6)\nHere, the pairwise term \u03c8 evaluates to 1 if the IoU over-\nlap a track correspondences T t,t+\u03c4 with the detection boxes Dti , D t+\u03c4 j is larger than 0.5. This is necessary, because the output of the track regressor does not have to exactly match the output of the box regressor.\nThe optimal path across a video can then be found by maximizing the scores over the duration T of the video [11]\nD\u0304?c = argmax D\u0304\n1\nT T \u2212\u03c4\u2211 t=1 sc(D t, Dt+\u03c4 , T t,t+\u03c4 ). (7)\nEq. (7) can be solved efficiently by applying the Viterbi algorithm [11]. Once the optimal tube D\u0304?c is found, the detections corresponding to that tube are removed from the set of regions and (7) is applied again to the remaining regions.\nAfter having found the class-specific tubes D\u0304c for one video, we re-weight all detection scores in a tube by adding the mean of the \u03b1 = 50% highest scores in that tube. We found that overall performance is largely robust to that parameter, with less than 0.5% mAP variation when varying 10% \u2264 \u03b1 \u2264 100%. Our simple tube-based re-weighting aims to boost the scores for positive boxes on which the detector fails. Using the highest scores of a tube for reweighting acts as a form of non-maximum suppression. It is also inspired by the hysteresis tracking in the Canny edge detector. Our reweighting assumes that the detector fails at most in half of a tubes frames, and improves robustness of the tracker, though the performance is quite insensitive to the proportion chosen (\u03b1). Note that our approach enforces the tube to span the whole video and, for simplicity, we do not prune any detections in time. Removing detections with subsequent low scores along a tube (e.g. [27, 33]) could clearly improve the results, but we leave that for future work. In the following section our approach is applied to the video object detection task."
        },
        {
            "heading": "5. Experiments",
            "text": ""
        },
        {
            "heading": "5.1. Dataset sampling and evaluation",
            "text": "We evaluate our method on the ImageNet [32] object detection from video (VID) dataset2 which contains 30 classes in 3862 training and 555 validation videos. The objects have ground truth annotations of their bounding box and track ID in a video. Since the ground truth for the test set is not publicly available, we measure performance as mean average precision (mAP) over the 30 classes on the validation set by following the protocols in [16, 17, 18, 42], as is standard practice.\nThe 30 object categories in ImageNet VID are a subset of the 200 categories in the ImageNet DET dataset. Thus we follow previous approaches [16, 17, 18, 42] and train our R-FCN detector on an intersection of ImageNet VID and DET set (only using the data from the 30 VID classes). Since the DET set contains large variations in the number of samples per class, we sample at most 2k images per class from DET. We also subsample the VID training set by using only 10 frames from each video. The subsampling reduces the effect of dominant classes in DET (e.g. there are 56K images for the dog class in the DET training set) and very long video sequences in the VID training set."
        },
        {
            "heading": "5.2. Training and testing",
            "text": "RPN. Our RPN is trained as originally proposed [31]. We attach two sibling convolutional layers to the stride-reduced ResNet-101 (Sect. 3.2) to perform proposal classification and bounding box regression at 15 anchors corresponding to 5 scales and 3 aspect ratios. As in [31] we also extract proposals from 5 scales and apply non-maximum suppression (NMS) with an IoU threshold of 0.7 to select the top\n2http://www.image-net.org/challenges/LSVRC/\n300 proposals in each frame for training/testing our R-FCN detector. We found that pre-training on the full ImageNet DET set helps to increase the recall; thus, our RPN is first pre-trained on the 200 classes of ImageNet DET before fine-tuning on only the 30 classes which intersect ImageNet DET and VID. Our 300 proposals per image achieve a mean recall of 96.5% on the ImageNet VID validation set.\nR-FCN. Our R-FCN detector is trained similar to [3, 42]. We use the stride-reduced ResNet-101 with dilated convolution in conv5 (see Sect. 3.2) and online hard example mining [34]. A randomly initialized 3 \u00d7 3, dilation 6 convolutional layer is attached to conv5 for reducing the feature dimension to 512 [42] (in the original R-FCN this is a 1 \u00d7 1 convolutional layer without dilation and an output dimension of 1024). For object detection and box regression, two sibling 1\u00d7 1 convolutional layers provide the Dcls = k\n2(C + 1) and Dreg = 4k2 inputs to the positionsensitive RoI pooling layer. We use a k\u00d7 k = 7\u00d7 7 spatial grid for encoding relative positions as in [3].\nIn both training and testing, we use single scale images with shorter dimension of 600 pixels. We use a batch size of 4 in SGD training and a learning rate of 10\u22123 for 60K iterations followed by a learning rate of 10\u22124 for 20K iterations. For testing we apply NMS with IoU threshold of 0.3.\nD & T. For training our D&T architecture we start with the R-FCN model from above and further fine-tune it on the full ImageNet VID training set with randomly sampling a set of two adjacent frames from a different video in each iteration. In each other iteration we also sample from the ImageNet DET training set to avoid biasing our model to the VID training set. When sampling from the DET set we send the same two frames through the network as there are no sequences available. Besides not forgetting the images from the DET training set, this has an additional beneficial effect of letting our model prefer small motions over large ones (e.g. the tracker in [13] samples motion augmentation from a Laplacian distribution with zero mean to bias a regression tracker on small displacements). Our correlation features (4) are computed at layers conv3, conv4 and conv5 with a maximum displacement of d = 8 and a stride of 2 in i, j for the the conv3 correlation. For training, we use a learning rate of 10\u22124 for 40K iterations and 10\u22125 for 20K iterations at a batch size of 4. During testing our architecture is applied to a sequence with temporal stride \u03c4 , predicting detections D and tracklets T between them. For objectcentred tracks, we use the regressed frame boxes as input of the ROI-tracking layer. We perform non-maximum suppression with bounding-box voting [8] before the tracklet linking step to reduce the number of detections per image and class to 25. These detections are then used in eq. (7) to extract tubes and the corresponding detection boxes are re-weighted as outlined in Sect. 4 for evaluation."
        },
        {
            "heading": "5.3. Results",
            "text": "We show experimental results for our models and the current state-of-the-art in Table 1. Qualitative results for difficult validation videos can be seen in Fig. 5 and also at http://www.robots.ox.ac.uk/\u02dcvgg/ research/detect-track/ Frame level methods. First we compare methods working on single frames without any temporal processing. Our R-FCN baseline achieves 74.2% mAP which compares favourably to the best performance of 73.9% mAP in [42]. We think our slightly better accuracy comes from the use of 15 anchors for RPN instead of the 9 anchors in [42]. The Faster R-CNN models working as single frame baselines in [18], [16] and [17] score with 45.3%, 63.0% and 63.9%, respectively. We think their lower performance is mostly due to the difference in training procedure and data sampling, and not originating from a weaker base ConvNet, since our frame baseline with a weaker ResNet-50 produces 72.1% mAP (vs. the 74.2% for ResNet-101). Next, we are interested in how our model performs after fine-tuning with the tracking loss, operating via RoI tracking on the correlation and track regression features (termed D (& T loss) in Table 1). The resulting performance for single-frame testing is 75.8% mAP. This 1.6% gain in accuracy shows that merely adding the tracking loss can aid the per-frame detection. A possible reason is that the correlation features propagate gradients back into the base ConvNet and therefore make the features more sensitive to important objects in the training data. We see significant gains for classes like panda, monkey, rabbit or snake which are likely to move. Video level methods. Next, we investigate the effect of multi-frame input during testing. In Table 1 we see that linking our detections to tubes based on our tracklets, D&T (\u03c4 = 1), raises performance substantially to 79.8% mAP. Some class-AP scores can be boosted significantly (e.g. cattle by 9.6, dog by 5.5, cat by 6, fox by 7.9, horse by 5.3, lion by 9.4, motorcycle by 6.4 rabbit by 8.9, red panda by 6.3 and squirrel by 8.5 points AP). This gain is mostly for the following reason: if an object is captured in an unconventional pose, is distorted by motion blur, or appears at a small scale, the detector might fail; however, if its tube is linked to other potentially highly scoring detections of the same object, these failed detections can be recovered (even though we use a very simple re-weighting of detections across a tube). The only class that loses AP is whale (\u22122.6 points) and this has an obvious explanation: in most validation snippets the whales successively emerge and submerge from the water and our detection rescoring based on tubes would assign false positives when they submerge for a couple of frames.\nWhen comparing our 79.8% mAP against the current state of the art, we make the following observations. The method in [18] achieves 47.5% by using a temporal con-\nvolutional network on top of the still image detector. An extended work [16] uses an encoder-decoder LSTM on top of a Faster R-CNN object detector which works on proposals from a tubelet proposal network, and produces 68.4% mAP. The ILSVRC 2015 winner [17] combines two Faster R-CNN detectors, multi-scale training/testing, context suppression, high confidence tracking [39] and optical-flowguided propagation to achieve 73.8%. And the winner from ILSVRC2016 [41] uses a cascaded R-FCN detector, context inference, cascade regression and a correlation tracker [25] to achieve 76.19% mAP validation performance with a single model (multi-scale testing and model ensembles boost their accuracy to 81.1%).\nOnline capabilities and runtime. The only component limiting online application is the tube rescoring (Sect. 4). We have evaluated an online version which performs only causal rescoring across the tracks. The performance for this method is 78.7%mAP, compared to the noncausal method (79.8%mAP). Since the correlation layer and track regressors are operating fully convolutional (no additional per-ROI computation is added except at the ROI-tracking layer), the extra runtime cost for testing a 1000x600 pixel image is 14ms (i.e. 141ms vs 127ms without correlation and ROI-tracking layers) on a Titan X GPU. The (unoptimized) tube linking (Sect. 4) takes on average 46ms per frame on a single CPU core).\nTemporally strided testing. We look at larger temporal strides \u03c4 during testing, which has recently been found useful for the related task of video action recognition [6, 7]. Our D & T architecture is evaluated only at every \u03c4 th frame of an input sequence and tracklets have to link detections over larger temporal strides. The performance for a temporal stride of \u03c4 = 10 is 78.6% mAP which is 1.2% below\nthe full-frame evaluation. We think that such a minor drop is remarkable as the duration for processing a video is now roughly reduced by a factor of 10.\nA potential point of improvement is to extend the detector to operate over multiple frames of the sequence. We found that such an extension did not have a clear beneficial effect on accuracy for short temporal windows (i.e. augmenting the detection scores at time t with the detector output at the tracked proposals in the adjacent frame at time t + 1 only raises the accuracy from 79.8 to 80.0% mAP). Increasing this window to frames at t \u00b1 1 by bidirectional detection and tracking from the tth frame did not lead to any gain. Interestingly, when testing with a temporal stride of \u03c4 = 10 and augmenting the detections from the current frame at time t with the detector output at the tracked proposals at t+10 raises the accuracy from 78.6 to 79.2% mAP.\nWe conjecture that the insensitivity of the accuracy for short temporal windows originates from the high redundancy of the detection scores from the centre frames with the scores at tracked locations. The accuracy gain for larger temporal strides, however, suggests that more complementary information is integrated from the tracked objects; thus, a potentially promising direction for improvement is to detect and track over multiple temporally strided inputs. Varying the base network. Finally, we compare different base networks for the Detect & Track architecture. Table 2\nshows the performance for using 50 and 101 layer ResNets [12], ResNeXt-101 [40], and Inception-v4 [37] as backbones. We report performance for frame-level Detection (D), video-level Detection and Tracking (D&T), as well as the variant that additionally classifies the tracked region and computes the detection confidence as the average of the scores in the current frame and the tracked region in the adjacent frame, (D&T, average). We observe that D&T benefits from deeper base ConvNets as well as specific design structures (ResNeXt and Inception-v4). The last row in Table 1 lists class-wise performance for D&T with an Inception-v4 backbone that seems to greatly boost certain categories, e.g., dog (+5.7 AP), domestic cat (+9.4 AP) , lion (+11.4 AP), lizard (+4.5 AP), rabbit (+4.4 AP), in comparison to ResNet-101."
        },
        {
            "heading": "6. Conclusion",
            "text": "We have presented a unified framework for simultaneous object detection and tracking in video. Our fully convolutional D&T architecture allows end-to-end training for detection and tracking in a joint formulation. In evaluation, our method achieves accuracy competitive with the winner of the last ImageNet challenge while being simple and efficient. We demonstrate clear mutual benefits of jointly performing the task of detection and tracking, a concept that can foster further research in video analysis. Acknowledgments. This work was partly supported by the Austrian Science Fund (FWF P27076) and by EPSRC Programme Grant Seebibyte EP/M013774/1."
        }
    ],
    "title": "Detect to Track and Track to Detect",
    "year": 2018
}