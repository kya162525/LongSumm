{
    "abstractText": "We consider the problem of detecting out-of-distribution images in neural networks. We propose ODIN, a simple and effective method that does not require any change to a pre-trained neural network. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can separate the softmax score distributions between inand out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach (Hendrycks & Gimpel, 2017) by a large margin, establishing a new state-of-the-art performance on this task. For example, ODIN reduces the false positive rate from the baseline 34.7% to 4.3% on the DenseNet (applied to CIFAR-10 and Tiny-ImageNet) when the true positive rate is 95%.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shiyu Liang"
        },
        {
            "affiliations": [],
            "name": "Yixuan Li"
        }
    ],
    "id": "SP:dc67238fd123d0aaf87f0612b4c364840967fbc0",
    "references": [
        {
            "authors": [
                "Dario Amodei",
                "Chris Olah",
                "Jacob Steinhardt",
                "Paul Christiano",
                "John Schulman",
                "Dan Man\u00e9"
            ],
            "title": "Concrete problems in ai safety",
            "venue": "arXiv preprint arXiv:1606.06565,",
            "year": 2016
        },
        {
            "authors": [
                "Jerone T.A Andrews",
                "Thomas Tanay",
                "Edward J. Morton",
                "Lewis D. Griffin"
            ],
            "title": "Transfer representationlearning for anomaly detection",
            "venue": "In ICML,",
            "year": 2016
        },
        {
            "authors": [
                "Kyunghyun Cho",
                "Bart Van Merri\u00ebnboer",
                "Caglar Gulcehre",
                "Dzmitry Bahdanau",
                "Fethi Bougares",
                "Holger Schwenk",
                "Yoshua Bengio"
            ],
            "title": "Learning phrase representations using rnn encoder-decoder for statistical machine",
            "year": 2014
        },
        {
            "authors": [
                "C Chow"
            ],
            "title": "On optimum recognition error and reject tradeoff",
            "venue": "IEEE Transactions on information theory,",
            "year": 1970
        },
        {
            "authors": [
                "Jesse Davis",
                "Mark Goadrich"
            ],
            "title": "The relationship between precision-recall and roc curves",
            "venue": "In ICML. ACM,",
            "year": 2006
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "In CVPR,",
            "year": 2009
        },
        {
            "authors": [
                "Luc Devroye",
                "L\u00e1szl\u00f3 Gy\u00f6rfi",
                "G\u00e1bor Lugosi"
            ],
            "title": "A probabilistic theory of pattern recognition, volume 31",
            "venue": "Springer Science & Business Media,",
            "year": 2013
        },
        {
            "authors": [
                "John Duchi",
                "Elad Hazan",
                "Yoram Singer"
            ],
            "title": "Adaptive subgradient methods for online learning and stochastic optimization",
            "venue": "Journal of Machine Learning Research,",
            "year": 2011
        },
        {
            "authors": [
                "Clement Farabet",
                "Camille Couprie",
                "Laurent Najman",
                "Yann LeCun"
            ],
            "title": "Learning hierarchical features for scene labeling",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 1929
        },
        {
            "authors": [
                "Tom Fawcett"
            ],
            "title": "An introduction to roc analysis",
            "venue": "Pattern recognition letters,",
            "year": 2006
        },
        {
            "authors": [
                "Amol Ghoting",
                "Srinivasan Parthasarathy",
                "Matthew Eric Otey"
            ],
            "title": "Fast mining of distance-based outliers in high-dimensional datasets",
            "venue": "Data Mining and Knowledge Discovery,",
            "year": 2008
        },
        {
            "authors": [
                "Ian J Goodfellow",
                "Jonathon Shlens",
                "Christian Szegedy"
            ],
            "title": "Explaining and harnessing adversarial examples",
            "venue": "ICLR,",
            "year": 2015
        },
        {
            "authors": [
                "Arthur Gretton",
                "Karsten M Borgwardt",
                "Malte J Rasch",
                "Bernhard Sch\u00f6lkopf",
                "Alexander Smola"
            ],
            "title": "A kernel two-sample test",
            "venue": "Journal of Machine Learning Research,",
            "year": 2012
        },
        {
            "authors": [
                "Chuan Guo",
                "Geoff Pleiss",
                "Yu Sun",
                "Kilian Q Weinberger"
            ],
            "title": "On calibration of modern neural networks",
            "venue": "arXiv preprint arXiv:1706.04599,",
            "year": 2017
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "year": 2016
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Gimpel"
            ],
            "title": "A baseline for detecting misclassified and out-of-distribution examples in neural networks",
            "year": 2017
        },
        {
            "authors": [
                "Geoffrey Hinton",
                "Oriol Vinyals",
                "Jeff Dean"
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "arXiv preprint arXiv:1503.02531,",
            "year": 2015
        },
        {
            "authors": [
                "Gao Huang",
                "Zhuang Liu",
                "Kilian Q Weinberger"
            ],
            "title": "Densely connected convolutional networks",
            "venue": "arXiv preprint arXiv:1608.06993,",
            "year": 2016
        },
        {
            "authors": [
                "Shuiwang Ji",
                "Wei Xu",
                "Ming Yang",
                "Kai Yu"
            ],
            "title": "3d convolutional neural networks for human action recognition",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2013
        },
        {
            "authors": [
                "Diederik Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Geoffrey E Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing",
            "year": 2012
        },
        {
            "authors": [
                "Christopher D Manning",
                "Hinrich Sch\u00fctze"
            ],
            "title": "Foundations of statistical natural language processing, volume 999",
            "year": 1999
        },
        {
            "authors": [
                "Seyed-Mohsen Moosavi-Dezfooli",
                "Alhussein Fawzi",
                "Omar Fawzi",
                "Pascal Frossard"
            ],
            "title": "Universal adversarial perturbations",
            "year": 2017
        },
        {
            "authors": [
                "Anh Nguyen",
                "Jason Yosinski",
                "Jeff Clune"
            ],
            "title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable",
            "year": 2015
        },
        {
            "authors": [
                "Gabriel Pereyra",
                "George Tucker",
                "Jan Chorowski",
                "\u0141ukasz Kaiser",
                "Geoffrey Hinton"
            ],
            "title": "Regularizing neural networks by penalizing confident output distributions",
            "year": 2017
        },
        {
            "authors": [
                "Marco AF Pimentel",
                "David A Clifton",
                "Lei Clifton",
                "Lionel Tarassenko"
            ],
            "title": "A review of novelty detection",
            "venue": "Signal Processing,",
            "year": 2014
        },
        {
            "authors": [
                "Mohammad Sabokrou",
                "Mohsen Fayyaz",
                "Mahmood Fathy"
            ],
            "title": "Fully convolutional neural network for fast anomaly detection in crowded scenes",
            "venue": "arXiv preprint arXiv:1609.00866,",
            "year": 2016
        },
        {
            "authors": [
                "Takaya Saito",
                "Marc Rehmsmeier"
            ],
            "title": "The precision-recall plot is more informative than the roc plot when evaluating binary classifiers on imbalanced datasets",
            "venue": "PloS one,",
            "year": 2015
        },
        {
            "authors": [
                "Thomas Schlegl",
                "Philipp Seeb\u00f6ck",
                "Sebastian M Waldstein",
                "Ursula Schmidt-Erfurth",
                "Georg Langs"
            ],
            "title": "Unsupervised anomaly detection with generative adversarial networks to guide marker discovery",
            "venue": "In International Conference on Information Processing in Medical Imaging,",
            "year": 2017
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "ICLR,",
            "year": 2015
        },
        {
            "authors": [
                "Bharath K Sriperumbudur",
                "Arthur Gretton",
                "Kenji Fukumizu",
                "Bernhard Sch\u00f6lkopf",
                "Gert RG Lanckriet"
            ],
            "title": "Hilbert space embeddings and metrics on probability measures",
            "venue": "Journal of Machine Learning Research,",
            "year": 2010
        },
        {
            "authors": [
                "Dougal J Sutherland",
                "Hsiao-Yu Tung",
                "Heiko Strathmann",
                "Soumyajit De",
                "Aaditya Ramdas",
                "Alex Smola",
                "Arthur Gretton"
            ],
            "title": "Generative models and model criticism via optimized maximum mean discrepancy",
            "year": 2016
        },
        {
            "authors": [
                "Christian Szegedy",
                "Wojciech Zaremba",
                "Ilya Sutskever",
                "Joan Bruna",
                "Dumitru Erhan",
                "Ian Goodfellow",
                "Rob Fergus"
            ],
            "title": "Intriguing properties of neural networks",
            "year": 2014
        },
        {
            "authors": [
                "Lucas Theis",
                "A\u00e4ron van den Oord",
                "Matthias Bethge"
            ],
            "title": "A note on the evaluation of generative models",
            "year": 2015
        },
        {
            "authors": [
                "Pascal Vincent",
                "Yoshua Bengio"
            ],
            "title": "Manifold parzen windows",
            "venue": "In Advances in neural information processing systems,",
            "year": 2003
        },
        {
            "authors": [
                "Pingmei Xu",
                "Krista A Ehinger",
                "Yinda Zhang",
                "Adam Finkelstein",
                "Sanjeev R Kulkarni",
                "Jianxiong Xiao"
            ],
            "title": "Turkergaze: Crowdsourcing saliency with webcam based eye tracking",
            "venue": "arXiv preprint arXiv:1504.06755,",
            "year": 2015
        },
        {
            "authors": [
                "Fisher Yu",
                "Yinda Zhang",
                "Shuran Song",
                "Ari Seff",
                "Jianxiong Xiao"
            ],
            "title": "Lsun: Construction of a largescale image dataset using deep learning with humans in the loop",
            "venue": "arXiv preprint arXiv:1506.03365,",
            "year": 2015
        },
        {
            "authors": [
                "Sergey Zagoruyko",
                "Nikos Komodakis"
            ],
            "title": "Wide residual networks",
            "venue": "arXiv preprint arXiv:1605.07146,",
            "year": 2016
        },
        {
            "authors": [
                "Chiyuan Zhang",
                "Samy Bengio",
                "Moritz Hardt",
                "Benjamin Recht",
                "Oriol Vinyals"
            ],
            "title": "Understanding deep learning requires rethinking generalization",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Modern neural networks are known to generalize well when the training and testing data are sampled from the same distribution (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; He et al., 2016; Cho et al., 2014; Zhang et al., 2017). However, when deploying neural networks in real-world applications, there is often very little control over the testing data distribution. Recent works have shown that neural networks tend to make high confidence predictions even for completely unrecognizable (Nguyen et al., 2015) or irrelevant inputs (Hendrycks & Gimpel, 2017; Szegedy et al., 2014; Moosavi-Dezfooli et al., 2017). It has been well documented (Amodei et al., 2016) that it is important for classifiers to be aware of uncertainty when shown new kinds of inputs, i.e., out-ofdistribution examples. Therefore, being able to accurately detect out-of-distribution examples can be practically important for visual recognition tasks (Krizhevsky et al., 2012; Farabet et al., 2013; Ji et al., 2013).\nA seemingly straightforward approach of detecting out-of-distribution images is to enlarge the training set of both in- and out-of-distribution examples. However, the number of out-of-distribution examples can be infinitely many, making the re-training approach computationally expensive and intractable. Moreover, to ensure that a neural network accurately classifies in-distribution samples into correct classes while correctly detecting out-of-distribution samples, one might need to employ exceedingly large neural network architectures, which further complicates the training process.\nHendrycks & Gimpel proposed a baseline method to detect out-of-distribution examples without further re-training networks. The method is based on an observation that a well-trained neural network tends to assign higher softmax scores to in-distribution examples than out-of-distribution\n\u2217Work done while at Cornell University.\nar X\niv :1\n70 6.\n02 69\n0v 5\n[ cs\n.L G\n] 3\n0 A\nexamples. In this paper, we go further. We observe that after using temperature scaling in the softmax function (Hinton et al., 2015; Pereyra et al., 2017) and adding small controlled perturbations to inputs, the softmax score gap between in - and out-of-distribution examples is further enlarged. We show that the combination of these two techniques (temperature scaling and input perturbation) can lead to better detection performance. For example, provided with a pre-trained DenseNet (Huang et al., 2016) on CIFAR-10 dataset (positive samples), we test against images from TinyImageNet dataset (negative samples). Our method reduces the False Positive Rate (FPR), i.e., the fraction of misclassified out-of-distribution samples, from 34.7% to 4.3%, when 95% of in-distribution images are correctly classified. We summarize the main contributions of this paper as the following:\n\u2022 We propose a simple and effective method, ODIN (Out-of-DIstribution detector for Neural networks), for detecting out-of-distribution examples in neural networks. Our method does not require re-training the neural network and is easily implementable on any modern neural architecture.\n\u2022 We test ODIN on state-of-the-art network architectures (e.g., DenseNet (Huang et al., 2016) and Wide ResNet (Zagoruyko & Komodakis, 2016)) under a diverse set of in- and out-distribution dataset pairs. We show ODIN can significantly improve the detection performance, and consistently outperforms the baseline method (Hendrycks & Gimpel, 2017) by a large margin.\n\u2022 We empirically analyze how parameter settings affect the performance, and further provide simple analysis that provides some intuition behind our method.\nThe outline of this paper is as follows. In Section 2, we present the necessary definitions and the problem statement. In Section 3, we introduce ODIN and present performance results in Section 4. We experimentally analyze the proposed method and provide some justification for our method in Section 5. We summarize the related works and future directions in Section 6 and conclude the paper in Section 7."
        },
        {
            "heading": "2 PROBLEM STATEMENT",
            "text": "In this paper, we consider the problem of distinguishing in- and out-of-distribution images on a pretrained neural network. Let PX and QX denote two distinct data distributions defined on the image space X . Assume that a neural network f is trained on a dataset drawn from the distribution PX . We call PX the in-distribution and QX the out-distribution, respectively. In testing, we draw new images from a mixture distribution PX\u00d7Z defined on X \u00d7 {0, 1}, where the conditional probability distributions PX|Z=0 = PX and PX|Z=1 = QX denote in- and out-distribution respectively. We consider the following problem: Given an image X drawn from the mixture distribution PX\u00d7Z , can we distinguish whether the image is from in-distribution PX or not?\nIn this paper, we focus on detecting out-of-distribution images. However, it is equally important to correctly classify an image into the right class if it is an in-distribution image. But this can be easily done: once it has been detected that an image is in-distribution, we can simply use the original image and run it through the neural network to classify it. Thus, we do not change the predictions of the neural network for in-distribution images and only focus on improving the detection performance for out-of-distribution images."
        },
        {
            "heading": "3 ODIN: OUT-OF-DISTRIBUTION DETECTOR",
            "text": "In this section, we present our method, ODIN, for detecting out-of-distribution samples. The detector is built on two components: temperature scaling and input preprocessing. We describe the details of both components below. Temperature Scaling. Assume that the neural network f = (f1, ..., fN ) is trained to classify N classes. For each input x, the neural network assigns a label y\u0302(x) = arg maxi Si(x;T ) by computing the softmax output for each class. Specifically,\nSi(x;T ) = exp (fi(x)/T )\u2211N j=1 exp (fj(x)/T ) , (1)\nwhere T \u2208 R+ is the temperature scaling parameter and set to 1 during the training. For a given input x, we call the maximum softmax probability, i.e., Sy\u0302(x;T ) = maxi Si(x;T ) the softmax score. In\nthis paper, we use notations Sy\u0302(x;T ) and S(x;T ) interchangeably. Prior works have established the use of temperature scaling to distill the knowledge in neural networks (Hinton et al., 2015) and calibrate the prediction confidence in classification tasks (Guo et al., 2017). As we shall see, using temperature scaling can separate the softmax scores between in- and out-of-distribution images, making out-of-distribution detection effective. Input Preprocessing. In addition to temperature scaling, we preprocess the input by adding small perturbations: x\u0303 = x\u2212 \u03b5sign(\u2212\u2207x logSy\u0302(x;T )), (2) where the parameter \u03b5 is the perturbation magnitude. The method is inspired by the idea of adversarial examples (Goodfellow et al., 2015), where small perturbations are added to decrease the softmax score for the true label and force the neural network to make a wrong prediction. Here, our goal and setting are the opposite: we aim to increase the softmax score of any given input, without the need for a class label at all. As we shall see later, the perturbation can have stronger effect on the indistribution images than that on out-of-distribution images, making them more separable. Note that the perturbations can be easily computed by back-propagating the gradient of the cross-entropy loss w.r.t the input. Out-of-distribution Detector. The detector combines the two components described above. For each image x, we first calculate the preprocessed image x\u0303 according to the equation (2). Next, we feed the preprocessed image x\u0303 into the neural network, calculate its calibrated softmax score S(x\u0303;T ) and compare the score to the threshold \u03b4. An image x is classified as in-distribution if the softmax score is greater than the threshold and vice versa. Mathematically, the out-of-distribution detector can be described as\ng(x; \u03b4, T, \u03b5) = { 1 if maxi p(x\u0303;T ) \u2264 \u03b4, 0 if maxi p(x\u0303;T ) > \u03b4.\nThe parameters T, \u03b5 and \u03b4 are chosen so that the true positive rate (i.e., the fraction of in-distribution images correctly classified as in-distribution images) is 95%."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "In this section, we demonstrate the effectiveness of ODIN on several computer vision benchmark datasets. We run all experiments with PyTorch1 and we release the code to reproduce all experimental results2."
        },
        {
            "heading": "4.1 TRAINING SETUP",
            "text": "Architectures and training configurations. We adopt two state-of-the-art neural network architectures, including DenseNet (Huang et al., 2016) and Wide ResNet (Zagoruyko & Komodakis, 2016). For DenseNet, our model follows the same setup as in (Huang et al., 2016), with depth L = 100, growth rate k = 12 (Dense-BC) and dropout rate 0. In addition, we evaluate the method on a Wide ResNet, with depth 28, width 10 (WRN-28-10) and dropout rate 0. The hyper-parameters of neural networks are set identical to the original Wide ResNet (Zagoruyko & Komodakis, 2016) and DenseNet (Huang et al., 2016) implementations. All neural networks are trained with stochastic gradient descent with Nesterov momentum (Duchi et al., 2011; Kingma & Ba, 2014). Specifically, we train Dense-BC for 300 epochs with batch size 64 and momentum 0.9; and Wide ResNet for 200 epochs with batch size 128 and momentum 0.9. The learning rate starts at 0.1, and is dropped by a factor of 10 at 50% and 75% of the training progress, respectively.\nAccuracy of pre-trained networks. Each neural network architecture is trained on CIFAR-10 (C-10) and CIFAR100 (C-100) datasets (Krizhevsky & Hinton, 2009), respectively. CIFAR-10 and CIFAR-100 images are drawn from 10 and 100 classes, respectively. Both datasets consist of 50,000 training images and 10,000 test images. The test error on CIFAR datasets are given in Table 1.\n1http://pytorch.org 2https://github.com/facebookresearch/odin"
        },
        {
            "heading": "4.2 OUT-OF-DISTRIBUTION DATASETS",
            "text": "At test time, the test images from CIFAR-10 (CIFAR-100) datasets can be viewed as the in-distribution (positive) examples. For out-of-distribution (negative) examples, we follow the setting in (Hendrycks & Gimpel, 2017) and test on several different natural image datasets and synthetic noise datasets. We consider the following out-of-distribution test datasets.\n(1) TinyImageNet. The Tiny ImageNet dataset3 consists of a subset of ImageNet images (Deng et al., 2009). It contains 10,000 test images from 200 different classes. We construct two datasets, TinyImageNet (crop) and TinyImageNet (resize), by either randomly cropping image patches of size 32\u00d7 32 or downsampling each image to size 32\u00d7 32.\n(2) LSUN. The Large-scale Scene UNderstanding dataset (LSUN) has a testing set of 10,000 images of 10 different scenes categories such as bedroom, kitchen room, living room, etc. (Yu et al., 2015). Similar to TinyImageNet, we construct two datasets, LSUN (crop) and LSUN (resize), by randomly cropping and downsampling the LSUN testing set, respectively.\n(3) Gaussian Noise. The synthetic Gaussian noise dataset consists of 10,000 random 2D Gaussian noise images, where each RGB value of every pixel is sampled from an i.i.d Gaussian distribution with mean 0.5 and unit variance. We further clip each pixel value into the range [0, 1].\n(4) Uniform Noise. The synthetic uniform noise dataset consists of 10,000 images where each RGB value of every pixel is independently and identically sampled from a uniform distribution on [0, 1].\nFor hyperparameter tuning, we use a separate validation dataset iSUN (Xu et al., 2015), which is independent from the OOD test datasets. iSUN (Xu et al., 2015) consists of natural scene images. We include the entire collection of 8925 images in iSUN and downsample each image to size 32 by 32."
        },
        {
            "heading": "4.3 EVALUATION METRICS",
            "text": "We adopt the following four different metrics to measure the effectiveness of a neural network in distinguishing in- and out-of-distribution images.\n(1) FPR at 95% TPR can be interpreted as the probability that a negative (out-of-distribution) example is misclassified as positive (in-distribution) when the true positive rate (TPR) is as high as 95%.\n(2) Detection Error, i.e., Pe measures the misclassification probability when TPR is 95%. The definition of Pe is given by Pe = 0.5(1\u2212 TPR) + 0.5FPR, where we assume that both positive and negative examples have the equal probability of appearing in the test set.\n3https://tiny-imagenet.herokuapp.com\n(3) AUROC is the Area Under the Receiver Operating Characteristic curve, which is also a thresholdindependent metric (Davis & Goadrich, 2006). The ROC curve depicts the relationship between TPR and FPR. The AUROC can be interpreted as the probability that a positive example is assigned a higher detection score than a negative example (Fawcett, 2006). A perfect detector corresponds to an AUROC score of 100%.\n(4) AUPR is the Area under the Precision-Recall curve, which is another threshold independent metric (Manning et al., 1999; Saito & Rehmsmeier, 2015). The PR curve is a graph showing the precision=TP/(TP+FP) and recall=TP/(TP+FN) against each other. The metric AUPR-In and AUPR-Out in Table 2 denote the area under the precision-recall curve where in-distribution and out-of-distribution images are specified as positives, respectively.\n4.4 EXPERIMENTAL RESULTS\nComparison with baseline. In Figure 1, we show the ROC curves when DenseNet-BC-100 is evaluated on CIFAR-10 (positive) images against TinyImageNet (negative) test examples. The red curve corresponds to the ROC curve when using baseline method (Hendrycks & Gimpel, 2017), whereas the blue curve corresponds to ODIN. We observe a strikingly large gap between the blue and red ROC curves. For example, when TPR= 95%, the FPR can be reduced from 34% to 4.2% by using our approach. Hyperparameters. We use a separate OOD validation dataset for hyperparameter selection, which is independent from the OOD test datasets. For temperature T , we select among 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000; and for perturbation magnitude \u03b5 we choose from 21 evenly spaced numbers starting from 0 and ending at 0.004. The optimal parameters are chosen to minimize the FPR at TPR 95% on the validation OOD dataset.\nMain results. The main results are summarized in Table 2, where we use iSUN (Xu et al., 2015) as validation set. We use T = 1000 for all settings. For DenseNet, we use \u03b5 = 0.0014 for CIFAR-10 and \u03b5 = 0.002 for CIFAR-100. We provide\nadditional details on the effect of parameters in Section 5. For each in- and out-of-distribution dataset pair, we report both the performance of the baseline (Hendrycks & Gimpel, 2017) and ODIN. In Table 2, we observe significant performance improvement across all dataset pairs. Parameter transferability. In Table 3, we show how the parameters tuned on one validation set can generalize across datasets. Specifically, we tune the parameters using one validation dataset and then evaluated on the remaining OOD test datasets. The results are very similar across different validation sets, which suggests the insensitivity of our method w.r.t the tuning set.\nData distributional distance vs. detection performance. To measure the statistical distance between in- and out-of-distribution datasets, we adopt a commonly used metric, maximum mean discrep-\nancy (MMD) with Gaussian RBF kernel (Sriperumbudur et al., 2010; Gretton et al., 2012; Sutherland et al., 2016). Specifically, given two image sets, V = {v1, ..., vm} and W = {w1, ..., wm}, the maximum mean discrepancy between V and Q is defined as\nM\u0302MD 2 (V,W ) = 1( m 2 ) \u2211 i 6=j k(vi, vj) + 1( m 2 ) \u2211 i6=j k(wi, wj)\u2212 2( m 2 ) \u2211 i 6=j k(vi, wj),\nwhere k(\u00b7, \u00b7) is the Gaussian RBF kernel, i.e., k(x, x\u2032) = exp ( \u2212\u2016x\u2212x\n\u2032\u201622 2\u03c32\n) . We use the same method\nused by Sutherland et al. (2016) to choose \u03c3, where 2\u03c32 is set to the median of all Euclidean distances between all images in the aggregate set V \u222aW . In Figure 2 (a)(b), we show how the performance of ODIN varies against the MMD distances between in- and out-of-distribution datasets. The datasets (on x-axis) are ranked in the descending order of MMD distances with CIFAR-100. There are two interesting observations can be drawn from these figures. First, we find that the MMD distances between the cropped datasets and CIFAR-100 tend to be larger. This is likely due to the fact that cropped images only contain local image context and are therefore more distinct from CIFAR-100 images, while resized images contain global patterns and are thus similar to images in CIFAR-100. Second, we observe that the MMD distance tends to be negatively correlated with the detection performance. This suggests that the detection task becomes harder as in- and out-of-distribution images are more similar to each other."
        },
        {
            "heading": "5 DISCUSSIONS",
            "text": ""
        },
        {
            "heading": "5.1 ANALYSIS ON TEMPERATURE SCALING",
            "text": "In this subsection, we analyze the effectiveness of the temperature scaling method. As shown in Figure 3 (a) and (b), we observe that a sufficiently large temperature yields better detection performance although the effects diminish when T is too large. To gain insight, we can use the Taylor expansion of the softmax score (details provided in Appendix B). When T is sufficiently large, we have\nSy\u0302(x;T ) \u2248 1 N \u2212 1T \u2211 i[fy\u0302(x)\u2212 fi(x)] + 12T 2 \u2211 i[fy\u0302(x)\u2212 fi(x)]2 , (3)\nby omitting the third and higher orders. For simplicity of notation, we define\nU1(x) = 1 N \u2212 1 \u2211 i 6=y\u0302 [fy\u0302(x)\u2212 fi(x)] and U2(x) = 1 N \u2212 1 \u2211 i 6=y\u0302 [fy\u0302(x)\u2212 fi(x)]2. (4)\nInterpretations of U1 and U2. By definition, U1 measures the extent to which the largest unnormalized output of the neural network deviates from the remaining outputs; while U2 measures the extent to which the remaining smaller outputs deviate from each other. We provide formal mathematical derivations in Appendix D. In Figure 5(a), we show the distribution of U1 for each out-of-distribution dataset vs. the in-distribution dataset (in red). We observe that the largest outputs of the neural\nnetwork on in-distribution images deviate more from the remaining outputs. This is likely due to the fact that neural networks tend to make more confident predictions on in-distribution images.\nFurther, we show in Figure 5(b) the expectation of U2 conditioned on U1, i.e., E[U2|U1], for each dataset. The red curve (in-distribution images) has overall higher expectation. This indicates that, when two images have similar values on U1, the in-distribution image tends to have a much higher value of U2 than the out-of-distribution image. In other words, for in-distribution images, the remaining outputs (excluding the largest output) tend to be more separated from each other compared to out-of-distribution datasets. This may happen when some classes in the in-distribution dataset share common features while others differ significantly. To illustrate this, in Figure 5 (f)(g), we show the outputs of each class using a DenseNet (trained on CIFAR-10) on a dog image from CIFAR-10,\nand another image from TinyImageNet (crop). For the image of dog, we can observe that the largest output for the label dog is close to the output for the label cat but is quite separated from the outputs for the label car and truck. This is likely due to the fact that, in CIFAR-10, images of dogs are very similar to the images of cats but are quite distinct from images of car and truck. For the image from TinyImageNet (crop), despite having one large output, the remaining outputs are close to each other and thus have a smaller deviation. The effects of T . To see the usefulness of adopting a large T , we can first rewrite the softmax score function in Equation (3) as S \u221d (U1 \u2212 U2/2T )/T . Hence the softmax score is largely determined by U1 and U2/2T . As noted earlier, U1 makes in-distribution images produce larger softmax scores than out-of-distribution images since S \u221d U1, while U2 has the exact opposite effect since S \u221d \u2212U2. Therefore, by choosing a sufficiently large temperature, we can compensate the negative impacts of U2/2T on the detection performance, making the softmax scores between in- and out-of-distribution images more separable. Eventually, when T is sufficiently large, the distribution of softmax score is almost dominated by the distribution of U1 and thus increasing the temperature further is no longer effective. This explains why we see in Figure 3 (a)(b) that the performance does not change when T is too large (e.g., T > 100). In Appendix C, we provide a formal proof showing that the detection error eventually converges to a constant number when T goes to infinity."
        },
        {
            "heading": "5.2 ANALYSIS ON INPUT PREPROCESSING",
            "text": "As noted previously, using the temperature scaling method by itself can be effective in improving the detection performance. However, the effectiveness quickly diminishes as T becomes very large. In order to make further improvement, we complement temperature scaling with input preprocessing. This has already been seen in Figure 4, where the detection performance is improved by a large margin on most datasets when T = 1000, provided with an appropriate perturbation magnitude \u03b5 is chosen. In this subsection, we provide some intuition behind this.\nTo explain, we can look into the first order Taylor expansion of the log-softmax function for the perturbed image x\u0303, which is given by\nlogSy\u0302(x\u0303;T ) = logSy\u0302(x;T ) + \u03b5 \u2016\u2207x logSy\u0302(x;T )\u20161 + o(\u03b5), where x is the original input.\nThe effects of gradient. In Figure 5 (c), we present the distribution of \u2016\u2207x logS(x;T )\u20161 \u2014 the 1-norm of gradient of log-softmax with respect to the input x \u2014 for all datasets. A salient observation is that CIFAR-10 images (in-distribution) tend to have larger values on the norm of gradient than most out-of-distribution images. To further see the effects of the norm of gradient on the softmax score, we provide in Figures 5 (d) the conditional expectation E[\u2016\u2207x logS(x;T )\u20161|S]. We can observe that, when an indistribution image and an out-of-distribution image have the same softmax score, the value of \u2016\u2207x logS(x;T )\u20161 for in-distribution image tends to be larger.\nWe illustrate the effects of the norm of gradient in Figure 6. Suppose that an in-distribution image x1 (blue) and an out-of-distribution image x2 (red) have similar softmax scores, i.e., S(x1) \u2248 S(x2).\nAfter input processing, the in-distribution image can have a much larger softmax score than the out-of-distribution image x2 since x1 results in a much larger value on the norm of softmax gradient than that of x2. Therefore, in- and out-of-distribution images are more separable from each other after input preprocessing4. The effect of \u03b5. When the magnitude \u03b5 is sufficiently small, adding perturbations does not change the predictions of the neural network, i.e., y\u0302(x\u0303) = y\u0302(x). However, when \u03b5 is not negligible, the gap of softmax scores between in- and out-of-distribution images can be affected by \u2016\u2207x logS(x;T )\u20161. Our observation is consistent with that in (Szegedy et al., 2014; Goodfellow et al., 2015; MoosaviDezfooli et al., 2017), which show that the softmax scores tend to change significantly if small perturbations are added to the in-distribution images. It is also worth noting that using a very large \u03b5\n4Similar observation can be seen when T = 1, where we present the conditional expectation of the norm of softmax gradient in Figure 5 (e).\ncan lead to performance degradation, as seen in Figure 4. This is likely due to the fact that the second and higher order terms in the Taylor expansion are no longer insignificant when the perturbation magnitude is too large."
        },
        {
            "heading": "6 RELATED WORKS AND FUTURE DIRECTIONS",
            "text": "The problem of detecting out-of-distribution examples in low-dimensional space has been well-studied in various contexts (see the survey by Pimentel et al. (2014)). Conventional methods such as density estimation, nearest neighbor and clustering analysis are widely used in detecting low-dimensional outof-distribution examples (Chow, 1970; Vincent & Bengio, 2003; Ghoting et al., 2008; Devroye et al., 2013), . The density estimation approach uses probabilistic models to estimate the in-distribution density and declares a test example to be out-of-distribution if it locates in the low-density areas. The clustering method is based on the statistical distance, and declares an example to be out-ofdistribution if it locates far from its neighborhood. Despite various applications in low-dimensional spaces, unfortunately, these methods are known to be unreliable in high-dimensional space such as image space (Wasserman, 2006; Theis et al., 2015). In recent years, out-of-distribution detectors based on deep models have been proposed. Schlegl et al. (2017) train a generative adversarial networks to detect out-of-distribution examples in clinical scenario. Sabokrou et al. (2016) train a convolutional network to detect anomaly in scenes. Andrews et al. (2016) adopt transfer representation-learning for anomaly detection. All these works require enlarging or modifying the neural networks. In a more recent work, Hendrycks & Gimpel (2017) found that pre-trained neural networks can be overconfident to out-of-distribution example, limiting the effectiveness of detection. Our paper aims to improve the performance of detecting out-of-distribution examples, without requiring any change to an existing well-trained model.\nOur approach leverages the following two interesting observations to help better distinguish between in- and out-of-distribution examples: (1) On in-distribution images, modern neural networks tend to produce outputs with larger variance across class labels, and (2) neural networks have larger norm of gradient of log-softmax scores when applied on in-distribution images. We believe that having a better understanding of these phenomenon can lead to further insights into this problem."
        },
        {
            "heading": "7 CONCLUSIONS",
            "text": "In this paper, we propose a simple and effective method to detect out-of-distribution data samples in neural networks. Our method does not require retraining the neural network and significantly improves on the baseline method Hendrycks & Gimpel (2017) on different neural architectures across various in and out-distribution dataset pairs. We empirically analyze the method under different parameter settings, and provide some insights behind the approach. Future work involves exploring our method in other applications such as speech recognition and natural language processing."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "The research reported here was supported by NSF Grant CPS ECCS 1739189."
        },
        {
            "heading": "B TAYLOR EXPANSION",
            "text": "In this section, we present the Taylor expansion of the soft-max score function:\nSy\u0302(x;T ) = exp (fy\u0302(x)/T )\u2211N i=1 exp(fi(x)/T )\n= 1\u2211N i=1 exp ( fi(x)\u2212fy\u0302(x) T ) =\n1\u2211N i=1 [ 1 + fi(x)\u2212fy\u0302(x) T + 1 2! (fi(x)\u2212fy\u0302(x))2 T 2 + o ( 1 T 2 )] by Taylor expansion \u2248 1 N \u2212 1T \u2211N i=1[fy\u0302(x)\u2212 fi(x)] + 12T 2 \u2211N i=1[fi(x)\u2212 fy\u0302(x)]2"
        },
        {
            "heading": "C PROPOSITION 1",
            "text": "The following proposition 1 shows that the detection error Pe(T, 0) \u2248 c if T is sufficiently large. Thus, increasing the temperature further can only slightly improve the detection performance. Proposition 1. There exists a constant c only depending on function U1, in-distribution PX and out-of-distribution QX such that limT\u2192\u221e Pe(T, \u03b5) = c, when \u03b5 = 0 (i.e., no input preprocessing).\nProof. Since\nSy\u0302(X;T ) = exp(fy\u0302(X)/T )\u2211N i=1 exp(fi(X)/T ) = 1 1 + \u2211 i 6=y\u0302 exp([fi(X)\u2212 fy\u0302(X)]/T )\nTherefore, for any X ,\nlim T\u2192\u221e T ( \u2212 1 Sy\u0302(X;T ) +N ) = lim T\u2192\u221e \u2211 i 6=y\u0302 T [ 1\u2212 exp ( fi(X)\u2212 fy\u0302(X) T )] = \u2211 i 6=y\u0302 [fy\u0302(X)\u2212 fi(X)] = (N \u2212 1)U1(X)\nThis indicates that the random variable\nT ( \u2212 1 Sy\u0302(X;T ) +N ) \u2192 (N \u2212 1)U1(X) a.s.\nas T \u2192\u221e. This means that for a specific \u03b1 > 0, choosing the threshold \u03b4T = 1/(N \u2212 \u03b1/T ), then the false positive rate\nFPR(T ) = QX(Sy\u0302(X;T ) > 1/(N \u2212 \u03b1/T )) = QX ( T ( N \u2212 1\nSy\u0302(X;T )\n) > \u03b1 ) T\u2192\u221e\u2212\u2212\u2212\u2212\u2192 QX ((N \u2212 1)U1(X) > \u03b1) ,\nand the true positive rate TPR(T ) = PX(Sy\u0302(X;T ) > 1/(N \u2212 \u03b1/T )) = PX ( T ( N \u2212 1\nSy\u0302(X;T )\n) > \u03b1 ) T\u2192\u221e\u2212\u2212\u2212\u2212\u2192 PX ((N \u2212 1)U1(X) > \u03b1) .\nChoosing \u03b1\u2217 such that PX ((N \u2212 1)U1(X) > \u03b1\u2217) = 0.95, then TPR(T ) \u2192 0.95 as T \u2192 \u221e and at the same time FPR(T ) \u2192 QX ((N \u2212 1)U1(X) > \u03b1\u2217) as T \u2192 \u221e. There exists a constant c depending on U1, PX , QX and PZ , such that\nlim T\u2192\u221e\nPe(T, 0) = 0.05P (Z = 0) + P (Z = 1)QX ((N \u2212 1)U1(X) > \u03b1\u2217) = c."
        },
        {
            "heading": "D ANALYSIS OF TEMPERATURE",
            "text": "For simplicity of the notations, let \u2206i = fy\u0302 \u2212 fi and thus \u2206 = {\u2206i}i 6=y\u0302. Besides, let \u2206\u0304 denote the mean of the set \u2206. Therefore,\n\u2206\u0304 = 1 N \u2212 1 \u2211 i 6=y\u0302 \u2206i = 1 N \u2212 1 \u2211 i6=y\u0302 [fy\u0302 \u2212 fi] = U1.\nEquivalently, U1 = Mean(\u2206).\nNext, we will show\nU2 = 1 N \u2212 1 \u2211 i 6=y\u0302 [fy\u0302 \u2212 fi]2 =\nVariance2(\u2206)\ufe37 \ufe38\ufe38 \ufe37 1\nN \u2212 1 \u2211 i 6=y\u0302 [\u2206i \u2212 \u2206\u0304]2 +\nMean2(\u2206)\ufe37\ufe38\ufe38\ufe37 \u2206\u03042 .\nSince\nU2 = 1 N \u2212 1 \u2211 i6=y\u0302 \u22062i by\u2206i = fy\u0302 \u2212 fi\n= 1 N \u2212 1 \u2211 i6=y\u0302 (\u2206i \u2212 \u2206\u0304 + \u2206\u0304)2\n= 1 N \u2212 1 \u2211 i6=y\u0302 [(\u2206i \u2212 \u2206\u0304)2 \u2212 2(\u2206i \u2212 \u2206\u0304)\u2206\u0304 + \u2206\u03042]\n= 1 N \u2212 1 \u2211 i6=y\u0302\n[\u2206i \u2212 \u2206\u0304]2\ufe38 \ufe37\ufe37 \ufe38 Variance2(\u2206)\n\u2212 2\u2206\u0304 N \u2212 1 \u2211 i6=y\u0302\n(\u2206i \u2212 \u2206\u0304)\ufe38 \ufe37\ufe37 \ufe38 =0 + \u2206\u03042\ufe38\ufe37\ufe37\ufe38 Mean2(\u2206)\nthen U2 = Variance2(\u2206) + Mean2(\u2206)"
        },
        {
            "heading": "E ADDITIONAL RESULTS ON DISTANCE MEASUREMENT",
            "text": "Apart from the Maximum Mean Discrepancy, we also calculate the Energy distance between in- and out-of-distribution datasets. Let P and Q denote two different distributions. Then the energy distance between distributions P and Q is defined as\nD2energy(P,Q) = 2EV\u223cP,W\u223cQ\u2016X \u2212 Y \u2016 \u2212 EV,V \u2032\u223cP \u2016X \u2212X \u2032\u2016 \u2212 EW,W \u2032\u223cQ\u2016Y \u2212 Y \u2032\u2016.\nTherefore, the energy distance between two datasets V = {V1, ..., Vm} iid\u223c P and W = {W1, ...,Wm} iid\u223c Q is defined as\nD\u0302energy 2 (P,Q) = 2\nm2 m\u2211 i=1 m\u2211 j=1 \u2016Vi \u2212Wj\u2016 \u2212 1( m 2 ) \u2211 i6=j \u2016Vi \u2212 Vj\u2016 \u2212 1( m 2 ) \u2211 i 6=j \u2016Wi \u2212Wj\u2016.\nIn the experiment, we use the 2-norm \u2016 \u00b7 \u20162.\nIn-distribution Out-of-distribution MMD Energy datasets Datasets Distance Distance\nCIFAR-100\nTiny-ImageNet (crop) 0.41 2.25 LSUN (crop) 0.43 2.31 Tiny-ImageNet (resize) 0.088 0.54 LSUN (resize) 0.12 0.63"
        },
        {
            "heading": "F ADDITIONAL DISCUSSIONS",
            "text": "In this section, we present additional discussion on the proposed method. We first empirically show how the threshold \u03b4 affects the detection performance. We next show how the proposed method performs when the parameters are tuned on a certain out-of-distribution dataset and are evaluated on other out-of-distribution datasets. Effects of the threshold. We analyze how the threshold affects the following metrics: (1) FPR, i.e., the fraction of out-of-distribution images misclassified as in-distribution images; (2) TPR, i.e, the fraction of in-distribution images correctly classified as in-distribution images. In Figure 10, we show how the thresholds affect FPR and TPR when the temperature and perturbation magnitude are chosen optimally (i.e., T = 1, 000, \u03b5 = 0.0014). From the figure, we can observe that the threshold corresponding to 95% TPR can produce small FPRs on all out-of-distribution datasets.\nDifficult-to-classify images and difficult-to-detect images. We analyze the correlation between the images that tend to be out-of-distribution and images on which the neural network tend to make incorrect predictions. To understand the correlation, we devise the following experiment. For the fixed temperature T and perturbation magnitude \u03b5, we first set \u03b4 to the softmax score threshold corresponding to a certain true positive rate. Next, we calculate the test accuracy on the images with softmax scores above \u03b4 and the test accuracy on the images with softmax score below \u03b4, respectively. We report the results in Figure 11(a) and (b). From these two figures, we can observe that the images that are difficult to detect are more likely to be the images that are difficult to classify. For example, the DenseNet can achieve up to 98.5% test accuracy on the images having softmax scores above the threshold corresponding to 80% TPR, but can only achieve around 82% test accuracy on the images having softmax scores below the threshold corresponding to 80% TPR."
        }
    ],
    "year": 2020
}