{
    "abstractText": "Centralized datacenter schedulers can make high-quality placement decisions when scheduling tasks in a cluster. Today, however, high-quality placements come at the cost of high latency at scale, which degrades response time for interactive tasks and reduces cluster utilization. This paper describes Firmament, a centralized scheduler that scales to over ten thousand machines at subsecond placement latency even though it continuously reschedules all tasks via a min-cost max-flow (MCMF) optimization. Firmament achieves low latency by using multiple MCMF algorithms, by solving the problem incrementally, and via problem-specific optimizations. Experiments with a Google workload trace from a 12,500-machine cluster show that Firmament improves placement latency by 20\u00d7 over Quincy [22], a prior centralized scheduler using the same MCMF optimization. Moreover, even though Firmament is centralized, it matches the placement latency of distributed schedulers for workloads of short tasks. Finally, Firmament exceeds the placement quality of four widely-used centralized and distributed schedulers on a real-world cluster, and hence improves batch task response time by 6\u00d7.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ionel Gog"
        },
        {
            "affiliations": [],
            "name": "Malte Schwarzkopf"
        },
        {
            "affiliations": [],
            "name": "Adam Gleave"
        },
        {
            "affiliations": [],
            "name": "Robert N. M. Watson"
        },
        {
            "affiliations": [],
            "name": "Steven Hand"
        }
    ],
    "id": "SP:dc69b14f1dfaa0d4d4949268bfb8a2e20135036e",
    "references": [
        {
            "authors": [
                "Mart\u0131\u0301n Abadi",
                "Paul Barham",
                "Jianmin Chen",
                "Zhifeng Chen",
                "Andy Davis",
                "Jeffrey Dean",
                "Matthieu Devin"
            ],
            "title": "TensorFlow: A system for large-scale machine learning",
            "venue": "Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI). Savannah, Georgia,",
            "year": 2016
        },
        {
            "authors": [
                "Ravindra K. Ahuja",
                "Thomas L. Magnanti",
                "James B. Orlin"
            ],
            "title": "Network flows: theory, algorithms, and applications",
            "year": 1993
        },
        {
            "authors": [
                "Luiz Andr\u00e9 Barroso",
                "Jimmy Clidaras",
                "Urs H\u00f6lzle"
            ],
            "title": "The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines, Second edition",
            "venue": "Synthesis Lectures on Computer Architecture",
            "year": 2013
        },
        {
            "authors": [
                "Dimitri P. Bertsekas",
                "Paul Tseng"
            ],
            "title": "Relaxation Methods for Minimum Cost Ordinary and Generalized Network Flow Problems",
            "venue": "In: Operations Research",
            "year": 1988
        },
        {
            "authors": [
                "Dimitri P. Bertsekas",
                "Paul Tseng"
            ],
            "title": "The Relax codes for linear minimum cost network flow problems",
            "venue": "Annals of Operations Research",
            "year": 1988
        },
        {
            "authors": [
                "Arka A. Bhattacharya",
                "David Culler",
                "Eric Friedman",
                "Ali Ghodsi",
                "Scott Shenker",
                "Ion Stoica"
            ],
            "title": "Hierarchical Scheduling for Diverse Datacenter Workloads",
            "venue": "Proceedings of the 4th Annual Symposium on Cloud Computing (SoCC). Santa Clara, California,",
            "year": 2013
        },
        {
            "authors": [
                "Eric Boutin",
                "Jaliya Ekanayake",
                "Wei Lin",
                "Bing Shi",
                "Jingren Zhou",
                "Zhengping Qian",
                "Ming Wu"
            ],
            "title": "Apollo: Scalable and Coordinated Scheduling for Cloud-Scale Computing",
            "venue": "Proceedings of the 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI)",
            "year": 2014
        },
        {
            "authors": [
                "Yanpei Chen",
                "Sara Alspaugh",
                "Randy Katz"
            ],
            "title": "Interactive Analytical Processing in Big Data Systems: A Cross-industry Study of MapReduce Workloads",
            "venue": "Proceedings of the VLDB Endowment",
            "year": 2012
        },
        {
            "authors": [
                "Pamela Delgado",
                "Diego Didona",
                "Florin Dinu",
                "Willy Zwaenepoel"
            ],
            "title": "Job-Aware Scheduling in Eagle: Divide and Stick to Your Probes",
            "venue": "Proceedings of the 7th ACM Symposium on Cloud Computing (SoCC). Santa Clara, California,",
            "year": 2016
        },
        {
            "authors": [
                "Pamela Delgado",
                "Florin Dinu",
                "Anne-Marie Kermarrec",
                "Willy Zwaenepoel"
            ],
            "title": "Hawk: Hybrid Datacenter Scheduling",
            "venue": "Proceedings of the USENIX Annual Technical Conference. Santa Clara, California,",
            "year": 2015
        },
        {
            "authors": [
                "Christina Delimitrou",
                "Christos Kozyrakis"
            ],
            "title": "Paragon: QoS-aware Scheduling for Heterogeneous Datacenters",
            "venue": "Proceedings of the 18th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)",
            "year": 2013
        },
        {
            "authors": [
                "Christina Delimitrou",
                "Christos Kozyrakis"
            ],
            "title": "Quasar: Resource-Efficient and QoS-Aware Cluster Management",
            "venue": "Proceedings of the 18th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)",
            "year": 2014
        },
        {
            "authors": [
                "Christina Delimitrou",
                "Daniel Sanchez",
                "Christos Kozyrakis"
            ],
            "title": "Tarcil: Reconciling Scheduling Speed and Quality in Large Shared Clusters",
            "venue": "Proceedings of the 6th ACM Symposium on Cloud Computing (SoCC). Kohala Coast, Hawaii,",
            "year": 2015
        },
        {
            "authors": [
                "Antonio Frangioni",
                "Antonio Manca"
            ],
            "title": "A Computational Study of Cost Reoptimization for Min- Cost Flow Problems",
            "venue": "INFORMS Journal on Computing",
            "year": 2006
        },
        {
            "authors": [
                "Andrey Goder",
                "Alexey Spiridonov",
                "Yin Wang"
            ],
            "title": "Bistro: Scheduling Data-Parallel Jobs Against Live Production Systems",
            "venue": "Proceedings of the USENIX Annual Technical Conference. Santa Clara, California,",
            "year": 2015
        },
        {
            "authors": [
                "Andrew V. Goldberg"
            ],
            "title": "An Efficient Implementation of a Scaling Minimum-Cost Flow Algorithm",
            "venue": "Journal of Algorithms",
            "year": 1997
        },
        {
            "authors": [
                "Andrew V. Goldberg",
                "Michael Kharitonov"
            ],
            "title": "On Implementing Scaling Push-Relabel Algorithms for the Minimum-Cost Flow Problem",
            "year": 1993
        },
        {
            "authors": [
                "Andrew V. Goldberg",
                "Robert E. Tarjan"
            ],
            "title": "Finding Minimum-Cost Circulations by Successive Approximation",
            "venue": "Mathematics of Operations Research 15.3 (Aug",
            "year": 1990
        },
        {
            "authors": [
                "Matthew P. Grosvenor",
                "Malte Schwarzkopf",
                "Ionel Gog",
                "Robert N.M. Watson",
                "Andrew W. Moore",
                "Steven Hand",
                "Jon Crowcroft"
            ],
            "title": "Queues don\u2019t matter when you can JUMP them!",
            "venue": "Proceedings of the 12th USENIX Symposium on Networked Systems Design and Implementation (NSDI). Oakland,",
            "year": 2015
        },
        {
            "authors": [
                "Benjamin Hindman",
                "Andy Konwinski",
                "Matei Zaharia",
                "Ali Ghodsi",
                "Anthony D. Joseph",
                "Randy Katz",
                "Scott Shenker"
            ],
            "title": "Mesos: A platform for fine-grained resource sharing in the data center",
            "venue": "Proceedings of the 8th USENIX Conference on Networked Systems Design and Implementation (NSDI)",
            "year": 2011
        },
        {
            "authors": [
                "Michael Isard",
                "Vijayan Prabhakaran",
                "Jon Currey",
                "Udi Wieder",
                "Kunal Talwar",
                "Andrew Goldberg"
            ],
            "title": "Quincy: fair scheduling for distributed computing clusters",
            "venue": "Proceedings of the 22nd ACM Symposium on Operating Systems Principles (SOSP). Big Sky,",
            "year": 2009
        },
        {
            "authors": [
                "Konstantinos Karanasos",
                "Sriram Rao",
                "Carlo Curino",
                "Chris Douglas",
                "Kishore Chaliparambil",
                "Giovanni Matteo Fumarola",
                "Solom Heddaya"
            ],
            "title": "Mercury: Hybrid Centralized and Distributed Scheduling in Large Shared Clusters",
            "venue": "Proceedings of the USENIX Annual Technical Conference. Santa Clara, California,",
            "year": 2015
        },
        {
            "authors": [
                "Zolt\u00e1n Kir\u00e1ly",
                "P. Kov\u00e1cs"
            ],
            "title": "Efficient implementations of minimum-cost flow algorithms",
            "year": 2012
        },
        {
            "authors": [
                "Morton Klein"
            ],
            "title": "A Primal Method for Minimal Cost Flows with Applications to the Assignment and Transportation Problems",
            "venue": "In: Management Science",
            "year": 1967
        },
        {
            "authors": [
                "Andreas L\u00f6bel"
            ],
            "title": "Solving Large-Scale Real-World Minimum-Cost Flow Problems by a Network Simplex Method",
            "venue": "Tech. rep. SC-96-07. Zentrum fu\u0308r Informationstechnik Berlin (ZIB),",
            "year": 1996
        },
        {
            "authors": [
                "Kay Ousterhout",
                "Aurojit Panda",
                "Joshua Rosen",
                "Shivaram Venkataraman",
                "Reynold Xin",
                "Sylvia Ratnasamy",
                "Scott Shenker"
            ],
            "title": "The case for tiny tasks in compute clusters",
            "venue": "Proceedings of the 14th USENIX Workshop on Hot Topics in Operating Systems (HotOS). Santa Ana Pueblo,",
            "year": 2013
        },
        {
            "authors": [
                "Kay Ousterhout",
                "Patrick Wendell",
                "Matei Zaharia",
                "Ion Stoica"
            ],
            "title": "Sparrow: Distributed, Low Latency Scheduling",
            "venue": "Proceedings of the 24th ACM Symposium on Operating Systems Principles (SOSP). Nemacolin Woodlands,",
            "year": 2013
        },
        {
            "authors": [
                "Jeff Rasley",
                "Konstantinos Karanasos",
                "Srikanth Kandula",
                "Rodrigo Fonseca",
                "Milan Vojnovic",
                "Sriram Rao"
            ],
            "title": "Efficient Queue Management for Cluster Scheduling",
            "venue": "Proceedings of the 11th ACM European Conference on Computer Systems (EuroSys)",
            "year": 2016
        },
        {
            "authors": [
                "Charles Reiss",
                "Alexey Tumanov",
                "Gregory R. Ganger",
                "Randy H. Katz",
                "Michael A. Kozuch"
            ],
            "title": "Heterogeneity and dynamicity of clouds at scale: Google trace analysis",
            "venue": "Proceedings of the 3rd ACM Symposium on Cloud Computing (SoCC). San Jose, California,",
            "year": 2012
        },
        {
            "authors": [
                "Malte Schwarzkopf"
            ],
            "title": "Operating system support for warehouse-scale computing",
            "venue": "PhD thesis. University of Cambridge Computer Laboratory,",
            "year": 2015
        },
        {
            "authors": [
                "Malte Schwarzkopf",
                "Andy Konwinski",
                "Michael Abd-El-Malek",
                "John Wilkes"
            ],
            "title": "Omega: flexible, scalable schedulers for large compute clusters",
            "venue": "Proceedings of the 8th ACM European Conference on Computer Systems (EuroSys). Prague, Czech Republic,",
            "year": 2013
        },
        {
            "authors": [
                "Alexey Tumanov",
                "Timothy Zhu",
                "Jun Woo Park",
                "Michael A. Kozuch",
                "Mor Harchol-Balter",
                "Gregory R. Ganger"
            ],
            "title": "TetriSched: global rescheduling with adaptive plan-ahead in dynamic heterogeneous clusters",
            "venue": "Proceedings of the 11th ACM European Conference on Computer Systems (EuroSys)",
            "year": 2016
        },
        {
            "authors": [
                "Vinod Kumar Vavilapalli",
                "Arun C. Murthy",
                "Chris Douglas",
                "Sharad Agarwal",
                "Mahadev Konar",
                "Robert Evans",
                "Thomas Graves"
            ],
            "title": "Apache Hadoop YARN: Yet Another Resource Negotiator",
            "venue": "Proceedings of the 4th Annual Symposium on Cloud Computing (SoCC). Santa Clara, California,",
            "year": 2013
        },
        {
            "authors": [
                "Abhishek Verma",
                "Luis David Pedrosa",
                "Madhukar Korupolu",
                "David Oppenheimer",
                "John Wilkes"
            ],
            "title": "Large scale cluster management at Google",
            "venue": "Proceedings of the 10th ACM European Conference on Computer Systems (EuroSys)",
            "year": 2015
        },
        {
            "authors": [
                "Xiao Zhang",
                "Eric Tune",
                "Robert Hagmann",
                "Rohit Jnagal",
                "Vrigo Gokhale",
                "John Wilkes"
            ],
            "title": "CPI2: CPU Performance Isolation for Shared Compute Clusters",
            "venue": "Proceedings of the 8th ACM European Conference on Computer Systems (EuroSys). Prague, Czech Republic,",
            "year": 2013
        }
    ],
    "sections": [
        {
            "text": "This paper describes Firmament, a centralized scheduler that scales to over ten thousand machines at subsecond placement latency even though it continuously reschedules all tasks via a min-cost max-flow (MCMF) optimization. Firmament achieves low latency by using multiple MCMF algorithms, by solving the problem incrementally, and via problem-specific optimizations.\nExperiments with a Google workload trace from a 12,500-machine cluster show that Firmament improves placement latency by 20\u00d7 over Quincy [22], a prior centralized scheduler using the same MCMF optimization. Moreover, even though Firmament is centralized, it matches the placement latency of distributed schedulers for workloads of short tasks. Finally, Firmament exceeds the placement quality of four widely-used centralized and distributed schedulers on a real-world cluster, and hence improves batch task response time by 6\u00d7."
        },
        {
            "heading": "1 Introduction",
            "text": "Many applications today run on large datacenter clusters [3]. These clusters are shared by applications of many organizations and users [6; 21; 35]. Users execute jobs, which each consist of one or more parallel tasks. The cluster scheduler decides how to place these tasks on cluster machines, where they are instantiated as processes, containers, or VMs.\nBetter task placements by the cluster scheduler lead to higher machine utilization [35], shorter batch job runtime, improved load balancing, more predictable application performance [12; 36], and increased fault tolerance [32]. Achieving high task placement quality is hard: it requires algorithmically complex optimization in multiple dimensions. This goal conflicts with the need for a\nlow placement latency, the time it takes the scheduler to place a new task. A low placement latency is required both to meet user expectations and to avoid idle cluster resources while there are waiting tasks. Shorter batch task runtimes and increasing cluster scale make it difficult to meet both conflicting goals [9; 10; 13; 23; 29]. Current schedulers thus choose one to prioritize.\nThree different cluster scheduler architectures exist today. First, centralized schedulers use elaborate algorithms to find high-quality placements [11; 12; 35], but have latencies of seconds or minutes [13; 32]. Second, distributed schedulers use simple algorithms that allow for high throughput, low latency parallel task placement at scale [13; 28; 29]. However, their uncoordinated decisions based on partial, stale state can result in poor placements. Third, hybrid schedulers split the workload across a centralized and a distributed component. They use sophisticated algorithms for long-running tasks, but rely on distributed placement for short tasks [9; 10; 23].\nIn this paper, we show that a centralized scheduler based on sophisticated algorithms can be fast and scalable for both current and future workloads. We built Firmament, a centralized scheduler that meets three goals:\n1. to maintain the same high placement quality as an existing centralized scheduler (viz. Quincy [22]); 2. to achieve sub-second task placement latency for all workloads in the common case; and 3. to cope well with demanding situations such as cluster oversubscription or large incoming jobs.\nOur key insight is that even centralized sophisticated algorithms for the scheduling problem can be fast (i) if they match the problem structure well, and (ii) if few changes to cluster state occur while the algorithm runs.\nFirmament generalizes Quincy [22], which represents the scheduling problem as a min-cost max-flow (MCMF) optimization over a graph (\u00a73) and continuously reschedules the entire workload. Quincy\u2019s original MCMF algorithm results in task placement latencies of minutes on a large cluster. Firmament, however, achieves placement\nUSENIX Association 12th USENIX Symposium on Operating Systems Design and Implementation 99\nlatencies of hundreds of milliseconds in the common case and reaches the same placement quality as Quincy.\nTo achieve this, we studied several MCMF optimization algorithms and their performance (\u00a74). Surprisingly, we found that relaxation [4], a seemingly inefficient MCMF algorithm, outperforms other algorithms on the graphs generated by the scheduling problem. However, relaxation can be slow in crucial edge cases, and we thus investigated three techniques to reduce Firmament\u2019s placement latency across different algorithms (\u00a75):\n1. Terminating the MCMF algorithms early to find approximate solutions generates unacceptably poor and volatile placements, and we reject the idea. 2. Incremental re-optimization improves the runtime of Quincy\u2019s original MCMF algorithm (cost scaling [17]), and makes it an acceptable fallback. 3. Problem-specific heuristics aid some MCMF algorithms to run faster on graphs of specific structure.\nWe combined these algorithmic insights with several implementation-level techniques to further reduce Firmament\u2019s placement latency (\u00a76). Firmament runs two MCMF algorithms concurrently to avoid slowdown in edge cases; it implements an efficient graph update algorithm to handle cluster state changes; and it quickly extracts task placements from the computed optimal flow.\nOur evaluation compares Firmament to existing distributed and centralized schedulers, both in simulation (using a Google workload trace) and on a local 40- machine cluster (\u00a77). In our experiments, we find that Firmament scales well: even with 12,500 machines and 150,000 live tasks eligible for rescheduling, Firmament makes sub-second placements. This task placement latency is comparable to those of distributed schedulers, even though Firmament is centralized. When scheduling workloads that consist exclusively of short, sub-second tasks, Firmament scales to over 1,000 machines, but suffers overheads for task runtimes below 5s at 10,000 machines. Yet, we find that Firmament copes well with realistic, mixed workloads that combine long-running services and short tasks even at this scale: Firmament keeps up with a 250\u00d7 accelerated Google workload. Finally, we show that Firmament\u2019s improved placement quality reduces short batch tasks\u2019 runtime by up to 6\u00d7 compared to other schedulers on a real-world cluster.\nFirmament is available as open-source software (\u00a79)."
        },
        {
            "heading": "2 Background",
            "text": "Cluster managers such as Mesos [21], YARN [34], Borg [35], and Kubernetes [14] automatically share and manage physical datacenter resources. Each one has a scheduler, which is responsible for placing tasks on machines. Figure 1 illustrates the lifecycle of a task in a cluster manager: after the user submits the task, it waits until the scheduler places it on a machine where it sub-\nsequently runs. The time between submission and task placement is the task placement latency, and to the total time between the task\u2019s submission and its completion is the task response time.1 The time a task spends being actively scheduled is the scheduler\u2019s algorithm runtime.\nFor each task, the scheduling algorithm typically first performs a feasibility check to identify suitable machines, then scores them according to a preference order, and finally places the task on the best-scoring machine. Scoring, i.e., rating the different placement choices for a task, can be expensive. Google\u2019s Borg, for example, relies on several batching, caching, and approximation optimizations to keep scoring tractable [35, \u00a73.4].\nHigh placement quality increases cluster utilization and avoids performance degradation due to overcommit. Poor placement quality, by contrast, increases task response time (for batch tasks), or decreases applicationlevel performance (for long-running services)."
        },
        {
            "heading": "2.1 Task-by-task placement",
            "text": "Most cluster schedulers, whether centralized or distributed, are queue-based and process one task at a time (per scheduler). Figure 2a illustrates how such a queuebased scheduler processes a new task. The task first waits in a queue of unscheduled tasks until it is dequeued and processed by the scheduler. In a busy cluster, a task may spend substantial time enqueued. Some schedulers also have tasks wait in a per-machine \u201cworker-side\u201d queue [29], which allows for pipelined parallelism.\nTask-by-task placement has the advantage of being amenable to uncoordinated, parallel decisions in distributed schedulers [9; 10; 13; 28]. On the other hand, processing one task at a time also has two crucial downsides: first, the scheduler commits to a placement early and restricts its choices for further waiting tasks, and second, there is limited opportunity to amortize work."
        },
        {
            "heading": "2.2 Batching placement",
            "text": "Both downsides of task-by-task placement can be addressed by batching. Processing several tasks in a batch\n1Task response time is primarily meaningful for batch tasks; longrunning service tasks\u2019 response times are conceptually infinite, and in practice are determined by failures and operational decisions.\n100 12th USENIX Symposium on Operating Systems Design and Implementation USENIX Association\nallows the scheduler to jointly consider their placement, and thus to find the best trade-off for the whole batch. A natural extension of this idea is to reconsider the entire existing workload (\u201crescheduling\u201d), and to preempt and migrate running tasks if prudent.\nFlow-based scheduling, introduced by Quincy [22], is an efficient batching technique. Flow-based scheduling uses a placement mechanism \u2013 min-cost max-flow (MCMF) optimization \u2013 with an attractive property: it guarantees overall optimal task placements for a given scheduling policy. Figure 2b illustrates how it proceeds. If a change to cluster state happens (e.g., task submission), the scheduler updates an internal graph representation of the scheduling problem. It waits for any running optimization to finish, and then runs a MCMF solver on the graph. This yields an optimal flow from which the scheduler extracts the task assignments.\nHowever, Figure 3 illustrates that Quincy, the current state-of-the-art flow-based scheduler, is too slow to meet our placement latency goal at scale. In this experiment, we replayed subsets of the public Google trace [30], which we augmented with locality preferences for batch\nprocessing jobs2 against our faithful reimplementation of Quincy\u2019s approach. We measured the scheduler algorithm runtime for clusters of increasing size with proportional workload growth. The algorithm runtime increases with scale, up to a median of 64s and a 99th percentile of 83s for the full Google cluster (12,500 machines). During this time, the scheduler must wait for the solver to finish, and cannot choose any placements for new tasks.\nThe goal of this paper is to build a flow-based scheduler that achieves equal placement quality to Quincy, but which does so at sub-second placement latency. As our experiment illustrates, we must achieve at least an orderof-magnitude speedup over Quincy to meet this goal."
        },
        {
            "heading": "3 Firmament approach",
            "text": "We chose to develop Firmament as a flow-based scheduler for three reasons. First, flow-based scheduling considers the entire workload, allowing us to support rescheduling and priority preemption. Second, flowbased scheduling achieves high placement quality and, consequently, low task response times [22, \u00a76]. Third, as a batching approach, flow-based scheduling amortizes work well over many tasks and placement decisions, and hence achieves high task throughput \u2013 albeit at a high placement latency that we aim to improve.\n2Details of our simulation are in \u00a77; in the steady-state, the 12,500- machine cluster runs about 150,000 tasks comprising about 1,800 jobs.\nUSENIX Association 12th USENIX Symposium on Operating Systems Design and Implementation 101"
        },
        {
            "heading": "3.1 Architecture",
            "text": "Figure 4 gives an overview of the Firmament scheduler architecture. Firmament, like Quincy, models the scheduling problem as a min-cost max-flow (MCMF) optimization over a flow network. The flow network is a directed graph whose structure is defined by the scheduling policy. In response to events and monitoring information, the flow network is modified according to the scheduling policy, and submitted to an MCMF solver to find an optimal (i.e., min-cost) flow. Once the solver completes, it returns the optimal flow, from which Firmament extracts the implied task placements. In the following, we first explain the basic structure of the flow network, and then discuss how to make the solver fast."
        },
        {
            "heading": "3.2 Flow network structure",
            "text": "A flow network is a directed graph whose arcs carry flow from source nodes to a sink node. A cost and capacity associated with each arc constrain the flow, and specify preferential routes for it.\nFigure 5 shows an example of a flow network that expresses a simple cluster scheduling problem. Each task node T j,i on the left hand side, representing the ith task of job j, is a source of one unit of flow. All such flow must be drained into the sink node (S) for a feasible solution to the optimization problem. To reach S, flow from T j,i can proceed through a machine node (Mm), which schedules the task on machine m (e.g., T0,2 on M1). Alternatively, the flow may proceed to the sink through an unscheduled aggregator node (U j for job j), which leaves the task unscheduled (as with T0,1) or preempts it if running.\nIn the example, a task\u2019s placement preferences are expressed as costs on direct arcs to machines. The cost to leave the task unscheduled, or to preempt it when run-\nning, is the cost on its arc to the unscheduled aggregator (e.g., 7 for T1,1). Given this flow network, an MCMF solver finds a globally optimal (i.e., minimum-cost) flow (shown in red in Figure 5). This optimal flow expresses the best trade-off between the tasks\u2019 unscheduled costs and their placement preferences. Task placements are extracted by tracing flow from the machines back to tasks.\nIn our example, tasks had only direct arcs to machines. The solver finds the best solution if every task has an arc to each machine scored according to the scheduling policy, but this requires thousands of arcs per task on a large cluster. Policy-defined aggregator nodes, similar to the unscheduled aggregators, reduce the number of arcs required to express a scheduling policy. Such aggregators group, e.g., machines in a rack, tasks with similar resource needs, or machines with similar capabilities. With aggregators, the cost of a task placement is the sum of all costs on the path from the task node to the sink."
        },
        {
            "heading": "3.3 Scheduling policies",
            "text": "Firmament generalizes flow-based scheduling over the single, batch-oriented policy proposed by Quincy. Cluster administrators use a policy API to configure Firmament\u2019s scheduling policy, which may incorporate e.g., multi-dimensional resources, fairness, and priority preemption [31, Ch. 6\u20137]. This paper focuses on Firmament\u2019s scalability, and we therefore use only three simplified, illustrative policies explained in the following: (i) a simple load-spreading policy, (ii) Quincy\u2019s slot-based, locality-oriented policy, and (iii) a network-aware policy that avoids overloading machines\u2019 network connections.\nLoad-spreading policy. Figure 6a shows a trivial use of an aggregator: all tasks have arcs to a cluster-wide aggregator (X). The cost on the outgoing arc from X to each machine node is proportional to the number of tasks already running on the machine (e.g., one task on M3). The effect is that the number of tasks on a machine only increases once all other machines have at least as many tasks (as e.g., in Docker SwarmKit). This policy neither requires or nor uses the full sophistication of flow-based scheduling. We use it to highlight specific edge cases in MCMF algorithms (see \u00a74.3).\nQuincy policy. Figure 6b depicts Quincy\u2019s original locality-oriented policy [22, \u00a74.2], which uses rack aggregators (Rr) and a cluster aggregator (X) to express data locality for batch jobs. Tasks have low-cost preference arcs to machines and racks on which they have local data, but fall back to scheduling via the cluster aggregator if their preferences are unavailable (e.g., T0,2). This policy is suitable for batch jobs, and optimizes for a tradeoff between data locality, task wait time, and preemption cost. We use it to illustrate MCMF algorithm performance and for head-to-head comparison with Quincy.\n102 12th USENIX Symposium on Operating Systems Design and Implementation USENIX Association\nNetwork-aware policy. Figure 6c illustrates a policy which avoids overcommitting machines\u2019 network bandwidth (which degrades task response time). Each task connects to a request aggregator (RA) for its network bandwidth request. The RAs have one arc for each task that fits on each machine with sufficient spare bandwidth (e.g., 650 MB/s of 1.25 GB/s on M2\u2019s 10G link). These arcs are dynamically adapted as the observed bandwidth use changes. Costs on the arcs to machines are the sum of the request and the currently used bandwidth, which incentivizes balanced utilization. We use this policy to illustrate Firmament\u2019s potential to make high-quality decisions, but a production policy would be more complex and extend it with a priority notion and additional resource dimensions (e.g., CPU/RAM) [31, \u00a77.3]."
        },
        {
            "heading": "4 Min-cost max-flow algorithms",
            "text": "A flow-based scheduler can use any MCMF algorithm, but some algorithms are better suited to the scheduling problem than others. In this section, we explain the MCMF algorithms that we implemented for Firmament, compare them empirically, and explain their sometimes unexpected performance.\nA min-cost max-flow algorithm takes a directed flow network G = (N,A) as input. Each arc (i, j) \u2208 A has a cost ci j and a maximum capacity ui j. Each node i \u2208 N also has an associated supply b(i); nodes with positive supply are sources, those with negative supply are sinks.\nInformally, MCMF algorithms must optimally route the flow from all sources (e.g., task nodes Ti, j) to sinks (e.g., the sink node S) without exceeding the capacity constraint on any arc. To understand the differences between MCMF algorithms, we need a slightly more formal definition: the goal is to find a flow f that minimizes\nEq. 1, while respecting the flow feasibility constraints of mass balance (Eq. 2) and capacity (Eq. 3):\nMinimize \u2211 (i, j)\u2208A ci j fi j subject to (1)\n\u2211 k:( j,k)\u2208A f jk\u2212 \u2211 i:(i, j)\u2208A fi j = b( j),\u2200 j \u2208 N (2)\nand 0\u2264 fi j \u2264 ui j,\u2200(i, j) \u2208 A (3) Some algorithms use an equivalent definition of the flow network, the residual network. In the residual network, each arc (i, j) \u2208 A with cost ci j and maximum capacity ui j is replaced by two arcs: (i, j) and ( j, i). Arc (i, j) has cost ci j and a residual capacity of ri j = ui j\u2212 fi j, while arc ( j, i) has cost \u2212ci j and a residual capacity r ji = fi j. The feasibility constraints also apply in the residual network.\nThe primal minimization problem (Eq. 1) also has an associated dual problem, which some algorithms solve more efficiently. In the dual min-cost max-flow problem, each node i\u2208N has an associated dual variable \u03c0(i) called the potential. The potentials are adjusted in different, algorithm-specific ways to meet optimality conditions. Moreover, each arc has a reduced cost with respect to the node potentials, defined as:\nc\u03c0i j = ci j\u2212\u03c0(i)+\u03c0( j) (4)\nA feasible flow is optimal if and only if at least one of three optimality conditions is met:\n1. Negative cycle optimality: no directed negativecost cycles exist in the residual network.\n2. Reduced cost optimality: there is a set of node potentials \u03c0 such that there are no arcs in the residual network with negative reduced cost (c\u03c0i j).\nUSENIX Association 12th USENIX Symposium on Operating Systems Design and Implementation 103\n3. Complementary slackness optimality: there is a set of node potentials \u03c0 such that the flow on arcs with c\u03c0i j > 0 is zero, and there are no arcs with both c\u03c0i j < 0 and available capacity.\nAlgorithms. The simplest MCMF algorithm is cycle canceling [25]. The algorithm first computes a maxflow solution, and then performs a series of iterations in which it augments flow along negative-cost directed cycles in the residual network. Pushing flow along such a cycle guarantees that the overall solution cost decreases. The algorithm finishes with an optimal solution once no negative-cost cycles remain (i.e., the negative cycle optimality condition is met). Cycle canceling always maintains feasibility and attempts to achieve optimality.\nUnlike cycle canceling, the successive shortest path algorithm [2, p. 320] maintains reduced cost optimality at every step and tries to achieve feasibility. It repeatedly selects a source node (i.e., b(i)> 0) and sends flow from it to the sink along the shortest path.\nThe relaxation algorithm [4; 5], like successive shortest path, augments flow from source nodes along the shortest path to the sink. However, unlike successive shortest path, relaxation optimizes the dual problem by applying one of two changes when possible:\n1. Keeping \u03c0 unchanged, the algorithm modifies the flow, f , to f \u2032 such that f \u2032 still respects the reduced cost optimality condition and the total supply decreases (i.e., feasibility improves). 2. It modifies \u03c0 to \u03c0 \u2032 and f to f \u2032 such that f \u2032 is still a reduced cost-optimal solution and the cost of that solution decreases (i.e., total cost decreases).\nThis allows relaxation to decouple the improvements in feasibility from reductions in total cost. When relaxation can reduce cost or improve feasibility, it reduces cost.\nCost scaling [17\u201319] iterates to reduce cost while maintaining feasibility, and uses a relaxed complementary slackness condition called \u03b5-optimality. A flow is \u03b5optimal if the flow on arcs with c\u03c0i j > \u03b5 is zero and there are no arcs with c\u03c0i j <\u2212\u03b5 on which flow can be sent. Initially, \u03b5 is equal to the maximum arc cost, but \u03b5 rapidly decreases as it is divided by a constant factor after every iteration that achieves \u03b5-optimality. Cost scaling finishes\nwhen 1n -optimality is achieved, since this is equivalent to the complementary slackness optimality condition [17]."
        },
        {
            "heading": "4.1 Algorithmic performance",
            "text": "Table 1 summarizes the worst-case complexities of the algorithms discussed. The complexities suggest that successive shortest path ought to work best, as long as U log(N)< M log(NC), which is the case as U M and C \u2265 1. However, since MCMF algorithms are known to have variable runtimes depending on the input graph [15; 24; 26], we decided to directly measure performance."
        },
        {
            "heading": "4.2 Measured performance",
            "text": "As in the experiment in Figure 3, we subsample the Google trace and replay it for simulated clusters of different sizes. We use the Quincy scheduling policy for batch jobs and prioritize service jobs over batch ones. Figure 7 shows the average runtime for each MCMF algorithm considered. Even though it has the best worst-case time complexity, successive shortest path outperforms only cycle canceling, and even on a modest cluster of 1,250 machines its algorithm runtime exceeds 100 seconds.\nMoreover, the relaxation algorithm, which has the highest worst-case time complexity, actually performs best in practice. It outperforms cost scaling (used in Quincy) by two orders of magnitude: on average, relaxation completes in under 200ms even on a cluster of 12,500 machines. One key reason for this perhaps surprising performance is that relaxation does minimal work when most scheduling choices are straightforward. This happens if the destinations for tasks\u2019 flow are uncontested, i.e., few new tasks have arcs to the same location and attempt to schedule there. In this situation, relaxation routes most of the flow in a single pass over the graph.\n104 12th USENIX Symposium on Operating Systems Design and Implementation USENIX Association"
        },
        {
            "heading": "4.3 Edge cases for relaxation",
            "text": "Yet, relaxation is not always the right choice. For example, it can perform poorly under the high load and oversubscription common in batch analytics clusters [29]. Figure 8 illustrates this: here, we push the simulated Google cluster closer to oversubscription. We take a snapshot of the cluster and then submit increasingly larger jobs. The relaxation runtime increases rapidly, and at about 93% cluster utilization, it exceeds that of cost scaling, growing to over 400s in the oversubscribed case.\nMoreover, some scheduling policies inherently create contention between tasks. Consider, for example, our load-spreading policy that balances the task count on each machine. This policy makes \u201cunder-populated\u201d machines a popular destination for tasks\u2019 flow, and thus creates contention. We illustrate this with an experi-\nment: we submit a single job with an increasing number of tasks to a cluster using the load-spreading policy. This corresponds to the rare-but-important arrival of very large jobs: for example, 1.2% of jobs in the Google trace have over 1,000 tasks, and some even over 20,000. Figure 9 shows that relaxation\u2019s runtime increases linearly in the number of tasks, and that it exceeds the runtime of cost scaling once the new job has over 3,000 tasks.\nTo make matters worse, a single overlong relaxation run can have a devastating effect on long-term placement latency. If many new tasks arrive during such a long run, the scheduler might again be faced with many unscheduled tasks when it next runs. Hence, relaxation may take a long time again, accumulate many changes, and in the worst case fail to ever recover to low placement latency."
        },
        {
            "heading": "5 MCMF optimizations for scheduling",
            "text": "Relaxation has promising common-case performance at scale for typical workloads. However, its edge-case behavior makes it necessary either (i) to fall back to other algorithms in these cases, or (ii) to reduce runtime in other ways. In the following, we use challenging graphs to investigate optimizations that either improve relaxation or the best \u201cfallback\u201d algorithm, cost scaling."
        },
        {
            "heading": "5.1 Approximate min-cost max-flow",
            "text": "MCMF algorithms return an optimal solution. For the cluster scheduling problem, however, an approximate solution may well suffice. For example, TetriSched [33] (based on an MILP solver), as well as Paragon [11] and Quasar [12] (based on collaborative filtering), terminate their solution search after a set time. We therefore investigated the solution quality of cost scaling and relaxation when they are terminated early. This would work well if the algorithms spent a long time on minor solution refinements with little impact on the overall outcome.\nUSENIX Association 12th USENIX Symposium on Operating Systems Design and Implementation 105\nIn our experiment, we use a highly-utilized cluster (cf. Figure 8) to investigate relaxation and cost scaling, but the results generalize. Figure 10 shows the number of \u201cmisplaced\u201d tasks as a function of how early we terminate the algorithms. We treat any task as misplaced if it is (i) preempted in the approximate solution but keeps running in the optimal one; (ii) scheduled on a different machine to where it is scheduled in the optimal solution. Both cost scaling and relaxation misplace thousands of tasks when terminated early, and tasks are still misplaced even in the final iteration before completion. Hence, early termination appears not to be a viable placement latency optimization for flow-based schedulers."
        },
        {
            "heading": "5.2 Incremental min-cost max-flow",
            "text": "Since cluster state does not change dramatically between subsequent scheduling runs, the MCMF algorithm might be able to reuse its previous state. In this section, we describe what changes are required to make MCMF algorithms work incrementally, and provide some intuition for which algorithms are suitable for incremental use.\nAll cluster events (e.g., task submissions, machine failures) ultimately reduce to three different types of graph change in the flow network:\n1. Supply changes at nodes when arcs or nodes which previously carried flow are removed (e.g., due to machine failure), or when nodes with supply are added to the graph (e.g., at task submission). 2. Capacity changes on arcs if machines fail or (re)join the cluster. Note that arc additions and removals can also be modeled as capacity changes from and to zero-capacity arcs. 3. Cost changes on an arc when the desirability of routing flow via that arc changes; when these happen exactly depends on the scheduling policy. Changes to the supply of a node, an arc\u2019s capacity, or its cost can invalidate the feasibility and optimality of an existing flow. Some MCMF algorithms require the flow to be feasible at every step and improve \u03b5-optimality, while others require optimality to always hold and improve feasibility (Table 2). A solution must be optimal and feasible because an infeasible solution fails to route all flow, which leaves tasks unscheduled or erroneously preempts them, while a non-optimal solution misplaces tasks.\nWe implemented incremental versions of the cost scaling and relaxation algorithms. Incremental cost scaling is up to 50% faster than running cost scaling from scratch (Figure 11). Incremental cost scaling\u2019s potential gains are limited because cost scaling requires the flow to be feasible and \u03b5-optimal before each intermediate iteration (Table 2). Graph changes can cause the flow to violate one or both requirements: for example, any addition or removal of task nodes adds supply and breaks feasibility. Table 3 shows the effect of different arc changes on the feasibility and optimality of the flow. A change that modifies the cost of an arc (i, j) from c\u03c0i j < 0 to c\u2032\u03c0i j > 0, for example, breaks optimality. Many changes break optimality and cause cost scaling to fall back to a higher \u03b5-optimality to compensate. To bring \u03b5 back down, cost scaling must do a substantial part of the work it would do from scratch. However, the limited improvement still helps reduce our fallback algorithm\u2019s runtime.\nIncremental relaxation ought to work better than incremental cost scaling, since the relaxation algorithm only needs to maintain reduced cost optimality (Table 2). In practice, however, it turns out not to work well. While the algorithm can be incrementalized with relative ease and often runs faster, it \u2013 counter-intuitively \u2013 can also be\n106 12th USENIX Symposium on Operating Systems Design and Implementation USENIX Association\nslower incrementally than when running from scratch. Relaxation requires reduced cost optimality to hold at every step of the algorithm and tries to achieve feasibility by pushing flow on zero-reduced cost arcs from source nodes to nodes with demand. The algorithm builds a tree of zero-reduced cost arcs from each source node in order to find such demand nodes. The tree is expanded by adding zero-reduced cost arcs to it. When running from scratch, the likelihood of zero-reduced cost arcs connecting two zero-reduced cost trees is low, as there are few such trees initially. Only when the solution is close to optimality, trees are joined into larger ones. Incremental relaxation, however, works with the existing, closeto-optimal state, which already contains large trees that must be extended for each source. Having to traverse these large trees many times, incremental relaxation can run slower than from scratch. This happens especially for graphs that relaxation already struggles with, e.g. ones that contain nodes with a lot of potential incoming flow. In practice, we found that incremental relaxation performs well only if tasks are not typically connected to a large zero-reduced cost tree."
        },
        {
            "heading": "5.3 Problem-specific heuristics",
            "text": "Our scheduler runs min-cost max-flow on a graph with specific properties, rather than the more general graphs typically used to evaluate MCMF algorithms [24, \u00a74]. For example, our graph has a single sink; it is a directed acyclic graph; and flow must always traverse unscheduled aggregators or machine nodes. Hence, problemspecific heuristics might help the algorithms find solutions more quickly. We investigated several such heuristics, and found two beneficial ones: (i) prioritization of promising arcs, and (ii) efficient task node removal."
        },
        {
            "heading": "5.3.1 Arc prioritization",
            "text": "The relaxation algorithm builds a tree of zero-reduced cost arcs for every source node (see \u00a75.2) in order to locate zero-reduced cost paths (i.e., paths that do not break reduced cost optimality) to nodes with demand. When this tree must be extended, any arc of zero reduced cost\nthat connects a node inside the tree to a node outside the tree can be used. However, some arcs are better choices for extension than others. The quicker we can find paths to nodes with demand, the sooner we can route the supply. We therefore prioritize arcs that lead to nodes with demand when extending the cut, adding them to the front of a priority queue to ensure they are visited sooner.\nIn effect, this heuristic implements a hybrid graph traversal that biases towards depth-first exploration when demand nodes can be reached, but uses breadth first exploration otherwise. Figure 12a shows that applying this heuristic reduces relaxation runtime by 45% when running over a graph with contended nodes."
        },
        {
            "heading": "5.3.2 Efficient task removal",
            "text": "Our second heuristic helps incremental cost scaling. It is based on the insight that removal of a running task is common (e.g., due to completion, preemption, or a machine failure), but breaks feasibility. This happens because the task node is removed, which creates demand at the machine node where the task ran, since the machine node still has outgoing flow in the intermediate solution. Breaking feasibility is expensive for cost scaling (\u00a75.2).\nHowever, we can reconstruct the task\u2019s flow through the graph, remove it, and drain the machine node\u2019s flow at the single sink node. This creates demand in a single place only (the sink), which accelerates the incremental solution. However, Figure 12b shows that this heuristic offers only modest gains: it improves runtime by 10%."
        },
        {
            "heading": "6 Firmament implementation",
            "text": "We implemented a new MCMF solver for Firmament. It supports the four algorithms discussed earlier (\u00a74) and incremental cost scaling. The solver consists of about 8,000 lines of C++. Firmament\u2019s cluster manager and our simulator are implemented in about 24,000 lines of C++, and are available at http://firmament.io.\nIn this section, we discuss implementation-level techniques that, in addition to our prior algorithmic insights, help Firmament achieve low task placement latency."
        },
        {
            "heading": "6.1 Algorithm choice",
            "text": "In \u00a74, we saw that the practical performance of MCMF algorithms varies. Relaxation often works best, but scales poorly in specific edge cases. Cost scaling, by contrast, scales well and can be incrementalized (\u00a75.2), but is usually substantially slower than relaxation.\nFirmament\u2019s MCMF solver always speculatively executes cost scaling and relaxation, and picks the solution offered by whichever algorithm finishes first. In the common case, this is relaxation; having cost scaling as well guarantees that Firmament\u2019s placement latency does not grow unreasonably large in challenging situations. We run both algorithms instead of developing a heuristic to choose the right one for two reasons: first, it is cheap, as\nUSENIX Association 12th USENIX Symposium on Operating Systems Design and Implementation 107\nthe algorithms are single-threaded and do not parallelize; second, predicting the right algorithm is hard and the heuristic would depend on both scheduling policy and cluster utilization (cf. \u00a74), making it brittle and complex."
        },
        {
            "heading": "6.2 Efficient algorithm switching",
            "text": "Firmament also applies an optimization that helps it efficiently transition state from relaxation to incremental cost scaling. Firmament\u2019s MCMF solver uses incremental cost scaling as it is faster than running cost scaling from scratch (\u00a75.2). Typically, however, the (fromscratch) relaxation algorithm finishes first. The next incremental cost scaling run must therefore use the solution from a prior relaxation as a starting point. Since relaxation and cost scaling use different reduced cost graph representations, this can be slow. Specifically, relaxation may converge on node potentials that fit poorly into cost scaling\u2019s complementary slackness requirement, since relaxation only requires reduced cost optimality.\nWe found that price refine [17], a heuristic originally developed for use within cost scaling, helps with this transition. Price refine reduces the node potentials without affecting solution optimality, and thus simplifies the problem for cost scaling. Figure 13 shows that applying price refine to the prior relaxation solution graph speeds up incremental cost scaling by 4\u00d7 in 90% of cases.\nWe apply price refine on the previous solution before we apply the latest cluster changes. This guarantees that price refine is able to find node potentials that satisfy complementary slackness optimality without modifying the flow. Consequently, cost scaling must start only at a value of \u03b5 equal to the costliest arc graph change."
        },
        {
            "heading": "6.3 Efficient solver interaction",
            "text": "So far, we have primarily focused on reducing the MCMF solver\u2019s algorithm runtime. To achieve low task placement latency, we must make two steps that fall out-\n1 to_visit = machine_nodes # list of machine nodes 2 node_flow_destinations = {} # auxiliary remember set 3 mappings = {} # final task mappings 4 while not to_visit.empty(): 5 node = to_visit.pop() 6 if node.type is not TASK_NODE: 7 # Visit the incoming arcs 8 for arc in node.incoming_arcs(): 9 moved_machines = 0\n10 # Move as many machines to the incoming arc\u2019s 11 # source node as there is flow on the arc 12 while assigned_machines < arc.flow: 13 node_flow_destinations[arc.source].append( 14 node_flow_destinations[node].pop()) 15 moved_machines += 1 16 # (Re)visit the incoming arc\u2019s source node 17 if arc.source not in to_visit: 18 to_visit.append(arc.source) 19 else: # node.type is TASK_NODE 20 mappings[node.task_id] = 21 node_flow_destinations[node].pop() 22 return mappings\nListing 1: Our efficient algorithm for extracting task placements from the optimal flow returned by the solver.\nside the solver runtime efficient as well. First, Firmament must efficiently update the flow network\u2019s nodes, arcs, costs, and capacities before every MCMF optimization to reflect the chosen scheduling policy. Second, Firmament must quickly extract task placements out of the flow network after the optimization finishes. We improve over the prior work on flow-based scheduling in Quincy for both aspects, as explained in the following.\nFlow network updates. Firmament does two breadthfirst traversals of the flow network to update it for a new solver run. The first traversal updates resource statistics associated with every entity, such as the memory available on a machine, its current load, or a task\u2019s resource request. The traversal starts from the nodes adjacent to the sink (usually machine nodes), and propagates statistics along each node\u2019s incoming arcs. Upon the first traversal\u2019s completion, Firmament runs a second traversal that starts at the task nodes. This pass allows the scheduling policy to update the flow network\u2019s nodes, arcs, costs and capacities using the statistics gathered in the first traversal. Hence, only two passes over the large graph must be made to prepare the next solver run. Their overhead is negligible compared to the solver runtime.\nTask placement extraction. At the end of a run, the solver returns an optimal flow through the given network and Firmament must extract the task placements implied by this flow. Since Firmament allows arbitrary aggregators in the flow network, paths from tasks to machines may be longer than in Quincy, where arcs necessarily pointed to machines or racks. Hence, we had to gen-\n108 12th USENIX Symposium on Operating Systems Design and Implementation USENIX Association\neralize Quincy\u2019s approach to this extraction [22, p. 275]. To extract task assignments efficiently, we devised the graph traversal algorithm shown in Listing 1. The algorithm starts from machine nodes and propagates a list of machines to which each node has sent flow via its incoming arcs. In the common case, the algorithm extracts the task placements in a single pass over the graph."
        },
        {
            "heading": "7 Evaluation",
            "text": "We now evaluate how well Firmament meets its goals:\n1. How do Firmament\u2019s task placement quality and placement latency compare to Quincy\u2019s? (\u00a77.2) 2. How does Firmament cope with demanding situations such as an overloaded cluster? (\u00a77.3) 3. At what operating points does Firmament fail to achieve sub-second placement latency? (\u00a77.4) 4. How does Firmament\u2019s placement quality compare to other cluster schedulers on a physical cluster running a mixed batch/service workload? (\u00a77.5)"
        },
        {
            "heading": "7.1 Methodology",
            "text": "Our experiments combine scale-up simulations with experiments on a local testbed cluster.\nIn simulations, we replay a public production workload trace from 12,500-machine Google cluster [30] against Firmament\u2019s implementation. Our simulator is similar to Borg\u2019s \u201cFauxmaster\u201d [35, \u00a73.1]: it runs Firmament\u2019s real code and scheduling logic against simulated machines, merely stubbing out RPCs and task execution. However, there are three important limitations to note. First, the Google trace contains multi-dimensional resource requests for each task. Firmament supports multidimensional feasibility checking (as in Borg [35, \u00a73.2]), but in order to fairly compare to Quincy, we use slotbased assignment. Second, we do not enforce task constraints for the same reason, even though they typically help Firmament\u2019s MCMF solver. Third, the Google trace lacks information about job types and input sizes. We use Omega\u2019s priority-based job type classification [32, \u00a72.1], and estimate batch task input sizes as a function of the known runtime using typical industry distributions [8].\nIn local cluster experiments, we use a homogeneous 40-machine cluster. Each machine has a Xeon E52430Lv2 CPU (12\u00d7 2.40GHz), 64 GB RAM, and uses a 1TB magnetic disk for storage. The machines are connected via 10 Gbps, full-bisection bandwidth Ethernet.\nWhen we compare with Quincy, we run Firmament with Quincy\u2019s scheduling policy and restrict the solver to use only cost scaling (as Quincy\u2019s cs2 solver does).\n7.2 Scalability vs. Quincy\nIn Figure 3, we illustrated that Quincy fails to scale to clusters of thousands of machines at an acceptable placement latency. We now repeat the same experiment using Firmament on the full-scale simulated Google clus-\nter. However, we increase the cluster slot utilization from the earlier experiment\u2019s 50% to 90% to make the setup more challenging for Firmament, and also tune the cost scaling-based MCMF solver for its best performance.3\nFigure 14 shows the results as a CDF of task placement latency, i.e., the time between a task being submitted to the cluster manager and the time when it has been placed (\u00a72). While Quincy takes between 25 and 60 seconds to place tasks, Firmament typically places tasks in hundreds of milliseconds and only exceeds a sub-second placement latency in the 90th percentile. Therefore, Firmament improves task placement latency by more than a 20\u00d7 over Quincy, but maintains the same placement quality as it also finds an optimal flow.\nFirmament\u2019s low placement latency comes because relaxation scales well even for large flow networks with the Google trace workload. This scalability allows us to afford scheduling policies with many arcs. As an illustrative example, we vary the data locality threshold in the Quincy scheduling policy. This threshold decides what fraction of a task\u2019s input data must reside on a machine or within a rack in order for the former to receive a preference arc to the latter. Quincy originally picked a threshold of a maximum of ten arcs per task. However, Figure 15a shows that even a lower threshold of 14% local data, which corresponds to at most seven preference arcs, yields algorithm runtimes of 20\u201340 seconds for Quincy\u2019s cost scaling. A low threshold allows the scheduler to exploit more fine-grained locality, but increases the number of arcs in the graph. Consequently, if we lower the threshold to 2% local data,4 the cost scaling runtime in-\n3Specifically, we found that an \u03b1-factor parameter value of 9, rather than the default of 2 used in Quincy, improves runtime by \u224830%.\n42% is a somewhat extreme value used for exposition here. The benefit of such a low threshold in a real cluster would likely be limited.\nUSENIX Association 12th USENIX Symposium on Operating Systems Design and Implementation 109\ncreases to well over 40 seconds. Firmament, on the other hand, still achieves sub-second algorithm even with a 2% threshold. This threshold yields an increase in data locality from 56% to 71% of total input data (Table 15b), which saves 4 TB of network traffic per simulated hour."
        },
        {
            "heading": "7.3 Coping with demanding situations",
            "text": "In the previous experiments, Firmament had a lower placement latency than Quincy because relaxation handles the Google workload well. As explained in \u00a74, there are situations in which this is not the case. In those situations, Firmament picks incremental cost scaling\u2019s solution as it finishes first (\u00a76). We now demonstrate the benefits of running two algorithms rather than just one.\nIn this experiment, we shrink the number of slots per cluster machine to reach 97% average utilization. Consequently, the cluster experiences transient periods of oversubscription. Figure 16 compares Firmament\u2019s automatic use of the fastest algorithm against using only one algorithm, either relaxation or cost scaling. During oversubscription, relaxation alone takes hundreds of seconds per run, while cost scaling alone completes in \u224830 seconds independent of cluster load. Firmament\u2019s incremental cost scaling finishes first in this situation, taking 10\u201315 seconds, which is about 2\u00d7 faster than using cost scaling only (as Quincy does). Firmament also recovers earlier from the overload situation starting at 2,200s: while the relaxation-only runtime returns to sub-second level only around 3,700s, Firmament recovers at 3,200s. Relaxation on its own takes longer to recover because\nmany tasks complete and free up slots during the long solver runs. These slots cannot be re-used until the next solver run completes, even though new, waiting tasks accumulate. Hence, Firmament\u2019s combination of algorithms outperforms either algorithm running alone."
        },
        {
            "heading": "7.4 Scalability to sub-second tasks",
            "text": "In the absence of oversubscription, we now investigate the scalability limit of Firmament\u2019s sub-second relaxation-based MCMF. To find Firmament\u2019s breaking point, we subject it to a worst-case workload consisting entirely of short tasks. This experiment is similar to Sparrow\u2019s breaking-point experiment for the centralized Spark scheduler [28, Fig. 12]. We submit jobs of 10 tasks at an interarrival time that keeps the cluster at a constant load of 80% if there is no scheduler overhead. We measure job response time, which is the maximum of the ten task response times for a job. In Figure 17, we plot job response time as a function of decreasing task duration. As\n110 12th USENIX Symposium on Operating Systems Design and Implementation USENIX Association\nwe reduce task duration, we also reduce task interarrival time to keep the load constant, hence increasing the task throughput faced by the scheduler. With an ideal scheduler, job response time would be equal to task runtime as the scheduler would take no time to choose placements. Hence, the breaking point occurs when job response time deviates from the diagonal. For example, Spark\u2019s centralized task scheduler in 2013 had its breaking point on 100 machines at a 1.35 second task duration [28, \u00a77.6].\nBy contrast, even though Firmament runs MCMF over the entire workload every time, Figure 17 shows that it achieves near-ideal job response time down to task durations as low as 5ms (100 machines) or 375ms (1,000 machines). This makes Firmament\u2019s response time competitive with distributed schedulers on medium-sized clusters that only run short tasks. At 10,000 machines, Firmament keeps up with task durations \u22655s. However, such large clusters usually run a mix of long-running and short tasks, rather than short tasks only [7; 10; 23; 35].\nWe therefore investigate Firmament\u2019s performance on a mixed workload. We speed up the Google trace by dividing all task runtimes and interarrival times by a speedup factor. This simulates a future workload of shorter batch tasks [27], while service jobs are still longrunning. For example, at a 200\u00d7 speedup, the median batch task takes 2.1 seconds, and the 90th and 99th percentile batch tasks take 18 and 92 seconds. We measure Firmament\u2019s placement latency across all tasks, and plot the distributions in Figure 18. Even at a speedup of 300\u00d7, Firmament keeps up and places 75% of the tasks at with sub-second latency. As before, a single MCMF algorithm does not scale: cost scaling\u2019s placement latency already exceeds 10s even without any speedup, and relaxation sees tail latencies well above 10 seconds beyond a 150\u00d7 speedup, while Firmament scales further."
        },
        {
            "heading": "7.5 Placement quality on a local cluster",
            "text": "We deployed Firmament on a local 40-machine cluster to evaluate its real-world performance. We run a workload of short batch analytics tasks that take 3.5\u20135 seconds to complete on an otherwise idle cluster. Each task reads inputs of 4\u20138 GB from a cluster-wide HDFS installation in this experiment, and Firmament uses the network-aware scheduling policy. This policy reflects current network bandwidth reservations and observed actual bandwidth use in the flow network, and strives to place tasks on machines with lightly-loaded network connections. In Figure 19a, we show CDFs of task response times obtained using different cluster managers\u2019 schedulers. We measure task response time, and compare to a baseline that runs each task in isolation on an otherwise idle network. Firmament\u2019s task response time comes closest to the baseline above the 80th percentile as it successfully avoids overcommitting machines\u2019 network bandwidth. Other schedulers make random assignments (Sparrow), perform simple load-spreading (SwarmKit), or do not consider network bandwidth (Mesos, Kubernetes). Since our cluster is small, Firmament\u2019s task placement latency is inconsequential at around 5ms in this experiment.\nReal-world clusters, however, run a mix of short, interactive tasks and long-running service and batch processing tasks. We therefore extend our workload with new long-running batch and service jobs to represent a similar mix. The long-running batch workloads are generated by fourteen iperf clients who communicate using UDP with seven iperf servers. Each iperf client generates 4 Gbps of sustained network traffic and simulates a batch job in a higher-priority network service class [20] than the short batch tasks (e.g., a TensorFlow [1] parameter server). Finally, we deploy three nginx web servers and seven HTTP clients as long-running service jobs. We run the cluster at about 80% network utilization, and again measure the task response time for the short batch analytics tasks. Figure 19b shows that Firmament\u2019s networkaware scheduling policy substantially improves the tail of the task response time distribution of short batch tasks. For example, Firmament\u2019s 99th percentile response time is 3.4\u00d7 better than the SwarmKit and Kubernetes ones, and 6.2\u00d7 better than Sparrow\u2019s. The tail matters, since the last task\u2019s response time often determines a batch job\u2019s overall response time (the \u201cstraggler\u201d problem)."
        },
        {
            "heading": "8 Related work",
            "text": "Many cluster schedulers exist, but Firmament is the first centralized one to offer high placement quality at subsecond placement latency on large clusters. We now briefly compare Firmament to existing schedulers.\nOptimization-based schedulers. Firmament retains the same optimality as Quincy [22], but achieves much\nUSENIX Association 12th USENIX Symposium on Operating Systems Design and Implementation 111\nlower placement latency. TetriSched [33] uses a mixed integer-linear programming (MILP) optimization and applies techniques similar to Firmament\u2019s (e.g., incremental restart from a prior solution) to reduce placement latency. Its placement quality degrades gracefully when terminated early (as required at scale), while Firmament always returns optimal solutions. Paragon [11], Quasar [12], and Bistro [16] also run expensive scoring computations (collaborative filtering, path selection), but scale the task placement by using greedy algorithms.\nCentralized schedulers. Mesos [21] and Borg [35] match tasks to resources greedily; Borg\u2019s scoring uses random sampling with early termination [35, \u00a73.4], which improves latency at the expense of placement quality. Omega [32] and Apollo [7] support multiple parallel schedulers to simplify their engineering and to improve scalability. Firmament shows that a single scheduler can attain scalability, but its MCMF optimization does not trivially admit multiple independent schedulers.\nDistributed schedulers. Sparrow [28] and Tarcil [13] are distributed schedulers developed for clusters that see a high throughput of very short, sub-second tasks. In \u00a77.4, we demonstrated that Firmament offers similarly low placement latency as Sparrow on clusters up to 1,000 machines, and beyond if only a part of the workload consists of short tasks. Mercury [23] is a hybrid scheduler that makes centralized, high-quality assignments for long tasks, and distributedly places short ones. With Firmament, we have shown that a centralized scheduler can scale even to short tasks, and that they benefit from the improved placement quality. Hawk [10] and Eagle [9] extend the hybrid approach with work-stealing and state gossiping techniques that improve placement\nquality; Yaq-d [29], by contrast, reorders tasks in workerside queues to a similar end. Firmament shows that even a centralized scheduler can quickly schedule short tasks in large clusters with mixed workloads, rendering such complex compensation mechanisms largely unnecessary."
        },
        {
            "heading": "9 Conclusions",
            "text": "Firmament demonstrates that centralized cluster schedulers can scale to large clusters at low placement latencies. It chooses the same high-quality placements as an advanced centralized scheduler, at the speed and scale typically associated with distributed schedulers.\nFirmament, our simulator, and our data sets are opensource and available from http://firmament.io. A Firmament scheduler plugin for Kubernetes [14] is currently under development."
        },
        {
            "heading": "Acknowledgements",
            "text": "We are grateful to M. Frans Kaashoek, Frank McSherry, Derek G. Murray, Rebecca Isaacs, Andrew Warfield, Robert Morris, and Pamela Delgado, as well as Jon Gjengset, Srivatsa Bhat, and the rest of MIT PDOS group for comments on drafts of this paper. We also thank Phil Gibbons, our shepherd, and the OSDI 2016 reviewers for their feedback. Their input much improved this paper.\nThis work was supported by a Google European Doctoral Fellowship, by NSF award CNS-1413920, and by the Defense Advanced Research Projects Agency (DARPA) and Air Force Research Laboratory (AFRL), under contract FA8750-11-C-0249. The views, opinions, and/or findings contained in this paper are those of the authors and should not be interpreted as representing the official views or policies, either expressed or implied, of DARPA or the Department of Defense.\n112 12th USENIX Symposium on Operating Systems Design and Implementation USENIX Association"
        }
    ],
    "title": "Firmament: Fast, Centralized Cluster Scheduling at Scale",
    "year": 2016
}