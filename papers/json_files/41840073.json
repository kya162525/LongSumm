{
    "abstractText": "Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, they have not been well visualized or understood. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models. In this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to object concepts using a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. We examine the contextual relationship between these units and their surroundings by inserting the discovered object concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in a scene. We provide open source interpretation tools to help researchers and practitioners better understand their GAN models\u2217.",
    "authors": [
        {
            "affiliations": [],
            "name": "GAN DISSECTION"
        },
        {
            "affiliations": [],
            "name": "ADVERSARIAL NETWORKS"
        },
        {
            "affiliations": [],
            "name": "David Bau"
        },
        {
            "affiliations": [],
            "name": "Jun-Yan Zhu"
        },
        {
            "affiliations": [],
            "name": "Hendrik Strobelt"
        },
        {
            "affiliations": [],
            "name": "Bolei Zhou"
        },
        {
            "affiliations": [],
            "name": "Joshua B. Tenenbaum"
        },
        {
            "affiliations": [],
            "name": "William T. Freeman"
        },
        {
            "affiliations": [],
            "name": "Antonio Torralba"
        }
    ],
    "id": "SP:2616837171cf349da37a9dcbc444aece48037d20",
    "references": [
        {
            "authors": [
                "Sebastian Bach",
                "Alexander Binder",
                "Gr\u00e9goire Montavon",
                "Frederick Klauschen",
                "Klaus-Robert M\u00fcller",
                "Wojciech Samek"
            ],
            "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation",
            "venue": "PloS one,",
            "year": 2015
        },
        {
            "authors": [
                "David Bau",
                "Bolei Zhou",
                "Aditya Khosla",
                "Aude Oliva",
                "Antonio Torralba"
            ],
            "title": "Network dissection: Quantifying interpretability of deep visual representations",
            "venue": "In CVPR, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Ali Borji"
            ],
            "title": "Pros and cons of gan evaluation measures",
            "venue": "arXiv preprint arXiv:1802.03446,",
            "year": 2018
        },
        {
            "authors": [
                "Emily L Denton",
                "Soumith Chintala",
                "Rob Fergus"
            ],
            "title": "Deep generative image models using a laplacian pyramid of adversarial networks",
            "venue": "In NIPS,",
            "year": 2015
        },
        {
            "authors": [
                "Laurent Dinh",
                "Jascha Sohl-Dickstein",
                "Samy Bengio"
            ],
            "title": "Density estimation using real nvp",
            "venue": "In ICLR,",
            "year": 2017
        },
        {
            "authors": [
                "Jeff Donahue",
                "Philipp Kr\u00e4henb\u00fchl",
                "Trevor Darrell"
            ],
            "title": "Adversarial feature learning",
            "venue": "In ICLR,",
            "year": 2017
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Thomas Brox"
            ],
            "title": "Generating images with perceptual similarity metrics based on deep networks",
            "venue": "In NIPS,",
            "year": 2016
        },
        {
            "authors": [
                "Vincent Dumoulin",
                "Ishmael Belghazi",
                "Ben Poole",
                "Alex Lamb",
                "Martin Arjovsky",
                "Olivier Mastropietro",
                "Aaron Courville"
            ],
            "title": "Adversarially learned inference",
            "venue": "In ICLR,",
            "year": 2017
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "In NIPS,",
            "year": 2014
        },
        {
            "authors": [
                "Ishaan Gulrajani",
                "Faruk Ahmed",
                "Martin Arjovsky",
                "Vincent Dumoulin",
                "Aaron C Courville"
            ],
            "title": "Improved training of wasserstein gans",
            "venue": "In NIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Martin Heusel",
                "Hubert Ramsauer",
                "Thomas Unterthiner",
                "Bernhard Nessler",
                "Sepp Hochreiter"
            ],
            "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "venue": "In NIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Judy Hoffman",
                "Eric Tzeng",
                "Taesung Park",
                "Jun-Yan Zhu",
                "Phillip Isola",
                "Kate Saenko",
                "Alexei A Efros",
                "Trevor Darrell"
            ],
            "title": "Cycada: Cycle-consistent adversarial domain adaptation",
            "venue": "In ICML,",
            "year": 2018
        },
        {
            "authors": [
                "Paul W Holland"
            ],
            "title": "Causal inference, path analysis and recursive structural equations models",
            "venue": "ETS Research Report Series,",
            "year": 1988
        },
        {
            "authors": [
                "Phillip Isola",
                "Jun-Yan Zhu",
                "Tinghui Zhou",
                "Alexei A Efros"
            ],
            "title": "Image-to-image translation with conditional adversarial networks",
            "venue": "In CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "Andrej Karpathy",
                "Justin Johnson",
                "Li Fei-Fei"
            ],
            "title": "Visualizing and understanding recurrent networks",
            "venue": "In ICLR,",
            "year": 2016
        },
        {
            "authors": [
                "Tero Karras",
                "Timo Aila",
                "Samuli Laine",
                "Jaakko Lehtinen"
            ],
            "title": "Progressive growing of gans for improved quality, stability, and variation",
            "venue": "In ICLR,",
            "year": 2018
        },
        {
            "authors": [
                "Been Kim",
                "Justin Gilmer",
                "Fernanda Viegas",
                "Ulfar Erlingsson",
                "Martin Wattenberg"
            ],
            "title": "Tcav: Relative concept importance testing with linear concept activation vectors",
            "venue": "arXiv preprint arXiv:1711.11279,",
            "year": 2017
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "ICLR,",
            "year": 2014
        },
        {
            "authors": [
                "Aravindh Mahendran",
                "Andrea Vedaldi"
            ],
            "title": "Understanding deep image representations by inverting them",
            "venue": "In CVPR,",
            "year": 2015
        },
        {
            "authors": [
                "Michael Mathieu",
                "Camille Couprie",
                "Yann LeCun"
            ],
            "title": "Deep multi-scale video prediction beyond mean square error",
            "venue": "In ICLR,",
            "year": 2016
        },
        {
            "authors": [
                "Takeru Miyato",
                "Toshiki Kataoka",
                "Masanori Koyama",
                "Yuichi Yoshida"
            ],
            "title": "Spectral normalization for generative adversarial networks",
            "venue": "In ICLR,",
            "year": 2018
        },
        {
            "authors": [
                "Ari S Morcos",
                "David GT Barrett",
                "Neil C Rabinowitz",
                "Matthew Botvinick"
            ],
            "title": "On the importance of single directions for generalization",
            "venue": "arXiv preprint arXiv:1803.06959,",
            "year": 2018
        },
        {
            "authors": [
                "Chris Olah",
                "Arvind Satyanarayan",
                "Ian Johnson",
                "Shan Carter",
                "Ludwig Schubert",
                "Katherine Ye",
                "Alexander Mordvintsev"
            ],
            "title": "The building blocks of interpretability",
            "venue": "Distill, 3(3):e10,",
            "year": 2018
        },
        {
            "authors": [
                "Alec Radford",
                "Luke Metz",
                "Soumith Chintala"
            ],
            "title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
            "venue": "In ICLR,",
            "year": 2016
        },
        {
            "authors": [
                "Ramprasaath R Selvaraju",
                "Michael Cogswell",
                "Abhishek Das",
                "Ramakrishna Vedantam",
                "Devi Parikh",
                "Dhruv Batra"
            ],
            "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
            "venue": "In ICCV,",
            "year": 2017
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrea Vedaldi",
                "Andrew Zisserman"
            ],
            "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
            "venue": "In ICLR Workshop,",
            "year": 2014
        },
        {
            "authors": [
                "Hendrik Strobelt",
                "Sebastian Gehrmann",
                "Hanspeter Pfister",
                "Alexander M. Rush"
            ],
            "title": "LSTMVis: A tool for visual analysis of hidden state dynamics in recurrent neural networks",
            "venue": "IEEE TVCG,",
            "year": 2018
        },
        {
            "authors": [
                "Mukund Sundararajan",
                "Ankur Taly",
                "Qiqi Yan"
            ],
            "title": "Axiomatic attribution for deep networks",
            "venue": "In PMLR,",
            "year": 2017
        },
        {
            "authors": [
                "Ting-Chun Wang",
                "Ming-Yu Liu",
                "Jun-Yan Zhu",
                "Guilin Liu",
                "Andrew Tao",
                "Jan Kautz",
                "Bryan Catanzaro"
            ],
            "title": "Video-to-video synthesis",
            "venue": "In NIPS,",
            "year": 2018
        },
        {
            "authors": [
                "Xiaolong Wang",
                "Abhinav Shrivastava",
                "Abhinav Gupta"
            ],
            "title": "A-fast-rcnn: Hard positive generation via adversary for object detection",
            "venue": "In CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "Dedy Rahman Wijaya",
                "Riyanarto Sarno",
                "Enny Zulaika"
            ],
            "title": "Information quality ratio as a novel metric for mother wavelet selection",
            "venue": "Chemometrics and Intelligent Laboratory Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Tete Xiao",
                "Yingcheng Liu",
                "Bolei Zhou",
                "Yuning Jiang",
                "Jian Sun"
            ],
            "title": "Unified perceptual parsing for scene understanding",
            "venue": "In ECCV,",
            "year": 2018
        },
        {
            "authors": [
                "Fisher Yu",
                "Ari Seff",
                "Yinda Zhang",
                "Shuran Song",
                "Thomas Funkhouser",
                "Jianxiong Xiao"
            ],
            "title": "Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop",
            "venue": "arXiv preprint arXiv:1506.03365,",
            "year": 2015
        },
        {
            "authors": [
                "Matthew D Zeiler",
                "Rob Fergus"
            ],
            "title": "Visualizing and understanding convolutional networks",
            "venue": "In ECCV,",
            "year": 2014
        },
        {
            "authors": [
                "Han Zhang",
                "Ian Goodfellow",
                "Dimitris Metaxas",
                "Augustus Odena"
            ],
            "title": "Self-attention generative adversarial networks",
            "venue": "arXiv preprint arXiv:1805.08318,",
            "year": 2018
        },
        {
            "authors": [
                "Bolei Zhou",
                "Aditya Khosla",
                "Agata Lapedriza",
                "Aude Oliva",
                "Antonio Torralba"
            ],
            "title": "Object detectors emerge in deep scene cnns",
            "venue": "In ICLR,",
            "year": 2015
        },
        {
            "authors": [
                "Bolei Zhou",
                "Hang Zhao",
                "Xavier Puig",
                "Sanja Fidler",
                "Adela Barriuso",
                "Antonio Torralba"
            ],
            "title": "Scene parsing through ade20k dataset",
            "venue": "In CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "Bolei Zhou",
                "David Bau",
                "Aude Oliva",
                "Antonio Torralba"
            ],
            "title": "Interpreting deep visual representations via network dissection",
            "venue": "PAMI, 2018a",
            "year": 2018
        },
        {
            "authors": [
                "Bolei Zhou",
                "Yiyou Sun",
                "David Bau",
                "Antonio Torralba"
            ],
            "title": "Interpretable basis decomposition for visual explanation",
            "venue": "In ECCV, pp",
            "year": 2018
        },
        {
            "authors": [
                "Tinghui Zhou",
                "Philipp Krahenbuhl",
                "Mathieu Aubry",
                "Qixing Huang",
                "Alexei A Efros"
            ],
            "title": "Learning dense correspondence via 3d-guided cycle consistency",
            "venue": "In CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "Jun-Yan Zhu",
                "Philipp Kr\u00e4henb\u00fchl",
                "Eli Shechtman",
                "Alexei A. Efros"
            ],
            "title": "Generative visual manipulation on the natural image manifold",
            "venue": "In ECCV,",
            "year": 2016
        },
        {
            "authors": [
                "Jun-Yan Zhu",
                "Taesung Park",
                "Phillip Isola",
                "Alexei A Efros"
            ],
            "title": "Unpaired image-to-image translation using cycle-consistent adversarial networks",
            "venue": "In ICCV,",
            "year": 2017
        },
        {
            "authors": [
                "Karras"
            ],
            "title": "Our method relies on having a segmentation function sc(x) that identifies pixels of class c in the output x. However, the segmentation model sc can perform poorly in the cases where x does not resemble the original training set of sc. This phenomenon is visible when analyzing earlier GAN models. For example, Figure 15 visualizes two units from a WGAN-GP model (Gulrajani et al., 2017) for LSUN bedrooms",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) have been able to produce photorealistic images, often indistinguishable from real images. This remarkable ability has powered many real-world applications ranging from visual recognition (Wang et al., 2017), to image manipulation (Isola et al., 2017; Zhu et al., 2017), to video prediction (Mathieu et al., 2016). Since its invention in 2014, many GAN variants have been proposed (Radford et al., 2016; Zhang et al., 2018), often producing more realistic and diverse samples with better training stability.\nDespite this tremendous success, many questions remain to be answered. For example, to produce a church image (Figure 1a), what knowledge does a GAN need to learn? Alternatively, when a GAN sometimes produces terribly unrealistic images (Figure 1f), what causes the mistakes? Why does one GAN variant work better than another? What fundamental differences are encoded in their weights?\nIn this work, we study the internal representations of GANs. To a human observer, a well-trained GAN appears to have learned facts about the objects in the image: for example, a door can appear on a building but not on a tree. We wish to understand how a GAN represents such a structure. Do the objects emerge as pure pixel patterns without any explicit representation of objects such as doors and trees, or does the GAN contain internal variables that correspond to the objects that humans perceive? If the GAN does contain variables for doors and trees, do those variables cause the generation of those objects, or do they merely correlate? How are relationships between objects represented?\n\u2217Interactive demos, video, code, and data are available at GitHub and gandissect.csail.mit.edu.\nar X\niv :1\n81 1.\n10 59\n7v 2\n[ cs\n.C V\n] 8\nD ec\n2 01\nWe present a general method for visualizing and understanding GANs at different levels of abstraction, from each neuron, to each object, to the contextual relationship between different objects. We first identify a group of interpretable units that are related to object concepts (Figure 1b). These units\u2019 featuremaps closely match the semantic segmentation of a particular object class (e.g., trees). Second, we directly intervene within the network to identify sets of units that cause a type of objects to disappear (Figure 1c) or appear (Figure 1d). We quantify the causal effect of these units using a standard causality metric. Finally, we examine the contextual relationship between these causal object units and the background. We study where we can insert the object concepts in new images and how this intervention interacts with other objects in the image (Figure 1d). To our knowledge, our work provides the first systematic analysis for understanding the internal representations of GANs.\nFinally, we show several practical applications enabled by this analytic framework, from comparing internal representations across different layers, GAN variants and datasets; to debugging and improving GANs by locating and ablating \u201cartifact\u201d units (Figure 1e); to understanding contextual relationships between objects in scenes; to manipulating images with interactive object-level control."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Generative Adversarial Networks. The quality and diversity of results from GANs (Goodfellow et al., 2014) has continued to improve, from generating simple digits and faces (Goodfellow et al., 2014), to synthesizing natural scene images (Radford et al., 2016; Denton et al., 2015), to generating 1k photorealistic portraits (Karras et al., 2018), to producing one thousand object classes (Miyato et al., 2018; Zhang et al., 2018). In addition to image generation, GANs have also enabled many applications such as visual recognition (Wang et al., 2017; Hoffman et al., 2018), image manipulation (Isola\net al., 2017; Zhu et al., 2017), and video generation (Mathieu et al., 2016; Wang et al., 2018). Despite the huge success, little work has been done to visualize what GANs have learned. Prior work (Radford et al., 2016; Zhu et al., 2016) manipulates latent vectors and observes how the results change accordingly.\nVisualizing deep neural networks. Various methods have been developed to understand the internal representations of networks, such as visualizations for CNNs (Zeiler & Fergus, 2014) and RNNs (Karpathy et al., 2016; Strobelt et al., 2018). We can visualize a CNN by locating and reconstructing salient image features (Simonyan et al., 2014; Mahendran & Vedaldi, 2015) or by mining patches that maximize hidden layers\u2019 activations (Zeiler & Fergus, 2014), or we can synthesize input images to invert a feature layer (Dosovitskiy & Brox, 2016). Alternately, we can identify the semantics of each unit (Zhou et al., 2015; Bau et al., 2017; Zhou et al., 2018a) by measuring agreement between unit activations and object segmentation masks. Visualization of an RNN has also revealed interpretable units that track long-range dependencies (Karpathy et al., 2016). Most previous work on network visualization has focused on networks trained for classification; our work explores deep generative models trained for image generation.\nExplaining the decisions of deep neural networks. We can explain individual network decisions using informative heatmaps (Zhou et al., 2018b; 2016; Selvaraju et al., 2017) or modified backpropagation (Simonyan et al., 2014; Bach et al., 2015; Sundararajan et al., 2017). The heatmaps highlight which regions contribute most to the categorical prediction given by the networks. Recent work has also studied the contribution of feature vectors (Kim et al., 2017; Zhou et al., 2018b) or individual channels (Olah et al., 2018) to the final prediction. Morcos et al. (2018) has examined the effect of individual units by ablating them. Those methods explain discriminative classifiers. Our method aims to explain how an image can be generated by a network, which is much less explored."
        },
        {
            "heading": "3 METHOD",
            "text": "Our goal is to analyze how objects such as trees are encoded by the internal representations of a GAN generator G : z\u2192 x. Here z \u2208 R|z| denotes a latent vector sampled from a low-dimensional\ndistribution, and x \u2208 RH\u00d7W\u00d73 denotes an H \u00d7W generated image. We use representation to describe the tensor r output from a particular layer of the generator G, where the generator creates an image x from random z through a composition of layers: r = h(z) and x = f(r) = f(h(z)) = G(z).\nSince r has all the data necessary to produce the image x = f(r), r certainly contains the information to deduce the presence of any visible class c in the image. Therefore the question we ask is not whether information about c is present in r \u2014 it is \u2014 but how such information is encoded in r. In particular, for any class from a universe of concepts c \u2208 C, we seek to understand whether r explicitly represents c in some way where it is possible to factor r at locations P into two components\nrU,P = (rU,P, rU,P), (1)\nwhere the generation of the object c at locations P depends mainly on the units rU,P, and is insensitive to the other units rU,P. Here we refer to each channel of the featuremap as a unit: U denotes the set of unit indices of interest and U is its complement; we will write U and P to refer to the entire set of units and featuremap pixels in r. We study the structure of r in two phases:\n\u2022 Dissection: starting with a large dictionary of object classes, we identify the classes that have an explicit representation in r by measuring the agreement between individual units of r and every class c (Figure 1b). \u2022 Intervention: for the represented classes identified through dissection, we identify causal\nsets of units and measure causal effects between units and object classes by forcing sets of units on and off (Figure 1c,d)."
        },
        {
            "heading": "3.1 CHARACTERIZING UNITS BY DISSECTION",
            "text": "We first focus on individual units of the representation. Recall that ru,P is the one-channel h \u00d7 w featuremap of unit u in a convolutional generator, where h\u00d7 w is typically smaller than the image size. We want to know if a specific unit ru,P encodes a semantic class such as a \u201ctree\u201d. For image classification networks, Bau et al. (2017) has observed that many units can approximately locate emergent object classes when the units are upsampled and thresholded. In that spirit, we select a universe of concepts c \u2208 C for which we have a semantic segmentation sc(x) for each class. Then we quantify the spatial agreement between the unit u\u2019s thresholded featuremap and a concept c\u2019s segmentation with the following intersection-over-union (IoU) measure:\nIoUu,c \u2261 Ez \u2223\u2223\u2223(r\u2191u,P > tu,c) \u2227 sc(x)\u2223\u2223\u2223 Ez \u2223\u2223\u2223(r\u2191u,P > tu,c) \u2228 sc(x)\u2223\u2223\u2223 ,where tu,c = argmaxt I(r\u2191u,P > t; sc(x)) H(r\u2191u,P > t, sc(x)) , (2)\nwhere \u2227 and \u2228 denote intersection and union operations, and x = G(z) denotes the image generated from z. The one-channel feature map ru,P slices the entire featuremap r = h(z) at unit u. As shown in Figure 2a, we upsample ru,P to the output image resolution as r \u2191 u,P. (r \u2191 u,P > tu,c) produces a binary mask by thresholding the r\u2191u,P at a fixed level tu,c. sc(x) is a binary mask where each pixel indicates the presence of class c in the generated image x. The threshold tu,c is chosen to be informative as possible by maximizing the information quality ratio I/H (using a separate validation set), that is, it maximizes the portion of the joint entropy H which is mutual information I (Wijaya et al., 2017).\nWe can use IoUu,c to rank the concepts related to each unit and label each unit with the concept that matches it best. Figure 3 shows examples of interpretable units with high IoUu,c. They are not the\nonly units to match tables and sofas: layer3 of the dining room generator has 31 units (of 512) that match tables and table parts, and layer4 of the living room generator has 65 (of 512) sofa units.\nOnce we have identified an object class that a set of units match closely, we next ask: which units are responsible for triggering the rendering of that object? A unit that correlates highly with an output object might not actually cause that output. Furthermore, any output will jointly depend on several parts of the representation. We need a way to identify combinations of units that cause an object."
        },
        {
            "heading": "3.2 MEASURING CAUSAL RELATIONSHIPS USING INTERVENTION",
            "text": "To answer the above question about causality, we probe the network using interventions: we test whether a set of units U in r cause the generation of c by forcing the units of U on and off.\nRecall that rU,P denotes the featuremap r at units U and locations P. We ablate those units by forcing rU,P = 0. Similarly, we insert those units by forcing rU,P = k, where k is a per-class constant, as described in Section S-6.4. We decompose the featuremap r into two parts (rU,P, rU,P), where rU,P are unforced components of r:\nOriginal image : x = G(z) \u2261 f(r) \u2261 f(rU,P, rU,P) (3) Image with U ablated at pixels P : xa = f(0, rU,P)\nImage with U inserted at pixels P : xi = f(k, rU,P)\nAn object is caused by U if the object appears in xi and disappears from xa. Figure 1c demonstrates the ablation of units that remove trees, and Figure 1d demonstrates insertion of units at specific locations to make trees appear. This causality can be quantified by comparing the presence of trees in xi and xa and averaging effects over all locations and images. Following prior work (Holland, 1988; Pearl, 2009), we define the average causal effect (ACE) of units U on the generation of on class c as:\n\u03b4U\u2192c \u2261 Ez,P[sc(xi)]\u2212 Ez,P[sc(xa)], (4)\nwhere sc(x) denotes a segmentation indicating the presence of class c in the image x at P. To permit comparisons of \u03b4U\u2192c between classes c which are rare, we normalize our segmentation sc by Ez,P[sc(x)]. While these measures can be applied to a single unit, we have found that objects tend to depend on more than one unit. Thus we need to identify a set of units U that maximize the average causal effect \u03b4U\u2192c for an object class c.\nFinding sets of units with high ACE. Given a representation r with d units, exhaustively searching for a fixed-size set U with high \u03b4U\u2192c is prohibitive as it has ( d |U| )\nsubsets. Instead, we optimize a continuous intervention \u03b1 \u2208 [0, 1]d, where each dimension \u03b1u indicates the degree of intervention for a unit u. We maximize the following average causal effect formulation \u03b4\u03b1\u2192c:\nImage with partial ablation at pixels P : x\u2032a = f((1\u2212\u03b1) rU,P, rU,P) (5) Image with partial insertion at pixels P : x\u2032i = f(\u03b1 k+ (1\u2212\u03b1) rU,P, rU,P) Objective : \u03b4\u03b1\u2192c = Ez,P [sc(x\u2032i)]\u2212 Ez,P [sc(x\u2032a)] ,\nwhere rU,P denotes the all-channel featuremap at locations P, rU,P denotes the all-channel featuremap at other locations P, and applies a per-channel scaling vector \u03b1 to the featuremap rU,P. We optimize \u03b1 over the following loss with an L2 regularization:\n\u03b1\u2217 = argmin \u03b1 (\u2212\u03b4\u03b1\u2192c + \u03bb||\u03b1||2), (6)\nwhere \u03bb controls the relative importance of each term. We add the L2 loss as we seek a minimal set of casual units. We optimize using stochastic gradient descent, sampling over both z and featuremap locations P and clamping the coefficient \u03b1 within the range [0, 1]d at each step (d is the total number of units). More details of this optimization are discussed in Section S-6.4. Finally, we can rank units by \u03b1\u2217u and achieve a stronger causal effect (i.e., removing trees) when ablating successively larger sets of tree-causing units as shown in Figure 4."
        },
        {
            "heading": "4 RESULTS",
            "text": "We study three variants of Progressive GANs (Karras et al., 2018) trained on LSUN scene datasets (Yu et al., 2015). To segment the generated images, we use a recent model (Xiao et al., 2018) trained on the ADE20K scene dataset (Zhou et al., 2017). The model can segment the input image into 336 object classes, 29 parts of large objects, and 25 materials. To further identify units that specialize in object parts, we expand each object class c into additional object part classes c-t, c-b, c-l, and c-r, which denote the top, bottom, left, or right half of the bounding box of a connected component.\nBelow, we use dissection for analyzing and comparing units across datasets, layers, and models (Section 4.1), and locating artifact units (Section 4.2). Then, we start with a set of dominant object classes and use intervention to locate causal units that can remove and insert objects in different images (Section 4.3 and 4.4). In addition, our video demonstrates our interactive tool."
        },
        {
            "heading": "4.1 COMPARING UNITS ACROSS DATASETS, LAYERS, AND MODELS",
            "text": "Emergence of individual unit object detectors We are particularly interested in any units that are correlated with instances of an object class with diverse visual appearances; these would suggest that GANs generate those objects using similar abstractions as humans. Figure 3 illustrates two such units. In the dining room dataset, a unit emerges to match dining table regions. More interestingly, the matched tables have different colors, materials, geometry, viewpoints, and levels of clutter: the only obvious commonality among these regions is the concept of a table. This unit\u2019s featuremap correlates to the fully supervised segmentation model (Xiao et al., 2018) with a high IoU of 0.34.\nInterpretable units for different scene categories The set of all object classes matched by the units of a GAN provides a map of what a GAN has learned about the data. Figure 5 examines units from GANs trained on four LSUN scene categories (Yu et al., 2015). The units that emerge are object classes appropriate to the scene type: for example, when we examine a GAN trained on kitchen scenes, we find units that match stoves, cabinets, and the legs of tall kitchen stools. Another striking phenomenon is that many units represent parts of objects: for example, the conference room GAN contains separate units for the body and head of a person.\nInterpretable units for different network layers. In classifier networks, the type of information explicitly represented changes from layer to layer (Zeiler & Fergus, 2014). We find a similar phenomenon in a GAN. Figure 6 compares early, middle, and late layers of a progressive GAN with 14 internal convolutional layers. The output of the first convolutional layer, one step away from the input z, remains entangled: individual units do not correlate well with any object classes except for two units that are biased towards the ceiling of the room. Mid-level layers 4 to 7 have many units that match semantic objects and object parts. Units in layers 10 and beyond match local pixel patterns such as materials, edges and colors. All layers are shown in Section S-6.7.\nInterpretable units for different GAN models. Interpretable units can provide insights about how GAN architecture choices affect the structures learned inside a GAN. Figure 7 compares three models from Karras et al. (2018): a baseline Progressive GANs, a modification that introduces minibatch stddev statistics, and a further modification that adds pixelwise normalization. By examining unit semantics, we confirm that providing minibatch stddev statistics to the discriminator increases not only the realism of results, but also the diversity of concepts represented by units: the number of\ntypes of objects, parts, and materials matching units increases by more than 40%. The pixelwise normalization increases the number of units that match semantic classes by 19%."
        },
        {
            "heading": "4.2 DIAGNOSING AND IMPROVING GANS",
            "text": "While our framework can reveal how GANs succeed in producing realistic images, it can also analyze the causes of failures in their results. Figure 8a shows several annotated units that are responsible for typical artifacts consistently appearing across different images. We can identify these units efficiently by human annotation: out of a sample of 1000 images, we visualize the top ten highest activating images for each unit, and we manually identify units with noticeable artifacts in this set. It typically takes 10 minutes to locate 20 artifact-causing units out of 512 units in layer4.\nMore importantly, we can fix these errors by ablating the above 20 artifact-causing units. Figure 8b shows that artifacts are successfully removed, and the artifact-free pixels stay the same, improving the generated results. In Table 1 we report two standard metrics, comparing our improved images\nto both the original artifact images and a simple baseline that ablates 20 randomly chosen units. First, we compute the widely used Fre\u0301chet Inception Distance (Heusel et al., 2017) between the generated images and real images. We use 50, 000 real images and generate 10, 000 images with high activations on these units. Second, we score 1, 000 images per method on Amazon MTurk, collecting 20, 000 human annotations regarding whether the modified image looks more realistic compared to the original. Both metrics show significant improvements. Strikingly, this simple manual change to a network beats state-of-the-art GANs models. The manual identification of \u201cartifact\u201d units can be approximated by an automatic scoring of the realism of each unit, as detailed in Section S-6.1."
        },
        {
            "heading": "4.3 LOCATING CAUSAL UNITS WITH ABLATION",
            "text": "Errors are not the only type of output that can be affected by directly intervening in a GAN. A variety of specific object types can also be removed from GAN output by ablating a set of units in a GAN. In Figure 9 we apply the method in Section 3.2 to identify sets of 20 units that have causal effects on common object classes in conference rooms scenes. We find that, by turning off these small sets of units, most of the output of people, curtains, and windows can be removed from the generated scenes. However, not every object can be erased: tables and chairs cannot be removed. Ablating those units will reduce the size and density of these objects, but will rarely eliminate them.\nThe ease of object removal depends on the scene type. Figure 10 shows that, while windows can be removed well from conference rooms, they are more difficult to remove from other scenes. In particular, windows are just as difficult to remove from a bedroom as tables and chairs from a conference room. We hypothesize that the difficulty of removal reflects the level of choice that a GAN has learned for a concept: a conference room is defined by the presence of chairs, so they\nTable 1: We compare generated images before and after ablating 20 \u201cartifacts\u201d units. We also report a simple baseline that ablates 20 randomly chosen units.\nFre\u0301chet Inception Distance (FID)\noriginal images 43.16 \u201cartifacts\u201d units ablated (ours) 27.14\nrandom units ablated 43.17\nHuman preference score original images\n\u201cartifacts\u201d units ablated (ours) 72.4% random units ablated 49.9%\nablate person units ablate curtain units\nablate table unitsablate window units ablate chair units\nFigure 9: Measuring the effect of ablating units in a GAN trained on conference room images. Five different sets of units have been ablated related to a specific object class. In each case, 20 (out of 512) units are ablated from the same GAN model. The 20 units are specific to the object class and independent of the image. The average causal effect is reported as the portion of pixels that are removed in 1 000 randomly generated images. We observe that some object classes are easier to remove cleanly than others: a small ablation can erase most pixels for people, curtains, and windows, whereas a similar ablation for tables and chairs only reduces object sizes without deleting them.\ncannot be altered. And modern building codes mandate that all bedrooms must have windows; the GAN seems to have caught on to that pattern."
        },
        {
            "heading": "4.4 CHARACTERIZING CONTEXTUAL RELATIONSHIPS VIA INSERTION",
            "text": "We can also learn about the operation of a GAN by forcing units on and inserting these features into specific locations in scenes. Figure 11 shows the effect of inserting 20 layer4 causal door units in church scenes. In this experiment, we insert these units by setting their activation to the fixed mean value for doors (further details in Section S-6.4). Although this intervention is the same in each case, the effects vary widely depending on the objects\u2019 surrounding context. For example, the doors added to the five buildings in Figure 11 appear with a diversity of visual attributes, each with an orientation, size, material, and style that matches the building.\nWe also observe that doors cannot be added in most locations. The locations where a door can be added are highlighted by a yellow box. The bar chart in Figure 11 shows average causal effects of insertions of door units, conditioned on the background object class at the location of the intervention. We find that the GAN allows doors to be added in buildings, particularly in plausible locations such as where a window is present, or where bricks are present. Conversely, it is not possible to trigger a door in the sky or on trees. Interventions provide insight on how a GAN enforces relationships between objects. Even if we try to add a door in layer4, that choice can be vetoed later if the object is not appropriate for the context. These downstream effects are further explored in Section S-6.5."
        },
        {
            "heading": "5 DISCUSSION",
            "text": "By carefully examining representation units, we have found that many parts of GAN representations can be interpreted, not only as signals that correlate with object concepts but as variables that have a causal effect on the synthesis of objects in the output. These interpretable effects can be used to compare, debug, modify, and reason about a GAN model. Our method can be potentially applied to other generative models such as VAEs (Kingma & Welling, 2014) and RealNVP (Dinh et al., 2017).\nWe have focused on the generator rather than the discriminator (as did in Radford et al. (2016)) because the generator must represent all the information necessary to approximate the target distribution, while\nthe discriminator only learns to capture the difference between real and fake images. Alternatively, we can train an encoder to invert the generator (Donahue et al., 2017; Dumoulin et al., 2017). However, this incurs additional complexity and errors. Many GANs also do not have an encoder.\nOur method is not designed to compare the quality of GANs to one another, and it is not intended as a replacement for well-studied GAN metrics such as FID, which estimate realism by measuring the distance between the generated distribution of images and the true distribution (Borji (2018) surveys these methods). Instead, our goal has been to identify the interpretable structure and provide a window into the internal mechanisms of a GAN.\nPrior visualization methods (Zeiler & Fergus, 2014; Bau et al., 2017; Karpathy et al., 2016) have brought new insights into CNN and RNNs research. Motivated by that, in this work we have taken a small step towards understanding the internal representations of a GAN, and we have uncovered many questions that we cannot yet answer with the current method. For example: why can a door not be inserted in the sky? How does the GAN suppress the signal in the later layers? Further work will be needed to understand the relationships between layers of a GAN. Nevertheless, we hope that our work can help researchers and practitioners better analyze and develop their own GANs.\nAcknowledgments We thank Zhoutong Zhang, Guha Balakrishnan, Didac Suris, Adria\u0300 Recasens, and Zhuang Liu for valuable discussions. We are grateful for the support of the MIT-IBM Watson AI Lab, the DARPA XAI program FA8750-18-C000, NSF 1524817 on Advancing Visual Recognition with Feature Visualizations, NSF BIGDATA 1447476, and a hardware donation from NVIDIA."
        },
        {
            "heading": "S-6 SUPPLEMENTARY MATERIAL",
            "text": ""
        },
        {
            "heading": "S-6.1 AUTOMATIC IDENTIFICATION OF ARTIFACT UNITS",
            "text": "In Section 4.2, we have improved GANs by manually identifying and ablating artifact-causing units. Now we describe an automatic procedure to identify artifact units using unit-specific FID scores.\nTo compute the FID score (Heusel et al., 2017) for a unit u, we generate 200, 000 images and select the 10, 000 images that maximize the activation of unit u, and this subset of 10, 000 images is compared to the true distribution (50, 000 real images) using FID. Although every such unit-maximizing subset of images represents a skewed distribution, we find that the per-unit FID scores fall in a wide range, with most units scoring well in FID while a few units stand out with bad FID scores: many of them were also manually flagged by humans, as they tend to activate on images with clear visible artifacts.\nFigure 12 shows the performance of FID scores as a predictor of manually flagged artifact units. The per-unit FID scores can achieve 50% precision and 50% recall. That is, of the 20 worst-FID units, 10 are also among the 20 units manually judged to have the most noticeable artifacts. Furthermore, repairing the model by ablating the highest-FID units works: qualitative results are shown in Figure 13 and quantitative results are shown in Table 2."
        },
        {
            "heading": "S-6.2 HUMAN EVALUATION OF DISSECTION",
            "text": "As a sanity check, we evaluate the gap between human labeling of object concepts correlated with units and our automatic segmentation-based labeling, for one model, as follows.\nFor each of 512 units of layer4 of a \u201cliving room\u201d Progressive GAN, 5 to 9 human annotations were collected (3728 labels in total). In each case, an AMT worker is asked to provide one or two words describing the highlighted patches in a set of top-activating images for a unit. Of the 512 units, 201 units were described by the same consistent word (such as \u201dsofa\u201d, \u201dfireplace\u201d or \u201dwicker\u201d) in 50% or more of the human labels. These units are interpretable to humans.\nApplying our segmentation-based dissection method, 154/201 of these units are also labeled with a confident label with IoU > 0.05 by dissection. In 104/154 cases, the segmentation-based model gave the same label word as the human annotators, and most others are slight shifts in specificity. For example, the segmentation labels \u201cottoman\u201d or \u201ccurtain\u201d or \u201cpainting\u201d when a person labels \u201csofa\u201d or \u201cwindow\u201d or \u201cpicture,\u201d respectively. A second AMT evaluation was done to rate the accuracy of both segmentation-derived and human-derived labels. Human-derived labels scored 100% (of the 201 human-labeled units, all of the labels were rated as consistent by most raters). Of the 154 segmentation-generated labels, 149 (96%) were rated by most AMT raters as accurate as well.\nThe five failure cases (where the segmentation is confident but rated as inaccurate by humans) arise from situations in which human evaluators saw one concept after observing only 20 top-activating images, while the algorithm, in evaluating 1000 images, counted a different concept as dominant. Figure 14a shows one example: in the top images, mostly sofas are highlighted and few ceilings, whereas in the larger sample, mostly ceilings are triggered.\nThere are also 47/201 cases where the segmenter is not confident while humans have consensus. Some of these are due to missing concepts in the segmenter. Figure 14b shows a typical example, where a unit is devoted to letterboxing (white stripes at the top and bottom of images), but the segmentation has no confident label to assign to these. We expect that as future semantic segmentation models are developed to be able to identify more concepts such as abstract shapes, more of these units can be automatically identified."
        },
        {
            "heading": "S-6.3 PROTECTING SEGMENTATION MODEL AGAINST UNREALISTIC IMAGES",
            "text": "Our method relies on having a segmentation function sc(x) that identifies pixels of class c in the output x. However, the segmentation model sc can perform poorly in the cases where x does not resemble the original training set of sc. This phenomenon is visible when analyzing earlier GAN models. For example, Figure 15 visualizes two units from a WGAN-GP model (Gulrajani et al., 2017) for LSUN bedrooms (this model was trained by Karras et al. (2018) as a baseline in the original paper). For these two units, the segmentation network seems to be confused by the distorted images.\nTo protect against such spurious segmentation labels, we can use a technique similar to that described in Section S-6.1: automatically identify units that produce unrealistic images, and omit those \u201cunrealistic\u201d units from semantic segmentation. An appropriate threshold to apply will depend on the distribution being modeled: in Figure 16, we show how applying a filter, ignoring segmentation on units with FID 55 or higher, affects the analysis of this base WGAN model. In general, fewer irrelevant labels are associated with units."
        },
        {
            "heading": "S-6.4 COMPUTING CAUSAL UNITS",
            "text": "In this section we provide more details about the ACE optimization described in Section 3.2.\nSpecifying the per-class positive intervention constant k. In Eqn. 3, the negative intervention is defined as zeroing the intervened units, and a positive intervention is defined as setting the intervened units to some big class-specific constant k. For interventions for class c, we set k to be mean featuremap activation conditioned on the presence of class c at that location in the output, with each pixel weighted by the portion of the featuremap locations that are covered by the class c. Setting all units at a pixel to k will tend to strongly cause the target class. The goal of the optimization is to find the subset of units that is causal for c.\nSampling c-relevant locations P. When optimizing the causal objective (Eqn. 5), the intervention locations P are sampled from individual featuremap locations. When the class c is rare, most featuremap locations are uninformative: for example, when class c is a door in church scenes, most regions of the sky, grass, and trees are locations where doors will not appear. Therefore, we focus the\noptimization as follows: during training, minibatches are formed by sampling locations P that are relevant to class c by including locations where the class c is present in the output (and are therefore candidates for removal by ablating a subset of units), and an equal portion of locations where class c is not present at P, but it would be present if all the units are set to the constant k (candidate locations for insertion with a subset of units). During the evaluation, causal effects are evaluated using uniform samples: the region P is set to the entire image when measuring ablations, and to uniformly sampled pixels P when measuring single-pixel insertions.\nInitializing \u03b1 with IoU. When optimizing causal \u03b1 for class c, we initialize with\n\u03b1u = IoUu,c\nmaxv IoUv,c (7)\nThat is, we set the initial \u03b1 so that the largest component corresponds to the unit with the largest IoU for class c, and we normalize the components so that this largest component is 1.\nApplying a learned intervention \u03b1 When applying the interventions, we clip \u03b1 by keeping only its top n components and zeroing the remainder. To compare the interventions of different classes an different models on an equal basis, we examine interventions where we set n = 20."
        },
        {
            "heading": "S-6.5 TRACING THE EFFECT OF AN INTERVENTION",
            "text": "To investigate the mechanism for suppressing the visible effects of some interventions seen in Section 4.4, in this section we insert 20 door-causal units on a sample of individual featuremap locations at layer4 and measure the changes caused in later layers.\nTo quantify effects on downstream features, the change in each feature channel is normalized by that channel\u2019s mean L1 magnitude, and we examine the mean change in these normalized featuremaps at each layer. In Figure 17, these effects that propagate to layer14 are visualized as a heatmap: brighter colors indicate a stronger effect on the final feature layer when the door intervention is in the neighborhood of a building instead of trees or sky. Furthermore, we plot the average effect on every layer at right in Figure 17, separating interventions that have a visible effect from those that do not. A small identical intervention at layer4 is amplified to larger changes up to a peak at layer12."
        },
        {
            "heading": "S-6.6 MONITORING GAN UNITS DURING TRAINING",
            "text": "Dissection can also be used to monitor the progress of training by quantifying the emergence, diversity, and quality of interpretable units. For example, in Figure 18 we show dissections of layer4 representations of a Progressive GAN model trained on bedrooms, captured at a sequence of checkpoints during training. As training proceeds, the number of units matching objects increases,\ncloud 0.01 buildingb 0.10 glass 0.04 floor 0.11\nwallb 0.13 floort 0.09 glass 0.14 wood 0.14\nceiling 0.01 bedt 0.10 window 0.18 painting 0.08\nlamp 0.05 bedt 0.17 window 0.25 painting 0.11\nlamp 0.05 bedt 0.20 window 0.30 painting 0.10\nlamp 0.05 bedt 0.19 window 0.32 painting 0.11\nlamp 0.05 bedt 0.23 window 0.30 painting 0.12\nIteration\u00a04800 27\u00a0object\u00a0units\u00a0 25\u00a0part\u00a0units\u00a0 23\u00a0material\u00a0units\u00a0 IoU\u00a0avg\u00a00.045\n1\n5\n10 un its\nsky mo un tai n flo ortre e bu ildi ngsky -t clo ud tre e-b bu ildi ng -b tre e-l mo un tai n-t\npo lish\ned st\non e pa int ed wo od gla ss\n5 ob\nje ct\ns\n6 pa\nrts\n4 m\nat er\nia ls\nIteration\u00a06400 45\u00a0object\u00a0units\u00a0 56\u00a0part\u00a0units\u00a0 32\u00a0material\u00a0units\u00a0 IoU\u00a0avg\u00a00.067\n1\n10\n19 un its\nce ilin\ng ea rthflo ortre e skyfen ce wa ll-t flo orr flo orb flo ort wa ll-b flo orl\nbu ildi\nng -b ce ilin g-bwa ll-r bu ildi ng -l bu ildi ng -t ce ilin g-t win do wt ce ilin g-r tre e-rsky -t win do wr ce ilin g-l gla ss pa int ed\npo lish\ned st\non e wo od me tal\n6 ob\nje ct\ns\n18 p\nar ts\n5 m\nat er\nia ls\nIteration\u00a08000 65\u00a0object\u00a0units\u00a0 146\u00a0part\u00a0units\u00a0 10\u00a0material\u00a0units\u00a0 IoU\u00a0avg\u00a00.070\n1\n14\n27 un its\nce ilin\ng be d win do w flo or pa int ing bu ildi ng be d-bbe d-t be d-r ce ilin g-bbe d-l win do wt ce ilin g-t flo orb win do wb ce ilin g-r wa ll-b win do wl flo orr flo orl ce ilin g-l pa int ing -b win do wr sky -t fab ric ca rpe t gla ss wo od\n6 ob\nje ct\ns\n18 p\nar ts\n4 m\nat er\nia ls\nIteration\u00a010001 70\u00a0object\u00a0units\u00a0 166\u00a0part\u00a0units\u00a0 6\u00a0material\u00a0units\u00a0 IoU\u00a0avg\u00a00.077\n1\n17\n34 un its\nwin do w ce ilin g flo or pa int ing cu rta in pill ow be d-bbe d-t be d-l be d-r wa ll-b ce ilin g-b win do wt ce ilin g-t ce ilin g-l win do wb ce ilin g-r wa ll-l win do wl flo orr flo orb pa int ing -b flo orl win do wr sky -t wa ll-r pill ow -t flo ort ca r-t cu rta inb cu rta inr ca rpe t wo od gla ss\n6 ob\nje ct\ns\n25 p\nar ts\n3 m\nat er\nia ls\nIteration\u00a015009 79\u00a0object\u00a0units\u00a0 182\u00a0part\u00a0units\u00a0 11\u00a0material\u00a0units\u00a0 IoU\u00a0avg\u00a00.084\n1\n20\n39 un its\nce ilin\ng\nwin do w cu rta in flo or pa int ing pill ow cu sh ion be d-bbe d-t be d-l be d-r win do wt wa ll-b ce ilin g-b win do wb ce ilin g-r ce ilin g-l win do wl ce ilin g-t wa ll-l cu rta int pa int ing -b wa ll-r flo ort win do wr cu rta inb flo orb flo orr bu ildi ng -l pill ow -t flo orl pa int ing -r sky -t pill ow -r pa int ing -t pa int ing -l bu ildi ng -t ca rpe t wo od\n7 ob\nje ct\ns\n30 p\nar ts\n2 m\nat er\nia ls\nIteration\u00a017413 87\u00a0object\u00a0units\u00a0 184\u00a0part\u00a0units\u00a0 12\u00a0material\u00a0units\u00a0 IoU\u00a0avg\u00a00.086\n1\n18\n36 un its\nce ilin\ng\nwin do\nw flo\nor cu rta in pill ow pa int ing cu sh ion bu ildi ng be d-bbe d-l be d-t wa ll-bbe d-r win do wt ce ilin g-b win do wb ce ilin g-r ce ilin g-l win do wl cu rta int ce ilin g-t wa ll-l wa ll-r cu rta inb flo orb win do wr flo orr pill ow -r pill ow -t flo orl pa int ing -t cu rta inl pa int ing -r pill ow -l pa int ing -b ca rpe t wo od gla ss\n8 ob\nje ct\ns\n27 p\nar ts\n3 m\nat er\nia ls\nIteration\u00a020000 95\u00a0object\u00a0units\u00a0 164\u00a0part\u00a0units\u00a0 12\u00a0material\u00a0units\u00a0 IoU\u00a0avg\u00a00.085\n1\n15\n29 un its\nwin do w ce ilin g flo or pa int ing cu rta in pill ow cu sh ionlam p be d be d-bbe d-t be d-l wa ll-b win do wt be d-r ce ilin g-b ce ilin g-r ce ilin g-l ce ilin g-t win do wl win do wb wa ll-l cu rta int win do wr cu rta inb pa int ing -b wa ll-r flo orr pill ow -t flo orl pa int ing -r cu rta inl pa int ing -l ca rpe t gla ss\n9 ob\nje ct\ns\n24 p\nar ts\n2 m\nat er\nia ls\nFigure 18: The evolution of layer4 of a Progressive GAN bedroom generator as training proceeds. The number and quality of interpretable units increases during training. Note that in early iterations, Progressive GAN generates images at a low resolution. The top-activating images for the same four selected units is shown for each iteration, along with the IoU and the matched concept for each unit at that checkpoint.\nas does the number of object classes with matching units, and the quality of object detectors as measured by average IoU over units increases. During this successful training, dissection suggests that the model is gradually learning the structure of a bedroom, as increasingly units converge to meaningful bedroom concepts."
        },
        {
            "heading": "S-6.7 ALL LAYERS OF A GAN",
            "text": "In Section 4.1 we show a small selection of layers of a GAN; in Figure 19 we show a complete listing of all the internal convolutional layers of that model (a Progressive GAN trained on LSUN living room images). As can be seen, the diversity of units matching high-level object concepts peaks at layer4-layer6, then declines in later layers, with the later layers dominated by textures, colors, and shapes.\n\u00a0 Units\u00a0in\u00a0layer \u00a0 Unit\u00a0class\u00a0distribution layer1\u00a0 512\u00a0units\u00a0total\n0\u00a0object\u00a0units\u00a0 2\u00a0part\u00a0units\u00a0 0\u00a0material\u00a0units\u00a0\n\u00a0\niou=0.10ceilingt\u00a0layer1\u00a0#457 iou=0.07ceilingt\u00a0layer1\u00a0#194\n\u00a0 layer2\u00a0 512\u00a0units\u00a0total\n97\u00a0object\u00a0units\u00a0 63\u00a0part\u00a0units\u00a0 5\u00a0material\u00a0units\u00a0\n\u00a0\niou=0.23window\u00a0layer2\u00a0#70 iou=0.25floor\u00a0layer2\u00a0#315\n\u00a0 layer3\u00a0 512\u00a0units\u00a0total\n86\u00a0object\u00a0units\u00a0 121\u00a0part\u00a0units\u00a0 10\u00a0material\u00a0units\u00a0\n\u00a0\niou=0.27window\u00a0layer3\u00a0#305 iou=0.24sofat\u00a0layer3\u00a0#55\n\u00a0 layer4\u00a0 512\u00a0units\u00a0total\n86\u00a0object\u00a0units\u00a0 149\u00a0part\u00a0units\u00a0 10\u00a0material\u00a0units\u00a0\n\u00a0\niou=0.28sofa\u00a0layer4\u00a0#37 iou=0.15fireplace\u00a0layer4\u00a0#23\n\u00a0 layer5\u00a0 512\u00a0units\u00a0total\n75\u00a0object\u00a0units\u00a0 153\u00a0part\u00a0units\u00a0 19\u00a0material\u00a0units\u00a0\n\u00a0\niou=0.29sofat\u00a0layer5\u00a0#190 iou=0.15paintingb\u00a0layer5\u00a0#133\n\u00a0 layer6\u00a0 512\u00a0units\u00a0total\n72\u00a0object\u00a0units\u00a0 129\u00a0part\u00a0units\u00a0 16\u00a0material\u00a0units\u00a0\n\u00a0\niou=0.31window\u00a0layer6\u00a0#393 iou=0.04bookcase\u00a0layer6\u00a0#308\n\u00a0 layer7\u00a0 256\u00a0units\u00a0total\n59\u00a0object\u00a0units\u00a0 48\u00a0part\u00a0units\u00a0 9\u00a0material\u00a0units\u00a0\n\u00a0\niou=0.23painting\u00a0layer7\u00a0#15 iou=0.07coffee\u00a0tablet\u00a0#247\n\u00a0 layer8\u00a0 256\u00a0units\u00a0total\n51\u00a0object\u00a0units\u00a0 52\u00a0part\u00a0units\u00a0 12\u00a0material\u00a0units\u00a0\n\u00a0\niou=0.17curtain\u00a0layer8\u00a0#186 iou=0.17foliage\u00a0layer8\u00a0#234\n\u00a0 layer9\u00a0 128\u00a0units\u00a0total\n26\u00a0object\u00a0units\u00a0 17\u00a0part\u00a0units\u00a0 9\u00a0material\u00a0units\u00a0\n\u00a0\niou=0.16window\u00a0layer9\u00a0#89 iou=0.23wood\u00a0layer9\u00a0#78\n\u00a0 layer10\u00a0 128\u00a0units\u00a0total\n19\u00a0object\u00a0units\u00a0 8\u00a0part\u00a0units\u00a0 11\u00a0material\u00a0units\u00a0\n\u00a0\niou=0.14carpet\u00a0layer10\u00a0#53 iou=0.21glass\u00a0layer10\u00a0#126\n\u00a0 layer11\u00a0 64\u00a0units\u00a0total\n9\u00a0object\u00a0units\u00a0 1\u00a0part\u00a0units\u00a0 7\u00a0material\u00a0units\u00a0\n\u00a0\niou=0.06sky\u00a0layer11\u00a0#14 iou=0.11ceiling\u00a0layer11\u00a0#49\n\u00a0 layer12\u00a0 64\u00a0units\u00a0total\n8\u00a0object\u00a0units\u00a0 1\u00a0part\u00a0units\u00a0 4\u00a0material\u00a0units\u00a0\n\u00a0\niou=0.23wood\u00a0layer12\u00a0#26 iou=0.04sky\u00a0layer12\u00a0#19\n\u00a0 layer13\u00a0 32\u00a0units\u00a0total\n6\u00a0object\u00a0units\u00a0 0\u00a0part\u00a0units\u00a0 3\u00a0material\u00a0units\u00a0\n\u00a0\niou=0.12carpet\u00a0layer13\u00a0#13 iou=0.17wood\u00a0layer13\u00a0#23\n\u00a0\n1\n2\nun its\ncei ling\n-t\n1 pa\nrt\n1\n22\n43 un its\ncei ling win dowsof a floo r sky cur tain\ncof fee\ntab le\nfire pla\nce floo r-b sof a-t cei ling -t sof a-b cei ling -r win dow -b cei ling -l floo r-r floo r-l win dow -r win dow -t fire pla cer sof a-l fire pla cel sky -t car pet\n8 ob\njec ts\n15 p\nar ts\n1 m\nat er\nial\n1\n18\n36 un its\ncei ling win dowsof a\ncof fee\ntab le\npai ntin g cur tainfloo r fire pla ce floo r-b sof a-b cei ling -t sof a-t cei ling -b win dow -b wa ll-t floo r-r win dow -t floo r-l sof a-l wa ll-b win dow -l wa ll-l floo r-t fire pla cet cei ling -r win dow -r sof a-r\ncof fee\ntab le-l cei ling -l\npai ntin\ng-bsky -t\ncur tain\n-t fire pla cel\ncof fee\ntab le-r\npai ntin\ng-r car pet\n8 ob\njec ts\n27 p\nar ts\n1 m\nat er\nial\n1\n12\n24 un its\ncei ling win dowsof a pai ntin g floo r cur tain\ncof fee\ntab le\nboo kca\nse\nfire pla\nceshe lf skysof a-t sof a-b floo r-b\nwin dow\n-t\ncei ling\n-t wa ll-bsof a-l floo r-r\ncei ling\n-b\nwin dow\n-b floo r-l wa ll-t\ncei ling\n-r\nwin dow\n-r wa ll-l\ncei ling\n-l sof a-r wa ll-r\npai ntin\ng-b\nfire pla\ncer\nfire pla\ncet win dow -l she lf-t cur tain -t pai ntin g-r\ncof fee\ntab le-r\ncof fee\ntab le-t\npai ntin\ng-l car petgla ss\n11 o\nbje cts\n28 p\nar ts\n2 m\nat er\nial s\n1\n15\n29 un its\ncei lingsof a win dow pai ntin g\nboo kca\nse cur tainfloo r\nfire pla\nce cof fee tab le cha nde lier floo r-b cei ling -t sof a-bsof a-t cei ling -b wa ll-bwa ll-t pai ntin g-b win dow -t pai ntin g-l sof a-rwa ll-l floo r-r floo r-l cei ling -l win dow -r fire pla cel fire pla cet wa ll-r cei ling -r floo r-t win dow -b sof a-lsky -t win dow -l pai ntin g-t fire pla cer cur tain -t cof fee tab le-t pai ntin g-r cof fee tab le-l fire pla ceb car petgla ss lea the r\n10 o\nbje cts\n32 p\nar ts\n3 m\nat er\nial s\n1\n10\n20 un its\ncei ling win dowsof a cur tain pai ntin g fire pla cefloo r boo kca se cus hio n mir ror skysof a-t sof a-b cei ling -t wa ll-b cei ling -b floo r-bwa ll-t win dow -t floo r-t cur tain -t pai ntin g-l pai ntin g-r floo r-r win dow -b floo r-l wa ll-r fire pla cet sof a-r win dow -l\ncof fee\ntab le-t\ncur tain\n-b fire pla ceb\ncof fee\ntab le-r cei ling -r cei ling -l win dow -r sky -t pai ntin g-t\ncof fee\ntab le-b\nfire pla\ncel car petgla\nss 11\no bje\ncts\n30 p\nar ts\n2 m\nat er\nial s\n1\n7\n14 un its\ncei ling win dowsof a floo r cur tain pai ntin g fire pla ce boo kca se sky\ncof fee\ntab le cha nde lier cus hio n car pet sof a-b cei ling -t floo r-b wa ll-b cei ling -b sof a-t pai ntin g-twa ll-l win dow -t\ncof fee\ntab le-t floo r-r\nwin dow\n-b floo r-t sof a-l\ncei ling\n-r\nwin dow\n-r pai ntin g-b\ncof fee\ntab le-r car petgla ss wo od\n13 o\nbje cts\n18 p\nar ts\n3 m\nat er\nial s\n1\n6\n11 un its\ncei ling win dowsof a cur tain pai ntin g skyfloo r cus hio n lam p wa ll-t sof a-b floo r-b wa ll-b win dow -t cei ling -b cei ling -t pai ntin g-l floo r-t pai ntin g-t pai ntin g-r floo r-l win dow -r cei ling -r sof a-t win dow -b\ncof fee\ntab le-t\ncur tain\n-b\ncur tain\n-t pai ntin g-bsky -t sky -b car petgla ss foli agewo od\n9 ob\njec ts\n22 p\nar ts\n4 m\nat er\nial s\n1\n4\n8\nun its\ncei ling cur tain win dow pai ntin g skysof a pla nt cei ling -t sof a-bfloo r-l win dow -t floo r-t floo r-bsky -t win dow -b cei ling -b cus hio n-b pai ntin g-bwo od car petgla ss\n7 ob\njec ts\n11 p\nar ts\n3 m\nat er\nial s\n1\n3\n6\nun its\ncei ling win dowpla nt sky pai ntin g cur tainfloo r win dow -t floo r-b wa ll-b cei ling -t win dow -b wo od gla ss car pet\n7 ob\njec ts\n5 pa\nrts\n3 m\nat er\nial s\n1\n3\n5\nun its\ncei ling win dow cur tainpla nt skywa ll-bwo od gla ss\n5 ob\njec ts\n1 pa\nrt\n2 m\nat er\nial s\n1\n2\nun its\ncei ling win dow cur tain skypla nt cei ling -t wo od gla ss\n5 ob\njec ts\n1 pa\nrt\n2 m\nat er\nial s\n1\n2\n3\nun its\nwin dowcei ling cur tainwo od car pet\n3 ob\njec ts\n2 m\nat er\nial s\nFigure 19: All layers of a Progressive GAN trained to generate LSUN living room images."
        }
    ],
    "title": "Preprint prepared for ArXiv submission",
    "year": 2018
}