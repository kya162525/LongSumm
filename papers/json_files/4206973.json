{
    "abstractText": "We develop a 2nd-order optimization method based on the \u201cHessian-free\u201d approach, and apply it to training deep auto-encoders. Without using pre-training, we obtain results superior to those reported by Hinton & Salakhutdinov (2006) on the same tasks they considered. Our method is practical, easy to use, scales nicely to very large datasets, and isn\u2019t limited in applicability to autoencoders, or any specific model class. We also discuss the issue of \u201cpathological curvature\u201d as a possible explanation for the difficulty of deeplearning and how 2nd-order optimization, and our method in particular, effectively deals with it.",
    "authors": [
        {
            "affiliations": [],
            "name": "James Martens"
        }
    ],
    "id": "SP:aa87371bdcec3cfdaef6b056306d319972acfce5",
    "references": [
        {
            "authors": [
                "S. Amari",
                "H. Park",
                "K. Fukumizu"
            ],
            "title": "Adaptive method of realizing natural gradient learning for multilayer perceptrons",
            "venue": "Neural Computation,",
            "year": 2000
        },
        {
            "authors": [
                "Y. Bengio",
                "P. Lamblin",
                "D. Popovici",
                "H. Larochelle"
            ],
            "title": "Greedy layer-wise training of deep networks",
            "venue": "In NIPS,",
            "year": 2007
        },
        {
            "authors": [
                "D. Erhan",
                "Y. Bengio",
                "A. Courville",
                "P. Manzagol",
                "P. Vincent",
                "S. Bengio"
            ],
            "title": "Why does unsupervised pre-training help deep learning",
            "venue": "Journal of Machine Learning Research,",
            "year": 2010
        },
        {
            "authors": [
                "G.E. Hinton",
                "R.R. Salakhutdinov"
            ],
            "title": "Reducing the dimensionality of data with neural networks",
            "year": 2006
        },
        {
            "authors": [
                "Y. LeCun",
                "L. Bottou",
                "G. Orr",
                "K. Muller"
            ],
            "title": "Efficient backprop",
            "venue": "Neural Networks: Tricks of the trade. Springer,",
            "year": 1998
        },
        {
            "authors": [
                "E. Mizutani",
                "S.E. Dreyfus"
            ],
            "title": "Second-order stagewise backpropagation for hessian-matrix analyses and investigation of negative curvature",
            "venue": "Neural Networks,",
            "year": 2008
        },
        {
            "authors": [
                "B.A. Pearlmutter"
            ],
            "title": "Fast exact multiplication by the hessian",
            "venue": "Neural Computation,",
            "year": 1994
        },
        {
            "authors": [
                "N.N. Schraudolph"
            ],
            "title": "Fast curvature matrix-vector products for second-order gradient descent",
            "venue": "Neural Computation,",
            "year": 2002
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Learning the parameters of neural networks is perhaps one of the most well studied problems within the field of machine learning. Early work on backpropagation algorithms showed that the gradient of the neural net learning objective could be computed efficiently and used within a gradientdescent scheme to learn the weights of a network with multiple layers of non-linear hidden units. Unfortunately, this technique doesn\u2019t seem to generalize well to networks that have very many hidden layers (i.e. deep networks). The common experience is that gradient-descent progresses extremely slowly on deep nets, seeming to halt altogether before making significant progress, resulting in poor performance on the training set (under-fitting).\nIt is well known within the optimization community that gradient descent is unsuitable for optimizing objectives that exhibit pathological curvature. 2nd-order optimization methods, which model the local curvature and correct for it, have been demonstrated to be quite successful on such objectives. There are even simple 2D examples such as the Rosenbrock function where these methods can demonstrate considerable advantages over gradient descent. Thus it is reasonable to suspect that the deep learning problem could be resolved by the application of such techniques. Unfortu-\nAppearing in Proceedings of the 27 th International Conference on Machine Learning, Haifa, Israel, 2010. Copyright 2010 by the author(s)/owner(s).\nnately, there has yet to be a demonstration that any of these methods are effective on deep learning problems that are known to be difficult for gradient descent.\nMuch of the recent work on applying 2nd-order methods to learning has focused on making them practical for large datasets. This is usually attempted by adopting an \u201con-line\u201d approach akin to the one used in stochastic gradient descent (SGD). The only demonstrated advantages of these methods over SGD is that they can sometimes converge in fewer training epochs and that they require less tweaking of metaparameters, such as learning rate schedules.\nThe most important recent advance in learning for deep networks has been the development of layer-wise unsupervised pre-training methods (Hinton & Salakhutdinov, 2006; Bengio et al., 2007). Applying these methods before running SGD seems to overcome the difficulties associated with deep learning. Indeed, there have been many successful applications of these methods to hard deep learning problems, such as auto-encoders and classification nets. But the question remains: why does pre-training work and why is it necessary? Some researchers (e.g. Erhan et al., 2010) have investigated this question and proposed various explanations such as a higher prevalence of bad local optima in the learning objectives of deep models.\nAnother explanation is that these objectives exhibit pathological curvature making them nearly impossible for curvature-blind methods like gradient-descent to successfully navigate. In this paper we will argue in favor of this explanation and provide a solution in the form of a powerful semi-online 2nd-order optimization algorithm which is practical for very large models and datasets. Using this technique, we are able to overcome the under-fitting problem encountered when training deep auto-encoder neural nets far more effectively than the pre-training + fine-tuning approach proposed by Hinton & Salakhutdinov (2006). Being an optimization algorithm, our approach doesn\u2019t deal specifically with the problem of over-fitting, however we show that this is only a serious issue for one of the three deep-auto encoder problems considered by Hinton & Salakhutdinov, and can be handled by the usual methods of regularization.\nThese results also help us address the question of why deep-learning is hard and why pre-training sometimes\nhelps. Firstly, while bad local optima do exist in deepnetworks (as they do with shallow ones) in practice they do not seem to pose a significant threat, at least not to strong optimizers like ours. Instead of bad local minima, the difficulty associated with learning deep auto-encoders is better explained by regions of pathological curvature in the objective function, which to 1st-order optimization methods resemble bad local minima."
        },
        {
            "heading": "2. Newton\u2019s method",
            "text": "In this section we review the canonical 2nd-order optimization scheme, Newton\u2019s method, and discuss its main benefits and why they may be important in the deep-learning setting. While Newton\u2019s method itself is impractical on large models due to the quadratic relationship between the size of the Hessian and the number of parameters in the model, studying it nevertheless informs us about how its more practical derivatives (i.e. quasi-Newton methods) might behave.\nNewton\u2019s method, like gradient descent, is an optimization algorithm which iteratively updates the parameters \u03b8 \u2208 RN of an objective function f by computing search directions p and updating \u03b8 as \u03b8+\u03b1p for some \u03b1. The central idea motivating Newton\u2019s method is that f can be locally approximated around each \u03b8, up to 2nd-order, by the quadratic:\nf(\u03b8 + p) \u2248 q\u03b8(p) \u2261 f(\u03b8) +\u2207f(\u03b8)\u22a4p+ 1\n2 p\u22a4Bp (1)\nwhere B = H(\u03b8) is the Hessian matrix of f at \u03b8. Finding a good search direction then reduces to minimizing this quadratic with respect to p. Complicating this idea is that H may be indefinite so this quadratic may not have a minimum, and moreover we don\u2019t necessarily trust it as an approximation of f for large values of p. Thus in practice the Hessian is \u201cdamped\u201d or re-conditioned so that B = H+ \u03bbI for some constant \u03bb \u2265 0."
        },
        {
            "heading": "2.1. Scaling and curvature",
            "text": "An important property of Newton\u2019s method is \u201cscale invariance\u201d. By this we mean that it behaves the same for any linear rescaling of the parameters. To be technically precise, if we adopt a new parameterization \u03b8\u0302 = A\u03b8 for some invertible matrix A, then the optimal search direction in the new parameterization is p\u0302 = Ap where p is the original optimal search direction. By contrast, the search direction produced by gradient descent has the opposite response to linear re-parameterizations: p\u0302 = A\u2212\u22a4p.\nScale invariance is important because, without it, poorly scaled parameters will be much harder to optimize. It also eliminates the need to tweak learning rates for individual parameters and/or anneal global learning-rates according to arbitrary schedules. Moreover, there is an implicit \u201cscaling\u201d which varies over the entire parameter space and is determined by the local curvature of the objective function.\nBy taking the curvature information into account (in the form of the Hessian), Newton\u2019s method rescales the gradient so it is a much more sensible direction to follow.\nIntuitively, if the curvature is low (and positive) in a particular descent direction d, this means that the gradient of the objective changes slowly along d, and so d will remain a descent direction over a long distance. It is thus sensible to choose a search direction p which travels far along d (i.e. by making p\u22a4d large), even if the amount of reduction in the objective associated with d (given by\u2212\u2207f\u22a4d) is relatively small. Similarly if the curvature associated with d is high, then it is sensible to choose p so that the distance traveled along d is smaller. Newton\u2019s method makes this intuition rigorous by computing the distance to move along d as its reduction divided by its associated curvature: \u2212\u2207f\u22a4d/d\u22a4Hd. This is precisely the point along d after which f is predicted by (1) to start increasing.\nNot accounting for the curvature when computing search directions can lead to many undesirable scenarios. First, the sequence of search directions might constantly move too far in directions of high curvature, causing an unstable \u201cbouncing\u201d behavior that is often observed with gradient descent and is usually remedied by decreasing the learning rate. Second, directions of low curvature will be explored much more slowly than they should be, a problem exacerbated by lowering the learning rate. And if the only directions of significant decrease in f are ones of low curvature, the optimization may become too slow to be practical and even appear to halt altogether, creating the false impression of a local minimum. It is our theory that the under-fitting problem encountered when optimizing deep nets using 1storder techniques is mostly due to such techniques becoming trapped in such false local minima.\nFigure 1 visualizes a \u201cpathological curvature scenario\u201d, where the objective function locally resembles a long narrow valley. At the base of the valley is a direction of low reduction and low curvature that needs to be followed in order to make progress. The smaller arrows represent the steps taken by gradient descent with large and small learning rates respectively, while the large arrow along the base of the valley represents the step computed by Newton\u2019s method. What makes this scenario \u201cpathological\u201d is not the presence of merely low or high curvature directions,\nAlgorithm 1 The Hessian-free optimization method 1: for n = 1, 2, ... do 2: gn \u2190 \u2207f(\u03b8n) 3: compute/adjust \u03bb by some method 4: define the function Bn(d) = H(\u03b8n)d+ \u03bbd 5: pn \u2190 CG-Minimize(Bn,\u2212gn) 6: \u03b8n+1 \u2190 \u03b8n + pn 7: end for\nbut the mixture of both of them together."
        },
        {
            "heading": "2.2. Examples of pathological curvature in neural nets",
            "text": "For a concrete example of pathological curvature in neural networks, consider the situation in which two units a and b in the same layer have nearly identical incoming and outgoing weights and biases. Let d be a descent direction which increases the value of one of a\u2019s outgoing weights, say parameter i, while simultaneously decreasing the corresponding weight for unit b, say parameter j, so that dk = \u03b4ik \u2212 \u03b4jk. d can be interpreted as a direction which \u201cdifferentiates\u201d the two units. The reduction associated with d is \u2212\u2207f\u22a4d = (\u2207f)j \u2212 (\u2207f)i \u2248 0 and the curvature is d\u22a4Hd = (Hii\u2212Hij)+ (Hjj \u2212Hji) \u2248 0+0 = 0. Gradient descent will only make progress along d which is proportional to the reduction, which is very small, whereas Newton\u2019s methods will move much farther, because the associated curvature is also very small.\nAnother example of pathological curvature, particular to deeper nets, is the commonly observed phenomenon where, depending on the magnitude of the initial weights, the gradients will either shrink towards zero or blow up as they are back-propagated, making learning of the weights before the last few layers nearly impossible. This difficulty in learning all but the last few layers is sometimes called the \u201cvanishing gradients\u201d problem and may be slightly mitigated by using heuristics to adapt the learning rates of each parameter individually. The issue here is not so much that the gradients become very small or very large absolutely, but rather that they become so relative to the gradients of the weights associated with units near the end of the net. Critically, the second-derivatives will shrink or blow up in an analogous way, corresponding to either very low or high curvature along directions which change the affected parameters. Newton\u2019s method thus will rescale these directions so that they are far more reasonable to follow."
        },
        {
            "heading": "3. Hessian-free optimization",
            "text": "The basis of the 2nd-order optimization approach we develop in this paper is a technique known as Hessianfree optimization (HF), aka truncated-Newton, which has been studied in the optimization community for decades (e.g. Nocedal & Wright, 1999), but never seriously applied within machine learning.\nIn the standard Newton\u2019s method, q\u03b8(p) is optimized by computing the N\u00d7N matrix B and then solving the system Bp = \u2212\u2207f(\u03b8). This is prohibitively expensive when N is large, as it is with even modestly sized neural networks. Instead, HF optimizes q\u03b8(p) by exploiting two simple ideas. The first is that for an N -dimensional vector d, Hd can be easily computed using finite differences at the cost of a single extra gradient evaluation via the identity:\nHd = lim \u03f5\u21920 \u2207f(\u03b8 + \u03f5d)\u2212\u2207f(\u03b8) \u03f5\nThe second is that there is a very effective algorithm for optimizing quadratic objectives (such as q\u03b8(p)) which requires only matrix-vector products with B: the linear conjugate gradient algorithm (CG). Now since in the worst case CG will require N iterations to converge (thus requiring the evaluation of N Bd-products), it is clearly impractical to wait for CG to completely converge in general. But fortunately, the behavior CG is such that it will make significant progress in the minimization of q\u03b8(p) after a much more practical number of iterations. Algorithm 1 gives the basic skeleton of the HF method.\nHF is appealing because unlike many other quasi-Newton methods it does not make any approximation to the Hessian. Indeed, the Hd products can be computed accurately by the finite differences method, or other more stable algorithms. HF differs from Newton\u2019s method only because it is performing an incomplete optimization (via un-converged CG) of q\u03b8(p) in lieu of doing a full matrix inversion.\nAnother appealing aspect of the HF approach lies in the power of the CG method. Distinct from the non-linear CG method (NCG) often used in machine learning, linear CG makes strong use of the quadratic nature of the optimization problem it solves in order to iteratively generate a set of \u201cconjugate directions\u201d di (with the property that d\u22a4i Adj = 0 for i \u0338= j) and optimize along these independently and exactly. In particular, the movement along each direction is precisely what Newton\u2019s method would select, the reduction divided by the curvature, i.e. \u2212\u2207f\u22a4di/d\u22a4i Adi, a fact which follows from the conjugacy property. On the other hand, when applying the non-linear CG method (which is done on f directly, not q\u03b8), the directions it generates won\u2019t remain conjugate for very long, even approximately so, and the line search is usually performed inexactly and at a relatively high expense.\nNevertheless, CG and NCG are in many ways similar and NCG even becomes equivalent to CG when it uses an exact line-search and is applied to a quadratic objective (i.e. one with constant curvature). Perhaps the most important difference is that when NCG is applied to a highly nonlinear objective f , the underlying curvature evolves with each new search direction processed, while when CG is applied to the local quadratic approximation of f (i.e. q\u03b8), the curvature remains fixed. It seems likely that the later condition would allow CG to be much more effective than NCG\nat finding directions of low reduction and curvature, as directions of high reduction and high curvature can be found by the early iterations of CG and effectively \u201csubtracted away\u201d from consideration via the conjugate-directions decomposition. NCG, on the other hand, must try to keep up with the constantly evolving curvature conditions of f , and therefore focus on the more immediate directions of highreduction and curvature which arise at each successively visited position in the parameter space."
        },
        {
            "heading": "4. Making HF suitable for machine learning problems",
            "text": "Our experience with using off-the-shelf implementations of HF is that they simply don\u2019t work for neural network training, or are at least grossly impractical. In this section we will describe the modifications and design choices we made to the basic HF approach in order to yield an algorithm which is both effective and practical on the problems we considered. Note that none of these enhancements are specific to neural networks, and should be applicable to other optimization problems that are of interest to machinelearning researchers."
        },
        {
            "heading": "4.1. Damping",
            "text": "The issue of damping, like with standard Newton\u2019s method, is of vital importance to the HF approach. Unlike methods such as L-BFGS where the curvature matrix is crudely approximated, the exact curvature matrix implicitly available to the HF method allows for the identification of directions with extremely low curvature. When such a direction is found that also happens to have a reasonable large reduction, CG will elect to move very far along it, and possibly well outside of the region where (1) is a sensible approximation. The damping parameter \u03bb can be interpreted as controlling how \u201cconservative\u201d the approximation is, essentially by adding the constant \u03bb\u2225d\u22252 to the curvature estimate for each direction d. Using a fixed setting of \u03bb is not viable for several reasons, but most importantly because the relative scale of B is constantly changing. It might also be the case that the \u201ctrustworthiness\u201d of the approximation varies significantly over the parameter space.\nThere are advanced techniques, known as Newton-Lanczos methods, for computing the value of \u03bb which corresponds to a given \u201ctrust-region radius\u201d \u03c4 . However, we found that such methods were very expensive and thus not costeffective in practice and so instead we used a simple Levenberg-Marquardt style heuristic for adjusting \u03bb directly: if \u03c1 < 14 : \u03bb \u2190 3 2\u03bb elseif \u03c1 > 3 4 : \u03bb \u2190 2 3\u03bb endif where \u03c1 is the \u201creduction ratio\u201d. The reduction ratio is a scalar quantity which attempts to measure the accuracy of q\u03b8 and is given by:\n\u03c1 = f(\u03b8 + p)\u2212 f(\u03b8) q\u03b8(p)\u2212 q\u03b8(0)"
        },
        {
            "heading": "4.2. Computing the matrix-vector products",
            "text": "While the product Hd can be computed using finitedifferences, this approach is subject to numerical problems and also requires the computationally expensive evaluation of non-linear functions. Pearlmutter (1994) showed that there is an efficient procedure for computing the product Hd exactly for neural networks and several other models such as RNNs and Boltzmann machines. This algorithm is like backprop as it involves a forward and backward pass, is \u201clocal\u201d, and has a similar computational cost. Moreover, for standard neural nets it can also be performed without the need to evaluate non-linear functions.\nIn the development of his on-line 2nd-order method \u201cSMD\u201d, Schraudolph (2002) generalized Pearlmutter\u2019s method in order to compute the product Gd where G is the Gauss-Newton approximation to the Hessian. While the classical Gauss-Newton method applies only to a sumof-squared-error objective, it can be extended to neural networks whose output units \u201cmatch\u201d their loss function (e.g. logistic units with cross-entropy error).\nWhile at first glance this might seem pointless since we can already compute Hd with relative efficiency, there are good reasons for using G instead of H. Firstly, the GaussNewton matrix G is guaranteed to be positive semi-definite, even when un-damped, which avoids the problem of negative curvature, thus guaranteeing that CG will work for any positive value of \u03bb. Mizutani & Dreyfus (2008) argue against using G and that recognizing and exploiting negative curvature is important, particularly for training neural nets. Indeed, some implementations of HF will perform a check for directions of negative curvature during the CG runs and if one is found they will abort CG and run a specialized subroutine in order to search along it. Based on our limited experience with such methods we feel that they are not particularly cost-effective. Moreover, on all of the learning problems we tested, using G instead of H consistently resulted in much better search directions, even in situations where negative curvature was not present. Another more mundane advantage of using G over H is that the associated matrix-vector product algorithm for G uses about half the memory and runs nearly twice as fast."
        },
        {
            "heading": "4.3. Handling large datasets",
            "text": "In general, the computational cost associated with computing the Bd products will grow linearly with the amount of training data. Thus for large training datasets it may be impractical to compute these vectors as many times as is needed by CG in order to sufficiently optimize q\u03b8(p). One obvious remedy is to just truncate the dataset when computing the products, but this is unsatisfying. Instead we seek something akin to \u201conline learning\u201d, where the dataset used for each gradient evaluation is a constantly changing subset of the total, i.e. a \u201cmini-batch\u201d.\nFortunately, there is a simple way to adapt the HF as an on-\nline algorithm which we have found works well in practice, although some care must be taken. One might be tempted to cycle through a sequence of mini-batches for the evaluation of each Bd product, but this is a bad strategy for several reasons. Firstly, it is much more efficient to compute these products if the unit activations for each training example can be cached and reused across evaluations, which would be much less viable if the set of training examples is constantly changing. Secondly, and more importantly, the CG algorithm is not robust to changes in the B matrix while it is running. The power of the CG method relies on the invariants it maintains across iterations, such as the conjugacy of its search directions. These will be quickly violated if the implicit definition of B is constantly changing as CG iterates. Thus the mini-batch should be kept constant during each CG run, cycling only at the end of each HF iteration. It might also be tempting to try using very small mini-batches, perhaps even of size 1. This strategy, too, is problematic since the B matrix, if defined using a very small mini-batch, will not contain enough useful curvature information to produce a good search direction.\nThe strategy we found that works best is to use relatively large mini-batches, the optimal size of which grows as the optimization progresses. And while the optimal minibatch size may be a function of the size of the model (we don\u2019t have enough data to make a determination) it critically doesn\u2019t seem to bear any relation to the total dataset size. In our experiments, while we used mini-batches to compute the Bd products, the gradients and log-likelihoods were computed using the entire dataset. The rationale for this is quite simple: each HF iteration involves a run of CG which may require hundreds of Bd evaluations but only 1 gradient evaluation. Thus it is cost-effective to obtain a much higher quality estimate of the gradient. And it should be noted that unlike SGD which performs tens of thousands of iterations, the number of iterations performed by our HF approach rarely exceeds 200."
        },
        {
            "heading": "4.4. Termination conditions for CG",
            "text": "Implementations of HF generally employ a convergence test for the CG runs of the form \u2225Bp + \u2207f(\u03b8)\u22252 < \u03f5 where the tolerance \u03f5 is chosen high enough so as to ensure that CG will terminate in a number of iterations that is practical. A popular choice seems to be \u03f5 = min(12 , \u2225\u2207f(\u03b8)\u2225 1 2 2 )\u2225\u2207f(\u03b8)\u22252, which is supported by some theoretical convergence results. In this section we will argue why this type of convergence test is bad and propose one which we have found works much better in practice.\nWhile CG is usually thought of as an algorithm for finding a least-squares solution to the linear system Ax = b, it is not actually optimizing the squared error objective \u2225Ax\u2212 b\u22252. Instead, it optimizes the quadratic \u03d5(x) = 12x\n\u22a4Ax\u2212 b\u22a4x, and is invoked within HF implementations by setting A = B and b = \u2212\u2207f(\u03b8). While \u03d5(x) and \u2225Ax \u2212 b\u22252 have the same global minimizer, a good but sub-optimal solution for\none may actually be a terrible solution for the other. On a typical run of CG one observes that the objective function \u03d5(x) steadily decreases with each iteration (which is guaranteed by the theory), while \u2225Ax\u2212b\u22252 fluctuates wildly up and down and only starts shrinking to 0 towards the very end of the optimization. Moreover, the original motivation for running CG within the HF method was to minimize the quadratic q\u03b8(p), and not \u2225Bp+\u2207f(\u03b8)\u22252.\nThus it is in our opinion surprising that the commonly used termination condition for CG used within HF is based on \u2225Ax \u2212 b\u22252. One possible reason is that while \u2225Ax \u2212 b\u22252 is bounded below by 0, it is not clear how to find a similar bound for \u03d5(x) that would generate a reasonable termination condition. We experimented with several obvious heuristics and found that the best one by far was to terminate the iterations once the relative per-iteration progress made in minimizing \u03d5(x) fell below some tolerance. In particular, we terminate CG at iteration i if the following condition is satisfied:\ni > k and \u03d5(xi) < 0 and \u03d5(xi)\u2212 \u03d5(xi\u2212k)\n\u03d5(xi) < k\u03f5\nwhere k determines how many iterations into the past we look in order to compute an estimate of the current periteration reduction rate. Choosing k > 1 is in general necessary because, while the average per-iteration reduction in \u03d5 tends to decrease over time, it also displays a considerable amount of variance and thus we need to average over many iterations to obtain a reliable estimate. In all of our experiments we set k = max(10, 0.1i) and \u03f5 = 0.0005, thus averaging over a progressively larger interval as i grows. Note that \u03d5 can be computed from the CG iterates at essentially no additional cost.\nIn practice this approach will cause CG to terminate in very few iterations when \u03bb is large (which makes q\u03b8(p) easy to optimize) which is the typical scenario during the early stages of optimization. In later stages, q\u03b8(p) begins to exhibit pathological curvature (as long as \u03bb is decayed appropriately), which reflects the actual properties of f , thus making it harder to optimize. In these situations our termination condition will permit CG to run much longer, resulting in a much more expensive HF iteration. But this is the price that seemingly must be paid in order to properly compensate for the true curvature in f .\nHeuristics which attempt to significantly reduce the number of iterations by terminating CG early (and we tried several, such as stopping when f(\u03b8 + p) increases), provide a speed boost early on in the optimization, but consistently seem to result in worse long-term outcomes, both in terms of generalization error and overall rate of reduction in f . A possible explanation for this is that shorter CG runs are more \u201cgreedy\u201d and do not pursue as many low-curvature directions, which seem to be of vital importance, both for reducing generalization error and for avoiding extreme-curvature scenarios such as unit saturation and\npoor differentiation between units."
        },
        {
            "heading": "4.5. Sharing information across iterations",
            "text": "A simple enhancement to the HF algorithm which we found improves its performance by an order of magnitude is to use the search direction pn\u22121 found by CG in the previous HF iteration as the starting point for CG in the current one. There are intuitively appealing reasons why pn\u22121 might make a good initialization. Indeed, the values of B and \u2207f(\u03b8) for a given HF iteration should be \u201csimilar\u201d in some sense to their values at the previous iteration, and thus the optimization problem solved by CG is also similar to the previous one, making the previous solution a potentially good starting point.\nIn typical implementations of HF, the CG runs are initialized with the zero vector, and doing this has the nice property that the initial value of \u03d5 will be non-positive (0 in fact). This in turn ensures that the search direction produced by CG will always provide a reduction in q\u03b8, even if CG is terminated after the first iteration. In general, if CG is initialized with a non-zero vector, the initial value of \u03d5 can be greater than zero, and we have indeed found this to be the case when using pn\u22121. However, judging an initialization merely by its \u03d5 value may be misleading, and we found that runs initialized from pn\u22121 rather than 0 consistently yielded better reductions in q\u03b8, even when \u03d5(pn\u22121)\u226b 0. A possible explanation for this finding is that pn\u22121 is \u201cwrong\u201d within the new quadratic objective mostly along the volatile high-curvature directions, which are quickly and easily discovered and \u201ccorrected\u201d by CG, thus leaving the harder-tofind low-curvature directions, which tend to be more stable over the parameter space, and thus more likely to remain descent directions between iterations of HF."
        },
        {
            "heading": "4.6. CG iteration backtracking",
            "text": "While each successive iteration of CG improves the value of p with respect to the 2nd-order model q\u03b8(p), these improvements are not necessarily reflected in the value of f(\u03b8 + p). In particular, if q\u03b8(p) is untrustworthy due to the damping parameter \u03bb being set too low or the current minibatch being too small or unrepresentative, then running CG past a certain number of iterations can actually be harmful. In fact, the dependency of the directions generated by CG on the \u201cquality\u201d of B should almost certainly increase CG iterates, as the basis from which CG generates directions (called the Krylov basis) expands to include matrix-vector products with increasingly large powers of B.\nBy storing the current solution for p at iteration \u2308\u03b3j\u2309 of CG for each j (where \u03b3 > 1 is a constant; 1.3 in our experiments), we can \u201cbacktrack\u201d along them after CG has terminated, reducing j as long as the \u2308\u03b3j\u22121\u2309th iterate of CG yields a lower value of f(x+ p) than the \u2308\u03b3j\u2309th. However, we have observed experimentally that, for best performance, the value of p used to initialize the next CG run (as\ndescribed in the previous sub-section) should not be backtracked in this manner. The likely explanation for this effect is that directions which are followed too strongly in p due to bad curvature information from an unrepresentative mini-batch, or \u03bb being too small, will be \u201ccorrected\u201d by the next run of CG, since it will use a different mini-batch and possibly a larger \u03bb to compute B."
        },
        {
            "heading": "4.7. Preconditioning CG",
            "text": "Preconditioning is a technique used to accelerate CG. It does this by performing a linear change of variables x\u0302 = Cx for some matrix C, and then optimizing the transformed quadratic objective given by \u03d5\u0302(x\u0302) = 1 2 x\u0302\n\u22a4C\u2212\u22a4AC\u22121x\u0302 \u2212 (C\u22121b)\u22a4x\u0302. \u03d5\u0302 may have more forgiving curvature properties than the original \u03d5, depending on the value of the matrix C. To use preconditioned CG one specifies M \u2261 C\u22a4C, with the understanding that it must be easy to solve My = x for arbitrary x. Preconditioning is somewhat of an application specific art, and we experimented with many possible choices. One that we found to be particularly effective was the diagonal matrix:\nM = [ diag ( D\u2211 i=1 \u2207fi(\u03b8)\u2299\u2207fi(\u03b8) ) + \u03bbI ]\u03b1\nwhere fi is the value of the objective associated with training-case i, \u2299 denotes the element-wise product and the exponent \u03b1 is chosen to be less than 1 in order to suppress \u201cextreme\u201d values (we used 0.75 in our experiments). The inner sum has the nice interpretation of being the diagonal of the empirical Fisher information matrix, which is similar in some ways to the G matrix. Unfortunately, it is impractical to use the diagonal of G itself, since the obvious algorithm has a cost similar to K backprop operations, where K is the size of the output layer, which is large for auto-encoders (although typically not for classification nets)."
        },
        {
            "heading": "5. Random initialization",
            "text": "Our HF approach, like almost any deterministic optimization scheme, is not completely immune to \u201cbad\u201d initializations, although it does tend to be far more robust to these than 1st-order methods. For example, it cannot break symmetry between two units in the same layer that are initialized with identical weights. However, as discussed in section 2.2 it has a far better chance than 1st-order methods of doing so if the weights are nearly identical.\nIn our initial investigations we tried a variety of random initialization schemes. The better ones, which were more careful about avoiding issues like saturation, seemed to allow the runs of CG to terminate after fewer iterations, mostly likely because these initializations resulted in more favorable local curvature properties. The best random initialization scheme we found was one of our own design,\n\u201csparse initialization\u201d. In this scheme we hard limit the number of non-zero incoming connection weights to each unit (we used 15 in our experiments) and set the biases to 0 (or 0.5 for tanh units). Doing this allows the units to be both highly differentiated as well as unsaturated, avoiding the problem in dense initializations where the connection weights must all be scaled very small in order to prevent saturation, leading to poor differentiation between units."
        },
        {
            "heading": "6. Related work on 2nd-order optimization",
            "text": "LeCun et al. (1998) have proposed several diagonal approximations of the H and G matrices for multi-layer neural nets. While these are easy to invert, update and store, the diagonal approximation may be overly simplistic since it neglects the very important interaction between parameters. For example, in the \u201cnearly identical units\u201d scenario considered in section 2.2, a diagonal approximation would not be able to recognize the \u201cdifferentiating direction\u201d as being one of low curvature.\nAmari et al. (2000) have proposed a 2nd-order learning algorithm for neural nets based on an empirical approximation of the Fisher information matrix (which can be defined for a neural net by casting it as a probabilistic model). Since Schraudolph\u2019s approach for computing Gd may be generalized to compute Fd, we were thus able to evaluate the possibility of using F as an alternative to G within our HF approach. The resulting algorithm wasn\u2019t able to make significant progress on the deep auto-encoder problems we considered, possibly indicating that F doesn\u2019t contain sufficient curvature information to overcome the problems associated with deep-learning. A more theoretical observation which supports the use of G over F in neural nets is that the ranks of F and G are D and DL respectively, where D is the size of the training set and L is the size of the output layer. Another observation is that G will converge to the Hessian as the error of the net approaches zero, a property not shared by F.\nBuilding on the work of Pearlmutter (1994), Schraudolph (2002) proposed a 2nd-order method called \u201cStochastic Meta-descent\u201d (SMD) which uses an on-line 2nd-order approximation to f and optimizes it via updates to p which are also computed on-line. This method differs from HF in several important ways, but most critically in the way it optimizes p. The update scheme used by SMD is a form of preconditioned gradient-descent given by:\npn+1 = pn +M \u22121rn rn \u2261 \u2207f(\u03b8n) + Bpn\nwhere M is a diagonal pre-conditioning matrix chosen to approximate B. Using the previously discussed method for computing Bd products, SMD is able to compute these updates efficiently. However, using gradient-descent instead of CG to optimize q\u03b8(p), even with a good diagonal preconditioner, is an approach likely to fail because q\u03b8(p) will exhibit the same pathological curvature as the objective function f that it approximates. And pathological curvature was\nthe primary reason for abandoning gradient-descent as an optimization method in the first place. Moreover, it can be shown that in the batch case, the updates computed by SMD lie in the same Krylov subspace as those computed by an equal number of CG iterations, and that CG finds the optimal solution of q\u03b8(p) within this subspace.\nThese 2nd-order methods, plus others we haven\u2019t discussed, have only been validated on shallow nets and/or toy problems. And none of them have been shown to be fundamentally more effective than 1st-order optimization methods on deep learning problems."
        },
        {
            "heading": "7. Experiments",
            "text": "We present the results from a series of experiments designed to test the effectiveness of our HF approach on the deep auto-encoder problems considered by Hinton & Salakhutdinov (2006) (abbr. H&S). We adopt precisely the same model architectures, datasets, loss functions and training/test partitions that they did, so as to ensure that our results can be directly compared with theirs.\nEach dataset consists of a collection of small grey-scale images of various objects such as hand-written digits and faces. Table 1 summarizes the datasets and associated experimental parameters, where size gives the size of the training set, K gives the size of minibatches used, and encoder dims gives the encoder network architecture. In each case, the decoder architecture is the mirror image of the encoder, yielding a \u201csymmetric autoencoder\u201d. This symmetry is required in order to be able to apply H&S\u2019s pre-training approach. Note that CURVES is the synthetic curves dataset from H&S\u2019s paper and FACES is the augmented Olivetti face dataset.\nWe implemented our approach using the GPU-computing MATLAB package Jacket. We also re-implemented, using Jacket, the precise approach considered by H&S, using their provided code as a basis, and then re-ran their experiments using many more training epochs than they did, and for far longer than we ran our HF approach on the same models. With these extended runs we were able to obtain slightly better results than they reported for both the CURVES and MNIST experiments. Unfortunately, we were not able to reproduce their results for the FACES dataset, as each net we pre-trained had very high generalization error, even before fine-tuning. We ran each method until it either seemed to converge, or started to overfit (which happened for MNIST and FACES, but not CURVES). We found that since our method was much better at fitting the training data, it was thus more prone to\noverfitting, and so we ran additional experiments where we introduced an \u21132 prior on the connection weights.\nTable 2 summarizes our results, where PT+NCG is the pre-training + non-linear CG fine-tuning approach of H&S, RAND+HF is our Hessian-free method initialized randomly, and PT+HF is our approach initialized with pretrained parameters. The numbers given in each entry of the table are the average sum of squared reconstruction errors on the training-set and the test-set. The *\u2019s indicate that an \u21132 prior was used, with strength 10\u22124 on MNIST and 10\u22122 on FACES. Error numbers for FACES which involve pre-training are missing due to our failure to reproduce the results of H&S on that dataset (instead we just give the testerror number they reported).\nFigure 2 demonstrates the performance of our implementations on the CURVES dataset. Pre-training time is included where applicable. This plot is not meant to be a definitive performance analysis, but merely a demonstration that our method is indeed quite efficient."
        },
        {
            "heading": "8. Discussion of results and implications",
            "text": "The most important implication of our results is that learning in deep models can be achieved effectively and efficiently by a completely general optimizer without any need for pre-training. This opens the door to examining a diverse range of deep or otherwise difficult-to-optimize architectures for which there are no effective pre-training methods, such as asymmetric auto-encoders, or recurrent neural nets.\nA clear theme which emerges from our results is that the HF optimized nets have much lower training error, implying that our HF approach does well because it is more effective than pre-training + fine-tuning approaches at solving the under-fitting problem. Because both the MNIST and FACES experiments used early-stopping, the training error numbers reported in Table 2 are actually much higher than can otherwise be achieved. When we initialized randomly and didn\u2019t use early-stopping we obtained a training error of 1.40 on MNIST and 12.9 on FACES.\nA pre-trained initialization benefits our approach in terms of optimization speed and generalization error. However, there doesn\u2019t seem to be any significant benefit in regards to under-fitting, as our HF approach seems to solve this problem almost completely by itself. This is in stark contrast to the situation with 1st-order optimization algorithms, where the main hurdle overcome by pre-training is that of under-fitting, at least in the setting of auto-encoders.\nBased on these results we can hypothesize that the way pretraining helps 1st-order optimization algorithms overcome the under-fitting problem is by placing the parameters in a region less affected by issues of pathological curvature in the objective, such as those discussed in section 2.2. This would also explain why our HF approach optimizes faster from pre-trained parameters, as more favorable local curvature conditions allow the CG runs to make more rapid progress when optimizing q\u03b8(p).\nFinally, while these early results are very encouraging, clearly further research is warranted in order to address the many interesting questions that arise from them, such as how much more powerful are deep nets than shallow ones, and is this power fully exploited by current pretraining/fine-tuning schemes?"
        },
        {
            "heading": "Acknowledgments",
            "text": "The author would like to thank Geoffrey Hinton, Richard Zemel, Ilya Sutskever and Hugo Larochelle for their helpful suggestions. This work was supported by NSERC and the University of Toronto."
        }
    ],
    "title": "Deep learning via Hessian-free optimization",
    "year": 2014
}