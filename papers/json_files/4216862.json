{
    "abstractText": "Deep residual networks [1] have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which further makes training easy and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/ resnet-1k-layers.",
    "authors": [
        {
            "affiliations": [],
            "name": "Kaiming He"
        },
        {
            "affiliations": [],
            "name": "Xiangyu Zhang"
        },
        {
            "affiliations": [],
            "name": "Shaoqing Ren"
        },
        {
            "affiliations": [],
            "name": "Jian Sun"
        }
    ],
    "id": "SP:1d84b437462b30bb28087f5e491caa3207438fe4",
    "references": [
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "CVPR.",
            "year": 2016
        },
        {
            "authors": [
                "V. Nair",
                "G.E. Hinton"
            ],
            "title": "Rectified linear units improve restricted boltzmann machines",
            "venue": "ICML.",
            "year": 2010
        },
        {
            "authors": [
                "O. Russakovsky",
                "J. Deng",
                "H. Su",
                "J. Krause",
                "S. Satheesh",
                "S. Ma",
                "Z. Huang",
                "A. Karpathy",
                "A. Khosla",
                "M. Bernstein",
                "A.C. Berg",
                "L. Fei-Fei"
            ],
            "title": "ImageNet Large Scale Visual Recognition Challenge",
            "venue": "IJCV",
            "year": 2015
        },
        {
            "authors": [
                "T.Y. Lin",
                "M. Maire",
                "S. Belongie",
                "J. Hays",
                "P. Perona",
                "D. Ramanan",
                "P. Doll\u00e1r",
                "C.L. Zitnick"
            ],
            "title": "Microsoft COCO: Common objects in context",
            "venue": "ECCV.",
            "year": 2014
        },
        {
            "authors": [
                "S. Hochreiter",
                "J. Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural computation",
            "year": 1997
        },
        {
            "authors": [
                "R.K. Srivastava",
                "K. Greff",
                "J. Schmidhuber"
            ],
            "title": "Highway networks",
            "venue": "ICML workshop.",
            "year": 2015
        },
        {
            "authors": [
                "R.K. Srivastava",
                "K. Greff",
                "J. Schmidhuber"
            ],
            "title": "Training very deep networks",
            "venue": "NIPS.",
            "year": 2015
        },
        {
            "authors": [
                "S. Ioffe",
                "C. Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "ICML.",
            "year": 2015
        },
        {
            "authors": [
                "Y. LeCun",
                "B. Boser",
                "J.S. Denker",
                "D. Henderson",
                "R.E. Howard",
                "W. Hubbard",
                "L.D. Jackel"
            ],
            "title": "Backpropagation applied to handwritten zip code recognition",
            "venue": "Neural computation",
            "year": 1989
        },
        {
            "authors": [
                "A. Krizhevsky"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "venue": "Tech Report",
            "year": 2009
        },
        {
            "authors": [
                "G.E. Hinton",
                "N. Srivastava",
                "A. Krizhevsky",
                "I. Sutskever",
                "R.R. Salakhutdinov"
            ],
            "title": "Improving neural networks by preventing co-adaptation of feature detectors",
            "venue": "arXiv:1207.0580",
            "year": 2012
        },
        {
            "authors": [
                "D.A. Clevert",
                "T. Unterthiner",
                "S. Hochreiter"
            ],
            "title": "Fast and accurate deep network learning by exponential linear units (ELUs)",
            "venue": "ICLR.",
            "year": 2016
        },
        {
            "authors": [
                "M. Lin",
                "Q. Chen",
                "S. Yan"
            ],
            "title": "Network in network",
            "venue": "ICLR.",
            "year": 2014
        },
        {
            "authors": [
                "C.Y. Lee",
                "S. Xie",
                "P. Gallagher",
                "Z. Zhang",
                "Z. Tu"
            ],
            "title": "Deeply-supervised nets",
            "venue": "AISTATS.",
            "year": 2015
        },
        {
            "authors": [
                "A. Romero",
                "N. Ballas",
                "S.E. Kahou",
                "A. Chassang",
                "C. Gatta",
                "Y. Bengio"
            ],
            "title": "Fitnets: Hints for thin deep nets",
            "venue": "ICLR.",
            "year": 2015
        },
        {
            "authors": [
                "D. Mishkin",
                "J. Matas"
            ],
            "title": "All you need is a good init",
            "venue": "ICLR.",
            "year": 2016
        },
        {
            "authors": [
                "C. Szegedy",
                "V. Vanhoucke",
                "S. Ioffe",
                "J. Shlens",
                "Z. Wojna"
            ],
            "title": "Rethinking the inception architecture for computer vision",
            "venue": "CVPR.",
            "year": 2016
        },
        {
            "authors": [
                "C. Szegedy",
                "S. Ioffe",
                "V. Vanhoucke"
            ],
            "title": "Inception-v4, inception-resnet and the impact of residual connections on learning",
            "venue": "arXiv:1602.07261",
            "year": 2016
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification",
            "venue": "ICCV.",
            "year": 2015
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Deep residual networks (ResNets) [1] consist of many stacked \u201cResidual Units\u201d. Each unit (Fig. 1 (a)) can be expressed in a general form:\nyl = h(xl) + F(xl,Wl), xl+1 = f(yl),\nwhere xl and xl+1 are input and output of the l-th unit, and F is a residual function. In [1], h(xl) = xl is an identity mapping and f is a ReLU [2] function.\nResNets that are over 100-layer deep have shown state-of-the-art accuracy for several challenging recognition tasks on ImageNet [3] and MS COCO [4] competitions. The central idea of ResNets is to learn the additive residual function F with respect to h(xl), with a key choice of using an identity mapping h(xl) = xl. This is realized by attaching an identity skip connection (\u201cshortcut\u201d).\nIn this paper, we analyze deep residual networks by focusing on creating a \u201cdirect\u201d path for propagating information \u2014 not only within a residual unit, but through the entire network. Our derivations reveal that if both h(xl) and f(yl) are identity mappings, the signal could be directly propagated from one unit to any other units, in both forward and backward passes. Our experiments empirically show that training in general becomes easier when the architecture is closer to the above two conditions.\nTo understand the role of skip connections, we analyze and compare various types of h(xl). We find that the identity mapping h(xl) = xl chosen in [1] achieves the fastest error reduction and lowest training loss among all variants\nar X\niv :1\n60 3.\n05 02\n7v 2\n[ cs\n.C V\n] 1\n2 0 1 2 3 4 5 6\nx 10 4\n0\n5\n10\n15\n20\nIterations\nTest Erro r (% )\n0.002\n0.02\n0.2\n2\nTr ai\nn in\ng L\no ss\nResNet\u22121001, original (error: 7.61%) ResNet\u22121001, proposed (error: 4.92%)\nBN\nReLU\nweight\nBN\nweight\naddition\nReLU\nxl\nxl+1\n(a) original\nReLU\nweight\nBN\nReLU\nweight\nBN\naddition\nxl\nxl+1\n(b) proposed\nFigure 1. Left: (a) original Residual Unit in [1]; (b) proposed Residual Unit. The grey arrows indicate the easiest paths for the information to propagate, corresponding to the additive term \u201cxl\u201d in Eqn.(4) (forward propagation) and the additive term \u201c1\u201d in Eqn.(5) (backward propagation). Right: training curves on CIFAR-10 of 1001-layer ResNets. Solid lines denote test error (y-axis on the right), and dashed lines denote training loss (y-axis on the left). The proposed unit makes ResNet-1001 easier to train.\nwe investigated, whereas skip connections of scaling, gating [5,6,7], and 1\u00d71 convolutions all lead to higher training loss and error. These experiments suggest that keeping a \u201cclean\u201d information path (indicated by the grey arrows in Fig. 1, 2, and 4) is helpful for easing optimization.\nTo construct an identity mapping f(yl) = yl, we view the activation functions (ReLU and BN [8]) as \u201cpre-activation\u201d of the weight layers, in contrast to conventional wisdom of \u201cpost-activation\u201d. This point of view leads to a new residual unit design, shown in (Fig. 1(b)). Based on this unit, we present competitive results on CIFAR-10/100 with a 1001-layer ResNet, which is much easier to train and generalizes better than the original ResNet in [1]. We further report improved results on ImageNet using a 200-layer ResNet, for which the counterpart of [1] starts to overfit. These results suggest that there is much room to exploit the dimension of network depth, a key to the success of modern deep learning."
        },
        {
            "heading": "2 Analysis of Deep Residual Networks",
            "text": "The ResNets developed in [1] are modularized architectures that stack building blocks of the same connecting shape. In this paper we call these blocks \u201cResidual Units\u201d. The original Residual Unit in [1] performs the following computations:\nyl = h(xl) + F(xl,Wl), (1) xl+1 = f(yl). (2)\n3 Here xl is the input feature to the l-th Residual Unit. Wl = {Wl,k|1\u2264k\u2264K} is a set of weights (and biases) associated with the l-th Residual Unit, and K is the number of layers in a Residual Unit (K is 2 or 3 in [1]). F denotes the residual function, e.g., a stack of two 3\u00d73 convolutional layers in [1]. The function f is the operation after element-wise addition, and in [1] f is ReLU. The function h is set as an identity mapping: h(xl) = xl.\nIf f is also an identity mapping: xl+1 \u2261 yl, we can put Eqn.(2) into Eqn.(1) and obtain:\nxl+1 = xl + F(xl,Wl). (3)\nRecursively (xl+2 = xl+1 + F(xl+1,Wl+1) = xl + F(xl,Wl) + F(xl+1,Wl+1), etc.) we will have:\nxL = xl + L\u22121\u2211 i=l F(xi,Wi), (4)\nfor any deeper unit L and any shallower unit l. Eqn.(4) exhibits some nice properties. (i) The feature xL of any deeper unit L can be represented as the\nfeature xl of any shallower unit l plus a residual function in a form of \u2211L\u22121\ni=l F , indicating that the model is in a residual fashion between any units L and l. (ii)\nThe feature xL = x0 + \u2211L\u22121\ni=0 F(xi,Wi), of any deep unit L, is the summation of the outputs of all preceding residual functions (plus x0). This is in contrast to a \u201cplain network\u201d where a feature xL is a series of matrix-vector products, say,\u220fL\u22121\ni=0 Wix0. Eqn.(4) also leads to nice backward propagation properties. Denoting the\nloss function as E , from the chain rule of backpropagation [9] we have:\n\u2202E \u2202xl = \u2202E \u2202xL \u2202xL \u2202xl = \u2202E \u2202xL\n( 1 + \u2202\n\u2202xl L\u22121\u2211 i=l F(xi,Wi)\n) . (5)\nEqn.(5) indicates that the gradient \u2202E\u2202xl can be decomposed into two additive terms: a term of \u2202E\u2202xL that propagates information directly without concerning any weight layers, and another term of \u2202E \u2202xL ( \u2202 \u2202xl \u2211L\u22121 i=l F ) that propagates through the weight layers. The additive term of \u2202E\u2202xL ensures that information is directly propagated back to any shallower unit l. Eqn.(5) also suggests that it is unlikely for the gradient \u2202E\u2202xl to be canceled out, because in general the term \u2202 \u2202xl \u2211L\u22121 i=l F cannot be always -1 for all xl. This implies that the gradient of a layer does not vanish even when the weights are arbitrarily small.\nDiscussions\nEqn.(4) and Eqn.(5) suggest that the signal can be directly propagated from any unit to one another, both forwardly and backwardly. The foundation of Eqn.(4) is two identity mappings: (i) the identity skip connection h(xl) = xl, and (ii) the condition that f is an identity mapping.\n4 These directly propagated information flows are represented by the grey arrows in Fig. 1, 2, and 4. And the above two conditions are true when these grey arrows cover no operations (expect addition) and thus are \u201cclean\u201d. In the following two sections we separately investigate the impacts of the two conditions.\nIt is noteworthy that there are Residual Units for increasing dimensions and reducing feature map sizes [1], which do not strictly follow the above formulations. But as there are only a very few such units (two on CIFAR and three on ImageNet, depending on image sizes [1]), we expect that they affect little on degrading propagation. On the other hand, one may think of the above derivations as applied to all Residual Units within the same feature map size."
        },
        {
            "heading": "3 On the Importance of Identity Skip Connections",
            "text": "Let\u2019s consider a simple modification, h(xl) = \u03bblxl, to break the identity shortcut:\nxl+1 = \u03bblxl + F(xl,Wl), (6)\nwhere \u03bbl is a modulating scalar (for simplicity we still assume f is identity). Recursively applying this formulation we obtain an equation similar to Eqn. (4):\nxL = \u220fL\u22121 i=l \u03bbixl + \u2211L\u22121 i=l \u220fL\u22121 j=i+1 \u03bbjF(xi,Wi), or simply:\nxL = L\u22121\u220f i=l \u03bbixl + L\u22121\u2211 i=l F\u0302(xi,Wi), (7)\nwhere the notation F\u0302 absorbs the scalars into the residual functions. Similar to Eqn.(5), we have backpropagation of the following form:\n\u2202E \u2202xl = \u2202E \u2202xL ( L\u22121\u220f i=l \u03bbi + \u2202 \u2202xl L\u22121\u2211 i=l F\u0302(xi,Wi) ) . (8)\nUnlike Eqn.(5), in Eqn.(8) the first additive term is modulated by a factor\u220fL\u22121 i=l \u03bbi. For an extremely deep network (L is large), if \u03bbi > 1 for all i, this factor can be exponentially large; if \u03bbi < 1 for all i, this factor can be exponentially small and vanish, which blocks the backpropagated signal from the shortcut and forces it to flow through the weight layers. This results in optimization difficulties as we show by experiments.\nIn the above analysis, the original identity skip connection in Eqn.(3) is replaced with a simple scaling h(xl) = \u03bblxl. If the skip connection h(xl) represents more complicated transforms (such as gating and 1\u00d71 convolutions), in Eqn.(8) the first term becomes \u220fL\u22121 i=l h \u2032 i where h\n\u2032 is the derivative of h. This product may also impede information propagation and hamper the training procedure as witnessed in the following experiments.\n5 (f) dropout shortcut(e) conv shortcut 3x3 conv 3x3 conv addition ReLU 1x1 conv ReLU 3x3 conv 3x3 conv addition dropout ReLU ReLU (d) shortcut-only gating(c) exclusive gating 3x3 conv 3x3 conv addition 1x1 conv sigmoid 1- ReLU ReLU 3x3 conv 3x3 conv addition 1x1 conv sigmoid 1- ReLU ReLU (a) original (b) constant scaling 3x3 conv 3x3 conv addition ReLU ReLU 3x3 conv 3x3 conv addition 0.5 0.5 ReLU ReLU\nFigure 2. Various types of shortcut connections used in Table 1. The grey arrows indicate the easiest paths for the information to propagate. The shortcut connections in (b-f) are impeded by different components. For simplifying illustrations we do not display the BN layers, which are adopted right after the weight layers for all units here."
        },
        {
            "heading": "3.1 Experiments on Skip Connections",
            "text": "We experiment with the 110-layer ResNet as presented in [1] on CIFAR-10 [10]. This extremely deep ResNet-110 has 54 two-layer Residual Units (consisting of 3\u00d73 convolutional layers) and is challenging for optimization. Our implementation details (see appendix) are the same as [1]. Throughout this paper we report the median accuracy of 5 runs for each architecture on CIFAR, reducing the impacts of random variations.\nThough our above analysis is driven by identity f , the experiments in this section are all based on f = ReLU as in [1]; we address identity f in the next section. Our baseline ResNet-110 has 6.61% error on the test set. The comparisons of other variants (Fig. 2 and Table 1) are summarized as follows:\nConstant scaling. We set \u03bb = 0.5 for all shortcuts (Fig. 2(b)). We further study two cases of scaling F : (i) F is not scaled; or (ii) F is scaled by a constant scalar of 1\u2212\u03bb = 0.5, which is similar to the highway gating [6,7] but with frozen gates. The former case does not converge well; the latter is able to converge, but the test error (Table 1, 12.35%) is substantially higher than the original ResNet-110. Fig 3(a) shows that the training error is higher than that of the original ResNet-110, suggesting that the optimization has difficulties when the shortcut signal is scaled down.\n6\nExclusive gating. Following the Highway Networks [6,7] that adopt a gating mechanism [5], we consider a gating function g(x) = \u03c3(Wgx + bg) where a transform is represented by weights Wg and biases bg followed by the sigmoid function \u03c3(x) = 11+e\u2212x . In a convolutional network g(x) is realized by a 1\u00d71 convolutional layer. The gating function modulates the signal by element-wise multiplication.\nWe investigate the \u201cexclusive\u201d gates as used in [6,7] \u2014 the F path is scaled by g(x) and the shortcut path is scaled by 1\u2212g(x). See Fig 2(c). We find that the initialization of the biases bg is critical for training gated models, and following the guidelines1 in [6,7], we conduct hyper-parameter search on the initial value of bg in the range of 0 to -10 with a decrement step of -1 on the training set by crossvalidation. The best value (\u22126 here) is then used for training on the training set, leading to a test result of 8.70% (Table 1), which still lags far behind the ResNet-110 baseline. Fig 3(b) shows the training curves. Table 1 also reports the results of using other initialized values, noting that the exclusive gating network does not converge to a good solution when bg is not appropriately initialized.\nThe impact of the exclusive gating mechanism is two-fold. When 1 \u2212 g(x) approaches 1, the gated shortcut connections are closer to identity which helps information propagation; but in this case g(x) approaches 0 and suppresses the function F . To isolate the effects of the gating functions on the shortcut path alone, we investigate a non-exclusive gating mechanism in the next.\nShortcut-only gating. In this case the function F is not scaled; only the shortcut path is gated by 1\u2212g(x). See Fig 2(d). The initialized value of bg is still essential in this case. When the initialized bg is 0 (so initially the expectation of 1 \u2212 g(x) is 0.5), the network converges to a poor result of 12.86% (Table 1). This is also caused by higher training error (Fig 3(c)).\n1 See also: people.idsia.ch/~rupesh/very_deep_learning/ by [6,7].\n7 0 1 2 3 4 5 6\nx 10 4\n0\n5\n10\n15\n20\nIterations\nTest Erro r (% )\n0.002\n0.02\n0.2\n2\nTr ai\nn in\ng L\no ss\n110, original 110, shortcut\u2212only gating (init b=0)\n0 1 2 3 4 5 6\nx 10 4\n0\n5\n10\n15\n20\nIterations\nTest Erro r (% )\n0.002\n0.02\n0.2\n2\nTr ai\nn in\ng L\no ss\n110, original 110, 1x1 conv shortcut\n(d)(c)\n(a) (b)\n0 1 2 3 4 5 6\nx 10 4\n0\n5\n10\n15\n20\nIterations\nTest Erro r (% )\n0.002\n0.02\n0.2 2 Tr ai n in g L o ss\n110, original 110, const scaling (0.5, 0.5)\n0 1 2 3 4 5 6\nx 10 4\n0\n5\n10\n15\n20\nIterations\nTest Erro r (% )\n0.002\n0.02\n0.2\n2\nTr ai\nn in\ng L\no ss\n110, original 110, exclusive gating (init b=\u22126)\nFigure 3. Training curves on CIFAR-10 of various shortcuts. Solid lines denote test error (y-axis on the right), and dashed lines denote training loss (y-axis on the left).\nWhen the initialized bg is very negatively biased (e.g., \u22126), the value of 1\u2212g(x) is closer to 1 and the shortcut connection is nearly an identity mapping. As such, the result (6.91%, Table 1) is much closer to the ResNet-110 baseline.\n1\u00d71 convolutional shortcut. Next we experiment with 1\u00d71 convolutional shortcut connections that replace the identity. This option has been investigated in [1] (known as option C) on a 34-layer ResNet (16 Residual Units) and shows good results, suggesting that 1\u00d71 shortcut connections could be useful. But we find that this is not the case when there are many Residual Units. The 110-layer ResNet has a poorer result (12.22%, Table 1) when using 1\u00d71 convolutional shortcuts. Again, the training error becomes higher (Fig 3(d)). When stacking so many Residual Units (54 for ResNet-110), even the shortest path may still impede signal propagation. We witnessed similar phenomena on ImageNet with ResNet-101 when using 1\u00d71 convolutional shortcuts.\nDropout shortcut. Last we experiment with dropout [11] (at a ratio of 0.5) which we adopt on the output of the identity shortcut (Fig. 2(f)). The network fails to converge to a good solution. Dropout statistically imposes a scale of \u03bb with an expectation of 0.5 on the shortcut, and similar to constant scaling by 0.5, it impedes signal propagation.\n8"
        },
        {
            "heading": "3.2 Discussions",
            "text": "As indicated by the grey arrows in Fig. 2, the shortcut connections are the most direct paths for the information to propagate. Multiplicative manipulations (scaling, gating, 1\u00d71 convolutions, and dropout) on the shortcuts can hamper information propagation and lead to optimization problems.\nIt is noteworthy that the gating and 1\u00d71 convolutional shortcuts introduce more parameters, and should have stronger representational abilities than identity shortcuts. In fact, the shortcut-only gating and 1\u00d71 convolution cover the solution space of identity shortcuts (i.e., they could be optimized as identity shortcuts). However, the degradation of these complex shortcut models is caused by optimization issues, instead of representational abilities."
        },
        {
            "heading": "4 On the Usage of Activation Functions",
            "text": "Experiments in the above section are consistent with the analysis in Eqn.(5) and Eqn.(8), both being derived under the assumption that the after-addition\n9 activation f is the identity mapping. But in the above experiments f is ReLU as designed in [1]. Next we investigate the impact of f .\nWe want to make f an identity mapping, which is done by re-arranging the activation functions (ReLU and/or BN). The original Residual Unit in [1] has a shape in Fig. 4(a) \u2014 BN is used after each weight layer, and ReLU is adopted after BN except that the last ReLU in a Residual Unit is after elementwise addition (f = ReLU). Fig. 4(b-e) show the alternatives we investigated, explained as following."
        },
        {
            "heading": "4.1 Experiments on Activation",
            "text": "In this section we experiment with ResNet-110 and a 164-layer Bottleneck [1] architecture (denoted as ResNet-164). A bottleneck Residual Unit consist of a 1\u00d71 layer for reducing dimension, a 3\u00d73 layer, and a 1\u00d71 layer for storing dimension. As designed in [1], its computational complexity is similar to the two-3\u00d73 Residual Unit. More details are in the appendix. The baseline ResNet164 has a competitive result of 5.93% on CIFAR-10 (Table 2).\nBN after addition. Before turning f into an identity mapping, we go the opposite way by adopting BN after addition (Fig. 4(b)). In this case f involves BN and ReLU. The results become considerably worse than the baseline (Table 2). Unlike the original design, now the BN layer alters the signal that passes through the shortcut and impedes information propagation, as reflected by the difficulties on reducing training loss at the beginning of training (Fib. 6 left).\nReLU before addition. A na\u0308\u0131ve choice of making f into an identity mapping is to move the ReLU before addition (Fig. 4(c)). However, this leads to a non-negative output from the transform F , while intuitively a \u201cresidual\u201d function should take values in (\u2212\u221e,+\u221e). As a result, the forward propagated signal is monotonically increasing. This may impact the representational ability, and the result is worse (7.84%, Table 2) than the baseline. We expect to have a residual function taking values in (\u2212\u221e,+\u221e). This condition is satisfied by other Residual Units including the following ones.\nPost-activation or pre-activation? In the original design (Eqn.(1) and Eqn.(2)), the activation xl+1 = f(yl) affects both paths in the next Residual Unit: yl+1 = f(yl) + F(f(yl),Wl+1). Next we develop an asymmetric form where an activation f\u0302 only affects the F path: yl+1 = yl + F(f\u0302(yl),Wl+1), for any l (Fig. 5 (a) to (b)). By renaming the notations, we have the following form:\nxl+1 = xl + F(f\u0302(xl),Wl), . (9)\nIt is easy to see that Eqn.(9) is similar to Eqn.(4), and can enable a backward formulation similar to Eqn.(5). For this new Residual Unit as in Eqn.(9), the new after-addition activation becomes an identity mapping. This design means that if a new after-addition activation f\u0302 is asymmetrically adopted, it is equivalent to recasting f\u0302 as the pre-activation of the next Residual Unit. This is illustrated in Fig. 5.\n10\n...\n...\n...\nThe distinction between post-activation/pre-activation is caused by the presence of the element-wise addition. For a plain network that has N layers, there are N \u2212 1 activations (BN/ReLU), and it does not matter whether we think of them as post- or pre-activations. But for branched layers merged by addition, the position of activation matters.\nWe experiment with two such designs: (i) ReLU-only pre-activation (Fig. 4(d)), and (ii) full pre-activation (Fig. 4(e)) where BN and ReLU are both adopted before weight layers. Table 2 shows that the ReLU-only pre-activation performs very similar to the baseline on ResNet-110/164. This ReLU layer is not used in conjunction with a BN layer, and may not enjoy the benefits of BN [8].\nSomehow surprisingly, when BN and ReLU are both used as pre-activation, the results are improved by healthy margins (Table 2 and Table 3). In Table 3 we report results using various architectures: (i) ResNet-110, (ii) ResNet-164, (iii) a 110-layer ResNet architecture in which each shortcut skips only 1 layer (i.e.,\n11\na Residual Unit has only 1 layer), denoted as \u201cResNet-110(1layer)\u201d, and (iv) a 1001-layer bottleneck architecture that has 333 Residual Units (111 on each feature map size), denoted as \u201cResNet-1001\u201d. We also experiment on CIFAR100. Table 3 shows that our \u201cpre-activation\u201d models are consistently better than the baseline counterparts. We analyze these results in the following."
        },
        {
            "heading": "4.2 Analysis",
            "text": "We find that the impact of pre-activation is twofold. First, the optimization is further eased (comparing with the baseline ResNet) because f is an identity mapping. Second, using BN as pre-activation improves regularization of the models.\nEase of optimization. This effect is particularly obvious when training the 1001-layer ResNet. Fig. 1 shows the curves. Using the original design in [1], the training error is reduced very slowly at the beginning of training. For f = ReLU, the signal is impacted if it is negative, and when there are many Residual Units, this effect becomes prominent and Eqn.(3) (so Eqn.(5)) is not a good approximation. On the contrary, when f is an identity mapping, the signal can be propagated directly between any two units. Our 1001-layer network reduces the training loss very quickly (Fig. 1). It also achieves the lowest loss among all models we investigated, suggesting the success of optimization.\nWe also find that the impact of f = ReLU is not severe when the ResNet has fewer layers (e.g., 164 in Fig. 6(right)). The training curve seems to suffer a little bit at the beginning of training, but goes into a healthy status soon. By monitoring the responses we observe that this is because after some training, the weights are adjusted into a status such that yl in Eqn.(1) is more frequently above zero and f does not truncate it (xl is always non-negative due to the previous ReLU, so yl is below zero only when the magnitude of F is very negative). The truncation, however, is more frequent when there are 1000 layers.\n12\nReducing overfitting. Another impact of using the proposed pre-activation unit is on regularization, as shown in Fig. 6 (right). The pre-activation version reaches slightly higher training loss at convergence, but produces lower test error. This phenomenon is observed on ResNet-110, ResNet-110(1-layer), and ResNet-164 on both CIFAR-10 and 100. This is presumably caused by BN\u2019s regularization effect [8]. In the original Residual Unit (Fig. 4(a)), although the BN normalizes the signal, this is soon added to the shortcut and thus the merged signal is not normalized. This unnormalized signal is then used as the input of the next weight layer. On the contrary, in our pre-activation version, the inputs to all weight layers have been normalized."
        },
        {
            "heading": "4.3 Comparisons with state-of-the-art results on CIFAR-10/100",
            "text": "Table 4 compares the state-of-the-art methods on CIFAR-10/100, where we achieve competitive results. We remark that we do not specially tailor the network filter sizes/numbers, nor use regularization techniques (such as dropout) which are very effective for these small datasets. We obtain these results via a simple but essential concept \u2014 going deeper. These results demonstrate the potential of pushing the limits of depth."
        },
        {
            "heading": "4.4 Experiments on ImageNet",
            "text": "Lastly we report experimental results on the 1000-class ImageNet dataset [3]. We have done preliminary experiments using the skip connections studied in Fig. 3 on ImageNet with ResNet-101 [1] , and observed similar optimization difficulties. The training error of these non-identity shortcut networks is obviously higher than the original ResNet at the first learning rate (similar to Fig. 3), and we\n13\ndecided to halt training due to limited resources. Nevertheless, we did finish a \u201cBN after addition\u201d version (Fig. 4(b)) of ResNet-101 on ImageNet and observed higher training loss and validation error. This model\u2019s single-crop (224\u00d7224) validation error is 24.6%/7.5%, vs. the original ResNet-101\u2019s 23.6%/7.1%2. This comparison is in line with CIFAR\u2019s results in Fig. 6 (left).\nTable 5 shows the results of ResNet-152 [1] and ResNet-2003, all trained from scratch. We notice that the original ResNet paper [1] trained the models using scale jittering with shorter side s \u2208 [256, 480], and so the test of a 224\u00d7224 crop on s = 256 (as did in [1]) is negatively biased. Instead, we test a single 320\u00d7320 crop from s = 320, for all original and our ResNets. Even though the ResNets are trained on smaller crops, they can be easily tested on larger crops because the ResNets are fully convolutional by design. This size is also close to 299\u00d7299 used by Inception v3 [17], allowing a fairer comparison.\nThe original ResNet-152 [1] has top-1 error of 21.3% on a 320\u00d7320 crop, and our pre-activation counterpart has 21.1%. The gain is not big on ResNet-152 because this model has not shown severe generalization difficulties. However, the original ResNet-200 has an error rate of 21.8%, higher than the baseline ResNet-152. On the other hand, the original ResNet-200 has lower training loss than ResNet-152, suggesting that it starts to overfit.\nOur pre-activation ResNet-200 has an error rate of 20.7%, which is 1.1% lower than the baseline ResNet-200 and also lower than the two versions of ResNet-152. The result of our ResNet-200 compares favorably to Inception v3 (Table 5). Concurrent with our work, an Inception-ResNet-v2 model [18] achieves a single-crop result of 19.9%/4.9%. We expect our observations and the proposed Residual Unit will help this type and generally other types of ResNets."
        },
        {
            "heading": "5 Conclusions",
            "text": "This paper investigates the propagation formulations behind the connection mechanisms of deep residual networks. Our derivations imply that identity shortcut connections and identity after-addition activation are essential for making\n2 https://github.com/KaimingHe/deep-residual-networks 3 The ResNet-200 has 16 more 3-layer bottleneck Residual Units than ResNet-152,\nwhich are added on the feature map of 28\u00d728.\n14\ninformation propagation smooth. Ablation experiments demonstrate phenomena that are consistent with our derivations. We also present 1000-layer deep networks that can be easily trained and achieve improved accuracy.\nAppendix: Implementation Details\nThe implementation details and hyper-parameters are the same as those in [1]. Specifically, on CIFAR we use only the translation and flipping augmentation in [1] for training. The learning rate starts from 0.1, and is divided by 10 at 32k and 48k iterations. Following [1], for all CIFAR experiments we warm up the training by using a smaller learning rate of 0.01 at the beginning 400 iterations and go back to 0.1 after that, although we remark that this is not necessary for our proposed Residual Unit. The mini-batch size is 128 on 2 GPUs (64 each), the weight decay is 0.0001, the momentum is 0.9, and the weights are initialized as in [19].\nOn ImageNet, we train the models using the same data augmentation as in [1]. The learning rate starts from 0.1 (no warming up), and is divided by 10 at 30 and 60 epochs. The mini-batch size is 256 on 8 GPUs (32 each). The weight decay, momentum, and weight initialization are the same as above.\nWhen using the pre-activation Residual Units (Fig. 4(d)(e) and Fig. 5), we pay special attention to the first and the last Residual Units of the entire network. For the first Residual Unit (that follows a stand-alone convolutional layer, conv1), we adopt the first activation right after conv1 and before splitting into two paths; for the last Residual Unit (followed by average pooling and a fullyconnected classifier), we adopt an extra activation right after its element-wise addition. These two special cases are the natural outcome when we obtain the pre-activation network via the modification procedure as shown in Fig. 5.\nThe bottleneck Residual Units (for ResNet-164/1001 on CIFAR) are constructed following [1]. For example, a [ 3\u00d73, 16 3\u00d73, 16 ] unit in ResNet-110 is replaced\nwith a  1\u00d71, 163\u00d73, 16\n1\u00d71, 64  unit in ResNet-164, both of which have roughly the same number of parameters. For the bottleneck ResNets, when reducing the feature map size we use projection shortcuts [1] for increasing dimensions, and when preactivation is used, these projection shortcuts are also with pre-activation."
        }
    ],
    "title": "Identity Mappings in Deep Residual Networks",
    "year": 2024
}