{
    "abstractText": "This paper presents a Neural Aggregation Network (NAN) for video face recognition. The network takes a face video or face image set of a person with a variable number of face images as its input, and produces a compact, fixed-dimension feature representation for recognition. The whole network is composed of two modules. The feature embedding module is a deep Convolutional Neural Network (CNN) which maps each face image to a feature vector. The aggregation module consists of two attention blocks which adaptively aggregate the feature vectors to form a single feature inside the convex hull spanned by them. Due to the attention mechanism, the aggregation is invariant to the image order. Our NAN is trained with a standard classification or verification loss without any extra supervision signal, and we found that it automatically learns to advocate high-quality face images while repelling low-quality ones such as blurred, occluded and improperly exposed faces. The experiments on IJB-A, YouTube Face, Celebrity-1000 video face recognition benchmarks show that it consistently outperforms naive aggregation methods and achieves the state-of-the-art accuracy.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiaolong Yang"
        },
        {
            "affiliations": [],
            "name": "Peiran Ren"
        },
        {
            "affiliations": [],
            "name": "Dongqing Zhang"
        },
        {
            "affiliations": [],
            "name": "Dong Chen"
        },
        {
            "affiliations": [],
            "name": "Fang Wen"
        },
        {
            "affiliations": [],
            "name": "Hongdong Li"
        },
        {
            "affiliations": [],
            "name": "Gang Hua"
        }
    ],
    "id": "SP:50316ccec5021aa0a4ec3b89c50bf360452bd837",
    "references": [
        {
            "authors": [
                "W. AbdAlmageed",
                "Y. Wu",
                "S. Rawls",
                "S. Harel",
                "T. Hassner",
                "I. Masi",
                "J. Choi",
                "J. Lekust",
                "J. Kim",
                "P. Natarajan"
            ],
            "title": "Face recognition using deep multi-pose representations",
            "venue": "In IEEE Winter Conference on Applications of Computer Vision (WACV),",
            "year": 2016
        },
        {
            "authors": [
                "O. Arandjelovic",
                "G. Shakhnarovich",
                "J. Fisher",
                "R. Cipolla",
                "T. Darrell"
            ],
            "title": "Face recognition with image sets using manifold density divergence",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2005
        },
        {
            "authors": [
                "H. Cevikalp",
                "B. Triggs"
            ],
            "title": "Face recognition based on image sets",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2010
        },
        {
            "authors": [
                "D. Chen",
                "G. Hua",
                "F. Wen",
                "J. Sun"
            ],
            "title": "Supervised transformer network for efficient face detection",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2016
        },
        {
            "authors": [
                "D. Chen",
                "S. Ren",
                "Y. Wei",
                "X. Cao",
                "J. Sun"
            ],
            "title": "Joint cascade face detection and alignment",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2014
        },
        {
            "authors": [
                "J.-C. Chen",
                "V.M. Patel",
                "R. Chellappa"
            ],
            "title": "Unconstrained face verification using deep cnn features",
            "venue": "In IEEE Winter Conference on Applications of Computer Vision (WACV),",
            "year": 2016
        },
        {
            "authors": [
                "J.-C. Chen",
                "R. Ranjan",
                "A. Kumar",
                "C.-H. Chen",
                "V. Patel",
                "R. Chellappa"
            ],
            "title": "An end-to-end system for unconstrained face verification with deep convolutional neural networks",
            "venue": "In IEEE International Conference on Computer Vision Workshops,",
            "year": 2015
        },
        {
            "authors": [
                "S. Chopra",
                "R. Hadsell",
                "Y. LeCun"
            ],
            "title": "Learning a similarity metric discriminatively, with application to face verification",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2005
        },
        {
            "authors": [
                "A.R. Chowdhury",
                "T.-Y. Lin",
                "S. Maji",
                "E. Learned- Miller"
            ],
            "title": "One-to-many face recognition with bilinear cnns",
            "venue": "In IEEE Winter Conference on Applications of Computer Vision (WACV),",
            "year": 2016
        },
        {
            "authors": [
                "N. Crosswhite",
                "J. Byrne",
                "O.M. Parkhi",
                "C. Stauffer",
                "Q. Cao",
                "A. Zisserman"
            ],
            "title": "Template adaptation for face verification and identification",
            "venue": "arXiv preprint arXiv:1603.03958,",
            "year": 2016
        },
        {
            "authors": [
                "Z. Cui",
                "W. Li",
                "D. Xu",
                "S. Shan",
                "X. Chen"
            ],
            "title": "Fusing robust face region descriptors via multiple metric learning for face recognition in the wild",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2013
        },
        {
            "authors": [
                "A. Graves",
                "G. Wayne",
                "I. Danihelka"
            ],
            "title": "Neural turing machines",
            "venue": "CoRR, abs/1410.5401,",
            "year": 2014
        },
        {
            "authors": [
                "P. Grother",
                "M. Ngan"
            ],
            "title": "Face recognition vendor test (frvt) performance of face identification algorithms. In Technical Report NIST IR 8009",
            "venue": "National Institute of Standards and Technology,",
            "year": 2014
        },
        {
            "authors": [
                "R. Hadsell",
                "S. Chopra",
                "Y. LeCun"
            ],
            "title": "Dimensionality reduction by learning an invariant mapping",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2006
        },
        {
            "authors": [
                "J. Hu",
                "J. Lu",
                "Y.-P. Tan"
            ],
            "title": "Discriminative deep metric learning for face verification in the wild",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2014
        },
        {
            "authors": [
                "J. Hu",
                "J. Lu",
                "J. Yuan",
                "Y.-P. Tan"
            ],
            "title": "Large margin multimetric learning for face and kinship verification in the wild",
            "venue": "In Asian Conference on Computer Vision (ACCV),",
            "year": 2014
        },
        {
            "authors": [
                "S. Ioffe",
                "C. Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "arXiv preprint arXiv:1502.03167,",
            "year": 2015
        },
        {
            "authors": [
                "T.-K. Kim",
                "O. Arandjelovi\u0107",
                "R. Cipolla"
            ],
            "title": "Boosted manifold principal angles for image set-based recognition",
            "venue": "Pattern Recognition,",
            "year": 2007
        },
        {
            "authors": [
                "B.F. Klare",
                "B. Klein",
                "E. Taborsky",
                "A. Blanton",
                "J. Cheney",
                "K. Allen",
                "P. Grother",
                "A. Mah",
                "M. Burge",
                "A.K. Jain"
            ],
            "title": "Pushing the frontiers of unconstrained face detection and recognition: Iarpa janus benchmark a",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2015
        },
        {
            "authors": [
                "K.-C. Lee",
                "J. Ho",
                "M.-H. Yang",
                "D. Kriegman"
            ],
            "title": "Videobased face recognition using probabilistic appearance manifolds",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2003
        },
        {
            "authors": [
                "H. Li",
                "G. Hua",
                "Z. Lin",
                "J. Brandt",
                "J. Yang"
            ],
            "title": "Probabilistic elastic matching for pose variant face verification",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2013
        },
        {
            "authors": [
                "H. Li",
                "G. Hua",
                "X. Shen",
                "Z. Lin",
                "J. Brandt"
            ],
            "title": "Eigen-PEP for video face recognition",
            "venue": "In Asian Conference on Computer Vision (ACCV),",
            "year": 2014
        },
        {
            "authors": [
                "L. Liu",
                "L. Zhang",
                "H. Liu",
                "S. Yan"
            ],
            "title": "Toward largepopulation face identification in unconstrained videos",
            "venue": "IEEE Transactions on Circuits and Systems for Video Technology,",
            "year": 2014
        },
        {
            "authors": [
                "I. Masi",
                "S. Rawls",
                "G. Medioni",
                "P. Natarajan"
            ],
            "title": "Pose-aware face recognition in the wild",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "I. Masi",
                "A.T. a. Tr\u00e3n",
                "J. Toy Leksut",
                "T. Hassner",
                "G. Medioni"
            ],
            "title": "Do we really need to collect millions of faces for effective face recognition",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2016
        },
        {
            "authors": [
                "H. Mendez-Vazquez",
                "Y. Martinez-Diaz",
                "Z. Chai"
            ],
            "title": "Volume structured ordinal features with background similarity measure for video face recognition",
            "venue": "In International Conference on Biometrics (ICB),",
            "year": 2013
        },
        {
            "authors": [
                "O.M. Parkhi",
                "K. Simonyan",
                "A. Vedaldi",
                "A. Zisserman"
            ],
            "title": "A compact and discriminative face track descriptor",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2014
        },
        {
            "authors": [
                "O.M. Parkhi",
                "A. Vedaldi",
                "A. Zisserman"
            ],
            "title": "Deep face recognition",
            "venue": "In British Machine Vision Conference (BMVC),",
            "year": 2015
        },
        {
            "authors": [
                "S. Ren",
                "X. Cao",
                "Y. Wei",
                "J. Sun"
            ],
            "title": "Face alignment at 3000 fps via regressing local binary features",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2014
        },
        {
            "authors": [
                "S. Sankaranarayanan",
                "A. Alavi",
                "C. Castillo",
                "R. Chellappa"
            ],
            "title": "Triplet probabilistic embedding for face verification and clustering",
            "venue": "arXiv preprint arXiv:1604.05417,",
            "year": 2016
        },
        {
            "authors": [
                "F. Schroff",
                "D. Kalenichenko",
                "J. Philbin"
            ],
            "title": "FaceNet: A unified embedding for face recognition and clustering",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2015
        },
        {
            "authors": [
                "S. Sukhbaatar",
                "J. Weston",
                "R. Fergus"
            ],
            "title": "End-to-end memory networks",
            "venue": "In Advances in Neural Information Processing Systems (NIPS),",
            "year": 2015
        },
        {
            "authors": [
                "Y. Sun",
                "X. Wang",
                "X. Tang"
            ],
            "title": "Deeply learned face representations are sparse, selective, and robust",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2015
        },
        {
            "authors": [
                "C. Szegedy",
                "W. Liu",
                "Y. Jia",
                "P. Sermanet",
                "S. Reed",
                "D. Anguelov",
                "D. Erhan",
                "V. Vanhoucke",
                "A. Rabinovich"
            ],
            "title": "Going deeper with convolutions",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2015
        },
        {
            "authors": [
                "Y. Taigman",
                "M. Yang",
                "M. Ranzato",
                "L. Wolf"
            ],
            "title": "DeepFace: Closing the gap to human-level performance in face verification",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2014
        },
        {
            "authors": [
                "T. Tieleman",
                "G. Hinton"
            ],
            "title": "RMSProp: Divide the gradient by a running average of its recent magnitude",
            "venue": "Technical report,",
            "year": 2012
        },
        {
            "authors": [
                "P. Turaga",
                "A. Veeraraghavan",
                "A. Srivastava",
                "R. Chellappa"
            ],
            "title": "Statistical computations on grassmann and stiefel manifolds for image and video-based recognition",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2011
        },
        {
            "authors": [
                "O. Vinyals",
                "S. Bengio",
                "M. Kudlur"
            ],
            "title": "Order matters: sequence to sequence for sets",
            "venue": "In International Conference on Learning Representation,",
            "year": 2016
        },
        {
            "authors": [
                "D. Wang",
                "C. Otto",
                "A.K. Jain"
            ],
            "title": "Face search at scale: 80 million gallery",
            "venue": "arXiv preprint arXiv:1507.07242,",
            "year": 2015
        },
        {
            "authors": [
                "R. Wang",
                "S. Shan",
                "X. Chen",
                "W. Gao"
            ],
            "title": "Manifold-manifold distance with application to face recognition based on image set",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2008
        },
        {
            "authors": [
                "Y. Wen",
                "K. Zhang",
                "Z. Li",
                "Y. Qiao"
            ],
            "title": "A discriminative feature learning approach for deep face recognition",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2016
        },
        {
            "authors": [
                "L. Wolf",
                "T. Hassner",
                "I. Maoz"
            ],
            "title": "Face recognition in unconstrained videos with matched background similarity",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2011
        },
        {
            "authors": [
                "L. Wolf",
                "N. Levy"
            ],
            "title": "The SVM-Minus similarity score for video face recognition",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2013
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Video face recognition has caught more and more attention from the community in recent years [42, 21, 43, 11, 26, 22, 23, 27, 15, 35, 31, 10]. Compared to image-based face recognition, more information of the subjects can be exploited from the input videos, which naturally incorporate faces of the same subject in varying poses and illumination conditions. The key issue in video face recognition is to build an appropriate representation of the video face, such that it can effectively integrate the information across different frames together, maintaining beneficial while discarding noisy information.\n\u2217Part of this work was done when J. Yang was an intern at MSR supervised by G. Hua.\nOne naive approach would be representing a video face as a set of frame-level face features such as those extracted from deep neural networks [35, 31], which have dominated face recognition recently [35, 28, 33, 31, 24, 41]. Such a representation comprehensively maintains the information across all frames. However, to compare two video faces, one needs to fuse the matching results across all pairs of frames between the two face videos. Let n be the average number of video frames, then the computational complexity is O(n2) per match operation, which is not desirable especially for large-scale recognition. Besides, such a setbased representation would incur O(n) space complexity per video face example, which demands a lot of memory storage and confronts efficient indexing.\nWe argue that it is more desirable to come with a compact, fixed-size feature representation at the video level, irrespective of the varied length of the videos. Such a representation would allow direct, constant-time computation of the similarity or distance without the need for frame-to-frame matching. A straightforward solution might be extracting a feature at each frame and then conducting a certain type of pooling to aggregate the frame-level features together to form a video-level representation.\n1\nar X\niv :1\n60 3.\n05 47\n4v 4\n[ cs\n.C V\n] 2\nA ug\n2 01\nThe most commonly adopted pooling strategies may be average and max pooling [28, 22, 7, 9]. While these naive pooling strategies were shown to be effective in the previous works, we believe that a good pooling or aggregation strategy should adaptively weigh and combine the framelevel features across all frames. The intuition is simple: a video (especially a long video sequence) or an image set may contain face images captured at various conditions of lighting, resolution, head pose etc., and a smart algorithm should favor face images that are more discriminative (or more \u201cmemorizable\u201d) and prevent poor face images from jeopardizing the recognition.\nTo this end, we look for an adaptive weighting scheme to linearly combine all frame-level features from a video together to form a compact and discriminative face representation. Different from the previous methods, we neither fix the weights nor rely on any particular heuristics to set them. Instead, we designed a neural network to adaptively calculate the weights. We named our network the Neural Aggregation Network (NAN), whose coefficients can be trained through supervised learning in a normal face recognition training task without the need for extra supervision signals.\nThe proposed NAN is composed of two major modules that could be trained end-to-end or one by one separately. The first one is a feature embedding module which serves as a frame-level feature extractor using a deep CNN model. The other is the aggregation module that adaptively fuses the feature vectors of all the video frames together.\nOur neural aggregation network is designed to inherit the main advantages of pooling techniques, including the ability to handle arbitrary input size and producing orderinvariant representations. The key component of this network is inspired by the Neural Turing Machine [12] and the work of [38], both of which applied an attention mechanism to organize the input through accessing an external memory. This mechanism can take an input of arbitrary size and work as a tailor emphasizing or suppressing each input element just via a weighted averaging, and very importantly it is order independent and has trainable parameters. In this work, we design a simple network structure of two cascaded attention blocks associated with this attention mechanism for face feature aggregation.\nApart from building a video-level representation, the neural aggregation network can also serve as a subject level feature extractor to fuse multiple data sources. For example, one can feed it with all available images and videos, or the aggregated video-level features of multiple videos from the same subject, to obtain a single feature representation with fixed size. In this way, the face recognition system not only enjoys the time and memory efficiency due to the compact representation, but also exhibits superior performance, as we will show in our experiments.\nWe evaluated the proposed NAN for both the tasks of\nvideo face verification and identification. We observed consistent margins in three challenging datasets, including the YouTube Face dataset [42], the IJB-A dataset [19], and the Celebrity-1000 dataset [23], compared to the baseline strategies and other competing methods.\nLast but not least, we shall point out that our proposed NAN can serve as a general framework for learning contentadaptive pooling. Therefore, it may also serve as a feature aggregation scheme for other computer vision tasks."
        },
        {
            "heading": "1.1. Related Works",
            "text": "Face recognition based on videos or image sets has been actively studied in the past. This paper is concerned with the input being an orderless set of face images. Existing methods exploiting temporal dynamics will not be considered here. For set based face recognition, many previous methods have attempted to represent the set of face images with appearance subspaces or manifolds and perform recognition via computing manifold similarity or distance [20, 2, 18, 40, 37]. These traditional methods may work well under constrained settings but usually cannot handle the challenging unconstrained scenarios where large appearance variations are present.\nAlong a different axis, some methods build video feature representation based on local features [21, 22, 27]. For example, the PEP methods [21, 22] take a part-based representation by extracting and clustering local features. The Video Fisher Vector Faces (VF2) descriptor [27] uses Fisher Vector coding to aggregate local features across different video frames together to form a video-level representation.\nRecently, state-of-the-art face recognition methods has been dominated by deep convolution neural networks [35, 31, 28, 7, 9]. For video face recognition, most of these methods either use pairwise frame feature similarity computation [35, 31] or naive (average/max) frame feature pooling [28, 7, 9]. This motivated us to seek for an adaptive aggregation approach.\nAs previously mentioned, this work is also related to the Neural Turing Machine [12] and the work of [38]. However, it is worth noting that while they use Recurrent Neural Networks (RNN) to handle sequential inputs/outputs, there is no RNN structure in our method. We only borrow their differentiable memory addressing/attention scheme for our feature aggregation."
        },
        {
            "heading": "2. Neural Aggregation Network",
            "text": "As shown in Fig. 1, the NAN network takes a set of face images of a person as input and outputs a single feature vector as its representation for the recognition task. It is built upon a modern deep CNN model for frame feature embedding, and becomes more powerful for video face recognition by adaptively aggregating all frames in the video into a compact vector representation."
        },
        {
            "heading": "2.1. Feature embedding module",
            "text": "The image embedding module of our NAN is a deep Convolution Neural Network (CNN), which embeds each frame of a video to a face feature representation. To leverage modern deep CNN networks with high-end performances, in this paper we adopt the GoogLeNet [34] with the Batch Normalization (BN) technique [17]. Certainly, other network architectures are equally applicable here as well. The GoogLeNet produces 128-dimension image features, which are first normalized to be unit vectors then fed into the aggregation module. In the rest of this paper, we will simply refer to the employed GoogLeNet-BN network as CNN."
        },
        {
            "heading": "2.2. Aggregation module",
            "text": "Consider the video face recognition task on n pairs of video face data (X i, yi)ni=1, where X i is a face video sequence or a image set with varying image number Ki, i.e. X i = {xi1,xi2, ...,xiKi} in which xik, k = 1, ...,Ki is the k-th frame in the video, and yi is the corresponding subject ID of X i. Each frame xik has a corresponding normalized feature representation f ik extracted from the feature embedding module. For better readability, we omit the upper index where appropriate in the remaining text. Our goal is to utilize all feature vectors from a video to generate a set of linear weights {ak}Kk=1, so that the aggregated feature representation becomes\nr = \u2211\nk\nakfk. (1)\nIn this way, the aggregated feature vector has the same size as a single face image feature extracted by the CNN.\nObviously, the key of Eq. 1 is its weights {ak}. If ak \u2261 1K , Eq. 1 will degrades to naive averaging, which\nis usually non-optimal as we will show in our experiments. We instead try to design a better weighting scheme.\nThree main principles have been considered in designing our aggregation module. First, the module should be able to process different numbers of images (i.e. different Ki\u2019s), as the video data source varies from person to person. Second, the aggregation should be invariant to the image order \u2013 we prefer the result unchanged when the image sequence are reversed or reshuffled. This way, the aggregation module can handle an arbitrary set of image or video faces without temporal information (e.g. that collected from different Internet locations). Third, the module should be adaptive to the input faces and has parameters trainable through supervised learning in a standard face recognition training task.\nOur solution is inspired by the memory attention mechanism described in [12, 32, 38]. The idea therein is to use a neural model to read external memories through a differentiable addressing/attention scheme. Such models are often coupled with Recurrent Neural Networks (RNN) to handle sequential inputs/outputs [12, 32, 38]. Although an RNN structure is not needed for our purpose, its memory attention mechanism is applicable to our aggregation task. In this work, we treat the face features as the memory and cast feature weighting as a memory addressing procedure. We employ in the aggregation module the \u201cattention blocks\u201d, to be described as follows."
        },
        {
            "heading": "2.2.1 Attention blocks",
            "text": "An attention block reads all feature vectors from the feature embedding module, and generate linear weights for them. Specifically, let {fk} be the face feature vectors, then an attention block filters them with a kernel q via dot product, yielding a set of corresponding significances {ek}. They are then passed to a softmax operator to generate positive\nweights {ak} with \u2211\nk ak = 1. These two operations can be described by the following equations, respectively:\nek = q T fk (2)\nak = exp(ek)\u2211 j exp(ej) . (3)\nIt can be seen that our algorithm essentially selects one point inside of the convex hull spanned by all the feature vectors. One related work is [3] where each face image set is approximated with a convex hull and set similarities are defined as the shortest path between two convex hulls.\nIn this way, the number of inputs {fk} does not affect the size of aggregation r, which is of the same dimension as a single feature fk. Besides, the aggregation result is invariant to the input order of fk: according to Eq. 1, 2, and 3, permuting fk and fk\u2032 has no effects on the aggregated representation r. Furthermore, an attention block is modulated by the filter kernel q, which is trainable through standard backpropagation and gradient descent.\nSingle attention block \u2013 Universal face feature quality measurement. We first try using one attention block for aggregation. In this case, vector q is the parameter to learn. It has the same size as a single feature f and serves as a universal prior measuring the face feature quality.\nWe train the network to perform video face verification (see Section 2.3 and Section 3 for details) in the IJB-A dataset [19] on the extracted face features, and Figure 2 shows the sorted scores of all the faces images in the dataset. It can be seen that after training, the network favors highquality face images, such as those of high resolutions and with relatively simple backgrounds. It down-weights face images with blur, occlusion, improper exposure and extreme poses. Table 1 shows that the network achieves higher accuracy than the average pooling baseline in the verification and identification tasks.\nCascaded two attention blocks \u2013 Content-aware aggregation. We believe a content-aware aggregation can perform even better. The intuition behind is that face image variation may be expressed differently at different geo-\ngraphic locations in the feature space (i.e. for different persons), and content-aware aggregation can learn to select features that are more discriminative for the identity of the input image set. To this end, we employ two attention blocks in a cascaded and end-to-end fashion described as follows.\nLet q0 be the kernel of the first attention block, and r0 be the aggregated feature with q0. We adaptively compute q1, the kernel of the second attention block, through a transfer layer taking r0 as the input:\nq1 = tanh(Wr0 + b) (4)\nwhere W and b are the weight matrix and bias vector of the neurons respectively, and tanh(x) = e\nx\u2212e\u2212x ex+e\u2212x imposes the\nhyperbolic tangent nonlinearity. The feature vector r1 generated by q1 will be the final aggregation results. Therefore, (q0,W,b) are now the trainable parameters of the aggregation module.\nWe train the network on the IJB-A dataset again, and Table 1 shows that the network obtained better results than using single attention block. Figure 3 shows some typical examples of the weights computed by the trained network for different videos or image sets.\nOur current full solution of NAN, based on which all the remaining experimental results are obtained, adopts such a cascaded two attention block design (as per Fig. 1)."
        },
        {
            "heading": "2.3. Network training",
            "text": "The NAN network can be trained either for face verification and identification tasks with standard configurations."
        },
        {
            "heading": "2.3.1 Training loss",
            "text": "For verification, we build a siamese neural aggregation network structure [8] with two NANs sharing weights, and minimize the average contrastive loss [14]: li,j = yi,j ||r1i \u2212 r1j ||22 + (1\u2212yi,j)max(0,m \u2212 ||r1i \u2212 r1j ||22), where yi,j =1 if the pair (i, j) is from the same identity and yi,j=0 otherwise. The constant m is set to 2 in all our experiments.\nFor identification, we add on top of NAN a fullyconnected layer followed by a softmax and minimize the average classification loss."
        },
        {
            "heading": "2.3.2 Module training",
            "text": "The two modules can be trained either simultaneously in an end-to-end fashion, or separately one by one. The latter option is chosen in this work. Specifically, we first train the CNN on single images with the identification task, then we train the aggregation module on top of the features extracted by CNN. More details can be found in Section 3.1.\nWe chose this separate training strategy mainly for two reasons. First, in this work we would like to focus on analyzing the effectiveness and performance of the aggregation module with the attention mechanism. Despite the huge success of applying deep CNN in image-based face recognition task, little attention has been drawn to CNN feature aggregation to our knowledge. Second, training a deep CNN usually necessitates a large volume of labeled data. While millions of still images can be obtained for training nowadays [35, 28, 31], it appears not practical to collect such amount of distinctive face videos or sets. We leave an endto-end training of the NAN as our future work."
        },
        {
            "heading": "3. Experiments",
            "text": "This section evaluates the performance of the proposed NAN network. We will begin with introducing our training details and the baseline methods, followed by reporting the results on three video face recognition datasets: the IARPA Janus Benchmark A (IJB-A) [19], the YouTube Face dataset [42], and the Celebrity-1000 dataset [23]."
        },
        {
            "heading": "3.1. Training details",
            "text": "As mentioned in Section 2.3, two networks are trained separately in this work. To train the CNN, we use about 3M face images of 50K identities crawled from the Internet to perform image-based identification. The faces are detected using the JDA method [5], and aligned with the LBF method [29]. The input image size is 224x224. After training, the CNN is fixed and we focus on analyzing the effectiveness of the neural aggregation module.\nThe aggregation module is trained on each video face dataset we tested on with standard backpropagation and an RMSProp solver [36]. An all-zero parameter initialization is used, i.e., we start from average pooling. The batch size, learning rate, and iteration are tuned for each dataset. As the network is quite simple and image features are compact (128-d), the training process is quite efficient: training on 5K video pairs with \u223c1M images in total only takes less than 2 minutes on a CPU of a desktop PC."
        },
        {
            "heading": "3.2. Baseline methods",
            "text": "Since our goal is compact video face representation, we compare the results with simple aggregation strategies such as average pooling. We also compare with some set-toset similarity measurements leveraging pairwise comparison on the image level. To keep it simple, we simply use the L2 feature distances for face recognition (all features are normalized), although it is possible to combine with an extra metric learning or template adaption technique [10] to further boost the performance on each dataset.\nIn the baseline methods, CNN+Min L2, CNN+Max L2, CNN+Mean L2 and CNN+SoftMin L2 measure the similarity of two video faces based on the L2 feature distances of all frame pairs. They necessitate storing all image features of a video, i.e., with O(n) space complexity. The first three use the minimum, maximum and mean pairwise distance respectively, thus having O(n2) complexity for similarity computation. CNN+SoftMin L2 corresponds to the SoftMax similarity score advocated in some works such as [24, 25, 1]. It has O(m\u00b7n2) complexity for computation1.\nCNN+MaxPool and CNN+AvePool are respectively max-pooling and average-pooling along each feature dimension for aggregation. These two methods as well as our NAN produce a 128-d feature representation for each video and compute the similarity in O(1) time."
        },
        {
            "heading": "3.3. Results on IJB-A dataset",
            "text": "The IJB-A dataset [19] contains face images and videos captured from unconstrained environments. It features full pose variation and wide variations in imaging conditions\n1m is the number of scaling factor \u03b2 used (see [25] for details). We tested 20 combinations of (negative) \u03b2\u2019s, including single [1] or multiple values [24, 25] and report the best results obtained.\nthus is very challenging. There are 500 subjects with 5,397 images and 2,042 videos in total and 11.4 images and 4.2 videos per subject on average. We detect the faces with landmarks using STN [4] face detector, and then align the face image with similarity transformation.\nIn this dataset, each training and testing instance is called a \u2018template\u2019, which comprises 1 to 190 mixed still images and video frames. Since one template may contain multiple medias and the dataset provides the media id for each image, another possible aggregation strategy is first aggregating the frame features in each media then the media features in the template [10, 30]. This strategy is also tested in this work with CNN+AvePool and our NAN. Note that media id may not be always available in practice.\nWe test the proposed method on both the \u2018compare\u2019 pro-\ntocol for 1:1 face verification and the \u2018search\u2019 protocol for 1:N face identification. For verification, the true accept rates (TAR) vs. false positive rates (FAR) are reported. For identification, the true positive identification rate (TPIR) vs. false positive identification rate (FPIR) and the Rank-N accuracies are reported. Table 2 presents the numerical results of different methods, and Figure 4 shows the receiver operating characteristics (ROC) curves for verification as well as the cumulative match characteristic (CMC) and decision error trade-off (DET) curves for identification. The metrics are calculated according to [19, 13] on the 10 splits.\nIn general, the CNN+MaxL2, CNN+MinL2 and CNN+MaxPool perform worst among the baseline methods. CNN+SoftMinL2 performs slightly better than CNN+MaxPool. The use of media id significantly improves\nthe performance of CNN+AvePool, but gives a relatively small boost to NAN. We believe NAN already has the robustness to templates dominated by poor images from a few media. Without the media aggregation, NAN outperforms all its baselines by appreciable margins, especially on the low FAR cases. For example, in the verification task, the TARs of our NAN at FARs of 0.001 and 0.01 are respectively 0.860 and 0.933, reducing the errors of the best results from its baselines by about 39% and 23%, respectively.\nTo our knowledge, with the media aggregation our NAN achieves top performances compared to previous methods. It has a same verification TAR at FAR=0.1 and identification Rank-10 CMC as the state-of-the-art method of [10], but outperforms it on all other metrics (e.g. 0.881 vs. 0.836 TARs at FAR=0.01, 0.817 vs. 0.774 TPIRs at FPIR=0.01 and 0.958 vs. 0.928 Rank-1 accuracy).\nFigure 3 has shown some typical examples of the weighting results. NAN exhibits the ability to choose high-quality and more discriminative face images while repelling poor face images."
        },
        {
            "heading": "3.4. Results on YouTube Face dataset",
            "text": "We then test our method on the YouTube Face (YTF) dataset [42] which is designed for unconstrained face verification in videos. It contains 3,425 videos of 1,595 different people, and the video lengths vary from 48 to 6,070 frames with an average length of 181.3 frames. Ten folds of 500 video pairs are available, and we follow the standard verification protocol to report the average accuracy with crossvalidation. We again use the STN and similarity transformation to align the face images.\nThe results of our NAN, its baselines, and other methods are presented in Table 3, with their ROC curves shown in Fig. 5. It can be seen that the NAN again outperforms all its\nbaselines. The gaps between NAN and the best-performing baselines are smaller compared to the results on IJB-A. This is because the face variations in this dataset are relatively small (compare the examples in Fig. 6 and Fig. 3), thus no much beneficial information can be extracted compared to naive average pooling or computing mean L2 distances.\nCompared to previous methods, our NAN achieves a mean accuracy of 95.72%, reducing the error of FaceNet by 12.3%. Note that FaceNet is also based on a GoogLeNet style network, and the average similarity of all pairs of 100 frames in each video (i.e., 10K pairs) was used [31]. To our knowledge, only the VGG-Face [28] achieves an accuracy (97.3%) higher than ours. However, that result is based on a further discriminative metric learning on YTF, without which the accuracy is only 91.5% [28]."
        },
        {
            "heading": "3.5. Results on Celebrity-1000 dataset",
            "text": "The Celebrity-1000 dataset [23] is designed to study the unconstrained video-based face identification problem. It contains 159,726 video sequences of 1,000 human subjects, with 2.4M frames in total (\u223c15 frames per sequence). We use the provided 5 facial landmarks to align the face images. Two types of protocols \u2013 open-set and close-set - exist on this dataset. More details about the protocols and the dataset can be found in [23].\nClose-set tests. For the close-set protocol, we first train the network on the video sequences with the identification loss. We take the FC layer output values as the scores and the subject with the maximum score as the result. We also train a linear classifier for CNN+AvePool to classify each video feature. As the features are built on video sequences, we call this approach \u2018VideoAggr\u2019 to distinguish it from another approach to be described next. Each subject in the dataset has multiple video sequences, thus we can build a single representation for a subject by aggregating all available images in all the training (gallery) video sequences. We call this approach \u2018SubjectAggr\u2019. This way, the linear classifier can be bypassed, and identification can be achieved simply by comparing the feature L2 distances.\nThe results are presented in Table 4. Note that [23] and [22] are not using deep learning and no deep network based method reported result on this dataset. So we mainly compare with our baselines in the following. It can be seen from Table 4 and Fig. 7 (a) that NAN consistently outperforms the baseline methods for both \u2018VideoAggr\u2019 and \u2018SubjectAggr\u2019. Significant improvements upon the baseline are achieved for the \u2018SubjectAggr\u2019 approach. It is interesting to see that, \u2018SubjectAggr\u2019 leads to a clear performance drop for CNN+AvePool compared to its \u2018VideoAggr\u2019. This indicates that the naive aggregation gets even worse when applied on the subject level with multiple videos. However, our NAN can benefit from \u2018SubjectAggr\u2019, yielding results consistently better than or on par with the \u2018VideoAggr\u2019 approach and delivers a considerable accuracy boost compared to the baseline. This suggests our NAN works quite well on handling large data variations.\nOpen-set tests. We then test our NAN with the close-set protocol. We first train the network on the provided training video sequences. In the testing stage, we take the \u2018SubjectAggr\u2019 approach described before to build a highly-compact face representation for each gallery subject. Identification is perform simply by comparing the L2 distances between aggregated face representations.\nThe results in both Table 5 and Fig. 7 (b) show that our NAN significantly reduces the error of the baseline CNN+AvePool. This again suggests that in the presence of large face variances, the widely used strategies such as average-pooling aggregation and the pairwise distance computation are far from optimal. In such cases, our learned NAN model is clearly more powerful, and the aggregated feature representation by it is more favorable for the video face recognition task."
        },
        {
            "heading": "4. Conclusions",
            "text": "We have presented a Neural Aggregation Network for video face representation and recognition. It fuses all input frames with a set of content adaptive weights, resulting in a compact representation that is invariant to the input frame order. The aggregation scheme is simple with small computation and memory footprints, but can generate quality face representations after training. The proposed NAN can be used for general video or set representation, and we plan to apply it to other vision tasks in our future work.\nAcknowledgments GH was partly supported by NSFC Grant 61629301. HL\u2019s work was supported in part by Australia ARC Centre of Excellence for Robotic Vision (CE140100016) and by CSIRO Data61."
        }
    ],
    "title": "Neural Aggregation Network for Video Face Recognition",
    "year": 2017
}