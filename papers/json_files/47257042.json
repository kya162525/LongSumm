{
    "abstractText": "A critical flaw of existing inverse reinforcement learning (IRL) methods is their inability to significantly outperform the demonstrator. This is because IRL typically seeks a reward function that makes the demonstrator appear near-optimal, rather than inferring the underlying intentions of the demonstrator that may have been poorly executed in practice. In this paper, we introduce a novel reward-learning-from-observation algorithm, Trajectory-ranked Reward EXtrapolation (T-REX), that extrapolates beyond a set of (approximately) ranked demonstrations in order to infer high-quality reward functions from a set of potentially poor demonstrations. When combined with deep reinforcement learning, T-REX outperforms state-of-the-art imitation learning and IRL methods on multiple Atari and MuJoCo benchmark tasks and achieves performance that is often more than twice the performance of the best demonstration. We also demonstrate that T-REX is robust to ranking noise and can accurately extrapolate intention by simply watching a learner noisily improve at a task over time.",
    "authors": [
        {
            "affiliations": [],
            "name": "Daniel S. Brown"
        },
        {
            "affiliations": [],
            "name": "Wonjoon Goo"
        },
        {
            "affiliations": [],
            "name": "Prabhat Nagarajan"
        },
        {
            "affiliations": [],
            "name": "Scott Niekum"
        }
    ],
    "id": "SP:4f984188c6891666ea11841872ce1e26cc5912ba",
    "references": [
        {
            "authors": [
                "P. Abbeel",
                "A.Y. Ng"
            ],
            "title": "Apprenticeship learning via inverse reinforcement learning",
            "venue": "In Proceedings of the 21st international conference on Machine learning,",
            "year": 2004
        },
        {
            "authors": [
                "R. Akrour",
                "M. Schoenauer",
                "M. Sebag"
            ],
            "title": "Preference-based policy learning",
            "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,",
            "year": 2011
        },
        {
            "authors": [
                "D. Amodei",
                "C. Olah",
                "J. Steinhardt",
                "P. Christiano",
                "J. Schulman",
                "D. Man\u00e9"
            ],
            "title": "Concrete problems in ai safety",
            "venue": "arXiv preprint arXiv:1606.06565,",
            "year": 2016
        },
        {
            "authors": [
                "B.D. Argall",
                "S. Chernova",
                "M. Veloso",
                "B. Browning"
            ],
            "title": "A survey of robot learning from demonstration",
            "venue": "Robotics and autonomous systems,",
            "year": 2009
        },
        {
            "authors": [
                "S. Arora",
                "P. Doshi"
            ],
            "title": "A survey of inverse reinforcement learning: Challenges, methods and progress",
            "venue": "arXiv preprint arXiv:1806.06877,",
            "year": 2018
        },
        {
            "authors": [
                "Y. Aytar",
                "T. Pfaff",
                "D. Budden",
                "T.L. Paine",
                "Z. Wang",
                "N. de Freitas"
            ],
            "title": "Playing hard exploration games by watching youtube",
            "venue": "arXiv preprint arXiv:1805.11592,",
            "year": 2018
        },
        {
            "authors": [
                "A. Boularias",
                "J. Kober",
                "J. Peters"
            ],
            "title": "Relative entropy inverse reinforcement learning",
            "venue": "In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,",
            "year": 2011
        },
        {
            "authors": [
                "R.A. Bradley",
                "M.E. Terry"
            ],
            "title": "Rank analysis of incomplete block designs: I. the method of paired comparisons",
            "year": 1952
        },
        {
            "authors": [
                "B. Burchfiel",
                "C. Tomasi",
                "R. Parr"
            ],
            "title": "Distance minimization for reward learning from scored trajectories",
            "venue": "In AAAI,",
            "year": 2016
        },
        {
            "authors": [
                "S. Choi",
                "K. Lee",
                "S. Oh"
            ],
            "title": "Robust learning from demonstrations with mixed qualities using leveraged gaussian processes",
            "venue": "IEEE Transactions on Robotics,",
            "year": 2019
        },
        {
            "authors": [
                "P.F. Christiano",
                "J. Leike",
                "T. Brown",
                "M. Martic",
                "S. Legg",
                "D. Amodei"
            ],
            "title": "Deep reinforcement learning from human preferences",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "L. El Asri",
                "B. Piot",
                "M. Geist",
                "R. Laroche",
                "O. Pietquin"
            ],
            "title": "Score-based inverse reinforcement learning",
            "venue": "In Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems,",
            "year": 2016
        },
        {
            "authors": [
                "C. Finn",
                "S. Levine",
                "P. Abbeel"
            ],
            "title": "Guided cost learning: Deep inverse optimal control via policy optimization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "J. Fu",
                "K. Luo",
                "S. Levine"
            ],
            "title": "Learning robust rewards with adversarial inverse reinforcement learning",
            "venue": "arXiv preprint arXiv:1710.11248,",
            "year": 2017
        },
        {
            "authors": [
                "Y. Gao",
                "J. Peters",
                "A. Tsourdos",
                "S. Zhifei",
                "E. Meng Joo"
            ],
            "title": "A survey of inverse reinforcement learning techniques",
            "venue": "International Journal of Intelligent Computing and Cybernetics,",
            "year": 2012
        },
        {
            "authors": [
                "Y. Gao",
                "J. Lin",
                "F. Yu",
                "S. Levine",
                "T Darrell"
            ],
            "title": "Reinforcement learning from imperfect demonstrations",
            "venue": "arXiv preprint arXiv:1802.05313,",
            "year": 2018
        },
        {
            "authors": [
                "W. Goo",
                "S. Niekum"
            ],
            "title": "One-shot learning of multi-step tasks from observation via activity localization in auxiliary video",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2019
        },
        {
            "authors": [
                "I. Goodfellow",
                "J. Pouget-Abadie",
                "M. Mirza",
                "B. Xu",
                "D. WardeFarley",
                "S. Ozair",
                "A. Courville",
                "Y. Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "In Advances in neural information processing systems,",
            "year": 2014
        },
        {
            "authors": [
                "S. Greydanus",
                "A. Koul",
                "J. Dodge",
                "A. Fern"
            ],
            "title": "Visualizing and understanding atari agents",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "D.H. Grollman",
                "A. Billard"
            ],
            "title": "Donut as i do: Learning from failed demonstrations",
            "venue": "In Robotics and Automation (ICRA),",
            "year": 2011
        },
        {
            "authors": [
                "P. Henderson",
                "Chang",
                "W.-D",
                "Bacon",
                "P.-L",
                "D. Meger",
                "J. Pineau",
                "D. Precup"
            ],
            "title": "Optiongan: Learning joint reward-policy options using generative adversarial inverse reinforcement learning",
            "venue": "In Thirty-Second AAAI Conference on Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "T. Hester",
                "M. Vecerik",
                "O. Pietquin",
                "M. Lanctot",
                "T. Schaul",
                "B. Piot",
                "D. Horgan",
                "J. Quan",
                "A. Sendonaris",
                "G Dulac-Arnold"
            ],
            "title": "Deep q-learning from demonstrations",
            "venue": "arXiv preprint arXiv:1704.03732,",
            "year": 2017
        },
        {
            "authors": [
                "J. Ho",
                "S. Ermon"
            ],
            "title": "Generative adversarial imitation learning",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "B. Ibarz",
                "J. Leike",
                "T. Pohlen",
                "G. Irving",
                "S. Legg",
                "D. Amodei"
            ],
            "title": "Reward learning from human preferences and demonstrations in atari",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "J. Kober",
                "J.R. Peters"
            ],
            "title": "Policy search for motor primitives in robotics",
            "venue": "In Advances in neural information processing systems,",
            "year": 2009
        },
        {
            "authors": [
                "Y. Liu",
                "A. Gupta",
                "P. Abbeel",
                "S. Levine"
            ],
            "title": "Imitation from observation: Learning to imitate behaviors from raw video via context translation",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2018
        },
        {
            "authors": [
                "R.D. Luce"
            ],
            "title": "Individual choice behavior: A theoretical analysis",
            "venue": "Courier Corporation,",
            "year": 2012
        },
        {
            "authors": [
                "V. Mnih",
                "K. Kavukcuoglu",
                "D. Silver",
                "A.A. Rusu",
                "J. Veness",
                "M.G. Bellemare",
                "A. Graves",
                "M. Riedmiller",
                "A.K. Fidjeland",
                "G Ostrovski"
            ],
            "title": "Human-level control through deep reinforcement learning",
            "year": 2015
        },
        {
            "authors": [
                "A.Y. Ng",
                "D. Harada",
                "S. Russell"
            ],
            "title": "Policy invariance under reward transformations: Theory and application to reward shaping",
            "venue": "In ICML,",
            "year": 1999
        },
        {
            "authors": [
                "D.A. Pomerleau"
            ],
            "title": "Efficient training of artificial neural networks for autonomous navigation",
            "venue": "Neural Computation,",
            "year": 1991
        },
        {
            "authors": [
                "M.L. Puterman"
            ],
            "title": "Markov decision processes: discrete stochastic dynamic programming",
            "year": 2014
        },
        {
            "authors": [
                "A.H. Qureshi",
                "M.C. Yip"
            ],
            "title": "Adversarial imitation via variational inverse reinforcement learning",
            "venue": "arXiv preprint arXiv:1809.06404,",
            "year": 2018
        },
        {
            "authors": [
                "D. Ramachandran",
                "E. Amir"
            ],
            "title": "Bayesian inverse reinforcement learning",
            "venue": "In Proceedings of the 20th International Joint Conference on Artifical intelligence,",
            "year": 2007
        },
        {
            "authors": [
                "S. Ross",
                "G. Gordon",
                "D. Bagnell"
            ],
            "title": "A reduction of imitation learning and structured prediction to no-regret online learning",
            "venue": "In Proceedings of the fourteenth international conference on artificial intelligence and statistics,",
            "year": 2011
        },
        {
            "authors": [
                "J. Schulman",
                "F. Wolski",
                "P. Dhariwal",
                "A. Radford",
                "O. Klimov"
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347,",
            "year": 2017
        },
        {
            "authors": [
                "P. Sermanet",
                "C. Lynch",
                "Y. Chebotar",
                "J. Hsu",
                "E. Jang",
                "S. Schaal",
                "S. Levine",
                "G. Brain"
            ],
            "title": "Time-contrastive networks: Selfsupervised learning from video",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2018
        },
        {
            "authors": [
                "K. Shiarlis",
                "J. Messias",
                "S. Whiteson"
            ],
            "title": "Inverse reinforcement learning from failure",
            "venue": "In Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems, pp. 1060\u20131068. International Foundation for Autonomous Agents and Multiagent Systems,",
            "year": 2016
        },
        {
            "authors": [
                "H. Sugiyama",
                "T. Meguro",
                "Y. Minami"
            ],
            "title": "Preference-learning based inverse reinforcement learning for dialog control",
            "venue": "In INTERSPEECH, pp",
            "year": 2012
        },
        {
            "authors": [
                "R.S. Sutton",
                "A.G. Barto"
            ],
            "title": "Introduction to reinforcement learning, volume 135",
            "venue": "MIT press Cambridge,",
            "year": 1998
        },
        {
            "authors": [
                "U. Syed",
                "R.E. Schapire"
            ],
            "title": "A game-theoretic approach to apprenticeship learning",
            "venue": "In Advances in neural information processing systems,",
            "year": 2008
        },
        {
            "authors": [
                "E. Todorov",
                "T. Erez",
                "Y. Tassa"
            ],
            "title": "Mujoco: A physics engine for model-based control",
            "venue": "In Intelligent Robots and Systems (IROS),",
            "year": 2012
        },
        {
            "authors": [
                "F. Torabi",
                "G. Warnell",
                "P. Stone"
            ],
            "title": "Behavioral cloning from observation",
            "venue": "In Proceedings of the 27th International Joint Conference on Artificial Intelligence (IJCAI),",
            "year": 2018
        },
        {
            "authors": [
                "F. Torabi",
                "G. Warnell",
                "P. Stone"
            ],
            "title": "Generative adversarial imitation from observation",
            "venue": "arXiv preprint arXiv:1807.06158,",
            "year": 2018
        },
        {
            "authors": [
                "A. Tucker",
                "A. Gleave",
                "S. Russell"
            ],
            "title": "Inverse reinforcement learning for video games",
            "venue": "In Proceedings of the Workshop on Deep Reinforcement Learning at NeurIPS,",
            "year": 2018
        },
        {
            "authors": [
                "C. Wirth",
                "J. F\u00fcrnkranz",
                "G. Neumann"
            ],
            "title": "Model-free preferencebased reinforcement learning",
            "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,",
            "year": 2016
        },
        {
            "authors": [
                "C. Wirth",
                "R. Akrour",
                "G. Neumann",
                "J. F\u00fcrnkranz"
            ],
            "title": "A survey of preference-based reinforcement learning methods",
            "venue": "Journal of Machine Learning Research,",
            "year": 2017
        },
        {
            "authors": [
                "M. Wulfmeier",
                "P. Ondruska",
                "I. Posner"
            ],
            "title": "Maximum entropy deep inverse reinforcement learning",
            "venue": "arXiv preprint arXiv:1507.04888,",
            "year": 2015
        },
        {
            "authors": [
                "T. Yu",
                "C. Finn",
                "A. Xie",
                "S. Dasari",
                "T. Zhang",
                "P. Abbeel",
                "S. Levine"
            ],
            "title": "One-shot imitation from observing humans via domain-adaptive meta-learning",
            "venue": "arXiv preprint arXiv:1802.01557,",
            "year": 2018
        },
        {
            "authors": [
                "J. Zheng",
                "S. Liu",
                "L.M. Ni"
            ],
            "title": "Robust bayesian inverse reinforcement learning with sparse behavior noise",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2014
        },
        {
            "authors": [
                "B.D. Ziebart",
                "A.L. Maas",
                "J.A. Bagnell",
                "A.K. Dey"
            ],
            "title": "Maximum entropy inverse reinforcement learning",
            "venue": "In Proceedings of the 23rd AAAI Conference on Artificial Intelligence,",
            "year": 2008
        },
        {
            "authors": [
                "Ibarz"
            ],
            "title": "2018) combine Deep Q-learning from demonstrations and active preference queries (DQfD+A). DQfD+A uses demonstrations consisting of (st, at, st+1)-tuples to initialize a policy using DQfD (Hester et al., 2017)",
            "year": 2017
        },
        {
            "authors": [
                "Ibarz"
            ],
            "title": "Results for T-REX are the best performance over 3 random seeds averaged over",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Due to advantages such as computational speed, precise manipulation, and exact timing, computers and robots are often superior to humans at performing tasks with well-defined goals and objectives. However, it can be difficult, even for experts, to design reward functions and objectives that lead to desired behaviors when designing autonomous agents (Ng et al., 1999; Amodei et al., 2016). When goals or rewards are difficult for a human to specify, inverse reinforcement learn-\n*Equal contribution 1Department of Computer Science, University of Texas at Austin, USA 2Preferred Networks, Japan. Correspondence to: Daniel S. Brown <dsbrown@cs.utexas.edu>, Wonjoon Goo <wonjoon@cs.utexas.edu>.\nProceedings of the 36 th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).\nFigure 1. T-REX takes a sequence of ranked demonstrations and learns a reward function from these rankings that allows policy improvement over the demonstrator via reinforcement learning.\ning (IRL) (Abbeel & Ng, 2004) techniques can be applied to infer the intrinsic reward function of a user from demonstrations. Unfortunately, high-quality demonstrations are difficult to provide for many tasks\u2014for instance, consider a non-expert user attempting to give kinesthetic demonstrations of a household chore to a robot. Even for relative experts, tasks such as high-frequency stock trading or playing complex video games can be difficult to perform optimally.\nIf a demonstrator is suboptimal, but their intentions can be ascertained, then a learning agent ought to be able to exceed the demonstrator\u2019s performance in principle. However, existing IRL algorithms fail to do this, typically searching for a reward function that makes the demonstrations appear near-optimal (Ramachandran & Amir, 2007; Ziebart et al., 2008; Finn et al., 2016; Henderson et al., 2018). Thus, when the demonstrator is suboptimal, IRL results in suboptimal behavior as well. Imitation learning approaches (Argall et al., 2009) that mimic behavior directly without reward inference, such as behavioral cloning (Torabi et al., 2018a), also suffer from the same shortcoming.\nTo overcome this critical flaw in current imitation learning methods, we propose a novel IRL algorithm, Trajectoryranked Reward EXtrapolation (T-REX)1 that utilizes ranked demonstrations to extrapolate a user\u2019s underlying intent be-\n1Code available at https://github.com/hiwonjoon/ ICML2019-TREX\nar X\niv :1\n90 4.\n06 38\n7v 5\n[ cs\n.L G\n] 9\nJ ul\n2 01\n9\nyond the best demonstration, even when all demonstrations are highly suboptimal. This, in turn, enables a reinforcement learning agent to exceed the performance of the demonstrator by learning to optimize this extrapolated reward function. Specifically, we use ranked demonstrations to learn a statebased reward function that assigns greater total return to higher-ranked trajectories. Thus, while standard inverse reinforcement learning approaches seek a reward function that justifies the demonstrations, we instead seek a reward function that explains the ranking over demonstrations, allowing for potentially better-than-demonstrator performance.\nUtilizing ranking in this way has several advantages. First, rather than imitating suboptimal demonstrations, it allows us to identify features that are correlated with rankings, in a manner that can be extrapolated beyond the demonstrations. Although the learned reward function could potentially overfit to the provided rankings, we demonstrate empirically that it extrapolates well, successfully predicting returns of trajectories that are significantly better than any observed demonstration, likely due to the powerful regularizing effect of having many pairwise ranking constraints between trajectories. For example, the degenerate all-zero reward function (the agent always receives a reward of 0) makes any given set of demonstrations appear optimal. However, such a reward function is eliminated from consideration by any pair of (non-equally) ranked demonstrations. Second, when learning features directly from high-dimensional data, this regularizing effect can also help to prevent overfitting to the small fraction of state space visited by the demonstrator. By utilizing a set of suboptimal, but ranked demonstrations, we provide the neural network with diverse data from multiple areas of the state space, allowing an agent to better learn both what to do and what not to do in a variety of situations.\nWe evaluate T-REX on a variety of standard Atari and MuJoCo benchmark tasks. Our experiments show that T-REX can extrapolate well, achieving performance that is often more than twice as high as the best-performing demonstration, as well as outperforming state-of-the-art imitation learning algorithms. We also show that T-REX performs well even in the presence of significant ranking noise, and provide results showing that T-REX can learn good policies simply by observing a novice demonstrator that noisily improves over time."
        },
        {
            "heading": "2. Related Work",
            "text": "The goal of our work is to achieve improvements over a suboptimal demonstrator in high-dimensional reinforcement learning tasks without requiring a hand-specified reward function or supervision during policy learning. While there is a large body of research on learning from demonstrations (Argall et al., 2009; Gao et al., 2012; Osa et al., 2018; Arora & Doshi, 2018), most work assumes access to action labels,\nwhile we learn only from observations. Additionally, little work has addressed the problem of learning from ranked demonstrations, especially when they are significantly suboptimal. To the best of our knowledge, our work is the first to show better-than-demonstrator performance in highdimensional tasks such as Atari, without requiring active human supervision or access to ground-truth rewards."
        },
        {
            "heading": "2.1. Learning from demonstrations",
            "text": "Early work on learning from demonstration focused on behavioral cloning (Pomerleau, 1991), in which the goal is to learn a policy that imitates the actions taken by the demonstrator; however, without substantial human feedback and correction, this method is known to have large generalization error (Ross et al., 2011). Recent deep learning approaches to imitation learning (Ho & Ermon, 2016) have used Generative Adversarial Networks (Goodfellow et al., 2014) to model the distribution of actions taken by the demonstrator.\nRather than directly learn to mimic the demonstrator, inverse reinforcement learning (IRL) (Gao et al., 2012; Arora & Doshi, 2018) seeks to find a reward function that models the intention of the demonstrator, thereby allowing generalization to states that were unvisited during demonstration. Given such a reward function, reinforcement learning (Sutton & Barto, 1998) techniques can be applied to learn an optimal policy. Maximum entropy IRL seeks to find a reward function that makes the demonstrations appear near-optimal, while further disambiguating inference by also maximizing the entropy of the resulting policy (Ziebart et al., 2008; Boularias et al., 2011; Wulfmeier et al., 2015; Finn et al., 2016). While maximum entropy approaches are robust to limited and occasional suboptimality in the demonstrations, they still fundamentally seek a reward function that justifies the demonstrations, resulting in performance that is explicitly tied to the performance of the demonstrator.\nSyed & Schapire (2008) proved that, given prior knowledge about which features contribute positively or negatively to the true reward, an apprenticeship policy can be found that is guaranteed to outperform the demonstrator. However, their approach requires hand-crafted, linear features, knowledge of the true signs of the rewards features, and also requires repeatedly solving a Markov decision process (MDP). Our proposed method uses deep learning and ranked demonstrations to automatically learn complex features that are positively and negatively correlated with performance, and is able to generate a policy that can outperform the demonstrator via the solution to a single RL problem.\nOur work can be seen as a form of preference-based policy learning (Akrour et al., 2011) and preference-based IRL (PBIRL) (Wirth et al., 2016; Sugiyama et al., 2012) which both seek to optimize a policy based on preference rankings over demonstrations. However, existing approaches only\nconsider reward functions that are linear in hand-crafted features and have not studied extrapolation capabilities. For a more complete overview survey of preference-based reinforcement learning, see the survey by Wirth et al. (2017). Other methods (Burchfiel et al., 2016; El Asri et al., 2016) have proposed the use of quantitatively scored trajectories as opposed to qualitative pairwise preferences over demonstrations. However, none of the aforementioned methods have been applied to the types of high-dimensional deep inverse reinforcement learning tasks considered in this paper."
        },
        {
            "heading": "2.2. Learning from observation",
            "text": "Recently there has been a shift towards learning from observations, in which the actions taken by the demonstrator are unknown. Torabi et al. (2018a) propose a state-of-the-art model-based approach to perform behavioral cloning from observation. Sermanet et al. (2018) and Liu et al. (2018) propose methods to learn directly from a large corpus of videos containing multiple view points of the same task. Yu et al. (2018) and Goo & Niekum (2019) propose metalearning-from-observation approaches that can learn from a single demonstration, but require training on a wide variety of similar tasks. Henderson et al. (2018) and Torabi et al. (2018b) extend Generative Adversarial Imitation Learning (Ho & Ermon, 2016) to remove the need for action labels. However, inverse reinforcement learning methods based on Generative Adversarial Networks (Goodfellow et al., 2014) are notoriously difficult to train and have been shown to fail to scale to high-dimensional imitation learning tasks such as Atari (Tucker et al., 2018)."
        },
        {
            "heading": "2.3. Learning from suboptimal demonstrations",
            "text": "Very little work has tried to learn good policies from highly suboptimal demonstrations. Grollman & Billard (2011) propose a method that learns from failed demonstrations where a human attempts, but is unable, to perform a task; however, demonstrations must be labeled as failures and manually clustered into two sets of demonstrations: those that overshoot and those that undershoot the goal. Shiarlis et al. (2016) demonstrate that if successful and failed demonstrations are labeled and the reward function is a linear combination of known features, then maximum entropy IRL can be used to optimize a policy to match the expected feature counts of successful demonstrations while not matching the feature counts of failed demonstrations. Zheng et al. (2014) and Choi et al. (2019) propose methods that are robust to small numbers of unlabeled suboptimal demonstrations, but require a majority of expert demonstrations in order to correctly identify which demonstrations are anomalous.\nIn reinforcement learning, it is common to initialize a policy from suboptimal demonstrations and then improve this policy using the ground truth reward signal (Kober & Peters,\n2009; Taylor et al., 2011; Hester et al., 2017; Gao et al., 2018). However, it is often still difficult to perform significantly better than the demonstrator (Hester et al., 2017) and designing reward functions for reinforcement learning can be extremely difficult for non-experts and can easily lead to unintended behaviors (Ng et al., 1999; Amodei et al., 2016)."
        },
        {
            "heading": "2.4. Reward learning for video games",
            "text": "Most deep learning-based methods for reward learning require access to demonstrator actions and do not scale to high-dimensional tasks such as video games (e.g. Atari) (Ho & Ermon, 2016; Finn et al., 2016; Fu et al., 2017; Qureshi & Yip, 2018). Tucker et al. (2018) tested state-of-the-art IRL methods on the Atari domain and showed that they are unsuccessful, even with near-optimal demonstrations and extensive parameter tuning.\nOur work builds on the work of Christiano et al. (2017), who proposed an algorithm that learns to play Atari games via pairwise preferences over trajectories that are actively collected during policy learning. However, this approach requires obtaining thousands of labels through constant human supervision during policy learning. In contrast, our method only requires an initial set of (approximately) ranked demonstrations as input and can learn a better-than-demonstrator policy without any supervision during policy learning. Ibarz et al. (2018) combine deep Q-learning from demonstrations (DQfD) (Hester et al., 2017) and active preference learning (Christiano et al., 2017) to learn to play Atari games using both demonstrations and active queries. However, Ibarz et al. (2018) require access to the demonstrator\u2019s actions in order to optimize an action-based, large-margin loss (Hester et al., 2017) and to optimize the state-action Q-value function using (s, a, s\u2032)-tuples from the demonstrations. Additionally, the large-margin loss encourages Q-values that make the demonstrator\u2019s actions better than alternative actions, resulting in performance that is often significantly worse than the demonstrator despite using thousands of active queries during policy learning.\nAytar et al. (2018) use video demonstrations of experts to learn good policies for the Atari domains of Montezuma\u2019s Revenge, Pitfall, and Private Eye. Their method first learns a state-embedding and then selects a set of checkpoints from a demonstration. During policy learning, the agent is rewarded only when it reaches these checkpoints. This approach relies on high-performance demonstrations, which their method is unable to outperform. Furthermore, while Aytar et al. (2018) do learn a reward function purely from observations, their method is inherently different from ours in that their learned reward function is designed to only imitate the demonstrations, rather than extrapolate beyond the capabilities of the demonstrator.\nTo the best of our knowledge, our work is the first to sig-\nnificantly outperform a demonstrator without using ground truth rewards or active preference queries. Furthermore, our approach does not require demonstrator actions and is able to learn a reward function that matches the demonstrator\u2019s intention without any environmental interactions\u2014given rankings, reward learning becomes a binary classification problem and does not require access to an MDP."
        },
        {
            "heading": "3. Problem Definition",
            "text": "We model the environment as a Markov decision process (MDP) consisting of a set of states S, actions A, transition probabilities P , reward function r : S \u2192 R, and discount factor \u03b3 (Puterman, 2014). A policy \u03c0 is a mapping from states to probabilities over actions, \u03c0(a|s) \u2208 [0, 1]. Given a policy and an MDP, the expected discounted return of the policy is given by J(\u03c0) = E[ \u2211\u221e t=0 \u03b3 trt|\u03c0].\nIn this work we are concerned with the problem of inverse reinforcement learning from observation, where we do not have access to the reward function of the MDP nor the actions taken by the demonstrator. Given a sequence of m ranked trajectories \u03c4t for t = 1, . . . ,m, where \u03c4i \u227a \u03c4j if i < j, we wish to find a parameterized reward function r\u0302\u03b8 that approximates the true reward function r that the demonstrator is attempting to optimize. Given r\u0302\u03b8, we then seek to optimize a policy \u03c0\u0302 that can outperform the demonstrations.\nWe only assume access to a qualitative ranking over demonstrations. Thus, we only require the demonstrator to have an internal goal or intrinsic reward. The demonstrator can rank trajectories using any method, such as giving pairwise preferences over demonstrations or by rating each demonstration on a scale. Note that even if the relative scores of the demonstrations are used for ranking, it is still necessary to infer why some trajectories are better than others, which is what our proposed method does."
        },
        {
            "heading": "4. Method",
            "text": "We now describe Trajectory-ranked Reward EXtrapolation (T-REX), an algorithm for using ranked suboptimal demonstrations to extrapolate a user\u2019s underlying intent beyond the best demonstration. Given a sequence of m demonstrations ranked from worst to best, \u03c41, . . . , \u03c4m, T-REX has two steps: (1) reward inference and (2) policy optimization.\nGiven the ranked demonstrations, T-REX performs reward inference by approximating the reward at state s using a neural network, r\u0302\u03b8(s), such that \u2211 s\u2208\u03c4i r\u0302\u03b8(s) < \u2211 s\u2208\u03c4j r\u0302\u03b8(s) when \u03c4i \u227a \u03c4j . The parameterized reward function r\u0302\u03b8 can be trained with ranked demonstrations using the generalized loss function:\nL(\u03b8) = E\u03c4i,\u03c4j\u223c\u03a0 [ \u03be ( P ( J\u0302\u03b8(\u03c4i) < J\u0302\u03b8(\u03c4j) ) , \u03c4i \u227a \u03c4j )] , (1)\nwhere \u03a0 is a distribution over demonstrations, \u03be is a binary classification loss function, J\u0302 is a (discounted) return defined by a parameterized reward function r\u0302\u03b8, and \u227a is an indication of the preference between the demonstrated trajectories.\nWe represent the probability P as a softmax-normalized distribution and we instantiate \u03be using a cross entropy loss:\nP ( J\u0302\u03b8(\u03c4i) < J\u0302\u03b8(\u03c4j) ) \u2248\nexp \u2211 s\u2208\u03c4j r\u0302\u03b8(s)\nexp \u2211 s\u2208\u03c4i r\u0302\u03b8(s) + exp \u2211 s\u2208\u03c4j r\u0302\u03b8(s) ,\n(2)\nL(\u03b8) = \u2212 \u2211 \u03c4i\u227a\u03c4j log\nexp \u2211 s\u2208\u03c4j r\u0302\u03b8(s)\nexp \u2211 s\u2208\u03c4i r\u0302\u03b8(s) + exp \u2211 s\u2208\u03c4j r\u0302\u03b8(s) . (3)\nThis loss function trains a classifier that can predict whether one trajectory is preferable to another based on the predicted returns of each trajectory. This form of loss function follows from the classic Bradley-Terry and Luce-Shephard models of preferences (Bradley & Terry, 1952; Luce, 2012) and has been shown to be effective for training neural networks from preferences (Christiano et al., 2017; Ibarz et al., 2018).\nTo increase the number of training examples, T-REX trains on partial trajectory pairs rather than full trajectory pairs. This results in noisy preference labels that are only weakly supervised; however, using data augmentation to obtain pairwise preferences over many partial trajectories allows T-REX to learn expressive neural network reward functions from only a small number of ranked demonstrations. During training we randomly select pairs of trajectories, \u03c4i and \u03c4j . We then randomly select partial trajectories \u03c4\u0303i and \u03c4\u0303j of length L. For each partial trajectory, we take each observation and perform a forward pass through the network r\u0302\u03b8 and sum the predicted rewards to compute the cumulative return. We then use the predicted cumulative returns as the logit values in the cross-entropy loss with the label corresponding to the higher ranked demonstration.\nGiven the learned reward function r\u0302\u03b8(s), T-REX then seeks to optimize a policy \u03c0\u0302 with better-than-demonstrator performance through reinforcement learning using r\u0302\u03b8."
        },
        {
            "heading": "5. Experiments and Results",
            "text": ""
        },
        {
            "heading": "5.1. Mujoco",
            "text": "We first evaluated our proposed method on three robotic locomotion tasks using the Mujoco simulator (Todorov et al., 2012) within OpenAI Gym (Brockman et al., 2016), namely HalfCheetah, Hopper, and Ant. In all three tasks, the goal of the robot agent is to move forward as fast as possible without falling to the ground."
        },
        {
            "heading": "5.1.1. DEMONSTRATIONS",
            "text": "To generate demonstrations, we trained a Proximal Policy Optimization (PPO) (Schulman et al., 2017) agent with the ground-truth reward for 500 training steps (64,000 simulation steps) and checkpointed its policy after every 5 training steps. For each checkpoint, we generated a trajectory of length 1,000. This provides us with different demonstrations of varying quality which are then ranked based on the ground truth returns. To evaluate the effect of different levels of suboptimality, we divided the trajectories into different overlapping stages. We used 3 stages for HalfCheetah and Hopper. For HalfCheetah, we used the worst 9, 12, and 24 trajectories, respectively. For Hopper, we used the worst 9, 12, and 18 trajectories. For Ant, we used two stages consisting of the worst 12 and 40 trajectories. We used the PPO implementation from OpenAI Baselines (Dhariwal et al., 2017) with the given default hyperparameters."
        },
        {
            "heading": "5.1.2. EXPERIMENTAL SETUP",
            "text": "We trained the reward network using 5,000 random pairs of partial trajectories of length 50, with preference labels based on the trajectory rankings, not the ground-truth returns. To prevent overfitting, we represented the reward function using an ensemble of five deep neural networks, trained separately with different random pairs. Each network has 3 fully connected layers of 256 units with ReLU nonlinearities. We train the reward network using the Adam optimizer (Kingma & Ba, 2014) with a learning rate of 1e-4 and a minibatch size of 64 for 10,000 timesteps.\nTo evaluate the quality of our learned reward, we then trained a policy to maximize the inferred reward function via PPO. The outputs of each the five reward networks in our ensemble, r\u0302(s), are normalized by their standard deviation to compensate for any scale differences amongst the models. The reinforcement learning agent receives the average of the ensemble as the reward, plus the control penalty used in OpenAI Gym (Brockman et al., 2016). This control penalty represents a standard safety prior over reward functions for robotics tasks, namely to minimize joint torques. We found that optimizing a policy based solely on this control penalty does not lead to forward locomotion, thus learning a reward function from demonstrations is still necessary."
        },
        {
            "heading": "5.1.3. RESULTS",
            "text": "Learned Policy Performance We measured the performance of the policy learned by T-REX by measuring the forward distance traveled. We also compared against Behavior Cloning from Observations (BCO) (Torabi et al., 2018a), a state-of-the-art learning-from-observation method, and Generative Adversarial Imitation Learning (GAIL) (Ho & Ermon, 2016), a state-of-the-art inverse reinforcement learning algorithm. BCO trains a policy via supervised learning,\nand has been shown to be competitive with state-of-the-art IRL (Ho & Ermon, 2016) on MuJoCo tasks without requiring action labels, making it one of the strongest baselines when learning from observations. We trained BCO using only the best demonstration among the available suboptimal demonstrations. We trained GAIL with all of the demonstrations. GAIL uses demonstrator actions, while T-REX and BCO do not.\nWe compared against three different levels of suboptimality (Stage 1, 2, and 3), corresponding to increasingly better demonstrations. The results are shown in Figure 2 (see the appendix for full details). The policies learned by T-REX perform significantly better than the provided suboptimal trajectories in all the stages of HalfCheetah and Hopper. This provides evidence that T-REX can discover reward functions that extrapolate beyond the performance of the demonstrator. T-REX also outperforms BCO and GAIL on all tasks and stages except for Stage 2 for Hopper and Ant. BCO and GAIL usually fail to perform better than the average demonstration performance because they explicitly seek to imitate the demonstrator rather than infer the demonstrator\u2019s intention.\nReward Extrapolation We next investigated the ability of T-REX to accurately extrapolate beyond the demonstrator. To do so, we compared ground-truth return and T-REXinferred return across trajectories from a range of performance qualities, including trajectories much better than the best demonstration given to T-REX. The extrapolation of the reward function learned by T-REX is shown in Figure 3. The plots in Figure 3 give insight into the performance of T-REX. When T-REX learns a reward function that has a strong positive correlation with the ground-truth reward function, then it is able to surpass the performance of the\nsuboptimal demonstrations. However, in Ant the correlation is not as strong, resulting in worse-than-demonstrator performance in Stage 2."
        },
        {
            "heading": "5.2. Atari",
            "text": ""
        },
        {
            "heading": "5.2.1. DEMONSTRATIONS",
            "text": "We next evaluated T-REX on eight Atari games shown in Table 1. To obtain a variety of suboptimal demonstrations, we generated 12 full-episode trajectories using PPO policies checkpointed every 50 training updates for all games except for Seaquest and Enduro. For Seaquest, we used every 5th training update due to the ability of PPO to quickly find a good policy. For Enduro, we used every 50th training update starting from step 3,100 since PPO obtained 0 return until after 3,000 steps. We used the OpenAI Baselines implementation of PPO with the default hyperparameters."
        },
        {
            "heading": "5.2.2. EXPERIMENTAL SETUP",
            "text": "We used an architecture for reward learning similar to the one proposed in (Ibarz et al., 2018), with four convolutional layers with sizes 7x7, 5x5, 3x3, and 3x3, with strides 3, 2, 1, and 1. Each convolutional layer used 16 filters and LeakyReLU non-linearities. We then used a fully connected layer with 64 hidden units and a single scalar output. We fed in stacks of 4 frames with pixel values normalized between 0 and 1 and masked the game score and number of lives.\nFor all games except Enduro, we subsampled 6,000 trajectory pairs between 50 and 100 observations long. We optimized the reward functions using Adam with a learning rate of 5e-5 for 30,000 steps. Given two full trajectories \u03c4i and \u03c4j such that \u03c4i \u227a \u03c4j , we first randomly sample a subtrajectory from \u03c4i. Let ti be the starting timestep for this subtrajectory. We then sample an equal length subtrajectory from \u03c4j such that ti \u2264 tj , where tj is the starting time step of the subtrajectory from \u03c4j . We found that this resulted in\nbetter performance than comparing randomly chosen subtrajectories, likely due to the fact that (1) it eliminates pairings that compare a later part of a worse trajectory with an earlier part of a better trajectory and (2) it encourages reward functions that are monotonically increasing as progress is made in the game. For Enduro, training on short partial trajectories was not sufficient to score any points and instead we used 2,000 pairs of down-sampled full trajectories (see appendix for details).\nWe optimized a policy by training a PPO agent on the learned reward function. To reduce reward scaling issues, we normalized predicted rewards by feeding the output of r\u0302\u03b8(s) through a sigmoid function before passing it to PPO. We trained PPO on the learned reward function for 50 million frames to obtain our final policy. We also compare against Behavioral Cloning from Observation (BCO) (Torabi et al., 2018a) and the state-of-the-art Generative Adversarial Imitation Learning (GAIL) (Ho & Ermon, 2016). Note that we give action labels to GAIL, but not to BCO or T-REX. We tuned the hyperparameters for GAIL to maximize performance when using expert demonstrations on Breakout and Pong. We gave the same demonstrations to both BCO and T-REX; however, we found that GAIL was very sensitive to poor demonstrations so we trained GAIL on 10 demonstrations using the policy checkpoint that generated the best demonstration given to T-REX."
        },
        {
            "heading": "5.2.3. RESULTS",
            "text": "Learned Policy Performance The average performance of T-REX under the ground-truth reward function and the best and average performance of the demonstrator are shown in Table 1. Table 1 shows that T-REX outperformed both BCO and GAIL in 7 out of 8 games. T-REX also outperformed the best demonstration in 7 out of 8 games. On four games (Beam Rider, Breakout, Enduro, and Q*bert) T-REX achieved score that is more than double the score of the best\ndemonstration. In comparison, BCO performed worse than the average performance of the demonstrator in all games, and GAIL only performed better than the average demonstration on Space Invaders. Despite using better training data, GAIL was unable to learn good policies on any of the Atari tasks. These results are consistent with those of Tucker et al. (2018) that show that current GAN-based IRL methods do not perform well on Atari. In the appendix, we compare our results against prior work (Ibarz et al., 2018) that uses demonstrations plus active feedback during policy training to learn control policies for the Atari domain.\nReward Extrapolation We also examined the extrapolation of the reward function learned using T-REX. Results are shown in Figure 4. We observed accurate extrapolation for Beam Rider, Breakout, Enduro, Seaquest, and Space Invaders\u2014five games where T-REX significantly outperform the demonstrator. The learned rewards for Pong, Q*bert, and Hero show less correlation. On Pong, T-REX overfits to the suboptimal demonstrations and ends up preferring longer games which do not result in a significant win or loss. T-REX is unable to score any points on Hero, likely due to poor extrapolation and the higher complexity of the game. Surprisingly, the learned reward function for Q*bert shows poor extrapolation, yet T-REX is able to outperform the demonstrator. We analyzed the resulting policy for Q*bert and found that PPO learns a repeatable way to score points by inducing Coily to jump off the edge. This behavior was not seen in the demonstrations. In the appendix, we plot the maximum and minimum predicted observations from the trajectories used to create Figure 4 along with attention maps for the learned reward functions."
        },
        {
            "heading": "5.2.4. HUMAN DEMONSTRATIONS",
            "text": "The above results used synthetic demonstrations generated from an RL agent. We also tested T-REX when given ground-truth rankings over human demonstrations. We used\nnovice human demonstrations from the Atari Grand Challenge Dataset (Kurin et al., 2017) for five Atari tasks. TREX was able to significantly outperform the best human demonstration in Q*bert, Space Invaders, and Video Pinball, but was unable to outperform the human in Montezuma\u2019s Revenge and Ms Pacman (see the appendix for details)."
        },
        {
            "heading": "5.3. Robustness to Noisy Rankings",
            "text": "All experiments described thus far have had access to ground-truth rankings. To explore the effects of noisy rankings we first examined the stage 1 Hopper task. We synthetically generated ranking noise by starting with a list of trajectories sorted by ground-truth returns and randomly swapping adjacent trajectories. By varying the number of swaps, we were able to generate different noise levels. Given n trajectories in a ranked list provides ( n 2 ) pairwise preferences over trajectories. The noise level is measured as a total order correctness: the fraction of trajectory pairs whose pairwise ranking after random swapping matches the original ground-truth pairwise preferences. The results of this experiment, averaged over 9 runs per noise level, are shown in Figure 5. We found that T-REX is relatively robust to noise of up to around 15% pairwise errors.\nTo examine the effect of noisy human rankings, we used the synthetic PPO demonstrations that were used in the previous Atari experiments and used Amazon Mechanical Turk to collect human rankings. We presented videos of the demonstrations in pairs along with a brief text description of the goal of the game and asked workers to select which demonstration had better performance, with an option for selecting \u201cNot Sure\u201d. We collected six labels per demonstration pair and used the most-common label as the label for training the reward function. We removed from the training data any pairings where there was a tie for the most-common label or where \u201cNot Sure\u201d was the most common label. We found that despite this preprocessing step, human labels added a\nsignificant amount of noise and resulted in pair-wise rankings with accuracy between 63% and 88% when compared to ground-truth labels. However, despite significant ranking noise, T-REX outperformed the demonstrator on 5 of the 8 Atari games (see the appendix for full details)."
        },
        {
            "heading": "5.3.1. LEARNING FROM TIME-BASED RANKINGS",
            "text": "Finally, we tested whether T-REX has the potential to work without explicit rankings. We took the same demonstrations used for the Mujoco tasks, and rather than sorting them based on ground-truth rankings, we used the order in which they were generated by PPO to produce a ranked list of trajectories, ordered by timestamp from earliest to latest.\nThis provides ranked demonstrations without any need for demonstrator labels, and enables us to test whether simply observing an agent learn over time allows us to extrapolate intention by assuming that later trajectories are preferable to trajectories produced earlier in learning. The results for Hopper are shown in Figure 5 and other task results are shown in the appendix. We found that T-REX is able to infer a meaningful reward function even when noisy, time-based rankings are provided. All the trained policies produced comparable results on most stages to the groundtruth rankings, and those policies outperform BCO and GAIL on all tasks and stages except for Ant Stage 2."
        },
        {
            "heading": "6. Conclusion",
            "text": "In this paper, we introduced T-REX, a reward learning technique for high-dimensional tasks that can learn to extrapolate intent from suboptimal ranked demonstrations. To the best of our knowledge, this is the first IRL algorithm that is able to significantly outperform the demonstrator without additional external knowledge (e.g. signs of feature contributions to reward) and that scales to high-dimensional Atari games. When combined with deep reinforcement learning, we showed that this approach achieves better-thandemonstrator performance as well as outperforming stateof-the-art behavioral cloning and IRL methods. We also demonstrated that T-REX is robust to modest amounts of ranking noise, and can learn from automatically generated labels, obtained by watching a learner noisily improve at a task over time."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work has taken place in the Personal AutonomousRobotics Lab (PeARL) at The University of Texas at Austin. PeARL research is supported in part by the NSF (IIS1724157, IIS-1638107, IIS-1617639, IIS-1749204) and ONR(N00014-18-2243)."
        },
        {
            "heading": "A. Code and Videos",
            "text": "Code as well as supplemental videos are available at https://github.com/hiwonjoon/ ICML2019-TREX."
        },
        {
            "heading": "B. T-REX Results on the MuJoCo Domain",
            "text": "B.1. Policy performance\nTable 1 shows the full results for the MuJoCo experiments. The T-REX (time-ordered) row shows the resulting performance of T-REX when demonstrations come from observing a learning agent and are ranked based on timestamps rather than using explicit preference rankings.\nB.2. Policy visualization\nWe visualized the T-REX-learned policy for HalfCheetah in Figure 1. Visualizing the demonstrations from different stages shows the specific way the policy evolves over time; an agent learns to crawl first and then begins to attempt to walk in an upright position. The T-REX policy learned from the highly suboptimal Stage 1 demonstrations results in a similar-style crawling gait; however, T-REX captures some of the intent behind the demonstration and is able to optimize a gait that resembles the demonstrator but with increased speed, resulting in a better-than-demonstrator policy. Similarly, given demonstrations from Stage 2, which are still highly suboptimal, T-REX learns a policy that resembles the gait of the best demonstration, but is able to optimize and partially stabilize this gait. Finally, given demonstrations from Stage 3, which are still suboptimal, T-REX is able to learn a near-optimal gait."
        },
        {
            "heading": "C. Behavioral Cloning from Observation",
            "text": "To build the inverse transition models used by BCO (Torabi et al., 2018a) we used 20,000 steps of a random policy to collect transitions with labeled states. We used the Adam optimizer with learning rate 0.0001 and L2 regularization of 0.0001. We used the DQN architecture (Mnih et al., 2015) for the classification network, using the same architecture to predict actions given state transitions as well as predict actions given states. When predicting P (a|st, st+1), we concatenate the state vectors obtaining an 8x84x84 input consisting of two 4x84x84 frames representing st and st+1.\nWe give both T-REX and BCO the full set of demonstrations. We tried to improve the performance of BCO by running behavioral cloning only on the bestX% of the demonstrations, but were unable to find a parameter setting that performed better than X = 100, likely due to a lack of training data when using very few demonstrations."
        },
        {
            "heading": "D. Atari reward learning details",
            "text": "We used the OpenAI Baselines implementation of PPO with default hyperparameters. We ran all of our experiments on an NVIDIA TITAN V GPU. We used 9 parallel workers when running PPO.\nWhen learning and predicting rewards, we mask the score and number of lives left for all games. We did this to avoid having the network learn to only look at the score and recognize, say, the number of significant digits, etc. We additionally masked the sector number and number of enemy ships left on Beam Rider. We masked the bottom half of the dashboard for Enduro to mask the position of the car in the race. We masked the number of divers found and the oxygen meter for Seaquest. We masked the power level and inventory for Hero.\nTo train the reward network for Enduro, we randomly downsampled full trajectories. To create a training set we repeatedly randomly select two full demonstrations, then randomly cropped between 0 and 5 of the initial frames from each trajectory and then downsampled both trajectories by only keeping every xth frame where x is randomly chosen between 3 and 6. We selected 2,000 randomly downsampled demonstrations and trained the reward network for 10,000 steps of Adam with a learning rate of 5e-5."
        },
        {
            "heading": "E. Comparison to active reward learning",
            "text": "In this section, we examine the ability of prior work on active preference learning to exceed the performance of the demonstrator. In Table 2, we denote the results that surpass the best demonstration with an asterisk (*). DQfD+A only surpasses the demonstrator in 3 out of 9 games tested, even with thousands of active queries. Note that DQfD+A extends the original DQfD algorithm (Hester et al., 2017), which uses demonstrations combined with RL on groundtruth rewards, yet is only able to surpass the best demonstration in 14 out of 41 Atari games. In contrast, we are able to leverage only 12 ranked demos to achieve betterthan-demonstrator performance on 7 out of 8 games tested, without requiring access to true rewards or access to thousands of active queries from an oracle.\nIbarz et al. (2018) combine Deep Q-learning from demonstrations and active preference queries (DQfD+A). DQfD+A uses demonstrations consisting of (st, at, st+1)-tuples to initialize a policy using DQfD (Hester et al., 2017). The algorithm then uses the active preference learning algorithm of Christiano et al. (2017) to refine the inferred reward function and initial policy learned from demonstrations. The first two columns of Table 2 compare the demonstration quality given to DQfD+A and T-REX. While our results make use of more demonstrations (12 for T-REX versus 4\u20137 for DQfD+A), our demonstrations are typically orders of magnitude worse than the demonstrations used by DQfD+A: on average the demonstrations given to DQfD+A are 38\ntimes better than those used by T-REX. However, despite this large gap in the performance of the demonstrations, TREX surpasses the performance of DQfD+A on Q*Bert, and Seaquest. We achieve these results using 12 ranked demonstrations. This requires only 66 comparisons (n \u00b7 (n\u2212 1)/2) by the demonstrator. In comparison, the DQfD+A results used 3,400 preference labels obtained during policy training using ground-truth rewards."
        },
        {
            "heading": "F. Human Demonstrations and Rankings",
            "text": "F.1. Human demonstrations\nWe used the Atari Grand Challenge data set (Kurin et al., 2017) to collect actual human demonstrations for five Atari games. We used the ground truth returns in the Atari Grand Challenge data set to rank demonstrations. To generate demonstrations we removed duplicate demonstrations (human demonstrations that achieved the same score). We then sorted the remaining demonstrations based on ground truth return and selected 12 of these demonstrations to form our training set. We ran T-REX using the same hyperparameters as described above.\nThe resulting performance of T-REX is shown in Table 3. T-REX is able to outperform the best human demonstration on Q*bert, Space Invaders, and Video Pinball; however, it is not able to learn a good control policy for Montezuma\u2019s Revenge or Ms Pacman. These games require maze navigation and balancing different objectives, such as collecting objects and avoiding enemies. This matches our results in the main text that show that T-REX is unable to learn a policy for playing Hero, a similar maze navigation task with multiple objectives such as blowing up walls, rescuing people, and destroying enemies. Extending T-REX to work in these types of settings is an interesting area of future work.\nF.2. Human rankings\nTo measure the effects of human ranking noise, we took the same 12 PPO demonstrations described above in the main text and had humans rank the demonstrations. We used Amazon Mechanical Turk and showed the workers two sideby-side demonstrations and asked them to classify whether video A or video B had better performance or whether they were unsure.\nWe took all 132 possible sequences of two videos across the 12 demonstrations and collected 6 labels for each pair of demonstrations. Because the workers are not actually giving the demonstrations and because some workers may exploit the task by simply selecting choices at random, we expect these labels to be a worst-case lower bound on the accuracy. To ameliorate the noise in the labels we take all 6 labels per pair and use the majority vote as the human label. If there is no majority or if the majority selects the \u201cNot Sure\u201d label, then we do not include this pair in our training data for T-REX.\nThe resulting accuracy and number of labels that had a majority preference are shown in Table 4. We ran T-REX using the same hyperparameters described in the main text. We ran PPO with 3 different seeds and report the performance of the best final policy averaged over 30 trials. We found that surprisingly, T-REX is able to optimize good policies for many of the games, despite noisy labels. However, we\ndid find cases such as Enduro, where the labels were too noisy to allow successful policy learning."
        },
        {
            "heading": "G. Atari Reward Visualizations",
            "text": "We generated attention maps for the learned rewards for the Atari domains. We use the method proposed by Greydanus et al. (2018), which takes a stack of 4 frames and passes a 3x3 mask over each of the frames with a stride of 1. The mask is set to be the default background color for each game. For each masked 3x3 region, we compute the absolute difference in predicted reward when the 3x3 region is not masked and when it is masked. This allows us to measure the influence of different regions of the image on the predicted reward. The sum total of absolute changes in reward for each pixel is used to generate an attention heatmap. We used the trajectories shown in the extrapolation plots in Figure 4 of the main text and performed a search using the learned reward function to find the observations with minimum and maximum predicted reward. We show the minimum and maximum observations (stacks of four frames) along with the attention heatmaps across all four stacked frames for the learned reward functions in figures 2\u20139. The reward function visualizations suggest that our networks are learning relevant features of the reward function."
        }
    ],
    "title": "Extrapolating Beyond Suboptimal Demonstrations via  Inverse Reinforcement Learning from Observations",
    "year": 2019
}