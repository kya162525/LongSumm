{
    "abstractText": "This paper studies a combination of generative Markov random field (MRF) models and discriminatively trained deep convolutional neural networks (dCNNs) for synthesizing 2D images. The generative MRF acts on higher-levels of a dCNN feature pyramid, controling the image layout at an abstract level. We apply the method to both photographic and non-photo-realistic (artwork) synthesis tasks. The MRF regularizer prevents over-excitation artifacts and reduces implausible feature mixtures common to previous dCNN inversion approaches, permitting synthezing photographic content with increased visual plausibility. Unlike standard MRF-based texture synthesis, the combined system can both match and adapt local features with considerable variability, yielding results far out of reach of classic generative MRF methods.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chuan Li"
        },
        {
            "affiliations": [],
            "name": "Johannes Gutenberg"
        },
        {
            "affiliations": [],
            "name": "Michael Wand"
        }
    ],
    "id": "SP:80d9a586c49ac6ec6a0b304ec1bb10d3f09fb526",
    "references": [
        {
            "authors": [
                "A. Agarwala",
                "M. Dontcheva",
                "M. Agrawala",
                "S. Drucker",
                "A. Colburn",
                "B. Curless",
                "D. Salesin",
                "M. Cohen"
            ],
            "title": "Inter- Figure 9: Cases where Gatys et al [8] works better. active digital photomontage",
            "venue": "ACM Trans. Graphics (Proc. Siggraph),",
            "year": 2004
        },
        {
            "authors": [
                "C. Barnes",
                "E. Shechtman",
                "A. Finkelstein",
                "D.B. Goldman"
            ],
            "title": "Patchmatch: A randomized correspondence algorithm for structural image editing",
            "venue": "ACM Trans. Graph. (Proc. Siggraph),",
            "year": 2009
        },
        {
            "authors": [
                "E.L. Denton",
                "R. Fergus",
                "A. Szlam",
                "S. Chintala"
            ],
            "title": "Deep generative image models using a laplacian pyramid of adversarial networks",
            "venue": "In Advances in Neural Information Processing Systems (NIPS)",
            "year": 2015
        },
        {
            "authors": [
                "C. Dong",
                "C.C. Loy",
                "K. He",
                "X. Tang"
            ],
            "title": "Image superresolution using deep convolutional networks, 2015",
            "venue": "ArXiv preprint: http://arxiv.org/abs/1501.00092",
            "year": 2015
        },
        {
            "authors": [
                "A.A. Efros",
                "W.T. Freeman"
            ],
            "title": "Image quilting for texture synthesis and transfer",
            "venue": "In Proc. ACM Siggraph,",
            "year": 2001
        },
        {
            "authors": [
                "A.A. Efros",
                "T.K. Leung"
            ],
            "title": "Texture synthesis by nonparametric sampling",
            "venue": "In Proc. IEEE Int. Conf. Computer Vision (ICCV),",
            "year": 1999
        },
        {
            "authors": [
                "L.A. Gatys",
                "A.S. Ecker",
                "M. Bethge"
            ],
            "title": "A neural algorithm of artistic style",
            "venue": "ArXiv preprint; http://arxiv.org/abs/ 1508.06576",
            "year": 2015
        },
        {
            "authors": [
                "L.A. Gatys",
                "A.S. Ecker",
                "M. Bethge"
            ],
            "title": "Texture synthesis and the controlled generation of natural stimuli using convolutional neural networks",
            "venue": "In Advances in Neural Information Processing Systems (NIPS)",
            "year": 2015
        },
        {
            "authors": [
                "J. Gauthier"
            ],
            "title": "Conditional generative adversarial nets for convolutional face generation",
            "venue": "http://www.foldl.me/2015/ conditional-gans-face-generation/,",
            "year": 2015
        },
        {
            "authors": [
                "I. Goodfellow",
                "J. Pouget-Abadie",
                "M. Mirza",
                "B. Xu",
                "D. Warde-Farley",
                "S. Ozair",
                "A. Courville",
                "Y. Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "In Advances in Neural Information Processing Systems (NIPS)",
            "year": 2014
        },
        {
            "authors": [
                "K. He",
                "J. Sun"
            ],
            "title": "Statistics of patch offsets for image completion",
            "venue": "In Proc. European Conference on Computer Vision (ECCV),",
            "year": 2012
        },
        {
            "authors": [
                "A. Hertzmann",
                "C.E. Jacobs",
                "N. Oliver",
                "B. Curless",
                "D.H. Salesin"
            ],
            "title": "Image analogies",
            "venue": "In Proc. ACM Siggraph",
            "year": 2001
        },
        {
            "authors": [
                "A. Krizhevsky",
                "I. Sutskever",
                "G.E. Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "In Advances in Neural Information Processing Systems (NIPS)",
            "year": 2012
        },
        {
            "authors": [
                "V. Kwatra",
                "I. Essa",
                "A. Bobick",
                "N. Kwatra"
            ],
            "title": "Texture optimization for example-based synthesis",
            "venue": "ACM Trans. Graph. (Proc. Siggraph),",
            "year": 2005
        },
        {
            "authors": [
                "V. Kwatra",
                "A. Sch\u00f6dl",
                "I. Essa",
                "G. Turk",
                "A. Bobick"
            ],
            "title": "Graphcut textures: Image and video synthesis using graph cuts",
            "venue": "ACM Trans. Graph. (Proc. Siggraph),",
            "year": 2003
        },
        {
            "authors": [
                "H. Lee",
                "S. Seo",
                "S. Ryoo",
                "K. Yoon"
            ],
            "title": "Directional texture transfer",
            "venue": "In Proc. Int. Symp. on Non-Photorealistic Animation and Rendering (NPAR),",
            "year": 2010
        },
        {
            "authors": [
                "C. Li",
                "M. Wand"
            ],
            "title": "Approximate translational building blocks for image decomposition and synthesis",
            "venue": "In ACM Transactions on Graphics,",
            "year": 2015
        },
        {
            "authors": [
                "Y. Lu",
                "S. Zhu",
                "Y.N. Wu"
            ],
            "title": "Learning FRAME models using CNN filters for knowledge visualization",
            "year": 2015
        },
        {
            "authors": [
                "A. Mahendran",
                "A. Vedaldi"
            ],
            "title": "Understanding deep image representations by inverting them",
            "venue": "In IEEE Conf. on Comp. Vision and Pattern Recognition (CVPR),",
            "year": 2015
        },
        {
            "authors": [
                "A. Mordvintsev",
                "C. Olah",
                "M. Tyka"
            ],
            "title": "Inceptionism: Going deeper into neural networks",
            "venue": "http://googleresearch.blogspot. com/2015/06/inceptionism-going-deeper-into-neural.html,",
            "year": 2015
        },
        {
            "authors": [
                "J. Portilla",
                "E.P. Simoncelli"
            ],
            "title": "A parametric texture model based on joint statistics of complex wavelet coefficients",
            "venue": "Int. J. Comput. Vision,",
            "year": 2000
        },
        {
            "authors": [
                "O. Russakovsky",
                "J. Deng",
                "H. Su",
                "J. Krause",
                "S. Satheesh",
                "S. Ma",
                "Z. Huang",
                "A. Karpathy",
                "A. Khosla",
                "M. Bernstein",
                "A.C. Berg",
                "L. Fei-Fei"
            ],
            "title": "ImageNet Large Scale Visual Recognition Challenge",
            "venue": "Int. J. Computer Vision (IJCV),",
            "year": 2015
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "Preprint; http://arxiv.org/abs/1409.1556",
            "year": 2014
        },
        {
            "authors": [
                "L. Theis",
                "M. Bethge"
            ],
            "title": "Generative image modeling using spatial lstms",
            "venue": "In Advances in Neural Information Processing Systems (NIPS)",
            "year": 2015
        },
        {
            "authors": [
                "L.-Y. Wei",
                "M. Levoy"
            ],
            "title": "Fast texture synthesis using treestructured vector quantization",
            "venue": "In Proc. ACM Siggraph",
            "year": 2000
        },
        {
            "authors": [
                "X. Xie",
                "F. Tian",
                "H.S. Seah"
            ],
            "title": "Feature guided texture synthesis (fgts) for artistic style transfer",
            "venue": "In Proc. Int. Conf. Digital Interactive Media in Entertainment and Arts (DIMEA),",
            "year": 2007
        },
        {
            "authors": [
                "L. Xu",
                "J.S. Ren",
                "C. Liu",
                "J. Jia"
            ],
            "title": "Deep convolutional neural network for image deconvolution",
            "venue": "In Advances in Neural Information Processing Systems (NIPS)",
            "year": 2014
        },
        {
            "authors": [
                "M.D. Zeiler",
                "R. Fergus"
            ],
            "title": "Visualizing and understanding convolutional networks",
            "venue": "In Proc. Europ. Conf. Comp. Vision (ECCV), pages 818\u2013833,",
            "year": 2014
        },
        {
            "authors": [
                "Y. Zhang",
                "J. Xiao",
                "J. Hays",
                "P. Tan"
            ],
            "title": "Framebreak: Dramatic image extrapolation by guided shift-maps",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2013
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "The problem of synthesizing content by example is a classic problem in computer vision and graphics. It is of fundamental importance to many applications, including creative tools such as high-level interactive photo editing [2, 3, 13], as well as scientific applications, such as generating stimuli in psycho-physical experiments [9].\nIn this paper, we specifically consider the problem of data-driven image synthesis: Given an example image, we want to fully automatically create a variant of the example image that looks similar but differs in structure. The intended deviation is controlled by additional constraints provided by the user, ranging from just changing image dimensions to detailed layout specifications. Concretely, we implement this by splitting up the input into a \u201cstyle\u201d image and a \u201ccontent\u201d image [9, 13]. The first describes the building blocks the image should be made of, the second constrains their layout. Figure 1 shows an example of style transferred images, where the input images are shown on\nthe left. Our results are are shown on the right. Notice our method produces plausible results for both art to photo and photo to art transfers. In particular, see how meso-structures in the style images, such as the mouth and eyes, are intentionally reused in the synthesized images.\nThe classic data-driven approach to generative image modeling is based on Markov random fields (MRFs): We assume that the most relevant statistical dependencies in an image are present at a local level and learn a distribution over the likelihood of local image patches by considering all local k \u00d7 k pixel patches in the example image(s). Usually, this is done using a simple nearest-neighbor scheme [7], and inference is performed by approximate MRF inference [6, 15, 16] or greedy approximations [3, 7, 13, 26].\nA critical limitation of MRFs texture synthesis is the difficulty of learning the distribution of plausible image patches from example data. Even the space of local k \u00d7 k image patches (typically: k \u2248 5...31) is already way too high-dimensional to be covered with simple sampling and nearest-neighbor estimation. The results are mismatched local pieces, which are subsequently stitched [16] or blended [15] together in order to minimize the perceptual impact of the lack of ability to generalize. The missing ingredient is a strong scheme for interpolating and adapting images from very sparse example sets of sample patches.\n1\nar X\niv :1\n60 1.\n04 58\n9v 1\n[ cs\n.C V\n] 1\n8 Ja\nn 20\nIn terms of invariance and ability to generalize, discriminatively trained deep convolutional neural networks have recently made dramatic impact [14, 24]. They are able to recognize complex classes of image features, modeling non-linear deformations and appearance variations far beyond the abilities of simple nearest neighbors search.\nHowever, the discriminative design poses a problem: The corresponding dCNNs compress image information progressively over multiple pooling layers to a very coarse representation, which encodes semantically relevant feature descriptors in a semi-distributed (not spatially localized) fashion (Figure 2). While an inverse process can be defined [4, 5, 8, 9, 10, 20, 28, 29], it reamins difficult to control: For example, simply maximizing the class-excitation of the network leads to hallucinatory patterns [21]. Rather than that, we need to reproduce the correct statistics for neural encoding in the synthesis images.\nAddressing this problem, Gatys et al. [8, 9] have recently demonstrated remarkable results for transferring styles to guiding \u201ccontent\u201d images: Their method uses the filter pyramid of the VGG network [24] as a higher-level representation of images, benefitting from the vast knowledge acquired through training the dCNN on millions of photographs. Then, feature layout is simply controlled by penalizing the difference of the high-level neural encoding of the newly synthesized image to that of the content image. Further, the process is regularized by matching feature statistics of the \u201cstyle\u201d image and the newly synthesized picture by matching the correlations across the various filter channels, captured by a Gram matrix. The method yields very impressive results for applying artistic styles of paintings to photographs [8]. However, strict local plausibility remains difficult. In particular, using photographs as styles does not yield plausible results because only per-pixel feature correlations are captured at different layers and the spatial layout is constrained too weakly.\nOur paper augments their framework by replacing the bag-of-feature-like statistics of Gram-matrix-matching by an MRF regularizer that maintains local patterns of the \u201cstyle\u201d exemplar: MRFs and dCNNs are a canonical combination \u2014 both models crucially rely on the assumption of\nlocally correlated information and translational invariance. This equips the encoding of features in a dCNN with approximate Markovian consistency properties: Local patches have characteristic arrangements of feature activations to describe objects, and higher-up encoding becomes more invariant under in-class variation (Figure 2). We therefore use the generative MRF model on such higher levels of the network (relu3 1 and relu4 1 of the 19-layer VGG network1). This prescribes a plausible local layout of objects in terms of more abstract categories and, importantly, tries to ensure a consistent encoding of these higher-level features. The actual task of generalizing within object categories and plausible blending is then performed by the dCNN\u2019s lower levels via inversion [20].\nTechnically, we implement the additional MRF prior by an additional energy term that models Markovian consistency of the upper layers of the dCNN feature pyramid. We then utilize the EM algorithm of Kwatra et al. [15] for MRF optimization: It easily integrates in the variational framework. Further, we will show that higher-level neural encodings are more perceptually linear, which matches well with the linear blending approach of the M-step.\nWe apply our method to a number of photo-realistic and non-photo-realistic image synthesis tasks, and show it is able to generalize among image patches far beyond the abilities of classic MRFs. In style-transfer scenarios, the combined method additionally benefits from the abilities of the dCNN to match semantically related image portions automatically, without user annotations. In comparison to previous methods that invert dCNNs, the MRF prior improves local plausibility of the feature layouts, avoiding hallucinatory artifacts and usually providing a more plausible mesostructure than the statistical approach of Gatys et al. [8]. In particular, we can strongly improve the plausibility of synthesizing photographs, which was not possible with the previous methods.\n1Notice that in Figure 2 layer relu5 1 shows the most discriminative encoding for single pixels. However, in practice we found using 3 \u00d7 3 patches at layer relu4 1 produce the best synthesis results. Intuitively, using patches from a slightly lower layer has similar matching performance, but permits overlapped MRFs and increased details in synthesis."
        },
        {
            "heading": "2. Related Work",
            "text": "Image synthesis with neural networks: The success of dCNNs in discriminative tasks [23] has also raised interest in generative variants. Zeiler et al. [29] introduce a deconvolutional network to back-project neuron activations to pixels. Similarly, Mahendran and Vedaldi [20] reconstruct images from the neural encoding in intermediate layers. The work of Gatys et al [8], detailed above, can also be employed in unguided settings [9], outperforming traditional parametric texture synthesis which only uses a linear feature bank and no statistical priors [22].\nA further approach is the framework of generative adversarial networks [11]. Here, two networks, one as the discriminator and other as the generator iteratively improve each other by playing a minnimax game. In the end the generator is able to produces more natural images than naive image synthesis. However, in many cases the output quality is still rather limited. Gauthier et al. [10] extend this model by a Laplacian pyramid. This leads to clear improvement on output quality. Nonetheless, training for large images remains expensive and the results often still lack structure. Denton et al. [4] extend the model to a conditional setting, limited to generating faces. It is also possible to re-train networks for specific generative tasks, such as image deblur [28], super-resolution [5], and class visualization [19].\nMRF-based image synthesis: MRFs are the classic framework for for non-parametric image synthesis [7]. As explained in the introduction, a key issue is adapting local patches beyond simple stitching [16] or blending [15], and our paper focuses on this issue. Aside from this, MRF models suffer from a second, significant limitation: Local image statistics is usually not sufficient for capturing complex image layouts at a global scale. While local details appear plausible, the global arrangement often merely resembles an unstructured \u201ctexture soup\u201d. Multi-resolution synthesis [13, 15, 26] provides some improvement here (and we adapt this in our method, too), but a principled solution requires additional high-level constraints. These can be either explicitly provided by the user [2, 3, 6, 8, 13, 17, 27], or learned from non-local image statistics [12, 18, 30]. Long range correlations have also been modeled by spatial LTSM neural networks; results so far are still limited to semiregular textures [25]. Our paper opts for the first, simpler solution of explicit layout constraints through a \u201ccontent\u201d image [8, 13] \u2014 in principle, learning of global layout constraints is mostly orthogonal to our approach."
        },
        {
            "heading": "3. Model",
            "text": "We now discuss our combined MRFs and dCNNs model for image synthesis. We assume that we are given a style image, denoted by xs \u2208 Rws\u00d7hs , and a content image xc \u2208 Rwc\u00d7hc for guidance. The (yet unknown) synthesized\nimage is denoted by x \u2208 Rwc\u00d7hc . We transfer the style of xs into the layout of xc by making the high-level neural encoding of x similar to xc, but using local patches similar to those of xs. The latter is the MRF prior that maintains the encoding of the style. Formally, x minimizes the following energy function:\nx = arg min x Es(\u03a6(x),\u03a6(xs)) +\n\u03b11Ec(\u03a6(x),\u03a6(xc)) + \u03b12\u03a5(x) (1)\nEs denotes the style loss function (MRFs constraint), where \u03a6(x) is x\u2019s feature map from some layer in the network. Ec is the content loss function. It computes the squared distance between the feature map of the synthesis image and that of the content guidance image xc. As shown in [8, 20], minimizing Ec generates an image that is contextually related to xc. The additional regularizer \u03a5(x) is a smoothness prior on the reconstruction. Next, we explain how to define these terms in details.\nMRFs loss function: Let \u03a8(\u03a6(x)) denote the list of all local patches extracted from \u03a6(x) \u2013 a specified set of feature maps of x. Each \u201cneural patch\u201d is indexed as \u03a8i(\u03a6(x)) and of the size k \u00d7 k \u00d7 C, where k is the width and height of the patch, and C is the number of channels for the layer where the patch is extracted from. We set the energy function to\nEs(\u03a6(x),\u03a6(xs)) = m\u2211 i=1 ||\u03a8i(\u03a6(x))\u2212\u03a8NN(i)(\u03a6(xs))||2\n(2) Here m is the cardinality of \u03a8(\u03a6(x)). For each patch \u03a8i(\u03a6(x)) we find its best matching patch \u03a8NN(i)(\u03a6(xs)) using normalized cross-correlation over all ms example patches in \u03a8(\u03a6(xs)):\nNN(i) := arg min j=1,...,ms \u03a8i(\u03a6(x)) \u00b7\u03a8j(\u03a6(xs)) |\u03a8i(\u03a6(x))| \u00b7 |\u03a8j(\u03a6(xs))|\n(3)\nWe use normalized cross-correlation to achieves stronger invariance. The matching process can be efficiently executed by an additional convolutional layer (explained in the implement details). Notice although we use normalized cross-correlation to find the best match, their Euclidean distance is minimized in Equation 2 for producing an image that is visually close to the reference style.\nContent loss function: Ec guides the content of the synthesized image by minimizing the squared Euclidean distance between \u03a6(x) and \u03a6(xc):\nEc(\u03a6(x),\u03a6(xc)) = ||\u03a6(x)\u2212 \u03a6(xc)||2 (4)\nRegularizer: There is significant amount of low-level image information discarded during the discriminative training of the network. In consequence, reconstructing an\nimage from its neural encoding can be noisy and unnatural. For this reason, we penalize the squared gradient norm [20] to encourage smoothness in the synthesized image:\n\u03a5(x) = \u2211 i,j ( (xi,j+1 \u2212 xi,j)2 + (xi+1,j \u2212 xi,j)2 ) (5)\nMinimization: We minimize Equation 1 using backpropagation with Limited-memory BFGS. In particular, the gradient of Es with respect to the feature maps is the element-wise difference between \u03a6(x) and their MRFs based reconstruction using patches from \u03a6(xs). Such a reconstruction is essentially a texture optimization process [15] that uses neural patches instead of pixel patches. It is crucially important to optimize this MRF energy at the neural level, as the traditional pixel based texture optimization will not be able produce results of comparable quality.\nWeight The \u03b11 and \u03b12 are weights for the content constraint and the natural image prior, respectively. We set \u03b11 = 0 for non-guided synthesis. By default we set \u03b11 = 1 for style transfer, while user can fine tune this value to interpolate between the content and the style. \u03b12 is fixed to 0.001 for all cases."
        },
        {
            "heading": "4. Analysis",
            "text": "Our key insight is that combining MRF priors with dCNN can significantly improve synthesis quality. This section provides a detailed analysis of our method from three perspectives: we first show compared to pixel values, neural activation leads to better patch matching and blending. We then show how MRFs can further improve the results."
        },
        {
            "heading": "4.1. Neural Matching",
            "text": "A key component of non-parametric image synthesis is to match the synthesized data with the example. (Figure 3 is a toy example that shows neural activation gives better matching than pixels. The task is to match two different car images. The first column contains the query patches from one car; every other column shows the best matching in the other car, found at different feature maps (including the pixel layer).\nIt is clear that patch matching using pixels or neural activation at the lower layers (such as relu2 1) is sensitive to appearance variation. The neural activations at layers relu3 1 and relu4 1 give better results. The top layers (relu5 1) seem to decrease the match quality due to the increasing invariance against appearance variation. This is not surprising because the features at middle layers are usually trained for recognizing object parts, as discussed in [29]. For these reason, we use neural patches at layers relu3 1 and relu4 1 as MRFs to constrain the synthesis process."
        },
        {
            "heading": "4.2. Neural Blending",
            "text": "The least-squared optimization for minimizing the texture term (Es in Equation 2) leads to a linear blending operation for overlapping patches. Here we show that blending neural patches often works better than directly blending pixel patches. Specifically, we compare two groups of blending results: The first method is to directly blend the pixels of two input patches. The second method passes these patches through the network and blend their neural activations at different layers. For each layer, we then reconstruct the blending result back into the pixel space using the method described in [20].\nFigure 4 compares the results of these two methods. The first two columns are the input patches A and B for blending. They are intentionally chosen to be semantically related and structurally similar, but are significantly different\nin pixel values. The third column shows the average of these two patches. Each of the remaining column shows the reconstruction of the blending at a different layer. It is clear that pixel based and neural based blending give very different results: averaging the pixels often gives strong ghost artifacts. This can be reduced as we dive deeper into the network: through experiments we observed that the middle level layers such as relu3 1 and relu4 1 often give more meaningful blendings. Lower layers such as relu2 1 behave similarly to pixels; reconstruction from layers beyond relu4 1 tends to be too fuzzy to be used for synthesis. This also matches our previous observation of middle layers\u2019 privilege for patch matching \u2013 because a representation that gives better discriminative performance is more robust to noise and enables better interpolation."
        },
        {
            "heading": "4.3. Effect of the MRF Prior",
            "text": "Despite the advantages in patch matching and blending, it is still possible for a dCNN to generate implausible results. For example, the matched patches from different layers might not always fire at the same place in the image space, indicated by the offsets between the patches found at layer relu3 1 and relu4 1 (Figure 3). The blending may also produce artifacts, for example, the destructed face of the dog (relu4 1, figure 4) and the ghosting eyes of the cat (relu3 1, figure 4). Extreme cases can be found at [21] which produces hallucinogenic images by simply stimulating neural activations without any constraint of the natural image statistics.\nMore natural results can be obtained through additional regularizer such as smothness (total variation) or the Gram matrix of pixel-wise filter responses [20, 8]. Our approach adds an MRF regularizer to the middle / upper levels of the network. To show the benefits of the MRFs prior, we compare the synthesis results with and without the constraint. We compare against the \u201cstyle constraint\u201d based on matching Gram matrices from [8]. Figure 5 validates the intended improvements in local consistency. The first row shows im-\nage patches cropped from our results. They are visually consistent to the patches in the original style images. In contrast, [8] produces artifacts such as distortions and smears. The new MRF prior reduces flexibility in local adaptation in favor of reproducing meso-scale features more faithfully."
        },
        {
            "heading": "5. Implementation Details",
            "text": "This section describes implementation details of our algorithm 2. We use the pre-trained 19-layer VGG-Network from [24]. The synthesis x is initialized as random noise, and iteratively updated by minimizing Equation 1 using back-propagation. We use layer relu3 1 and relu4 1 for MRFs prior, and layer relu4 2 for content constraint.\nFor both layer relu3 1 and relu4 1 we use 3\u00d7 3 patches. To achieve the best synthesis quality we set the stride to one so patches are very densely sampled. The patch matching (Equation 3) is implemented as an additional convolutional layer for fast computation. In this case patches sampled from the style image are treated as the filters. The best matching of a query patch is the filter that gives the maximum response. We can pre-computed the magnitude of the filters (||\u03a8i(\u03a6(xt))|| in Equation 3) as they will not change during the synthesis. Unfortunately the magnitude of patches in the synthesized image (||\u03a8i(\u03a6(x))||) needs to be computed on the fly.\nIn practice we use a multi-resolution process: We built a image pyramid using the scaling factor of two, and stop when the longest dimension of the synthesized image is less than 64 pixels. The reference texture and reference content images are scaled accordingly. We perform 200 iterations for each resolution, and the output of the previous resolution is bi-linearly up-sampled as the initialization for the next resolution. We implement our algorithm under the Torch framework. Our algorithm take about three minutes to synthesis an image of size 384\u00d7384 with a Titan X GPU.\nTo partially overcome the perspective and scale difference between the style and the content images, we sample patches from a number of copies of the style image with different rotations and scales: we use seven scales {0.85, 0.9, 0.95, 1, 1.05, 1.1, 1.15}, and for each scale we create five rotational copies {\u2212 \u03c012 ,\u2212 \u03c0 24 , 0, \u03c0 24 , \u03c0 12}. Since increasing the number of patches is computational expensive, in practice we only use the rotational copies for objects that can deform \u2013 for example faces."
        },
        {
            "heading": "6. Results",
            "text": "This section discusses our results. Here we focus on style transfer and refer readers to our supplementary material for the results of un-guided synthesis. As discussed later in this section, our method has strong restriction with the input data, so a formal user study is less meaningful. For\n2We release code at: https://github.com/chuanli11/CNNMRF\nthis reason, our evaluation is performed by demonstrating our method with a number of typical successful and failure cases, with comparisons and discussions related to the state of the art neural based image stylization method [8]. We refer readers to our supplementary report for more results.\nFigure 6 shows two examples of stylizing photos by artwork. The input photos (left) are stylized using Pablo Picasso\u2019s \u201cSelf-portrait 1907\u201d and Wassily Kandinsky\u2019s \u201cComposition VIII\u201d, respectively. In both cases, Gatys et al.\u2019s [8] method (middle) achieves interesting results, preserving the content of the photos and the overall look and feel of the artworks. However, there are weaknesses on closer inspection: The first result (top row) contains many unnecessary details, and the eyes look unnatural. Their second result lost the characteristic shapes in the original painting and partially blends with the content exemplar. In contrast, our first result synthesized more plausible facial features. In the second result, our method also resembles the style better: notice the important facial features such as eyes and the mouth are synthesized as simple shapes, and the hair as regions of dark color.\nFigure 7 shows two examples of photorealsitic synthesis. We transfer the style of a vintage car to two different modern vehicles. Notice the lack of photo-realistic details and strong smears in [8]\u2019s results. With the MRFs constraint (right) our results are closer to photorealistic.\nFrom an informal user study, we observed that [8] usually keeps the content better, and our method produces more accurate styles. Figure 8 gives detailed analysis between these two different characters. Here we show three patches from the content image (red boxes), and their closet matches\nfrom the style image and the synthesis images (using neural level matching). Notice our method produces more plausible results when a good match can be found between the content and the style images (the first patch), and performs less well when mis-matching happens (the second patch, where our synthesis deviates from the content image). For the third patch, there is no matching can be found for the car. In this case, our method replaces the car with texture synthesis, while [8] keeps the car and renders it with artifacts. In general, our method creates more stylish images, but may contain artifacts when the MRFs do not fit the content. In contrast, the parametric method [8] is more adaptable to the content, but at the cost of deviating from the style."
        },
        {
            "heading": "6.1. Limitations",
            "text": "Our method is an interesting extension for image based style transfer, especially for photorealistic styles. Nonetheless, it has many limitations. First and for most, it is more restricted to the input data: it only works if the content image can be re-assembled by the MRFs in the style image. For example, images of strong perspective or structure difference are not suitable for our method.\nFigure 9 shows two further example cases where [8] works better: The first example aim to transfer the style of a white dog according to the content of a yellow dog. Notice how the dogs\u2019 facial expression differ in these images. In this case our result (right) failed to reproduce the open mouth in the content image. The second example shows a artistic style that can be more easily transferred by the parametric method: Notice how the textures adapt to the content more naturally in [8]\u2019s method. In contrast, our method tries to \u201creshuffle\u201d building blocks in the style image and lost important features in the content image. In general, our method works better for subjects that allows structural deformation, such as faces and cars. For subjects that have strict symmetry properties such as architectures, it is often that our method will generate structural artifacts. In this case, structural statistics such as [12] may be used for regularizing the synthesized image.\nAlthough our method achieved improvement for photorealistic synthesis, it is still not as sharp as the original photos. This is due to the loss of non-discriminative image details during the training of the network. This opens an interesting future work that how the dCNN can be retrained, or incorporated with stitching based texture synthesis such as [16] for achieving pixel-level photorealism."
        },
        {
            "heading": "7. Conclusions",
            "text": "The key insight of this paper is that we can combine the discriminative power of a deep neural network with classical MRFs based texture synthesis. We developed a simple method that is able to produce encouraging new results for style transfer between images. We analyzed our results with\na number of typical successful and failure cases, and discussed its pros and cons compared to the state of the art neural based method for transferring image styles. Importantly, our results often preserve better mesostructures in the synthesized image. For this first time, this permits transferring photo-realistic styles with some plausibility. The stricter control of the mesostructure is also the biggest limitation at this point: The MRF prior only offers advantages when style and content images consists of similarly shaped elements without strong changes in perspective, size, or shape, as covered by the invariance of the high-level neural encoding. Otherwise, artifacts might occur. For pure artistic styles, the increased rigidity can then be a disadvantage.\nOur work is only one step in the direction of leveraging deep convolutional neural networks for improving image synthesis. It opens many interesting questions and future work such as how to resolve the incompatibility between the structure guidance and the MRFs [12]; how to more ef-\nficiently study and transfer the middle level style across a big dataset; and how to generate pixel-level photorealistic images by incorporating with stitching based texture synthesis [16], or with generative training of the network."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work has been partially supported by the Intel Visual Computing Institute and by the International Max Planck Research School for Computer Science. We thank Hao Su and Daniel Franzen for inspiring discussions."
        }
    ],
    "title": "Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis",
    "year": 2016
}