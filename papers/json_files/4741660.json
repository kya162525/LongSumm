{
    "abstractText": "Filters in convolutional neural networks are sensitive to their initialization. The random numbers used to initialize filters are a bias and determine if you will \u201cwin\u201d and converge to a satisfactory local minimum so we call this The Filter Lottery. We observe that the 28x28 Inception-V3 model without Batch Normalization fails to train 26% of the time when varying the random seed alone. This is a problem that affects the trial and error process of designing a network. Because random seeds have a large impact it makes it hard to evaluate a network design without trying many different random starting weights. This work aims to reduce the bias imposed by the initial weights so a network converges more consistently. We propose to evaluate and replace specific convolutional filters that have little impact on the prediction. We use the gradient norm to evaluate the impact of a filter on error, and re-initialize filters when the gradient norm of its weights falls below a specific threshold. This consistently improves accuracy on the 28x28 Inception-V3 with a median increase of +3.3%. In effect our method RandomOut increases the number of filters explored without increasing the size of the network. We observe that the RandomOut method has more consistent generalization performance, having a standard deviation of 1.3% instead of 2% when varying random seeds, and does so faster and with fewer parameters.",
    "authors": [
        {
            "affiliations": [],
            "name": "Joseph Paul Cohen"
        },
        {
            "affiliations": [],
            "name": "Henry Z. Lo"
        },
        {
            "affiliations": [],
            "name": "Wei Ding"
        }
    ],
    "id": "SP:7b4a71ea28f6ff289047617320ec62ab12e6dcc1",
    "references": [
        {
            "authors": [
                "L. Bandeira",
                "W. Ding",
                "T.F. Stepinski"
            ],
            "title": "Automatic Detection of Sub-km Craters Using Shape and Texture Information",
            "venue": "In Proceedings of the 41st Lunar and Planetary Science Conference,",
            "year": 2010
        },
        {
            "authors": [
                "Chen",
                "Tianqi"
            ],
            "title": "MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems",
            "venue": "Neural Information Processing Systems, Workshop on Machine Learning Systems,",
            "year": 2015
        },
        {
            "authors": [
                "Cohen",
                "Joseph Paul",
                "Lo",
                "Henry Z",
                "Lu",
                "Tingting",
                "Ding",
                "Wei"
            ],
            "title": "Crater Detection via Convolutional Neural Networks",
            "venue": "In Lunar and Planetary Institute Science Conference Abstracts,",
            "year": 2016
        },
        {
            "authors": [
                "Dauphin",
                "Yann N",
                "Pascanu",
                "Razvan",
                "Gulcehre",
                "Caglar",
                "Cho",
                "Kyunghyun",
                "Ganguli",
                "Surya",
                "Bengio",
                "Yoshua"
            ],
            "title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2014
        },
        {
            "authors": [
                "Glorot",
                "Xavier",
                "Bengio",
                "Yoshua"
            ],
            "title": "Understanding the difficulty of training deep feedforward neural networks",
            "venue": "In International conference on artificial intelligence and statistics,",
            "year": 2010
        },
        {
            "authors": [
                "Ioffe",
                "Sergey",
                "Szegedy",
                "Christian"
            ],
            "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
            "year": 2015
        },
        {
            "authors": [
                "Kingma",
                "Diederik",
                "Ba",
                "Jimmy"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Krizhevsky",
                "Alex",
                "Hinton",
                "Geoffrey"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Krizhevsky",
                "Alex",
                "Sutskever",
                "Ilya",
                "Hinton",
                "Geoffrey E"
            ],
            "title": "ImageNet Classification with Deep Convolutional Neural Networks",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2012
        },
        {
            "authors": [
                "YA LeCun",
                "L Bottou",
                "GB Orr",
                "M\u00fcller",
                "KR"
            ],
            "title": "Efficient backprop",
            "venue": "Neural networks: tricks of the trade,",
            "year": 1998
        },
        {
            "authors": [
                "LeCun",
                "Yann",
                "Bengio",
                "Yoshua"
            ],
            "title": "Convolutional networks for images, speech, and time series",
            "venue": "The handbook of brain theory and neural networks,",
            "year": 1995
        },
        {
            "authors": [
                "Szegedy",
                "Christian",
                "Vanhoucke",
                "Vincent",
                "Ioffe",
                "Sergey",
                "Shlens",
                "Jonathon",
                "Wojna",
                "Zbigniew"
            ],
            "title": "Rethinking the Inception Architecture for Computer Vision",
            "venue": "arXiv preprint arXiv:1512.00567,",
            "year": 2015
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "In convolutional neural networks (LeCun & Bengio, 1995; LeCun et al., 2015) different random seeds (ceteris paribus) greatly affect both the quality of the learned convolutional filters as measured by generalization error on the testing set. We call this issue The Filter Lottery because the\n1Montreal Institute for Learning Algorithms (MILA), Universite de Montre\u0301al 2University of Massachusetts Boston. Correspondence to: Joseph Paul Cohen <cohenjos@iro.umontreal.ca>.\nFigure 1. The Filter Lottery. This is a problem that affects the trail and error process of designing a network. Because random seeds have a large impact it makes it hard to evaluate a network design without trying many different random starting weights. This work aims to reduce the bias imposed by the initial weights so a network converges more consistently.\nrandom numbers used to initialize the network determine if you will \u201cwin\u201d and converge to a satisfactory test error. The issue was mentioned in (LeCun et al., 1998) and continues to be a challenge when training deep models which results in the typical workflow shown in Figure 1. In this work we explore it with a concrete example and propose a solution.\nBy simply changing the random initialization seed of a model we observe high variation in testing accuracy. For example a 28x28 Inception-V3 model without Batch Normalization trained on CIFAR-10 fails 26% of the time with an error was as low as random chance (Szegedy et al., 2015; Krizhevsky & Hinton, 2009). The same phenomena was observed 5% of the time when training a compact CraterCNN network on a dataset of Martian crater images (Cohen et al., 2016b; Bandeira et al., 2010; Cohen et al., 2016a). These results are to be expected because we are minimizing a non-convex loss function which we expect to have many local minima or saddle points that cause convergence behavior similar to that of local minima (Dauphin et al., 2014).\nWe suspect this is due to the network not constructing the filters needed to extract the most discriminative features. Figure 2 shows that varying the random seed can change how the filters will converge and also a large bias imposed by the random seed itself. When training the CraterCNN on different examples but the same random seed, the network learned almost identical filters. However, training on the same examples but with a different random seed the learned filters are drastically different, which has a strong impact on the testing accuracy. This indicates that bad random\nar X\niv :1\n60 2.\n05 93\n1v 3\n[ cs\n.C V\n] 2\n9 M\nay 2\n01 7\nseeds inhibit the learning of useful filters. We call these filters \u201cabandoned\u201d by the network because they contribute little to minimizing the error.\nOur experiments in \u00a74 indicate that wider networks (with more filters) have better performance. We believe that this is because more filters allows the network to successfully capture more discriminative features (similar to buying more lottery tickets). This would be unnecessary if all filters were utilized instead of being abandoned.\nCarefully scaled initialization (Glorot & Bengio, 2010) and better optimization is not sufficient to solve this problem; the network may still start with bad filters. The random weights (not just the distribution they are drawn from) have an impact on the potential accuracy of the network. This problem is a result of the iterative methods used to train neural networks. Adding Batch Normalization (Ioffe & Szegedy, 2015) layers resolve the problem in almost all cases but incur added runtime costs and parameters required after the training. These added layers lead to slower forward and back propagation because they are blocking operations that delay the next layers of the computation graph from processing. It is desirable to produce a model with minimum depth at test time and offload any added cost to training time.\nWe propose the method called RANDOMOUT in \u00a72 that scores filters and replaces them at training time if they have been abandoned by the network. This can be thought of as a regularizer for convolutional filters to keep their gradients high. If a filter\u2019s contribution to the objective is insignificant, then we re-initialize it with random values and continue learning. This allows the weights to learn a completely different filter which will give the network another chance to reach an acceptable stationary point. RANDOMOUT allows us to increase the number of filters explored without increasing the size of the network. This can potentially yield more compact networks which can train and\npredict with less computation."
        },
        {
            "heading": "2. RandomOut",
            "text": "What information can the gradient contain? The derivative for each convolutional filter weight \u2202L\u2202wi gives insight into the potential for that filter to influence the output. Here eachwi is a weight in the network with a differentiable path to a loss function L. During backprop this value is calculated and used to determine its contribution to the output of the loss function. It is calculated by identifying the unique path of operations that are applied to wi and multiplying the local gradient at every operation.\nConvolutional layers are no different in terms of the backprop algorithm. However for each convolutional filter we can calculate a holistic representation of it\u2019s influence to the loss function. We define the Convolutional Gradient Norm (CGN) in order to evaluate how much a filter (Lets call it k) will change as the network learns:\nCGN(k) = \u2211 i \u2223\u2223\u2223\u2223 \u2202L\u2202wki \u2223\u2223\u2223\u2223\nWe calculate this with respect to each minibatch. When the network error L is low then it is expected that gradients will be low because we have converged. A low CGN and a low overall error implies a filter was learned correctly. However, a low CGN and a high overall error can imply the filter:\n1. was abandoned by the network, decreasing its influence on the prediction in order to reduce error.\n2. overfit the training batch and learned some artifact of the training data.\n3. extracts a useful feature but cannot be adjusted to correct the current erroneous predictions.\nWhat drives gradients to be low and parts of a computation graph to be abandoned? To explain the intuition we use the example in Figure 3 which shows a small neural network with ReLU activations. Here a gradient of 1 for \u2202f \u2202f is backpropagated down the network. Lets first focus on relu2. The output of plus3 is negative, so relu2 routes its incoming gradient to the 0 term of the max(0,\u22121) and a 0 gradient is routed to that portion of the network. This drives the gradients that directly depend on it (\u2202w4, \u2202w3, and \u2202w5) to be 0. These weights will never be updated unless some data in the future can cause the ReLU to activate. Alternatively, the weights w4, w3, and w5 could be randomized to produce a better representation of x0 and x1 which could be more useful to minimize error.\nWe can also look at relu1 which is routing its gradient down the graph. The gradient applied to its downstream weights is low because the weightw6 is very low, causing the gradient passed to relu1 to be low. This weight may be increased ifw0, w1, andw2 are transforming the inputs x0 and x1 into something that will minimize error. If not further training epochs would decrease their influence by reducing w6 further.\nThe RANDOMOUT approach is to reinitialize weights for abandoned parts of the network if their CGN is below a threshold \u03c4 near 0. Because the gradients in these sections are low the impact is not very disruptive to the output. This is a concern because if the gradients to this region of the network were high, drastic changes to the weights would cause drastic changes to the output which would then cause large gradients to be sent back down the network which could cause havoc in the network. If the filters are randomized to a value that is used later in the network to reduce error its gradients will gradually increase and the section\nwill slowly be introduced back into the network.\nFormulating this into an algorithm, RANDOMOUT has two hyperparameters, a threshold \u03c4 and a \u201c% of epochs active\u201d P . During training each filter k is checked at regular intervals to see if CGN(k) < \u03c4 and, if so, filter k is re-initialized. The motivation for the threshold is that the CGN is hardly ever 0, because learning rates are fractional so update rules only approach 0, but will become very close when the network has stopped learning a filter. For our networks, \u03c4 = 10\u22128 yielded good results. It is also necessary to consider the proportion of epochs from the start in which filter randomization should occur; we refer to this as \u201c% of epochs active\u201d or P . Re-initializing filters too late in the training process may damage the network, and it will need time to retrain itself."
        },
        {
            "heading": "3. Experimental Setup",
            "text": "Two networks are used. The first is a small network used for filter visualizations, hyperparameter search, and testing capacity improvements called CraterCNN (Cohen et al., 2016b) implemented in Deeplearning4j (Team, 2015) which is applied to Martian crater data. The second is a 28x28 Inception-V3 network which we use with and without Batch Normalization (Chen et al., 2015) (Szegedy et al., 2015) from the MXNet repository.\nCraterCNN has two convolutional layers, followed by a fully connected layer, then softmax. The input is 15x15 and grayscale. Each crater candidate example is scaled to this size. The convolutional layers contain stride-1 4x4 filters. The network uses ReLU activations as in (Krizhevsky et al., 2012). The initial weights throughout the network are initialized using the Xavier initialization (Glorot & Bengio,\n2010) scheme. It is trained using standard stochastic gradient descent with a fixed learning rate.\nThe Martian Crater dataset (Bandeira et al., 2010) is used because it is challenging enough, while fast enough to perform the training of>14,000 networks (>2 million epochs) for hyperparameter tuning using grid search. We use three subsets of the data (the East, Center, and West regions) for our experiments which are split each region 50/50 into a training and test set. The East region contains 458/765 positive/negative crater examples and the West contains 1121/1385.\nThe 28x28 Inception-V3 network is used as implemented in MXNet example repository (Chen et al., 2015) (Szegedy et al., 2015) and applied to CIFAR-10. It contains 6848 convolutional filters of sizes 3x3 and 1x1, trained using Adam (Kingma & Ba, 2014). Typically the network has Batch Normalization nodes after the output of every convolutional layer but we remove these layers to demonstrate RANDOMOUT because the methods are not compatible. The 28x28 Inception-V3 network without Batch Normalization is referred to as Base and is referred to as BatchNorm when those layers are included.\nFull code examples are available online1"
        },
        {
            "heading": "4. Experiments",
            "text": "Overall results are presented in Figure 4. The resulting test error of the 28x28 Inception-V3 network in three experimental conditions are shown. The base network fails to converge to a satisfactory local minimum 26% of the time. We draw the reader\u2019s attention to the effectiveness of our method in recovering from all bad seeds and increasing accuracy overall. RANDOMOUT allows the network to recover from being initialized with bad weights. However using BatchNorm appears to perform even better except for 3 random seeds where it is about equal. We also observe more consistency in generalization performance when using RANDOMOUT. BatchNorm test accuracy has a standard deviation of 2% while RANDOMOUT is almost half at 1.3%.\nIn order achieve these results we needed to determine how to set the hyperparameters \u03c4 and P . In order to perform a hyperparameter search we used the smaller CraterCNN network and the small Crater dataset. We evaluate RANDOMOUT on 50 random seeds used for initialization over 150 training epochs. Test accuracy when varying \u03c4 and P is shown in Figure 5. We observe that generally for a fixed P a lower threshold \u03c4 value results in a higher average gain in network accuracy. This is because these filters have been\n1https://github.com/ieee8023/ NeuralNetwork-Examples/tree/master/mxnet/ randomout\ncorrectly identified as being abandoned and brought back to life to improve the network. We also find that increasing P , provided a low threshold \u03c4 , yields a higher gain. We understand this to mean that the lower the threshold is the lower the risk of randomizing a filter that has learned an important feature. It is significant to note that these improvements are not brittle and just for specific hyperparameters but can be observed over large areas of the parameter spaces as shown in the green and blue cells in Figure 5.\nNow we use the hyperparameters P = 1.0 and \u03c4 = 10\u221212 and vary the number of filters used in each layer of the CraterCNN network configuration in Figure 6. The goal of this figure is to determine if we achieve an increase in the potential of a network to that of one with more filters without incurring the cost of adding more filters. This network is very small with only two convolutional layers so a 1 in this plot means only 1 filter in each layer. The mean accuracy is used from 50 networks each using a different random seed and trained for 100 full size batch epochs.\nWe can observe the performance CNNs with RANDOMOUT lead those without it by around 1 to 2 filters consistently. These results indicate that RANDOMOUT achieves an increase in performance equivalent to about 1 or 2 added\nfilters for this network. For example on the West region using 1 filter with RANDOMOUT achieves the same performance of using 2 filters without it. We can also observe a smoothing effect on the accuracy at 6 filters in every region. It is unknown what caused this dip in performance but it is clear that RANDOMOUT mitigated this negative effect. These finding support our statement that these networks have abandoned filters because randomizing them achieves similar performance to making them wider.\nNext we go deeper into how RANDOMOUT is performing during training. In Figure 7 the average CGN is shown when training the 28x28 inception-v3 network in three test conditions. RANDOMOUT continuously increases the gradients while BatchNorm constrains them. The large spikes in the beginning can be explained by Figure 8 which shows the number of filters that fall below \u03c4 . This represents how many are reinitialized by RANDOMOUT and how many would be reinitialized for the base and BatchNorm lines. There are a large number of resets at the start of training and then this decreases quickly as training continues.\nWe can then look at the training and testing accuracy of the 28x28 Inception model under the three test conditions in Figure 9. Using BatchNorm causes testing accuracy to be unstable while at times achieving the highest accuracy. BatchNorm accelerates training a reaches a higher training and testing accuracy faster. RANDOMOUT is slower to surpass the Base condition but achieves a consistent gain in testing accuracy. We draw the reader\u2019s attention to the stable training and testing accuracy while RANDOMOUT is continually reinitializing filter weights."
        },
        {
            "heading": "5. Conclusion",
            "text": "We introduced RANDOMOUT, a method of detecting and repairing abandoned conv. filters with the goal of reducing the bias by initial weights. We analyse the causes of the issue and hyperparameters on examples and then demonstrate an increase in performance on the well known CIFAR-10 dataset and the 28x28 Inception model. We conclude that the hyperparameters are easy to config. with P = 1 and \u03c4 very close to 0.\nAlthough the addition of BatchNorm layers yields higher accuracy, it incurs added runtime costs and parameters, unlike in RANDOMOUT. We study the effect of RandomOut when varying the number of filters in a network and conclude that RandomOut increases accuracy of a network consistently. We study the CGN value with RandomOut and BatchNorm at every batch to find that these methods produce drastically different patterns of gradients while training which demonstrates the difficulty in merging these two methods. We finally demonstrate the stability of testing accuracy on models produced with RANDOMOUT compared to BatchNorm. We expected that resetting weights might damage the network but discovered that because we are resetting weights with small gradients the impact on the network is low."
        }
    ],
    "title": "RandomOut: Using a convolutional gradient normto rescue convolutional filters",
    "year": 2017
}