{
    "abstractText": "Learning interpretable and transferable subpolicies and performing task decomposition from a single, complex task is difficult. Some traditional hierarchical reinforcement learning techniques enforce this decomposition in a top-down manner, while meta-learning techniques require a task distribution at hand to learn such decompositions. This paper presents a framework for using diverse suboptimal world models to decompose complex task solutions into simpler modular subpolicies. This framework performs automatic decomposition of a single source task in a bottom up manner, concurrently learning the required modular subpolicies as well as a controller to coordinate them. We perform a series of experiments on high dimensional continuous action control tasks to demonstrate the effectiveness of this approach at both complex single task learning and lifelong learning. Finally, we perform ablation studies to understand the importance and robustness of different elements in the framework and limitations to this approach.",
    "authors": [
        {
            "affiliations": [],
            "name": "Bohan Wu"
        },
        {
            "affiliations": [],
            "name": "Jayesh K. Gupta"
        },
        {
            "affiliations": [],
            "name": "Mykel J. Kochenderfer"
        }
    ],
    "id": "SP:4a74e7a54bd5f31f20c78d8553cc39fd88834b49",
    "references": [
        {
            "authors": [
                "Pierre-Luc Bacon",
                "Jean Harb",
                "Doina Precup"
            ],
            "title": "The Option-critic Architecture",
            "venue": "In AAAI Conference on Artificial Intelligence (AAAI)",
            "year": 2017
        },
        {
            "authors": [
                "L C Baird"
            ],
            "title": "Reinforcement Learning in Continuous Time: Advantage Updating",
            "venue": "In IEEE International Conference on Neural Networks (ICNN),",
            "year": 1994
        },
        {
            "authors": [
                "Andr\u00e9 Barreto",
                "Will Dabney",
                "R\u00e9mi Munos",
                "Jonathan J Hunt",
                "Tom Schaul",
                "Hado P van Hasselt",
                "David Silver"
            ],
            "title": "Successor Features for Transfer in Reinforcement Learning",
            "venue": "In Advances in Neural Information Processing Systems (NIPS)",
            "year": 2017
        },
        {
            "authors": [
                "Emma Brunskill",
                "Lihong Li"
            ],
            "title": "PAC-inspired Option Discovery in Lifelong Reinforcement Learning",
            "venue": "In International Conference on Machine Learning (ICML)",
            "year": 2014
        },
        {
            "authors": [
                "Peter Dayan"
            ],
            "title": "Improving Generalization for Temporal Difference Learning: The Successor Representation",
            "venue": "Neural Computation",
            "year": 1993
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A Large-scale Hierarchical Image Database",
            "venue": "In IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
            "year": 2009
        },
        {
            "authors": [
                "Chelsea Finn",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Model-Agnostic Meta- Learning for Fast Adaptation of Deep Networks",
            "venue": "In International Conference on Machine Learning (ICML)",
            "year": 2017
        },
        {
            "authors": [
                "Kevin Frans",
                "Jonathan Ho",
                "Xi Chen",
                "Pieter Abbeel",
                "John Schulman"
            ],
            "title": "Meta Learning Shared Hierarchies",
            "venue": "In International Conference on Learning Representations (ICLR)",
            "year": 2018
        },
        {
            "authors": [
                "D. Ha",
                "J. Schmidhuber"
            ],
            "title": "2018",
            "venue": "World Models. CoRR abs/1803.10122 ",
            "year": 2018
        },
        {
            "authors": [
                "G. Zacharias Holland"
            ],
            "title": "Erik Talvitie",
            "venue": "and Michael Bowling. 2018. The Effect of Planning Shape on Dyna-style Planning in High-dimensional State Spaces. CoRR abs/1806.01825 ",
            "year": 2018
        },
        {
            "authors": [
                "Georg B. Keller",
                "Tobias Bonhoeffer",
                "Mark H\u00c3ijbener"
            ],
            "title": "Sensorimotor Mismatch Signals in Primary Visual Cortex of the Behaving Mouse",
            "venue": "Neuron 74,",
            "year": 2012
        },
        {
            "authors": [
                "James Kirkpatrick",
                "Razvan Pascanu",
                "Neil Rabinowitz",
                "Joel Veness",
                "Guillaume Desjardins",
                "Andrei A Rusu",
                "Kieran Milan",
                "John Quan",
                "Tiago Ramalho",
                "Agnieszka Grabska-Barwinska"
            ],
            "title": "Overcoming Catastrophic Forgetting in Neural Networks",
            "venue": "Proceedings of the National Academy of Sciences 114,",
            "year": 2017
        },
        {
            "authors": [
                "Marcus Leinweber",
                "Daniel R. Ward",
                "Jan M. Sobczak",
                "Alexander Attinger",
                "Georg B. Keller"
            ],
            "title": "A Sensorimotor Circuit in Mouse Cortex for Visual Flow Predictions",
            "venue": "Neuron 95,",
            "year": 2017
        },
        {
            "authors": [
                "Saeed Masoudnia",
                "Reza Ebrahimpour"
            ],
            "title": "Mixture of Experts: A Literature Survey",
            "venue": "Artificial Intelligence Review 42,",
            "year": 2014
        },
        {
            "authors": [
                "Michael McCloskey",
                "Neal J Cohen"
            ],
            "title": "Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem",
            "venue": "In Psychology of Learning and Motivation, Gordon H Bower (Ed.)",
            "year": 1989
        },
        {
            "authors": [
                "Gerhard Neumann",
                "Christian Daniel",
                "Alexandros Paraschos",
                "Andras Kupcsik",
                "Jan Peters"
            ],
            "title": "Learning Modular Policies for Robotics",
            "venue": "Frontiers of Computational Neuroscience 8,",
            "year": 2014
        },
        {
            "authors": [
                "Dan Rosenbaum",
                "Yair Weiss"
            ],
            "title": "The Return of the Gating Network: Combining Generative Models and Discriminative Training in Natural Image Priors",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2015
        },
        {
            "authors": [
                "John Schulman",
                "Philipp Moritz",
                "Sergey Levine",
                "Michael Jordan",
                "Pieter Abbeel"
            ],
            "title": "High-dimensional Continuous Control Using Generalized Advantage Estimation",
            "venue": "CoRR abs/1506.02438",
            "year": 2015
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov"
            ],
            "title": "Proximal Policy Optimization Algorithms",
            "year": 2017
        },
        {
            "authors": [
                "Chen Sun",
                "Abhinav Shrivastava",
                "Saurabh Singh",
                "Abhinav Gupta"
            ],
            "title": "Revisiting Unreasonable Effectiveness of Data inDeep Learning Era",
            "venue": "In IEEE International Conference on Computer Vision (ICCV)",
            "year": 2017
        },
        {
            "authors": [
                "Richard S Sutton",
                "Andrew G Barto"
            ],
            "title": "Reinforcement learning: An Introduction",
            "year": 1998
        },
        {
            "authors": [
                "Eric Talvitie"
            ],
            "title": "Self-Correcting Models for Model-Based Reinforcement Learning",
            "venue": "In AAAI Conference on Artificial Intelligence (AAAI)",
            "year": 2017
        },
        {
            "authors": [
                "Fumihide Tanaka",
                "Masayuki Yamamura"
            ],
            "title": "Multitask Reinforcement Learning on the Distribution of MDPs",
            "venue": "In IEEE International Symposium on Computational Intelligence in Robotics and Automation,",
            "year": 2003
        },
        {
            "authors": [
                "Yuval Tassa",
                "Yotam Doron",
                "Alistair Muldal",
                "Tom Erez",
                "Yazhe Li",
                "Diego de Las Casas",
                "David Budden",
                "Abbas Abdolmaleki",
                "Josh Merel",
                "Andrew Lefrancq",
                "Timothy P. Lillicrap",
                "Martin A. Riedmiller"
            ],
            "title": "DeepMind Control Suite",
            "year": 2018
        },
        {
            "authors": [
                "Yee Teh",
                "Victor Bapst",
                "Wojciech M Czarnecki",
                "John Quan",
                "James Kirkpatrick",
                "Raia Hadsell",
                "Nicolas Heess",
                "Razvan Pascanu"
            ],
            "title": "Distral: Robust Multitask Reinforcement Learning",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2017
        },
        {
            "authors": [
                "Sebastian Thrun",
                "Lorien Pratt"
            ],
            "title": "Learning to Learn: Introduction and Overview",
            "venue": "In Learning to Learn, Sebastian Thrun and Lorien Pratt (Eds.)",
            "year": 1998
        },
        {
            "authors": [
                "Emanuel Todorov",
                "Tom Erez",
                "Yuval Tassa"
            ],
            "title": "MuJoCo: A Physics Engine for Model-based Control",
            "venue": "In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
            "year": 2012
        },
        {
            "authors": [
                "Alexander Sasha Vezhnevets",
                "Simon Osindero",
                "Tom Schaul",
                "Nicolas Heess",
                "Max Jaderberg",
                "David Silver",
                "Koray Kavukcuoglu"
            ],
            "title": "FeUdal Networks for Hierarchical Reinforcement Learning",
            "venue": "In International Conference on Machine Learning (ICML)",
            "year": 2017
        },
        {
            "authors": [
                "Aaron Wilson",
                "Alan Fern",
                "Soumya Ray",
                "Prasad Tadepalli"
            ],
            "title": "Multi-task Reinforcement Learning: A Hierarchical Bayesian Approach",
            "venue": "In International Conference on Machine Learning (ICML)",
            "year": 2007
        }
    ],
    "sections": [
        {
            "text": "KEYWORDS Reinforcement learning; Task decomposition; Transfer; Lifelong learning ACM Reference Format: Bohan Wu, Jayesh K. Gupta, and Mykel J. Kochenderfer. 2019. Model Primitive Hierarchical Lifelong Reinforcement Learning . In Proc. of the 18th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2019), Montreal, Canada, May 13\u201317, 2019, IFAAMAS, 9 pages."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "In the lifelong learning setting, we want our agent to solve a series of related tasks drawn from some task distribution rather than a single, isolated task. Agents must be able to transfer knowledge gained in previous tasks to improve performance on future tasks. This setting is different from multi-task reinforcement learning [25, 27, 31] and variousmeta-reinforcement learning settings [7, 8], where the agent jointly trains on multiple task environments. Not only do such nonincremental settings make the problem of discovering common structures between tasks easier, they allow the methods to ignore the problem of catastrophic forgetting [16], which is the inability to solve previous tasks after learning to solve new tasks in a sequential learning setting.\nOur work takes a step towards solutions for such incremental settings. We draw on the idea of modularity [17]. While learning to perform a complex task, we force the agent to break its solution down into simpler subpolicies instead of learning a single monolithic policy. This decomposition allows our agent to rapidly learn another related task by transferring these subpolicies. We\nProc. of the 18th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2019), N. Agmon, M. E. Taylor, E. Elkind, M. Veloso (eds.), May 13\u201317, 2019, Montreal, Canada. \u00a9 2019 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.\nhypothesize that many complex tasks are heavily structured and hierarchical in nature. The likelihood of transfer of an agent\u2019s solution increases if it can capture such shared structure.\nA key ingredient of our proposal is the idea of world models [10, 12, 14] \u2014 transition models that can predict future sensory data given the agent\u2019s current actions. The world however is complex, and learning models that are consistent enough to plan with is not only hard [24], but planning with such one-step models is suboptimal [11]. We posit that the requirement that these world models be good predictors of the world state is unnecessary, provided we have a multiplicity of such models. We use the termmodel primitives to refer to these suboptimal world models. Since each model primitive is only relatively better at predicting the next states within a certain region of the environment space, we call this area the model primitive\u2019s region of specialization.\nModel primitives allow the agent to decompose the task being performed into subtasks according to their regions of specialization and learn a specialized subpolicy for each subtask. The same model primitives are used to learn a gating controller to select, improve, adapt, and sequence the various subpolicies to solve a given task in a manner very similar to a mixture of experts framework [15].\nOur framework assumes that at least a subset of model primitives are useful across a range of tasks and environments. This assumption is less restrictive than that of successor representations [3, 5]. Even though successor representations decouple the state transitions from the rewards (representing the task or goals), the transitions learned are policy dependent and can only transfer across tasks with the same environment dynamics.\nThere are alternative approaches to learning hierarchical spatiotemporal decompositions from the rewards seen while interacting with the environment. These approaches include meta-learning algorithms like Meta-learning Shared Hierarchies (MLSH) [8], which require a multiplicity of pretrained subpolicies and joint training on related tasks. Other approaches include the option-critic architecture [1] that allows learning such decompositions in a single task environment. However, this method requires regularization hyperparameters that are tricky to set. As observed by Vezhnevets et al. [30], its learning often collapses to a single subpolicy. Moreover, we posit that capturing the shared structure across task-environments can be more useful in the context of transfer for lifelong learning than reward-based task specific structures.\nTo summarize our contributions:\n\u2022 Given diverse suboptimalworldmodels, we propose amethod to leverage them for task decomposition. \u2022 We propose an architecture to jointly train decomposed subpolicies and a gating controller to solve a given task. \u2022 We demonstrate the effectiveness of this approach at both single-task and lifelong learning in complex domains with high-dimensional observations and continuous actions.\nar X\niv :1\n90 3.\n01 56\n7v 1\n[ cs\n.L G\n] 4\nM ar\n2 01\n9"
        },
        {
            "heading": "2 PRELIMINARIES",
            "text": "We assume the standard reinforcement learning (RL) formulation: an agent interacts with an environment to maximize the expected reward [23]. The environment is modeled as a Markov decision process (MDP), which is defined by \u27e8S,A,R,T ,\u03b3 \u27e9 with a state space S, an action space A, a reward function R : S \u00d7 A \u2192 R, a dynamics modelT : S\u00d7A \u2192 \u03a0(S), and a discount factor\u03b3 \u2208 [0, 1). Here, \u03a0(\u00b7) defines a probability distribution over a set. The agent acts according to stationary stochastic policies \u03c0 : S \u2192 \u03a0(A), which specify action choice probabilities for each state. Each policy \u03c0 has a corresponding Q\u03c0 : S \u00d7 A \u2192 R function that defines the expected discounted cumulative reward for taking an action a from state s and following the policy \u03c0 from that point onward.\nLifelong Reinforcement Learning: In a lifelong learning setting, the agent must interact with multiple tasks and successfully solve each of them. Adopting the framework from Brunskill and Li [4], in lifelong RL, the agent receives S,A, initial state distribution \u03c10 \u2208 \u03a0(S), horizon H , discount factor \u03b3 , and an unknown distribution over reward-transition function pairs, D. The agent samples (Ri ,Ti ) \u223c D and interacts with the MDP \u27e8S,A,Ri ,Ti ,\u03b3 \u27e9 for a maximum ofH timesteps, starting according to the initial state distribution \u03c10. After solving the given MDP or after H timesteps, whichever occurs first, the agent resamples from D and repeats.\nThe fundamental question in lifelong learning is to determine what knowledge should be captured by the agent from the tasks it has already solved so that it can improve its performance on future tasks. When learning with functional approximation, this translates to learning the right representation \u2014 the one with the right inductive bias for the tasks in the distribution. Given the assumption that the set of related tasks for lifelong learning share a lot of structure, the ideal representation should be able to capture this shared structure.\nThrun and Pratt [28] summarized various representation decomposition methods into two major categories. Modern approaches to avoiding catastrophic forgetting during transfer tend to fall into either category. The first category partitions the parameter space into task-specific parameters and general parameters [19]. The second category learns constraints that can be superimposed when learning a new function [13].\nA popular approachwithin the first category is to usewhat Thrun and Pratt [28] term as recursive functional decomposition. This approach assumes that solution to tasks can be decomposed into a function of the form fi = hi \u25e6 \u0434, where hi is task-specific whereas \u0434 is the same for all fi . This scheme has been particularly effective in computer vision where early convolutional layers in deep convolutional networks trained on ImageNet [6, 22] become a very effective \u0434 for a variety of tasks. However, this approach to decomposition often fails in DeepRL because of two main reasons. First, the gradients used to train such networks are noisier as a result of Monte Carlo sampling. Second, the i.i.d. assumption for training data often fails.\nWe instead focus on devising an effective piecewise functional decomposition of the parameter space, as defined by Thrun and Pratt [28]. The assumption behind this decomposition is that each function fi can be represented by a collection of functions h1, . . . ,hm ,\nwherem \u226a N , and N is the number of tasks to learn. Our hypothesis is that this decomposition is much more effective and easier to learn in RL."
        },
        {
            "heading": "3 MODEL PRIMITIVE HIERARCHICAL RL",
            "text": "This section outlines the Model Primitive Hierarchical Reinforcement Learning (MPHRL) framework (Figure 1) to address the problem of effective piecewise functional decomposition for transfer across a distribution of tasks."
        },
        {
            "heading": "3.1 Model Primitives and Gating",
            "text": "The key assumption in MPHRL is access to several diverse world models of the environment dynamics. These models can be seen as instances of learned approximations to the true environment dynamics T . In reality, these dynamics can even be non-stationary. Therefore, the task of learning a complete model of the environment dynamics might be too difficult. Instead, it can be much easier to train multiple approximate models that specialize in different parts of the environment. We use the term model primitives to refer to these approximate world models.\nSuppose we have access to K model primitives: T\u0302k : S \u00d7 A \u2192 \u03a0(S). For simplicity, we can assign a labelMk to each T\u0302k , such that their predictions of the environment\u2019s transition probabilities can be denoted by T\u0302 (st+1 | st ,at ,Mk ).\n3.1.1 Subpolicies. The goal of the MPHRL framework is to use these suboptimal predictions from different model primitives to decompose the task space into their regions of specialization, and learn different subpolicies \u03c0k : S \u2192 \u03a0(A) that can focus on these regions. In the function approximation regime, each subpolicy \u03c0k belongs to a fixed class of smoothly parameterized stochastic policies {\u03c0\u03b8k | \u03b8k \u2208 \u0398}, where \u0398 is a set of valid parameter vectors.\nModel primitives are suboptimal and make incorrect predictions about the next state. Therefore we do not use them for planning or model-based learning of subpolicies directly. Instead, model primitives give rise to useful functional decompositions and allow subpolicies to be learned in a model-free way.\n3.1.2 Gating Controller. Taking inspiration from the mixture-ofexperts literature [15], where the output from multiple experts can be combined using probabilistic gating functions, MPHRL decomposes the solution for a given task into multiple \u201cexpert\u201d subpolicies and a gating controller that can compose them to solve the task. We\nwant this switching behavior to be probabilistic and continuous to avoid abrupt transitions. During learning, we want this controller to help assign the reward signal to the correct blend of subpolicies to ensure effective learning as well as decomposition.\nSince the gating controller\u2019s goal is to choose the subpolicy whose corresponding model primitive makes the best prediction for a given transition, using Bayes\u2019 rule we can write:\nP(Mk | st ,at , st+1) \u221d P(Mk | st )\u03c0k (at | st )T\u0302 (st+1 | st ,at ,Mk ) (1)\nbecause \u03c0k (at | st ) = \u03c0 (at | st ,Mk ). The agent only has access to the current state st during execution. Therefore, the agent needs to marginalize out st+1 and at such that the model choice only depends on the current state st :\nP(Mk | st ) = \u222b\nst+1\u2208S \u222b at \u2208A P(Mk | st ,at , st+1)\nP(st+1,at )datdst+1 (2) This is equivalent to:\nP(Mk | st ) = Est+1,at\u223cP (st+1,at ) [P(Mk | st ,at , st+1)] (3) Unfortunately, computing these integrals requires expensive Monte Carlomethods. However, we can use an approximatemethod to achieve the same objective with discriminative learning [18].\nWe parameterize the gating controller (GC) as a categorical distribution P\u03d5 (Mk | st ) = P(Mk | st ;\u03d5) and minimize the conditional cross entropy loss between Est+1,at\u223cP (st+1,at ) [P(Mk | st ,at , st+1)] and P\u03d5 (Mk | st ) for all sampled transitions (st ,at , st+1) in a rollout:\nminimize \u03d5\nLGC (4)\nwhere LGC = \u2211 st \u2211 k \u2212 (\u2211 st+1 \u2211 at P(Mk | st ,at , st+1) ) \u00d7 log P(Mk | st ;\u03d5) (5) This is equivalent to an implicit Monte Carlo integration to compute the marginal if st+1,at \u223c P(st+1,at ). Although we cannot query or sample from P(st+1,at ) directly, st ,at , and st+1 can be sampled according to their respective distributionswhilewe perform rollouts in the environment. Despite the introduced bias in our estimates, we find Eq. 4 sufficient for achieving task decomposition.\n3.1.3 Subpolicy Composition. Taking inspiration from mixtureof-experts, the gating controller composes the subpolicies into a mixture policy:\n\u03c0 (at | st ) = K\u2211 k=1 P\u03d5 (Mk | st )\u03c0k (at | st ) (6)\n3.1.4 Decoupling Cross Entropy from Action Distribution. During a rollout, the agent samples as follows:\nat \u223c \u03c0 (at | st ) (7) st+1 \u223c T(st+1 | st ,at ) (8)\nThe \u03c0k from Eq. 1 gets coupled with this sampling distribution, making the target distribution in Eq. 5 no longer stationary and the\napproximation process difficult. We alleviate this issue by ignoring \u03c0k , effectively treating it as a distribution independent of k . This transforms Eq. 1 into:\nP\u0302(Mk | st ,at , st+1) \u221d P(Mk | st )T\u0302 (st+1 | st ,at ,Mk ) (9)"
        },
        {
            "heading": "3.2 Learning",
            "text": "Since the focus of this work is on difficult continuous action problems, we mostly concentrate on the issue of policy optimization and how it integrates with the gating controller. The standard policy (SP) optimization objective is:\nmaximize \u03b8\nLSP = E\u03c10,\u03c0\u03b8 [\u03c0\u03b8 (at | st )Q\u03c0\u03b8 (st ,at )] (10)\nWith baseline subtraction for variance reduction, this turns into [20]:\nmaximize \u03b8\nLPG = E\u03c10,\u03c0\u03b8 [\u03c0\u03b8 (at | st )A\u0302t ] (11)\nwhere A\u0302t is an estimator of the advantage function [2]. In MPHRL, we directly use the mixture policy as defined by Eq. 6. The standard policy gradients (PG) get weighted by the probability outputs of the gating controller, enforcing the required specialization by factorizing into:\n\u0434\u0302k = E\u03c10,\u03c0\u03b8k [ P\u03d5 (Mk | st )\u2207\u03b8k log\u03c0\u03b8k (at | st )A\u0302t ] (12)\nIn practice, we use the Clipped PPO objective [21] instead to perform stable updates by limiting the step size. This includes adding a baseline estimator (BL) parameterized by \u03c8 for value prediction and variance reduction. We optimize\u03c8 according to the following loss:\nLBL = E [ V\u03c8 \u2212V\u03c0\u03b8 2] (13)\nWe summarize this single-task learning algorithm in Algorithm 1, which results in a set of decomposed subpolicies, \u03c0\u03b81 , . . . \u03c0\u03b8K , and a gating controller P\u03d5 that can modulate between them to solve the task under consideration.\nAlgorithm 1 MPHRL: single-task learning\n1: Initialize P\u03d5 ,\u03c0\u03b8 = {\u03c0\u03b81 , . . . ,\u03c0\u03b8K }, V\u03c8 2: while not converged do 3: Rollout trajectories \u03c4 \u223c \u03c0\u03b8,\u03d5 4: Compute advantage estimates A\u0302\u03c4 5: Optimize LPG wrt \u03b81, . . . ,\u03b8K with expectations taken over \u03c4 6: Optimize LBL wrt\u03c8 with expectations taken over \u03c4 7: Optimize LGC wrt \u03d5\nwith expectations taken over \u03c4\nLifelong learning: We have shown how MPHRL can decompose a single complex task solution into different functional components. Complex tasks often share structure and can be decomposed into similar sets of subtasks. Different tasks however require different recomposition of similar subtasks. Therefore, we transfer the subpolicies to learn target tasks, but not the gating controller or the baseline estimator. We summarize the lifelong learning algorithm in Algorithm 2, with the global variable RESET set to true.\nAlgorithm 2MPHRL: lifelong learning\n1: Initialize P\u03d5 , \u03c0\u03b8 = {\u03c0\u03b81 , . . . ,\u03c0\u03b8K },V\u03c8 2: for Tasks (Ri ,Ti ) \u223c D do 3: if RESET then 4: Initialize P\u03d5 , V\u03c8 5: while not converged do 6: Rollout trajectories \u03c4 \u223c \u03c0\u03b8,\u03d5 7: Compute advantage estimates A\u0302\u03c4 8: Optimize LPG wrt \u03b81, . . . ,\u03b8K with expectations taken over \u03c4 9: Optimize LBL wrt\u03c8\nwith expectations taken over \u03c4 10: Optimize LGC wrt \u03d5\nwith expectations taken over \u03c4"
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "Our experiments aim to answer two questions: (a) can model primitives ensure task decomposition? (b) does such decomposition improve transfer for lifelong learning?\nWe evaluate our approach in two challenging domains: a MuJoCo [29] ant navigating different mazes and a Stacker [26] arm picking up and placing different boxes. In our experiments, we use subpolicies that have Gaussian action distributions, with mean given by a multi-layer perceptron taking observations as input and standard deviations given by a different set of parameters. MPHRL\u2019s gating controller outputs a categorical distribution and is parameterized by another multi-layer perceptron. We also use a separate multi-layer perceptron for the baseline estimator. We use the standard PPO algorithm as a baseline to compare against MPHRL. Transferring network weights empirically led to worse performance for standard PPO. Hence, we re-initialize its weights for every task. For fair comparison, we also shrink the hidden layer size of MPHRL\u2019s subpolicy networks from 64 to 16. We conduct each experiment across 5 different seeds. Error bars represent the standard deviation from the mean.\nThe focus of this work is on understanding the usefulness of model primitives for task decomposition and the resulting improvement in sample efficiency from transfer. To conduct controlled experiments with interpretable results, we hand-designed model primitives using the true next state provided by the environment simulator. Concretely, we apply distinct multivariate Gaussian noise models with covariance \u03c3\u03a3 to the true next state. We then sample from this distribution to obtain the mean of the probability distribution of a model primitive\u2019s next state prediction, using \u03a3 as its covariance. Here, \u03c3 is the noise scaling factor that distinguishes model primitives, while \u03a3 refers to the empirical covariance of the sampled next states:\n\u00b5 \u223c N(st+1,\u03c3k\u03a3) (14) T\u0302 (st+1 | st ,at ,Mk ) = N(\u00b5,\u03c3k\u03a3) (15)\nUsing \u03a3 as opposed to a constant covariance is essential for controlled experiments because different elements of the observation space have different orders of magnitude. Sampling \u00b5 from a distribution effectively adds random bias to the model primitive\u2019s next state probability distribution.\nHyperparameter details are in Table 1, and our code is freely available at http://github.com/sisl/MPHRL."
        },
        {
            "heading": "4.1 Single-task Learning",
            "text": "First, we focus on two single-task learning experiments where MPHRL learns a number of interpretable subpolicies to solve a single task. Both the L-Maze and D-Maze (Figure 2a) tasks require the ant to learn to walk and reach the green goal within a finite 18-Pickup&Place. 2Single task refers to L-Maze and D-Maze; source and target tasks refer to the first task and all subsequent tasks in a lifelong learning taskset, respectively. 3Baseline network hyperparameters apply to both MPHRL and baseline PPO; model primitive networks are for experiments with learned model primitives only. 4The baseline PPO has no subpolicies, so the subpolicy network is the policy network. 5Baseline and subpolicy networks only.\nhorizon. For both tasks, both the goal and the initial ant locations are fixed. For the L-Maze, the agent has access to two model primitives, one specializing in the horizontal (E, W) corridor and the other specializing in the vertical (N, S) corridor of the maze. Similarly for the D-Maze, the agent has access to four model primitives, one specializing in each N, S, E, W corridor of the maze. In their specialized corridors, the noise scaling factor \u03c3 = 0. Outside of their regions of specialization, \u03c3 = 0.5. The observation space includes the standard joint angles and positions, lidar information tracking distances from walls on each side, and the Manhattan distance to the goal. Figure 2b shows the experimental results on these environments. Notice that using model primitives can make the learning problem more difficult and increase the sample complexity on a single task. This is expected, since we are forcing the agent to decompose the solution, which could be unnecessary for easy tasks. However, we will observe in the following section that this decomposition can lead to remarkable improvements in transfer performance during lifelong learning."
        },
        {
            "heading": "4.2 Lifelong Learning",
            "text": "To evaluate our framework\u2019s performance at lifelong learning, we introduce two tasksets.\n4.2.1 10-Maze. To evaluate MPHRL\u2019s performance in lifelong learning, we generate a family of 10 random mazes for the MuJoCo Ant environment, referred to as the 10-Maze taskset (Figure 4) hereafter. The goal, the observation space, the Gaussian noise models, and the model primitives remain the same as in D-Maze. The agent has a maximum of 3 \u00d7 107 timesteps to reach 80% success rate in each of the 10 tasks. As shown in Figure 3a, MPHRL requires nearly double the number of timesteps to learn the decomposed subpolicies in the first task. However, this cost gets heavily amortized over the entire taskset, with MPHRL taking half the total number of timesteps of the baseline PPO, exhibiting strong subpolicy transfer.\n4.2.2 8-Pickup&Place. Wemodify the Stacker task [26] to create the 8-Pickup&Place taskset. As shown in Figure 5, a robotic arm is tasked to bring 2 boxes to their respective goal locations in a certain\norder. Marked by colors red, green, and blue, the goal locations reside within two short walls forming a \u201cstack\u201d.\nEach of the 8 tasks has a maximum of 3 goal locations. The observation space of the agent includes joint angles and positions, box and goal locations, their relative distances to each other, and the current stage of the task encoded as one-hot vectors. The agent has access to six model primitives for each box that specialize in reaching above, lowering to, grasping, picking up, carrying, and dropping a certain box. Similar to 10-Maze, model primitives have \u03c3 of 0 within their specialized stages and \u03c3 of 0.5 otherwise. Figure 3b shows MPHRL\u2019s experimental performance by learning twelve useful subpolicies for this taskset. We notice again the strong transfer performance due to the decomposition forced by the model primitives. Note that this taskset is much more complex than 10-Maze such that MPHRL even accelerates the learning of the first task."
        },
        {
            "heading": "4.3 Ablation",
            "text": "We conduct ablation experiments to answer the following questions: (1) How much gain in sample efficiency is achieved by transfer-\nring subpolicies? (2) Can MPHRL learn the task decomposition even when the\nmodel primitives are quite noisy or when the source task does not cover all \u201ccases\u201d? (3) When does MPHRL fail to decompose the solution? (4) What kind of diversity in the model primitives is essential for\nperformance? (5) When does MPHRL lead to negative transfer? (6) Is MPHRL\u2019s gain in sample efficiency a result of hand-crafted\nmodel primitives and how does it perform with actual learned model primitives?\n4.3.1 Model Noise. MPHRL has the ability to decompose the solution even given bad model primitives. Since the learning is done model-free, these suboptimal model primitives should not strongly affect the learning performance so long as they remain sufficiently distinct. To investigate the limitations to this claim, we conduct five experiments using various sets of noisy model primitives. Below, the first value corresponds to the noise scaling factor \u03c3 within their individual regions of specialization, while the second value corresponds to \u03c3 outside of their regions of specialization. (a) 0.4 and 0.5: good models with limited distinction (b) 0.5 and 1.0: good models with reasonable distinction\n(c) 5.0 and 10.0: bad models with reasonable distinction (d) 9.0 and 10.0: bad models with limited distinction (e) 0.5 and 0.5: good models with no distinction\nShown in Figure 6a, while (a), (b), (c), and (d) exhibit limited degradation in performance, (d) experiences the most performance degradation on average. On the other hand, in (e) MPHRL took 22.0\u00b14.6million timesteps to solve the first task and 2.8 \u00b1 1.6 million timesteps to solve the second task, but failed to solve the third task within 30 million timesteps. This is because the model primitives are identical and provide no information about task decomposition. In summary, MPHRL is robust against bad model primitives so long as the they maintain some relative distinction. Similar observations hold true for the 8-Pickup&Place taskset where noise models with distinctive models with large noise of \u03c3 = 5 and \u03c3 = 20 show little deterioration in performance, taking 15.8 \u00b1 5.5 million timesteps to reach 75% average success rate.\n4.3.2 Overlapping Model Primitives. We next test the condition when there is substantial overlap in regions of specialization between different model primitives. For the 10-Maze taskset, the most plausible region for this confusion is at the corners. In this experiment, within each corner, the two model primitives whose specialized corridors share the corner have \u03c3 = 0 while the other two have \u03c3 = 0.5. Figure 6b shows the performance for model primitive confusion against the standard set of model primitives with no confusion. We observe that despite some performance degradation, MPHRL continues to outperform the PPO baseline.\n4.3.3 Model Diversity. Having tested MPHRL against noises, we experimented with undesirable model primitives for 10-Maze: (a) Extra: a fifth model primitive that specializes in states where\nthe ant is moving horizontally; (b) H-V corridors: 2 model primitives specializing in horizontal (E,\nW) and vertical (N, S) corridors respectively; (c) Velocity: 2 model primitives specializing in states where the\nant is moving horizontally or vertically; and for 8-Pickup&Place: (a) Box-only: 2 model primitives for all actions on 2 boxes; (b) Action-only: 6 model primitives for 6 actions performed on\nboxes: reach above, lower to, grasp, pick up, carry, and drop. Table 2 shows MPHRL is susceptible to performance degradation given undesirable sets of model primitives. However, MPHRL still outperforms baseline PPO when given an extra, undesirable model primitive. This indicates that for best transfer, the model primitives need to approximately capture the structure present in the taskset.\n4.3.4 Negative Transfer and Catastrophic Forgetting. Lifelong learning agents with neural network function approximators face the problem of negative transfer and catastrophic forgetting. Ideally, they should find the solution quickly if the task has already been seen. More generally, given two sets of tasks T and T \u2032 such that T \u2282 T \u2032, after being exposed to T \u2032 the agent should perform no worse, and preferably better, than had it been exposed to T only.\nIn this experiment, we restore the subpolicy checkpoints after solving the 10 tasks and evaluate MPHRL\u2019s learning performance for the first 9 tasks. Similarly, we restore the subpolicy checkpoints after solving 6 tasks and evaluate MPHRL\u2019s performance on the\nfirst 5 tasks. The gating controller is reset for each task as in earlier experiments. We summarize the results in Table 3. Subpolicies trained sequentially on 6 or 10 tasks quickly relearn the required behavior for all previously seen tasks, implying no catastrophic forgetting. Moreover, if we compare the 10-task result to the 6-task result, we see remarkable improvements at transfer. This implies negative transfer is limited with this approach.\n4.3.5 Oracle Gating Controller. One might suspect that all gains in sample efficiency come from hand-crafted model primitives because they allow the agent to learn a perfect gating controller. However, Figure 7 shows the reward curves for an experiment where the gating controller is already perfectly known. This setup is unable to\nlearn any 10-Maze task. Since the 10-Maze taskset is composed of sequential subtasks, only one subpolicy will be learned in the first corridor when the gating controller is perfect. When transitioning to the second corridor, the second policy needs to be learned from scratch, making the ant\u2019s survival rate very low. This discourages the first subpolicy from entering the second corridor and activating the second subpolicy. Eventually, the ant stops moving forward close to the intersection between the first two corridors. In contrast, MPHRL\u2019s natural curriculum for gradual specialization allows multiple subpolicies to learn the basic skills for survival initially.\n4.3.6 Partial Decomposition. To confirm that the ordering of tasks does not significantly affect MPHRL\u2019s performance, we modified 10-Maze to create the 10-Maze-v2 taskset (Figure 9), in which the source task does not allow for complete decomposition into all useful subpolicies for the subsequent tasks. Again, we observe large improvement in sample efficiency over standard PPO (Figure 8).\n4.3.7 Learned Model Primitives. This paper focuses on evaluating suboptimal models for task decomposition in controlled experiments using hand-designed model primitives. Here, we show one way to obtain each model primitive for 10-Maze-v2 using three corridor environments demonstrated in Figure 10. Concretely, we parameterize each model primitive using a multivariate Gaussian distribution. We learn the mean of this distribution via a multilayer perceptron using a weighted mean square error in dynamics prediction as the loss. The standard deviation is still derived from the empirical covariance \u03a3 as described earlier. Even though the diversity in these learned model primitives is much more difficult to quantify and control, their sample efficiency substantially outperforms standard PPO and slightly underperforms hand-designed model primitives with 0 and 0.5 model noises (Figure 8).\n4.3.8 Gating Controller Transfer. To explore factors that lead to negative transfer, we tested MPHRL without re-initializing the gating controller in target tasks, as shown in Figure 6c. Although the mean sample efficiency remains stable, its standard deviation increases dramatically, indicating volatility due to negative transfer.\n4.3.9 Subpolicy Transfer. To measure how much gain in sample efficiency MPHRL has achieved by transferring subpolicies alone, we conducted a 10-Maze experiment by re-initializing all network\nweights for every new task. As shown in Figure 6c, sample complexity more than quintuples when subpolicies are re-initialized (in green).\n4.3.10 Coupling between Cross Entropy and Action Distribution. To validate using P\u0302(Mk | st ,at , st+1) in Eq. 9 as opposed to P(Mk | st ,at , st+1) from Eq. 1, we tested MPHRL with Eq. 1 on 10-Maze. All runs with different seeds failed to solve the first 5 tasks (Table 4). As the gating controller is re-initialized during transfer, most actions were chosen incorrectly. The gating controller is thus presented with the incorrect cross entropy target, which worsens the action distribution. The resulting vicious cycle forces the gating controller to converge to a suboptimal equilibrium against the incorrect target."
        },
        {
            "heading": "5 CONCLUSIONS",
            "text": "We showed how imperfect world models can be used to decompose a complex task into simpler ones. We introduced a framework that uses these model primitives to learn piecewise functional decompositions of solutions to complex tasks. The learned decomposed subpolicies can then be used to transfer to a variety of related tasks, reducing the overall sample complexity required to learn complex behaviors. Our experiments showed that such structured decomposition avoids negative transfer and catastrophic interference, a major concern for lifelong learning systems.\nOur approach does not require access to accurate world models. Neither does it need a well-designed task distribution or the incremental introduction of individual tasks. So long as the set of model primitives are useful across the task distribution, MPHRL is robust to other imperfections.\nNevertheless, learning useful and diverse model primitives, subpolicies and task decomposition all simultaneously is left for future work. The recently introduced Neural Processes [9] can potentially be an efficient approach to build upon."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "We are thankful to Kunal Menda and everyone at SISL for useful comments and suggestions. This work is supported in part by DARPA under agreement number D17AP00032. The content is solely the responsibility of the authors and does not necessarily represent the official views of DARPA. We are also grateful for the support from Google Cloud in scaling our experiments."
        }
    ],
    "title": "Model Primitive Hierarchical Lifelong Reinforcement Learning ",
    "year": 2019
}