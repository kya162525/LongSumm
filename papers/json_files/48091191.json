{
    "abstractText": "Cloud services have become the backbone of today\u2019s computing world. Runtime incidents, which adversely affect the expected service operations, are extremely costly in terms of user impacts and engineering efforts required to resolve them. Hence, such incidents are the target of much research effort. Unfortunately, there is limited understanding about cloud service incidents that actually happen during production runs: what cause them and how they are resolved. In this work, we carefully study hundreds of highseverity incidents that occurred recently during the production runs of many Microsoft Azure services. We find software bugs to be amajor cause behind these incidents, and make interesting observations about the types of software bugs that cause cloud incidents and how these bug-related incidents are resolved, providing motivation and guidance to future research in tackling cloud bugs and improving the cloud-service availability.",
    "authors": [
        {
            "affiliations": [],
            "name": "Haopeng Liu"
        },
        {
            "affiliations": [],
            "name": "Shan Lu"
        },
        {
            "affiliations": [],
            "name": "Madan Musuvathi"
        },
        {
            "affiliations": [],
            "name": "Suman Nath"
        }
    ],
    "id": "SP:a0fd9505651d7ba0d0f3568036720eb6b0ffcfbd",
    "references": [
        {
            "authors": [
                "Ramnatthan Alagappan",
                "Aishwarya Ganesan",
                "Yuvraj Patel",
                "Andrea C Arpaci-Dusseau",
                "Remzi H Arpaci- Dusseau"
            ],
            "title": "Correlated crash vulnerabilities",
            "venue": "In OSDI,",
            "year": 2016
        },
        {
            "authors": [
                "Vaggelis Atlidakis",
                "Patrice Godefroid",
                "Marina Polishchuk"
            ],
            "title": "Restler: Stateful rest api fuzzing",
            "venue": "In ICSE,",
            "year": 2019
        },
        {
            "authors": [
                "Ryan Beckett",
                "Aarti Gupta",
                "Ratul Mahajan",
                "David Walker"
            ],
            "title": "A general approach to network configuration verification",
            "venue": "In ACM SIGCOMM,",
            "year": 2017
        },
        {
            "authors": [
                "Nikolaj Bj\u00f8rner",
                "Karthick Jayaraman"
            ],
            "title": "Checking cloud contracts in microsoft azure",
            "venue": "In International Conference on Distributed Computing and Internet Technology,",
            "year": 2015
        },
        {
            "authors": [
                "Ella Bounimova",
                "Patrice Godefroid",
                "David Molnar"
            ],
            "title": "Billions and billions of constraints: Whitebox fuzz testing in production",
            "venue": "In ICSE,",
            "year": 2013
        },
        {
            "authors": [
                "Lucas Brutschy",
                "Dimitar Dimitrov",
                "Peter M\u00fcller",
                "Martin T. Vechev"
            ],
            "title": "Serializability for eventual consistency: criterion, analysis, and applications",
            "venue": "In Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages",
            "year": 2017
        },
        {
            "authors": [
                "Sebastian Burckhardt",
                "Pravesh Kothari",
                "Madanlal Musuvathi",
                "Santosh Nagarakatte"
            ],
            "title": "A randomized scheduler with probabilistic guarantees of finding bugs",
            "venue": "In Proceedings of the Fifteenth Edition of ASPLOS on Architectural Support for Programming Languages and Operating Systems, ASPLOS XV,",
            "year": 2010
        },
        {
            "authors": [
                "Andy Chou",
                "Junfeng Yang",
                "Benjamin Chelf",
                "Seth Hallem",
                "Dawson R. Engler"
            ],
            "title": "An empirical study of operating system errors",
            "venue": "In Proceedings of the 18th ACM Symposium on Operating System Principles (SOSP),",
            "year": 2001
        },
        {
            "authors": [
                "Kirk Glerum",
                "Kinshuman Kinshumann",
                "Steve Greenberg",
                "Gabriel Aul",
                "Vince Orgovan",
                "Greg Nichols",
                "David Grant",
                "Gretchen Loihle",
                "andGalen C. Hunt"
            ],
            "title": "Debugging in the (very) large: ten years of implementation and experience",
            "venue": "In Proceedings of the 22nd ACM Symposium on Operating Systems Principles",
            "year": 2009
        },
        {
            "authors": [
                "Jim Gray"
            ],
            "title": "Why do computers stop and what can be done about it? Tandem Technical report",
            "year": 1985
        },
        {
            "authors": [
                "Haryadi S Gunawi",
                "Mingzhe Hao",
                "Tanakorn Leesatapornwongsa",
                "Tiratat Patana-anake",
                "Thanh Do",
                "Jeffry Adityatama",
                "Kurnia J Eliazar",
                "Agung Laksono",
                "Jeffrey F Lukman",
                "Vincentius Martin"
            ],
            "title": "What bugs live in the cloud? a study of 3000+ issues in cloud systems",
            "venue": "In Proceedings of the ACM Symposium on Cloud Computing,",
            "year": 2014
        },
        {
            "authors": [
                "Haryadi S. Gunawi",
                "Mingzhe Hao",
                "Riza O. Suminto",
                "Agung Laksono",
                "Anang D. Satria",
                "Jeffry Adityatama",
                "Kurnia J. Eliazar"
            ],
            "title": "Why does the cloud stop computing?: Lessons from hundreds of service outages",
            "venue": "In Proceedings of the Seventh ACM Symposium on Cloud Computing,",
            "year": 2016
        },
        {
            "authors": [
                "Peng Huang",
                "Chuanxiong Guo",
                "Jacob R. Lorch",
                "Lidong Zhou",
                "Yingnong Dang"
            ],
            "title": "Capturing and enhancing in situ system observability for failure detection",
            "venue": "In 13th USENIX Symposium on Operating Systems Design and Implementation,",
            "year": 2018
        },
        {
            "authors": [
                "Peng Huang",
                "Chuanxiong Guo",
                "Lidong Zhou",
                "Jacob R. Lorch",
                "Yingnong Dang",
                "Murali Chintalapati",
                "Randolph Yao"
            ],
            "title": "Gray failure: The achilles\u2019 heel of cloud-scale systems",
            "venue": "In Proceedings of the 16th Workshop on Hot Topics in Operating Systems (HotOS),",
            "year": 2017
        },
        {
            "authors": [
                "Leslie Lamport"
            ],
            "title": "The tla+ home",
            "venue": "page. http:// lamport.azurewebsites.net/tla/tla.html,",
            "year": 2018
        },
        {
            "authors": [
                "Tanakorn Leesatapornwongsa",
                "Mingzhe Hao",
                "Pallavi Joshi",
                "Jeffrey F. Lukman",
                "Haryadi S. Gunawi"
            ],
            "title": "SAMC: semantic-aware model checking for fast discovery of deep bugs in cloud systems",
            "venue": "In 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI),",
            "year": 2014
        },
        {
            "authors": [
                "Tanakorn Leesatapornwongsa",
                "Jeffrey F Lukman",
                "Shan Lu",
                "Haryadi S Gunawi"
            ],
            "title": "TaxDC: A taxonomy of non-deterministic concurrency bugs in datacenter distributed systems",
            "year": 2016
        },
        {
            "authors": [
                "Tanakorn Leesatapornwongsa",
                "Cesar A. Stuardo",
                "Riza O. Suminto",
                "Huan Ke",
                "Jeffrey F. Lukman",
                "Haryadi S. Gunawi"
            ],
            "title": "Scalability bugs: When 100node testing is not enough",
            "venue": "In Proceedings of the 16th Workshop on Hot Topics in Operating Systems (HotOS),",
            "year": 2017
        },
        {
            "authors": [
                "Haopeng Liu",
                "Guangpu Li",
                "Jeffrey F Lukman",
                "Jiaxin Li",
                "Shan Lu",
                "Haryadi S Gunawi",
                "Chen Tian"
            ],
            "title": "DCatch: Automatically detecting distributed concurrency bugs in cloud systems",
            "year": 2017
        },
        {
            "authors": [
                "Haopeng Liu",
                "XuWang",
                "Guangpu Li",
                "Shan Lu",
                "Feng Ye",
                "Chen Tian"
            ],
            "title": "Fcatch: Automatically detecting time-of-fault bugs in cloud systems",
            "venue": "In Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Lanyue Lu",
                "Andrea C. Arpaci-Dusseau",
                "Remzi H. Arpaci-Dusseau",
                "Shan Lu"
            ],
            "title": "A study of linux HotOS \u201919",
            "venue": "May 13\u201315,",
            "year": 2019
        },
        {
            "authors": [
                "Shan Lu",
                "Soyeon Park",
                "Eunsoo Seo",
                "Yuanyuan Zhou"
            ],
            "title": "Learning from mistakes: A comprehensive study on real world concurrency bug characteristics",
            "venue": "In Proceedings of the 13th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS),",
            "year": 2008
        },
        {
            "authors": [
                "IHS Markit"
            ],
            "title": "Businesses losing $700 billion a year to it downtime",
            "venue": "says ihs. http://news.ihsmarkit.com/ press-release/technology/businesses-losing-700billion-year-it-downtime-says-ihs",
            "year": 2016
        },
        {
            "authors": [
                "Madanlal Musuvathi",
                "Shaz Qadeer",
                "Thomas Ball",
                "Gerard Basler",
                "Piramanayagam Arumuga Nainar",
                "Iulian Neamtiu"
            ],
            "title": "Finding and reproducing heisenbugs in concurrent programs",
            "venue": "In Proceedings of the 8th USENIX Conference on Operating Systems Design and Implementation,",
            "year": 2008
        },
        {
            "authors": [
                "David Oppenheimer",
                "Archana Ganapathi",
                "David A. Patterson"
            ],
            "title": "Why do internet services fail, and what can be done about it",
            "venue": "USITS,",
            "year": 2003
        },
        {
            "authors": [
                "Nicolas Palix",
                "Ga\u00ebl Thomas",
                "Suman Saha",
                "Christophe Calv\u00e8s",
                "Julia Lawall",
                "Gilles Muller"
            ],
            "title": "Faults in linux: Ten years later",
            "venue": "In ASPLOS,",
            "year": 2011
        },
        {
            "authors": [
                "Tianyin Xu",
                "Jiaqi Zhang",
                "Peng Huang",
                "Jing Zheng",
                "Tianwei Sheng",
                "Ding Yuan",
                "Yuanyuan Zhou",
                "Shankar Pasupathy"
            ],
            "title": "Do not blame users for misconfigurations",
            "venue": "In SOSP,",
            "year": 2013
        },
        {
            "authors": [
                "Ding Yuan",
                "Yu Luo",
                "Xin Zhuang",
                "Guilherme Renna Rodrigues",
                "Xu Zhao",
                "Yongle Zhang",
                "Pranay Jain",
                "Michael Stumm"
            ],
            "title": "Simple testing can prevent most critical failures: An analysis of production failures in distributed data-intensive systems",
            "venue": "In 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI),",
            "year": 2014
        },
        {
            "authors": [
                "Yongle Zhang",
                "Serguei Makarov",
                "Xiang Ren",
                "David Lion",
                "Ding Yuan"
            ],
            "title": "Pensieve: Non-intrusive failure reproduction for distributed systems using the event chaining approach",
            "venue": "In Proceedings of the 26th Symposium on Operating Systems Principles (SOSP),",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "severity incidents that occurred recently during the production runs of many Microsoft Azure services. We find software bugs to be amajor cause behind these incidents, and make interesting observations about the types of software bugs that cause cloud incidents and how these bug-related incidents are resolved, providing motivation and guidance to future research in tackling cloud bugs and improving the cloud-service availability.\nCCS CONCEPTS \u2022 Software and its engineering \u2192 Software defect analysis; Software testing and debugging; \u2022 Computer systems organization \u2192 Cloud computing.\nKEYWORDS Cloud system, production incident, reliability, bug characteristics"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": ""
        },
        {
            "heading": "1.1 Motivations",
            "text": "Cloud services such as distributed computing infrastructures and distributed storage systems have become the\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. HotOS \u201919, May 13\u201315, 2019, Bertinoro, Italy \u00a9 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6727-1/19/05. . . $15.00 https://doi.org/10.1145/3317550.3321438\nbackbone of today\u2019s computing world. The availability of cloud services is crucial, with minutes of service outages costing millions of dollars [11, 26]. Although much research has attempted to improve the availability of cloud services through automated bug detection [19, 22, 8], failure diagnosis [33], fault detection [16], and others, there is still a lack of understanding about cloud service incidents that actually happen in the wild \u2014 what cause them and how they are resolved.\nEmpirical studies have always been crucial in motivating and guiding the improvement of software availability. Many studies were conducted to understand failure causes and failure resolutions in operating systems [10, 29, 12], multi-threaded software [25], file systems [24], and others [13].\nIn recent years, empirical studies were also conducted for cloud systems. They mainly use two types of data sources: (1) news reports about cloud outages [15], which contain detailed outage-impact information; (2) opensource bug databases [14, 15, 20, 32, 21], which contain detailed information about bugs found during both inhouse code review/testing and production uses. The focus of these studies has been (1) specific types of bugs (e.g., timing bugs [20], scalability bugs [21], gray component failures [17]); (2) high-level cause categorization (e.g., hardware faults vs. software bugs [14, 15]); and (3) error and failure symptoms (e.g., the scope, propagation, and duration of cloud errors and failures [32, 15, 17]).\nAlthough useful, previous studies have not and cannot, due to the limitations of their data sources, provide indepth understanding about production-run cloud service incidents, answering fundamental questions like: (1) What caused production-run service incidents \u2014 what types of software bugs escaped in-house testing? (2) Howwere production incidents resolved \u2014 is there any chance to automate them in the future? Answers to these questions would be crucial to improving the availability of cloud services."
        },
        {
            "heading": "1.2 Contributions",
            "text": "In this work, we systematically studied all1 the highseverity production-run incidents during a recent span of 6 months in Microsoft Azure services, which cover a wide range of services including computation, storage,\n1Except for a few without clear root-cause description (Section 2).\ndata management, data analytics, IoT, media services, etc., and identified software bugs as the most common cause of cloud incidents (close to 40%). We then did an in-depth study of all the 112 high-severity production incidents that are caused by software bugs.\nOur study sheds lights on what types of software bugs lead to production cloud incidents, how these incidents are resolved, and how they differ from failures in singlemachine systems (Table 1). What caused incidents?Among all incidents caused by bugs, the most common causes are (1) incorrect or missing detection and handling of component failures (31 %) and (2) inconsistent data-format assumptions held by different software components or versions (21 %). Timing bugs are also common (13 %), with many of them related to conflicting accesses to not only in-memory data but also persistent resources. Finally, incorrectly set constant values are non-negligible (7 %).\nProbably related to the rise of bugs related to component failures, we observed the percentage of incidents caused by hardware failures to be significantly smaller (less than 5%) than those in non-cloud systems [13].\nHowwere incidents resolved?Different from bugs in open-source software bug databases, production incidents were more often to get resolved through a mitigation mechanism without a code patch. Those mitigation mechanisms, which we further categorize into code mitigation like rolling back to an older version, running-environment mitigation like killing a process, and data mitigation like deleting a temporary directory, have not been well studied before. Without requiring new code, they provide a good opportunity for incident auto-healing and hence better service availability.\nThe resolving strategies are different among incidents caused by different types of bugs. For example, runningenvironment mitigation is the most common resolving\nstrategy for incidents caused by fault-related bugs and timing bugs, but is never used for incidents caused by data-format bugs and constant-value bugs. How to pick the suitable resolving strategy for a production cloud incident is an open problem for future research.\nImplications While there has been decades of research on bug detection, in cloud systems, some well studied bugs are much less common (e.g., memory bugs) or are taking new forms (e.g., timing bugs), yet some notso-well studied bugs (e.g., data-format bugs and faultrelated bugs) are taking predominant fractions. Many production-run incidents are resolved through mitigation techniques, instead of patches. Automation techniques that support mitigation would be helpful."
        },
        {
            "heading": "2 METHODOLOGY",
            "text": ""
        },
        {
            "heading": "2.1 Incidents in our study",
            "text": "Microsoft Azure production incidents can be reported by (Microsoft internal or external) users or by system watchdogs that keep monitoring if certain systemmetric goes beyond a pre-configured threshold. Every incident is recorded in the incident database, associated with information such as user description or watchdog report, developers\u2019 discussion, severity-level tag, root cause description, work items issued to developer teams (if any), the incident-impact duration, etc. The remainder of the paper focuses on a set of 112 incidents. They are all the incidents that satisfy the following four conditions during a 6-month period (March 5th, 2018 \u2013 September 5th, 2018): (1) the incident is not a false alarm and its severity level indicates that new features cannot commit into production environment until this incident is resolved; (2) the incident led to changes in the cloud service, such as bug-fixing patches, test enhancement, etc; (3) the incident report contains enough information for us to judge the root cause of the incident; (4) the root cause of the incident are software bugs. Note that not all the 112 incidents we studied affected Microsoft \u2019s external customers. Many incidents affected Microsoft \u2019s internal users and many others were detected by internal users and automated watchdogs and mitigated before external customers reported them."
        },
        {
            "heading": "2.2 Threats to validity",
            "text": "The results of our study have to be interpreted with our methodology in mind. The types of bugs we observed in production are biased by the fact that Microsoft uses effective tools (e.g., [4, 5, 6, 7]) to mostly eliminate many types of bugs before they can manifest in production, and hence our study includes zero\nor few of such bugs. For example, we observed only a small number of configuration bugs caused by misspecification of configuration entries in configuration files, even though such bugs were reported to be common in other settings[28, 31]). Our observation may not represent incidents in Microsoft that we did not study, and may not represent incidents in other cloud services. We analyzed only incidents whose reports contain enough information for us to judge root causes, which may lead us to miss incidents with complicated root causes and little information about the causes."
        },
        {
            "heading": "3 WHAT ARE THE BUGS?",
            "text": "Every incident report contains a \u201cdiscussion\u201d section and a \u201croot cause\u201d section, by reading these sections, and sometimes the work item description, we figure out the root cause of each incident, and categorize them into data-format bug incidents (21 %), fault-related bug incidents (31 %), timing bug incidents (13 %), constantvalue bug incidents (7 %), and others (28 %)."
        },
        {
            "heading": "3.1 Data-format incidents",
            "text": "Different components of cloud services interact with each other through various types of \u201cdata\u201d, including inter-process/node messages, persistent files, and so on. At the same time, cloud software goes through frequent updates. As a result, different software components in the cloud could hold conflicting assumptions about the format of certain data, leading to service incidents. We refer to these as data-format bugs. They have not been a type of common bugs in traditional software systems [10, 29, 13, 28], but are among the most common ones in our study (21 % of all software bug incidents). We can categorize these bugs based on the type of data whose format becomes incompatible with newer versions of the software.\n(1) Local or global files (about 40% of data-format incidents): different parties assume different formats about certain files or database tables. For example, a service, let\u2019s call it Service-X, allows users to store their customized configuration in the cloud. After a feature upgrade, Service-X changes the format of such customized configuration files \u2014 a reference to the source configuration has to exist in the customized configuration file. This reference is added to any configuration created by the new-version of Service-X. However, no such reference exists in customized configuration created by earlier versions of Service-X. Therefore, loading old customized configuration files lead to null-reference exceptions and then service incidents.\n(2) Message interfaces (about 60% of data-format incidents): a service changes the interface of its externalfacing message APIs; consequently, the other process or node that uses this message API got unexpected results. For example, a service, let\u2019s call it Service-Y, provides a REST API that returns a list of active instances. In the past, Service-Y used to return an error-code 200 together with an empty list when there were no active instances. In a newer version, Service-Y changed the API to return the error code 404 when there are no active instances, causing incidents in consuming services that could not handle this new return code.\nWe can also break down these bugs based on different roles of the conflicting parties. They could be caused by inconsistencies between data producers and data consumers (83 % of the cases), as well as between two data consumers (17 % of the cases). In the former case, data produced by one part of the system cannot be properly consumed by another part of the system; in the latter case, different system components draw inconsistent conclusions about whether some user data is valid or not. In most cases, these bugs are triggered by software updates that fail to fully consider the data-format assumptions held by all stakeholders.\nDiscussion: Among all the bugs we studied, only one of them occurs inside one process, and the other ones all involve multiple processes and/or nodes. This is probably not a coincidence: persistent data related bugs are more likely to exist in multi-process systems; message related bugs are probably unique to networked systems. The large scale, frequent updates, and long running natures of cloud services likely have facilitated the occurrence of these bugs. Techniques are needed to automatically extract assumptions about data formats, so that we can automatically detect data-format bugs or automatically raise warnings about inconsistent code versions among different software components."
        },
        {
            "heading": "3.2 Fault-related incidents",
            "text": "Component failures (i.e., faults) are inevitable in cloud environment, and 31 % of software bug incidents are about not detecting or handling faults correctly.\nIn our study, a component can refer to a user request, a user/system job, a node in the system, a file, and so on. There are three main types of component failures (i.e., faults) that lead to fault-related incidents in our study: (1) Error component: a specific task or job fails and reports an error that cannot be handled by the cloud service platform (43 % among all fault-related incidents);\n(2) Unresponsive component: a hanging job or a disabled node is not handled by the service platform (i.e., no error code ever returned) and eventually leads to a timeout perceived by users or watchdogs (29 % among all fault-related incidents);\n(3) Silent corruption: persistent data or cached persistent data became corrupted or inconsistent without any error code and led to incorrect results returned to users (17 % among all fault-related incidents).\nWe observed three main reasons for a fault in component F not being detected by a component G (in most cases,G then waits infinitely for an operation o, not realizing that o will never occur due the fault in F ). 1)G did not contain any fault detection code for potential fault in F ; 2)G typically checks certain signal or log to detect faults in F , not realizing that the signal/log itself could disappear due to the fault in F . 3)G typically checks certain signal or log to detect faults in F , not realizing that the signal/log could disappear along the transmission path from F toG . This problem sometimes happens due to file and process re-location after a component failure.\nWe observed three main types of fault handling problems: (1) handler ignores the error report (35 %); (2) handler over-reacts and causes incidents (35 %); (3) handler contains bugs like infinite loops, timing bugs, etc (30 %). The first two types of problems have also been reported in open-source systems [32]. Discussion: Fault detection and handling is usually not an issue for single-machine systems, but is a major problem in cloud services. This finding is consistent with previous studies about open-source cloud systems [14]. The predominance of fault-related problems confirms our expectation that these bugs only show up in scale and are not likely to be exposed during in-house testing. Moreover, recent research has looked at various aspects of fault/exception handling problems in distributed systems, including detecting empty error handlers and certain type of over reaction handlers [32], dealing with gray component failures [17, 16], detecting fault-related timing bugs [23], fault injection testing [3], and others. Our study indicates that even with intensive faultinjection testing inside Microsoft (Section 5), fault related bugs are still common, and hence call for more research to help detect and handle faults."
        },
        {
            "heading": "3.3 Timing incidents",
            "text": "Overall, there are 13 % timing incidents in our study set. Among all timing incidents in our study, 72 % incidents are non-deadlock issue and 14 % incidents are deadlock issue. In the remaining 14 % incidents, we only know\nthey are caused by a race condition without any detailed information.\nThere are twomain differences between non-deadlock timing bugs in our study from those in traditional concurrent systems [25].\nFirst, half of these bugs are about race conditions between multiple nodes rather than multiple threads in traditional bugs. Even when a race is among multiple threads, at least one of the threads is an event/message handling thread that is serving the request from a different node like the message-timing bugs discussed in previous empirical studies [20]. Second, half of these bugs are racing on persistent data like cached firewall rules, configuration entries, znodes in Zookeeper, database data, and others, instead of shared memory variables that traditional timing bugs race on. For example, in one case, two system processes read and write the same entry in the machine\u2019s configuration file. Races between these two processes\u2019 reads and writes led to repeated machine restarts.\nDiscussion: Timing bugs continue to be a threat to system availability in the cloud. Traditional timing-bug detection techniques need to be adapted to tackle races on persistent data and races between different nodes."
        },
        {
            "heading": "3.4 Constant-value setting incidents",
            "text": "These incidents are caused by an incorrect setting, including typos, of constant variables in the software. They contribute to 7 % of all software bug incidents. These constant variables include hard-coded configurations, special-purpose strings like URLs, and enumtyped values. For example, cloud software often contains state machines for every node, every job, and so on. The variable that represents the current state of a state machine often has enum type. In some cases, an incorrect constant value of the state variable causes the execution to enter incorrect code path.\nDiscussion: Comparing with generic and arbitrary typos and semantic bugs, these constant-setting bugs might be easier to automatically discover and fix: some of these bugs are essentially misconfiguration problems; some of these bugs are very easy to fix as there are very few choices for the constant values (considering an enum-typed value)."
        },
        {
            "heading": "3.5 Other software bugs",
            "text": "There are about one quarter of the bugs that do not belong to the above four categories. They include 7 resource leak bugs and then 24 miscellaneous semantic bugs. These 7 resource leak incidents include two out-ofmemory incidents, four Virtual Machine resource leaks,\nand one lock leak. Different from that in traditional software systems, memory bugs, other than memory leaks, did not appear at all in our study."
        },
        {
            "heading": "4 HOWWERE THEY RESOLVED?",
            "text": "Facing tight time pressure, more often than not, softwarebug incidents were resolved through a variety ofmitigation techniques (56%) without patching the buggy code (44%), providing quick solutions to users and maximizing service availability. Note that, it is possible that an incident first got resolved by a mitigation technique and later led to a software patch that was not tracked by the incident report. Q1. What are the common strategies for mitigating\nsoftware-bug incidents? We categorize all mitigation techniques into three categories: code mitigation, data mitigation, and runningenvironment mitigation. As shown in Figure 1, these three strategies are all widely used, with environment mitigation the most common in our study.\nCode mitigation mainly involves rolling back the software to an older version, or disabling certain code snippets such as an unnecessary/outdated sanity check that failed users\u2019 requests and caused severe incidents.\nData mitigation involves manually restoring, cleaning up, or deleting data in a file, a cloud table, etc.\nRunning-environment mitigation cleans up dynamic environment through killing/restarting processes, migrating workloads, adding fail-over resources, etc. Q2. Are different types of bugs resolved differently? Figure 2 shows how incidents with different root causes are resolved. As we can see, different types of bugs are indeed resolved differently. Constant-value bugs, and data-related bug incidents are mainly resolved by software patches. On the other hand, environment mitigation is widely used to resolve fault-related bugs, and timing bugs, probably due to the transient nature of many of these incidents and the complexity of handling faults and timing correctly in software.\nData-format Fault-related Timing Constant-value Others\nIncident Resolving Time\n1\n10\n20\n30\n40\n50\n0\n155\n160\nFigure 3: Resolving time for incidents caused by different types of bugs (Y-axis shows the normalized resolving time with the median resolving time of all software-bug incidents as 1; each box represents 25\u201375 percentile of each type)\nQ3.Do different types of incidents take different amount of time to get resolved?\nFigure 3 compares the normalized resolving time among incidents caused by different types of software bugs, with the median resolving time among all software-bug incidents as \u201c1\u201d. As we can see, although the resolving time varies a lot from incident to incident, there is no significant difference among incidents caused by different types of software bugs.\nDiscussion: Much recent work looked at how to automatically generate new patches. In comparison, automatically generating mitigation steps has not been well studied and worth more attention in the future."
        },
        {
            "heading": "5 PAST AND FUTURE",
            "text": "Many tools have been proposed to detect software bugs, and much focus has been put to avoid bugs during software development. We believe these efforts are reflected in and have influenced the software bug characteristics that we have seen in earlier sections.\nThe low rate of some bugs is probably related to the tools or languages that are currently used. For example, most of Microsoft Azure is written in .Net managed languages such as C#, and in C/C++, with most C/C++ code inside well-tested legacy components. This is likely the reason that we have seen few memory leak problems and other types of memory problems in our study. Tools like CHESS [27] and PCT [9] are used to expose share-memory concurrency bugs, which contribute to the relatively low rate of those bugs in our study. TLA+ [18, 30] is used to model concurrent and distributed system protocols that allow developers to eliminate high level design/semantic bugs. At the same time, some types of bugs exist despite the tools and testing already used in house. For example, many Azure services are built on top of Service Fabric [2], which provides Fault Analysis Service [1] that supports various types of fault injections, such as node restart, data migration, random faults, during testing. Although this has been effective in catching fault related problems, the large ratio of fault related bugs indicates that more research is needed.\nThere are also bugs that have not been tackled by existing tools and deserve future research attention. These include data-format bugs, distributed concurrency bugs on persistent data, and constant-value bugs.\nAs discussed earlier, much recent research has looked at how to automatically generate patches, a very challenging problem. Our study indicates a likely easier but as important, if not more, direction \u2014 how to automatically generate mitigation schemes."
        },
        {
            "heading": "6 RELATEDWORK",
            "text": "Given space constraints, we discuss below a few closely related studies on cloud/internet service failures. A recent paper [15] studied headline news and public post-mortem reports of 597 unplanned outages in 32 different production-run cloud services within a 7 year span. The different data sources led to different focuses and findings in our study and that work. Particularly, accordingly to that study, most (76%) public reports do not discuss details about how outages were resolved, and many (60%) do not explain outage root causes. Consequently, that study focused on outage duration and coarse-granularity cause breakdowns (e.g., upgrade problems versus load problems and so on). Regarding software bugs, it focuses on providing examples of interesting bugs and fixes, yet it cannot and did not answer questions like how common are different types of bugs and resolving strategies.\nYuan et. al. [32] studied 198 user reported failures in 5 open source cloud systems (Cassandra, HBase, etc.). That study intentionally did not look at the root-cause bug types and instead focused on how errors propagate and eventually manifest as failures. Consequently, their study and ours are orthogonal. Gunawi et. al. [14] studied 3000 issues in the issue system of open source cloud systems, coming from developers\u2019 code review, in-house testing, and users\u2019 reports (2011 \u2013 2014). They could not check which issues actually caused production incidents and how they were resolved during production (all issues ended up with code patches). Their study found relatively more hardware issues (13%); among software issues, they found less fault-related bugs, although still common (18%), more miscellaneous logic bugs, and did not report data format issues, persistent data timing issues, constant-value issues, and so on. The different observations are likely due to different data sources and study methodology. There was a study about internet service incidents 15 years ago [28]. Since the authors did not have access to detailed bug reports, their study about incident root causes also stayed at coarse granularity \u2014 operator versus hardware versus software. They did not have data about how incidents were resolved during production."
        },
        {
            "heading": "7 CONCLUSION",
            "text": "This paper presented an in-depth study about root causes and resolving strategies of incidents caused by software bugs in production-run cloud services. We hope findings in our study can provide a guidance for future academic and industrial efforts in this field."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "We would like to thank the anonymous reviewers for their insightful comments. This research is supported by ( CCF-1837120, CNS-1764039, 1563956, 1514256, IIS1546543) and generous support from Microsoft."
        }
    ],
    "title": "What bugs cause production cloud incidents?",
    "year": 2019
}