{
    "abstractText": "Classification of social media data is an important approach in understanding user behavior on the Web. Although information on social media can be of different modalities such as texts, images, audio or videos, traditional approaches in classification usually leverage only one prominent modality. Techniques that are able to leverage multiple modalities are often complex and susceptible to the absence of some modalities. In this paper, we present simple models that combine information from different modalities to classify social media content and are able to handle the above problems with existing techniques. Our models combine information from different modalities using a pooling layer and an auxiliary learning task is used to learn a common feature space. We demonstrate the performance of our models and their robustness to the missing of some modalities in the emotion classification domain. Our approaches, although being simple, can not only achieve significantly higher accuracies than traditional fusion approaches but also have comparable results when only one modality is available.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chi Thang Duong"
        },
        {
            "affiliations": [],
            "name": "Remi Lebret"
        },
        {
            "affiliations": [],
            "name": "Karl Aberer"
        }
    ],
    "id": "SP:3c3bb40eba8fc1d0b885e23a8e3d54d60ae3aee6",
    "references": [
        {
            "authors": [
                "K. Barnard",
                "P. Duygulu",
                "D. Forsyth",
                "Freitas",
                "N.d.",
                "D.M. Blei",
                "M.I. Jordan"
            ],
            "title": "Matching words and pictures",
            "venue": "JMLR 3(Feb)",
            "year": 2003
        },
        {
            "authors": [
                "D. Borth",
                "T. Chen",
                "R. Ji",
                "S.F. Chang"
            ],
            "title": "Sentibank: large-scale ontology and classifiers for detecting sentiment and emotions in visual content",
            "venue": "MM",
            "year": 2013
        },
        {
            "authors": [
                "M. Chen",
                "L. Zhang",
                "J.P. Allebach"
            ],
            "title": "Learning deep features for image emotion classification",
            "venue": "ICIP. IEEE",
            "year": 2015
        },
        {
            "authors": [
                "D.C. Ciresan",
                "U. Meier",
                "J. Masci",
                "L. Maria Gambardella",
                "J. Schmidhuber"
            ],
            "title": "Flexible, high performance convolutional neural networks for image classification",
            "venue": "IJCAI",
            "year": 2011
        },
        {
            "authors": [
                "B. Cui",
                "A.K. Tung",
                "C. Zhang",
                "Z. Zhao"
            ],
            "title": "Multiple feature fusion for social media applications",
            "venue": "SIGMOD. ACM",
            "year": 2010
        },
        {
            "authors": [
                "E. Hoffer",
                "I. Hubara",
                "N. Ailon"
            ],
            "title": "Deep unsupervised learning through spatial contrasting",
            "venue": "arXiv preprint arXiv:1610.00243",
            "year": 2016
        },
        {
            "authors": [
                "N.Q.V. Hung",
                "D.C. Thang",
                "M. Weidlich",
                "K. Aberer"
            ],
            "title": "Minimizing efforts in validating crowd answers",
            "venue": "SIGMOD. pp. 999\u20131014",
            "year": 2015
        },
        {
            "authors": [
                "G. Irie",
                "T. Satou",
                "A. Kojima",
                "T. Yamasaki",
                "K. Aizawa"
            ],
            "title": "Affective audio-visual words and latent topic driving model for realizing movie affective scene classification",
            "venue": "TMM",
            "year": 2010
        },
        {
            "authors": [
                "J. Jia",
                "S. Wu",
                "X. Wang",
                "P. Hu",
                "L. Cai",
                "J. Tang"
            ],
            "title": "Can we understand van gogh\u2019s mood?: learning to infer affects from images in social networks",
            "venue": "MM",
            "year": 2012
        },
        {
            "authors": [
                "A. Joulin",
                "E. Grave",
                "P. Bojanowski",
                "T. Mikolov"
            ],
            "title": "Bag of tricks for efficient text classification",
            "venue": "arXiv preprint arXiv:1607.01759",
            "year": 2016
        },
        {
            "authors": [
                "T.K. Kim",
                "J. Kittler",
                "R. Cipolla"
            ],
            "title": "Discriminative learning and recognition of image set classes using canonical correlations",
            "venue": "TPAMI 29(6)",
            "year": 2007
        },
        {
            "authors": [
                "G. Koch",
                "R. Zemel",
                "R. Salakhutdinov"
            ],
            "title": "Siamese neural networks for one-shot image recognition",
            "venue": "ICML Deep Learning Workshop. vol. 2",
            "year": 2015
        },
        {
            "authors": [
                "X. Lu",
                "P. Suryanarayan",
                "R.B. Adams Jr",
                "J. Li",
                "M.G. Newman",
                "J.Z. Wang"
            ],
            "title": "On shape and the computability of emotions",
            "venue": "MM",
            "year": 2012
        },
        {
            "authors": [
                "C. Lutkewitte"
            ],
            "title": "Multimodal composition: A critical sourcebook",
            "venue": "Bedford/St. Martin\u2019s",
            "year": 2013
        },
        {
            "authors": [
                "J. Machajdik",
                "A. Hanbury"
            ],
            "title": "Affective image classification using features inspired by psychology and art theory",
            "venue": "MM",
            "year": 2010
        },
        {
            "authors": [
                "T. Mikolov",
                "K. Chen",
                "G. Corrado",
                "J. Dean"
            ],
            "title": "Efficient estimation of word representations in vector space",
            "venue": "arXiv preprint arXiv:1301.3781",
            "year": 2013
        },
        {
            "authors": [
                "Q.V.H. Nguyen",
                "C.T. Duong",
                "T.T. Nguyen",
                "M. Weidlich",
                "K. Aberer",
                "H. Yin",
                "X. Zhou"
            ],
            "title": "Argument discovery via crowdsourcing",
            "venue": "VLDBJ",
            "year": 2017
        },
        {
            "authors": [
                "T.T. Nguyen",
                "C.T. Duong",
                "M. Weidlich",
                "H. Yin",
                "Q.V.H. Nguyen"
            ],
            "title": "Retaining data from streams of social platforms with minimal regret",
            "venue": "IJCAI",
            "year": 2017
        },
        {
            "authors": [
                "T.T. Nguyen",
                "Q.V.H. Nguyen",
                "M. Weidlich",
                "K. Aberer"
            ],
            "title": "Result selection and summarization for web table search",
            "venue": "ICDE. pp. 231\u2013242",
            "year": 2015
        },
        {
            "authors": [
                "J. Pennington",
                "R. Socher",
                "C.D. Manning"
            ],
            "title": "Glove: Global vectors for word representation",
            "venue": "EMNLP",
            "year": 2014
        },
        {
            "authors": [
                "R. Plutchik"
            ],
            "title": "The nature of emotions human emotions have deep evolutionary roots, a fact that may explain their complexity and provide tools for clinical practice",
            "venue": "AS",
            "year": 2001
        },
        {
            "authors": [
                "M.E. Sargin",
                "Y. Yemez",
                "E. Erzin",
                "A.M. Tekalp"
            ],
            "title": "Audiovisual synchronization and fusion using canonical correlation analysis",
            "venue": "IEEE Trans. Multimedia",
            "year": 2007
        },
        {
            "authors": [
                "A. Sharif Razavian",
                "H. Azizpour",
                "J. Sullivan",
                "S. Carlsson"
            ],
            "title": "Cnn features off-the-shelf: an astounding baseline for recognition",
            "venue": "CVPR",
            "year": 2014
        },
        {
            "authors": [
                "C. Szegedy",
                "W. Liu",
                "Y. Jia",
                "P. Sermanet",
                "S. Reed",
                "D. Anguelov",
                "D. Erhan",
                "V. Vanhoucke",
                "A. Rabinovich"
            ],
            "title": "Going deeper with convolutions",
            "venue": "CVPR",
            "year": 2015
        },
        {
            "authors": [
                "Q. You",
                "L. Cao",
                "H. Jin",
                "J. Luo"
            ],
            "title": "Robust visual-textual sentiment analysis: When attention meets tree-structured recursive neural networks",
            "venue": "MM",
            "year": 2016
        },
        {
            "authors": [
                "Q. You",
                "J. Luo",
                "H. Jin",
                "J. Yang"
            ],
            "title": "Building a large scale dataset for image emotion recognition: The fine print and the benchmark",
            "venue": "arXiv preprint arXiv:1605.02677",
            "year": 2016
        },
        {
            "authors": [
                "Q. You",
                "J. Luo",
                "H. Jin",
                "J. Yang"
            ],
            "title": "Cross-modality consistent regression for joint visualtextual sentiment analysis of social multimedia",
            "venue": "WSDM",
            "year": 2016
        },
        {
            "authors": [
                "M. Zeppelzauer",
                "D. Schopfhauser"
            ],
            "title": "Multimodal classification of events in social media",
            "venue": "Image and Vision Computing 53",
            "year": 2016
        },
        {
            "authors": [
                "S. Zhao",
                "Y. Gao",
                "X. Jiang",
                "H. Yao",
                "T.S. Chua",
                "X. Sun"
            ],
            "title": "Exploring principles-of-art features for image emotion recognition",
            "venue": "MM",
            "year": 2014
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "With the advance of social media networks, people are sharing user-generated contents in an unprecedented scale [18]. Social media networks have shifted from being specialized as users can only share either texts or images to general-purpose as users can now share texts, images, audio segments or even video clips. As recent statistics show that posts with images get more interaction 1 while video tweets fuels discovery and drives engagement 2, we expect an enormous increase of multimodal posts. The contents posted by users on social networks are a way for them to express themselves such as their emotions, feelings or to share life events. As a result, these posts provide precious information about the users that if these information is analyzed systematically, we would have a strong understanding about the users. For instance, knowing the emotions of the users by analyzing these contents can bring enormous benefits. An ad campaign can customize its ad based on the emotion expressed in a recent post by a user, which would reduce marketing cost but increase user engagement. A further step is to induce an emotion in a user proactively using a specific image or piece of text. A use case would be using happy images or joyful texts to encourage users to buy a product. There are many approaches to social media analysis and classification is usually the tool of choice when we want to study an aspect of the user base in depth such as their sentiment or emotion.\n1blog.bufferapp.com/the-power-of-twitters-new-expanded-images[..] 2blog.twitter.com/2015/new-research-twitter-users[..]\nar X\niv :1\n70 8.\n02 09\n9v 1\n[ cs\n.C L\n] 7\nA ug\n2 01\n7\nAlthough classifying social media content for user understanding is highly important, traditional works tend to focus on one modality[5]. For instance, works in emotion classification leverage only the visual information, textual information, which is also important, is often left out. Although images can express users\u2019 emotions more vividly, textual information can also convey various emotional aspects. Table 1 shows some posts with the availability of both image and text. In the first example, both the text and the image express the same emotion joy, which is also the emotion expressed by the post. On the other hand, in the second example, using only the text, it is easy for us to tell it conveys the emotion joy but the visual signal is the dominant one in this case and it expresses the emotion fear. In the last example, without considering the text, the image may describe the emotion contentment due to the presence of a pet. However, the text provides the context which changes the meaning and the emotion expressed in the image completely. These examples have shown that when information is available in different modalities, only by leveraging all of them can give us a complete picture.\nGiven the drawbacks of unimodal approaches, several works have been done on combining multimodal information for social media analysis. These works are able to improve the classification accuracy significantly[28]. However, most of these approaches are usually complicated and tailored to specific problems and modalities, which makes them hard to apply to new problems or add other modalities[5]. In addition, these classification techniques also assume the availability of all the required modalities, which is not the case in practice. For instance, a tweet classification technique that combines visual and textual information to classify may not work if the tweets contain only texts. Inspired by these observations, we propose techniques to combine information from multiple modalities based on neural network models. Our models combine information from different modalities using a pooling layer and an auxiliary learning task is used to learn a common feature space for all modalities. Our techniques are able to achieve high accuracy without the use of complex model while allowing to classify even when some modalities are missing. In addition, our techniques are easily scalable to new problems or more modalities.\nWe demonstrate the performance of our approaches in the emotion classification domain. Emotion classification is a multiclass classification problem with many applications in practice. We focus on two prominent modalities which are textual and visual resources. To the best of our knowledge, we are the first to propose techniques that combine information from different modalities in the emotion classification domain. In this paper, we make the following contributions.\n\u2013 We propose novel generic techniques that can combine information from multiple modalities to classify social media content. In particular, we leverage advances in neural networks to build our fusion approaches that are scalable to new modalities and new problems. \u2013 Our proposed techniques are also robust to different types of input as they are able to handle the missing of some modalities. In the emotion classification demonstration, they can tackle three cases: only image or text or both are available. \u2013 We construct a dataset which contains data from textual and visual modalities to test our techniques. The dataset also contains strong labels for training and testing. We also enrich an image-only dataset with textual data. These datasets will be made available to foster research in the emotion analysis field.\nThe rest of the paper is organized as follows. Section 2 introduces related works on the field of multimodal classification and emotion analysis. Section 3 discusses a general fusion model and traditional fusion approaches. Section 4 explains in detail our proposed techniques. Experimental evaluation and analysis are presented in Section 5 while Section 6 concludes the paper."
        },
        {
            "heading": "2 Related Work",
            "text": "Multimodal classification for social media. Multimodal classification techniques can be classified into two main classes depending on how the information from multiple modalities are combined. In late fusion, separate classification results obtained from different modalities are constructed first and the fusion is done based on these results at the decision level[5]. Late fusion implies an assumption that the underlying data from different modalities are independent. This assumption is not practical as information from different modalities still describe the same event/object, which means they could be correlated[22]. In a recent work[27], a variant of late fusion is discussed where the authors used KL divergence to enforce the results from different modalities to be similar. Early fusion takes a different approach as the fusion is done at the feature level where information from different modalities are appended. Classification is then done on the appended representation[28]. Various variants of early fusion are proposed to classify social media content. In sentiment analysis, instead of concatenating information from different modalities, the work in [25] use LSTM to combine visual and textual information. One approach[28] in social event classification constructed a hierarchical classifier on the concatenated features to classify non-event and other event types. Several intermediate fusion techniques are also available such as using LDA to extract joint latent topics[1] or by using statistical methods such as CCA[11] for image classification. Existing techniques for multimodal classification are usually complex and they often require the presence of all modalities. Although our proposed approaches can be considered as a variant of early fusion, they differ from previous techniques as they can handle the absence of some modalities while being simpler. Our model seems to be similar to a Siamese network [12] which contains two identical subnetworks with the same parameters and weights. However, our approach differs from a Siamese network as the image and text subnetworks architecture are different, hence, their parameters are completely different.\nEmotion analysis. Techniques for analyzing emotions can be classified into handcrafted features and deep features. Early works in emotion analysis leverage features based on art and psychology theory. These artistic features are usually low-level such as shape [13], color and texture [15]. Several higher-level features which are the combination of these low-level features are also used [8,9,2,17]. In a recent work [29], the authors proposed several features based on principles of arts in which combinations of these features can evoke different emotions. After these features are defined, a classifier such as SVM is constructed on top of these features to classify the emotions. The inherent drawback of techniques that uses hand-crafted features is the required availability of these features. However, designing these features is a tedious process which requires expert knowledge and it is not guaranteed that all features that can capture the emotions are covered. Given this drawback, techniques that do not require a feature design process are proposed. These features are constructed automatically using CNNs. Only a recent work[3] has leveraged deep features to emotion analysis and already achieved better accuracy in comparison with hand-crafted features. However, the technique in [3] only deals with images, which leaves out the textual information completely. As we show in Table1, considering only textual or visual information is not enough to analyze emotions. This work is also the closest to our work as we also use build image representations using Convolutional Neural Networks(CNNs). However, we differ from this work significantly as we also take into account textual information and the combination of them. To the best of our knowledge, our technique is the first to combine visual and textual information to classify emotions."
        },
        {
            "heading": "3 Multimodal Classification for Analysing Social Media",
            "text": "In this section, we denote the modalities in social media that we use in our models, and we present the traditional approaches for combining them in classification."
        },
        {
            "heading": "3.1 Multimodality in Social Media",
            "text": "Multimodality describes communication practices in terms of the textual, aural, linguistic, spatial, and visual resources - or modes - used to compose messages (or posts) [14]. In this paper, we focus on combining the two prominent modalities of social media, i.e. textual and visual resources. It is worth mentioning that the proposed models is easily scalable to more modalities.\nWe define x \u2208 X a post that can be composed of an image or a text, or both an image and a text. When both image and text are present, we assume that they are semantically related, e.g. the text describes the image. Each image i is represented with an image feature vector \u03b3(i) \u2208 Rn. Thanks to the recent progress in computer vision using deep learning, \u03b3(i) can be extracted from CNNs trained on millions of images. A text can be as long as a paragraph or as short as a phrase. We represent each piece of text s with a textual feature vector \u03c8(s) \u2208 Rm. Such textual feature vectors can be obtained with classical bag-of-words models or with approaches based on word embeddings."
        },
        {
            "heading": "3.2 Multimodal Classification using Feature Level Fusion",
            "text": "Given a post x\u2208X and a set of classes Y , we define a model that assigns probabilities to all y \u2208 Y . The predicted class is then the one with the highest probability,\ny\u0302 = argmax y P(Y = y|x). (1)\nWhen x = i or x = s, unimodal classifiers are trained on the entire training set in each modality. As we are interested in combining image and text (i.e. when x = {i,s}), we discuss in this part two traditional fusion approaches: late fusion and early fusion. The difference in these approaches is when the fusion is done in the classification process. In early fusion, the fusion happens before the classifier is constructed (hence the name early) while in late fusion, the fusion is done after the classifiers are created.\nLate fusion Late fusion requires the construction of two independent classifiers: one for image and one for text. The predicted class is then the highest product between the two classifiers,\ny\u0302 = argmax y P(Y = y|i)P(Y = y|s). (2)\nEarly fusion Early fusion, on the other hand, does not require the construction of two separate classifiers as the fusion applies in the feature space. More precisely, it requires modeling the post x as a feature vector:\nx = [\u03b3(i);\u03c8(s)] (3) where x \u2208 R(n+m) is the vector for the post x and [v1;v2] denotes vector concatenation. The model to classify x can be constructed on top of the feature vectors x. Classification techniques such as SVM can be used. In our work, the classification is done using neural network as discussed in the next section."
        },
        {
            "heading": "4 Joint Fusion with Neural Network Models",
            "text": "Although late fusion and early fusion are able to combine visual and textual data for classification, they also suffer from some drawbacks. Late fusion requires the construc-\ntion of two classifiers. In many cases, the construction of two separate classifiers would negate the purpose of combining them as the improvement (if any) from using only one classifier could be insignificant. On the other hand, while early fusion does not need two separate classifiers, it requires the availability of both the image and the text, which may be too stringent. In this section, we propose two fusion approaches that enjoys the simplicity of early fusion and the flexibility of late fusion. We call these approaches joint fusion and common space fusion. Our fusion approaches are based on neural networks as they provide many advantages: 1) neural networks allow to train the classifier in an end-to-end manner without involving the tedious feature engineering process, 2) neural networks are also highly-customizable as the model can be changed easily by adding or removing some layers."
        },
        {
            "heading": "4.1 Mathematical notations and layers",
            "text": "A feedforward neural network estimates P(Y = y|x) with a parametric function \u03c6\u03b8 (Equation 1), where \u03b8 refers to all learnable parameters of the network. Given an input x, this function \u03c6\u03b8 applies a combination of functions such as \u03c6\u03b8(x) = \u03c6L(\u03c6L\u22121(. . .\u03c61(x) . . .)), (4) with L the total number of layers in the network.\nWe denote matrices as bold upper case letters (X, Y, Z), and vectors as bold lowercase letters (a, b, c). Ai represents the ith row of matrix A and [a]i denotes the ith element of vector a. Unless otherwise stated, vectors are assumed to be column vectors. We now introduce the two standard layers when training linear classifiers with neural networks: the linear layer and the softmax layer.\nLinear layer This layer applies a linear transformation to its inputs x: \u03c6l(x) = Wlx+bl (5)\nwhere Wl and bl are the trainable parameters with Wl being the weight matrix, and bl is the bias term. For instance, when training an image classifier, x = \u03b3(i) \u2208 Rn with Wl \u2208 R|Y |\u00d7n and bl \u2208 Rn. And x = \u03c8(s) \u2208 Rm with Wl \u2208 R|Y |\u00d7m and bl \u2208 Rm, when training a classifier with only textual content.\nSoftmax layer Given an input x, the penultimate layer outputs a score for each class y \u2208 Y , \u03c6L\u22121(x) \u2208 R|Y |. The probability distribution is obtained by applying the softmax activation function:\nP(Y = y|x) \u221d \u03c6\u03b8(x,y) = exp(\u03c6L\u22121(x,y))\n\u2211|Y |k=1 exp(\u03c6 L\u22121(x,yk))\n(6)\nUsing the above layers, early fusion can be described as a neural network as in the following example.\nExample 1 (Early fusion as a neural network model). Early fusion can be modelled as a neural network with 3 layers: \u03c6\u03b8(x) = \u03c63(\u03c62(\u03c61(x))). The first layer \u03c61 is a fusion layer that applies the concatenation operation on the image and text feature vectors, \u03b3(i),\u03c8(s). The output of this layer is the post vector x = [\u03b3(i);\u03c8(s)]. The second layer\n\u03c62 is a linear layer that takes the concatenation of both image and text feature vectors, x = [\u03b3(i);\u03c8(s)] \u2208 R(n+m). The learnable parameters of this layer are W2 \u2208 R|Y |\u00d7(n+m) and b2 \u2208R(n+m). The final layer \u03c63 is a softmax layer which converts the score for each class obtained from the second layer to probability. The parameters of early fusion are \u03b8 = {W2,b2}"
        },
        {
            "heading": "4.2 Feature Vectors",
            "text": "Before diving into the detail of our fusion models, we discuss the process to represent each image and text as feature vectors (\u03b3(i),\u03c8(s)) as early fusion and our proposed approaches require an underlying image and text representation. A good image and text representation can affect the performance of these approaches heavily.\nImage Representations To represent the images, we use a CNN to extract features, i.e. represent each image i as a feature vector \u03b3(i) \u2208 Rn. This is motivated by the fact that CNNs are able to obtain state-of-the-art results in many object classification tasks[4]. Therefore, the CNN may capture features which are suitable to classify objects[23]. These features may also be applicable for other types of classification, such as emotion. Technically, we take a CNN[24] trained on the ImageNet dataset for the object classification task and remove the last fully-connected layer while keeping the other layers the same. By feeding each image through this pretrained CNN, we obtain an image vector in the output.\nIt is worth noting that by keeping other layers the same, we use no parameter to obtain the image vector. However, we can also retrain the CNN with our dataset. In this case, the parameter for \u03b3(i) is all the parameters of the CNN. They will be trained together with other parameters of the joint fusion models.\nText Representations The process of converting a text s from its original format (i.e. words) to a d-dimensional vector is modelled by a function \u03c8\u00b5(s), which takes a sequence of words s = \u3008w1,w2, ...,wn\u3009 as input, where each word wt comes from a predefined vocabulary V . This function is a composition of an embedding layer and an aggregation layer.\nEmbedding layer. Given a parameter matrix E \u2208 R|V |\u00d7d , the embedding layer is a lookup table that performs an array indexing operation: \u03c81(wt) = Et \u2208 Rd , (7) where Et corresponds to the embedding of the element wi at row i. This matrix is usually initialized with pretrained word embeddings. d is the dimensionality of the word vectors which is a hyperparameter chosen by the users.\nBy feeding each word of a text s = \u3008w1,w2, ...,wn\u3009 through the lookup table, we obtain the following output matrix:\n\u03c81(s) = (E1,E2, ...,En) \u2208 Rd\u00d7n. (8)\nAggregation layer. The second layer \u03c82 of the network is an aggregation layer that takes the matrix \u03c81 from the previous layer as input and returns a vector representation\nfor the text s. Formally, the aggregation layer applies the following operation: \u03c82(s) = agg { \u03c81(wt),\u2200wt \u2208 s } , (9)\nwhere agg is an aggregation function. This function can be either the average or a component-wise max. More complex functions could be used for aggregating the word embeddings, such as a convolutional or recurrent neural network. But it has been shown that these simpler aggregating function with no parameters have similar performance on classification tasks [10].\nAs the network \u03c8\u00b5 has only two layers and the second layer has no parameter, \u00b5 = {E}. This parameter will be trained together with the parameters of the classifier."
        },
        {
            "heading": "4.3 Joint Fusion Models",
            "text": "As discussed above, our motivation for joint fusion is to have models that can classify posts having only image or text or both without constructing two classifiers like late fusion. To achieve this goal, our joint fusion models change how the post vector x is constructed from the image and text vector.\nJoint Fusion Learning In early fusion, the post vector x is constructed in the fusion layer by concatenating the image vector \u03b3(i) and the text vector \u03c8(s). This way of combining the vectors has two important implications. First, both the image and text vector must be available, which prevents our intention to classify using only image or text. Second, we still consider the textual and visual information as separate as each of them affects the classification independently. In joint fusion models, we change the way the text and image vectors are fused in the fusion layer while keeping other layers similar to early fusion.\nFusion layer. The fusion layer takes the image vector and text vector \u03b3(i),\u03c8(s) as input and applies the pooling operation to obtain the post vector x:\nx = pooling(\u03b3(i),\u03c8(s)) (10) The pooling function can be either a component-wise max pooling, or an average pooling.\nIt is worth noting that the pooling operation requires the vectors \u03b3(i) \u2208 Rn and \u03c8(s) \u2208 Rm to have the same size. In Figure 1c, we assume the image vector has higher dimension and we project its feature vector into the textual feature space. This can be done by adding an extra linear layer to \u03b3 (i.e. the network that extracts image feature vector). Assuming n > m, the linear layer is as follows:\n\u03b3\u0303(i) = W\u0303\u03b3(i)+ b\u0303 (11) where \u03b3\u0303(i) \u2208 Rm, W\u0303 \u2208 Rn\u00d7m and b\u0303 \u2208 Rm. The input to the fusion layer is then two vectors \u03b3\u0308(i) and \u03c8(s).\nThe parameters of joint fusion are \u03b8 = {W2,b2,W\u0303, b\u0303}. Training. The parameter \u03b8 is obtained by training the joint fusion neural network by minimizing the negative log-likelihood using stochastic gradient descent(SGD):\nL(\u03b8) = \u2211 (x,y) \u2212 logP(Y = y|x) \u221d \u2211 (x,y) \u2212 log\n( \u03c6\u03b8(x,y) ) . (12)\nCommon Feature Space Fusion Although joint fusion allows to classify even when only image or text is available, the accuracy of these unimodal classifiers are not the same (c.f. Section 5) as joint fusion still considers visual and textual signals of the same post as different. Motivated by this observation, we propose common space fusion which aims to enforce visual and textual vectors of the same post to be similar i.e. to be in the same feature space. We achieve this using an auxiliary learning task in addition to the classification main task. The neural network of common space fusion is similar to that of joint fusion with the exception of the addition of the auxiliary task. Auxiliary learning task. The goal of the auxiliary learning task is to make the image and text vector \u03b3(i),\u03c8(s) of the post x to be similar while the image vector \u03b3(i) of the post x and the text vectors {\u03c8(s\u2212)} of posts from different classes are different. The vectors \u3008\u03b3(i),\u03c8(s+)\u3009 of the post x is called a positive pair while the vectors \u3008\u03b3(i),\u03c8(s\u2212)\u3009 is called a negative pair. We measure the similarity and difference between the pairs using a distance metric d(\u03b3(i),\u03c8(s)). Intuitively, we want the distance of the positive pair to be low while the distance of the negative pairs to be high. This objective is captured by the loss function for the auxiliary task. Training. Traditionally, the objective of the auxiliary task is captured using a marginbased loss function. However, a recent work has shown that using a probabilistic interpretation of the margin-based loss function yields better result[6]. The loss function for the auxiliary task is defined as follows:\nL(i,s+,{s\u2212j } j=1,g) = \u2212 \u2211 j=1,g\nlog ( exp(\u2212d(\u03b3(i),\u03c8(s+)))\nexp(\u2212d(\u03b3(i),\u03c8(s+)))+ exp(\u2212d(\u03b3(i),\u03c8(s\u2212j ))) ) (13)\nwhere g is the number of negative pairs to be used in training. With \u03b8 = {W2,b2,W\u0303, b\u0303}, we minimize the following loss function involving the auxiliary learning task and the classification main task for a given training sample (x,y), where x = {i,s+}:\nL(x,y;\u03b8)=\u2212\u03bb log ( \u03c6\u03b8(x,y) ) \u2212 \u2211\nj=1,g log ( exp(\u2212d(\u03b3(i),\u03c8(s+))) exp(\u2212d(\u03b3(i),\u03c8(s+)))+ exp(\u2212d(\u03b3(i),\u03c8(s\u2212j ))) ) (14)\nwhere the g negative text samples s\u2212 are chosen randomly at each iteration, and \u03bb is a hyperparameter specifying the weight of the classification main task."
        },
        {
            "heading": "5 Experiments with Emotion Classification",
            "text": "In this section, we evaluate our proposed fusion approaches on the emotion classification application. To the best of our knowledge, we are the first one to study combining visual and textual data for emotion classification."
        },
        {
            "heading": "5.1 Emotion as Discrete Categories",
            "text": "In emotion classification, the problem we want to solve is given a post x, we want to construct a classifier to find out the post\u2019s membership i.e., x belongs to which emotion\nclass. The set of emotion classes is denoted as Y = {y1, ...,yk} where each class yi \u2208 Y and \u2200i, j : yi\u2229 y j = /0. For the emotion classes, we use the most well-known Plutchik\u2019s classification of emotions [21]. An illustration of the Plutchik\u2019s wheel of emotions is shown in Figure 2."
        },
        {
            "heading": "5.2 Datasets",
            "text": "To the best of our knowledge, there is no large-scale dataset for emotion classification that contains both visual and textual data. Even for visual content only, most of the datasets are relatively small except for the flickr dataset[26]. This motivates us to build a dataset from scratch while also adding textual data to the flickr dataset. Enriching an image-only dataset. The flickr dataset was produced by You et. al.[26], which contains images from a popular image sharing website3. To obtain the labels for these images, at least 5 workers from Amazon Mechanical Turks were hired to label them into 8 emotion classes: amusement, anger, awe, contentment, disgust, fear and sadness. We first crawl the images from this dataset while keeping only the ones where the majority of the workers agree that the image belongs to a specific emotion class. Then, for each image, we collect its title and description to use as textual data. Although these information are not provided in the dataset, they are still available from the image sharing website. We only keep the images with English title and description and the total of words in the title and description is more than 5. The statistics of the flickr dataset is shown in Table 2. Building an emotion dataset from scratch. In order to test our approaches in different settings, we build another emotion dataset by crawling data from Reddit4. Reddit is a discussion website where discussions are organized by topics (i.e. subreddits). Reddit\n3www.flickr.com 4www.reddit.com\nTable 2: Statistics of the two datasets\nemotion #posts\nreddit joy 1119 fear 697 anger 613 disgust 810\nTotal 3239\nemotion #posts\nflickr amusement 1259 anger 407 awe 1561 contentment 2389 disgust 852 excitement 1451 fear 434 sadness 984\nTotal 9337\nTable 3: Model hyperparameters\nParameter Value\nWord embedding size d = 200 Hidden vector space h = 100 # negative samples (reddit) g = 3 # negative samples (flickr) g = 4 Classification main task weight \u03bb = 3 Aggregation layer function max Fusion layer function max\nalso has a reputation system where submissions are vote up or down. This reputation system enforces that 1) submissions in a subreddit are always belong to a topic, 2) each submission is \u201clabelled\u201d by a number of users. These characteristics make Reddit an attractive social media to build an emotion dataset as it provides strongly-labelled data while a submission always contains text in its title and sometimes contains image. We focus on 4 popular subreddits (happy, creepy, rage, gore) which are related to emotion and contain high amount of image submissions. These subreddits correspond to the emotion joy, fear, anger, disgust in the Plutchik\u2019s model for emotion classification.\nFor each subreddit, we crawled 1000 submissions with the highest number of upvotes. We kept only posts containing image and discarded the rest. This created an imbalance in the number of posts between the emotion classes. For this reason, we collected submissions with at least 100 upvotes for two classes with the least amount of posts. It is worth noting that one post may contain several images. All the images are then converted to jpg format. The statistics of the reddit dataset is shown in Table 2."
        },
        {
            "heading": "5.3 Baselines",
            "text": "We compare our proposed approaches (joint fusion and common space fusion) with two unimodal baselines (a text-based and an image-based classifier) and two traditional multimodal fusion techniques (early and late fusion). Regarding the text-based classifier, we chose to use fastText[10] which is a shallow network model for text classification. FastText is able to achieve state-of-the-art result without sacrificing much performance in comparison with other deep models[10]. For the image-based classifier, we use a pretrained InceptionNet[24] which is a CNN for object classification task. To make an image classifier for our setting, we replace the last layer in InceptionNet with a linear layer and train this layer with our dataset while keeping other layers the same. In addition to serving as a baseline, we also use InceptionNet to extract the image features as mentioned in Section 4, which results in a total of 2048 features per image. Regarding late fusion, for comparison purpose, we also reuse the text-based and image-based classifiers to obtain the class probabilities. It is worth noting that we have considered other baselines such as using SVM as the classifier. However, as these techniques have worse performance than deep-learning approaches, we do not include them in the paper."
        },
        {
            "heading": "5.4 Experimental settings",
            "text": "For the embedding layer, we used the GloVe word vectors[20] trained on Twitter data as they are in the same social media domain as Flickr and Reddit. For regularization, we used a dropout layer with a dropout probability of 0.25 right after the lookup table to reduce overfitting. In addition, it is reported that factorizing the linear classifier into low rank matrices may improve the classification accuracy[16]. We also followed this approach by adding a linear layer right before the last layer to map the concatenated vector (in the case of early fusion) and the pooled vector (in the case of joint fusion models) to a hidden vector space with a size of h. Regarding the hyperparameters, we tested different values of them on the validation set and select the ones that gave the best results. Table 3 describes other hyperparameters. Our models were trained with a learning rate set to 0.01.\nThe models were trained on a server equipped with a Tesla GPU. For testing, we use the top 10% posts with the highest number of upvotes for the reddit dataset and the highest number of agreements for the flickr dataset. The next 10% of these dataset are used for validation. We use the same splits for all the models in our experiments. All the source codes and the datasets are available at https://emoclassifier.github.io/"
        },
        {
            "heading": "5.5 Quantitative analysis",
            "text": "Results on the Reddit dataset Table 4 shows the experimental results for the Reddit dataset. The results show that visual and textual classifiers have similar accuracy on the reddit dataset as their difference in accuracy is only 1%. In addition, all the fusion techniques have better accuracy than the baselines. The traditional fusion approaches improve the accuracy by at least 5%. This clearly demonstrates the benefits of combing visual and textual data. Our proposed techniques perform the best as they improves the accuracy by 8% and 2% in comparison with using single modality and early/late fusion, respectively. Among the proposed techniques, common space fusion has higher accuracy. However, the difference is small as it is only 0.6%.\nResults on the Flickr dataset The experimental results for the Flickr dataset is also shown in Table 4. The dataset is interesting as there is a large discrepancy in accuracy (30%) between image-based and text-based classifiers. On the contrary, the discrepancy\nbetween these classifiers in the Reddit dataset is only 1%. The reasons for this could be 1) the title and description of an image post from flickr are more descriptive as they tend to be longer than the title of a submission from reddit, 2) the labels from the reddit dataset are more reliable as they are \u201clabelled\u201d by at least 100 users in comparison with only 5 for the flickr dataset. Similar to the reddit dataset, all the fusion approaches have better accuracy than unimodal classifiers and our proposed approaches outperform the rest with the common space fusion has better accuracy.\nThe experimental results on two datasets with different characteristics show that combining textual and visual data can improve the accuracy of classification significantly. In addition, our proposed techniques are robust with different setting as they always achieve the highest accuracy throughout two datasets.\nHandling the absence of one modality The goal of joint and common space fusion is not only able to achieve higher accuracy by combining visual and textual signals but also able to provide correct classification when only either text or image is available. In this section, we analyze the performance of our proposed approaches when only one modality is used as input. Table 5 shows the experimental results for two different datasets, we also replicate the results with the image and text-only classifiers for comparison purposes. The results clearly demonstrate the benefits of creating a common space as the difference in accuracy between using only image or text as input for common space fusion is significantly lower than that of joint fusion. For the reddit dataset, the difference is only 0.8% for common space fusion while it is 34% for joint fusion. The discrepancy for common space fusion is lower as it considers image and text vector of a post as equally important. As a result, using either image or text to classify, we get similar accuracy. On the other hand, joint fusion considers textual information as more important. This makes the classification using text significantly better than using only image. Another key finding is that using only image or text as input, the common space fusion has better result on the flickr dataset than single-input classifier. We achieve a gain of 13% for image and 0.8% for text. As the common space contains information from two modalities, even when we use only one modality as input, we can also leverage information from other modality. However, by enforcing a common space from two modalities, we may lose information if one modality is less informative than the other. This happens with the reddit dataset where the titles are shorter and less descriptive than the descriptions of flickr images."
        },
        {
            "heading": "5.6 Qualtitative analysis",
            "text": "Results with multimodal input We also compare and analyze the results to find out in which case our proposed approaches, image-only or text-only approach is better. Table 6 shows some noteworthy examples. The first two examples demonstrate the benefits of creating the common space as it allows the classifier to put more weights on the visual signal. As the image descriptors of these examples are sarcasm, focusing only on the text give incorrect predictions. For instance, the image descriptor contains the word \u201cpresent\u201d which is a strong indicator for the emotion joy. However, the image shows that the present is a creepy doll in a closet, which expresses the emotion fear. Only by taking into account both the image and the text, a classifier can make a correct prediction i.e. this is the advantage of fusion approaches. The next two examples shows that when a signal is stronger than the other, the fusion approaches will make the same prediction as the stronger signal. In very rare cases as in the fifth example, the stronger signal is an incorrect one, which makes the fusion\u2019s prediction incorrect.\nResults with the absence of one modality Table 7 shows several examples where either image or text is used as input to our fusion approaches. As one type of input is missing, single-modality fusion approaches may misclassify. This is illustrated in the first two example where unimodal classifiers classify incorrectly while fusion approaches give correct results. In the first example, the presence of the words \u201cwife\u201d,\u201cbaby\u201d usually indicates the emotion joy while the image indicates the emotion fear. In the next two examples, common space fusion makes correct predictions while other approaches fail. The text from the third example contains the words \u201cwake up\u201d, \u201cmorning\u201d, which indicates the emotion joy. This makes joint fusion misclassify as it considers textual signal as more important. On the other hand, common space fusion is able to balance between textual and visual signals and come up with correct predictions. The fifth example shows a case where the image-only common space fusion is even better than the image classifier. While the image classifier predicts the emotion fear due to the presence of the color red and people\u2019s skin, the image actually expresses the emotion contentment, which is correctly classified by the image-only common space fusion."
        },
        {
            "heading": "6 Conclusions",
            "text": "In this paper, we propose simple models that are able to combine information from different modalities to analyze social media. Our models are robust with different types of input as they can handle the missing of some modalities. In addition, we show that our models, despite being simple, can achieve high accuracy on the emotion classification application. In order to showcase our models, we also constructed two multimodal datasets which allow us to test our approaches in different settings. Future research directions will go towards analyzing the performance of our models in problems that involve other modalities such as structured data [19] and user-annotated data [7] and other applications beside emotion classification. Acknowledgement. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan X GPU used for this research. We would also like to thank the IT support team for their help."
        }
    ],
    "title": "Multimodal Classification for Analysing Social Media",
    "year": 2017
}