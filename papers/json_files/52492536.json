{
    "abstractText": "Gatys et al. (2015) showed that optimizing pixels to match features in a convolutional network is a way to render images of high visual quality. Unrolling this gradient-based optimization can be thought of as a recurrent computation, that creates images by incrementally adding onto a visual \u201ccanvas\u201d. Inspired by this view we propose a recurrent generative model that can be trained using adversarial training. In order to quantitatively compare adversarial networks we also propose a new performance measure, that is based on letting the generator and discriminator of two models compete against each other.",
    "authors": [
        {
            "affiliations": [],
            "name": "Daniel Jiwoong Im"
        },
        {
            "affiliations": [],
            "name": "Chris Dongjoo Kim"
        }
    ],
    "id": "SP:47790bc4468b07a72f1666b7e14f2b237a105bfa",
    "references": [
        {
            "authors": [
                "Emily Denton",
                "Soumith Chintala",
                "Arthur Szlam",
                "Rob Fergus"
            ],
            "title": "Deep generative image models using a laplacian pyramid of adversarial networks",
            "venue": "In Proceedings of the Neural Information Processing Systems (NIPS),",
            "year": 2015
        },
        {
            "authors": [
                "Leon A Gatys",
                "Alexander S Ecker",
                "Matthias Bethge"
            ],
            "title": "A neural algorithm of artistic style",
            "venue": "arXiv preprint arXiv:1508.06576,",
            "year": 2015
        },
        {
            "authors": [
                "Jon Gauthier"
            ],
            "title": "Conditional generative adversarial nets for convolutional face generation. In Class Project for Stanford CS231N: Convolutional Neural Networks for Visual Recognition, Winter semester",
            "year": 2014
        },
        {
            "authors": [
                "Ian J. Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "In Proceedings of the Neural Information Processing Systems (NIPS),",
            "year": 2014
        },
        {
            "authors": [
                "Karol Gregor",
                "Ivo Danihelka",
                "Alex Graves",
                "Danilo Jimenez Rezende",
                "Daan Wierstra"
            ],
            "title": "Draw: A recurrent neural network for image generation",
            "venue": "In Proceedings of the International Conference on Machine Learning (ICML),",
            "year": 2015
        },
        {
            "authors": [
                "Geoffrey E. Hinton",
                "Simon Osindero",
                "Yee-Whye Teh"
            ],
            "title": "A fast learning algorithm for deep belief nets",
            "venue": "Neural Computation,",
            "year": 2006
        },
        {
            "authors": [
                "Aapo Hyvrinen",
                "Jarmo Hurri",
                "Patrick O. Hoyer"
            ],
            "title": "Natural Image Statistics: A Probabilistic Approach to Early Computational Vision",
            "venue": "Springer Publishing Company, Incorporated,",
            "year": 2009
        },
        {
            "authors": [
                "Daniel Jiwoong Im",
                "Sungjin Ahn",
                "Roland Memisevic",
                "Yoshua Bengio"
            ],
            "title": "Denoising criterion for variational auto-encoding framework",
            "venue": "In http://arxiv.org/abs/1511.06406,",
            "year": 2015
        },
        {
            "authors": [
                "Sergey Ioffe",
                "Christian Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "In http://arxiv.org/pdf/1502.03167.pdf,",
            "year": 2015
        },
        {
            "authors": [
                "Diederik Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),",
            "year": 2014
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding varational bayes",
            "venue": "In Proceedings of the Neural Information Processing Systems (NIPS),",
            "year": 2014
        },
        {
            "authors": [
                "Andrew L Maas",
                "Awni Y Hannun",
                "Andrew Y. Ng"
            ],
            "title": "Rectifier nonlinearities improve neural network acoustic models",
            "venue": "In Proceedings of the International Conference on Machine Learning (ICML),",
            "year": 2013
        },
        {
            "authors": [
                "Mehdi Mirza",
                "Simon Osindero"
            ],
            "title": "Conditional generative adversarial nets",
            "venue": "In Proceedings of the Neural Information Processing Systems Deep learning Workshop(NIPS),",
            "year": 2014
        },
        {
            "authors": [
                "John Nash"
            ],
            "title": "Non-cooperative games",
            "venue": "The Annals of Mathematics,",
            "year": 1951
        },
        {
            "authors": [
                "Alec Radford",
                "Luke Metz",
                "Soumith Chintala"
            ],
            "title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
            "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),",
            "year": 2015
        },
        {
            "authors": [
                "Marc\u2019Aurelio Ranzato",
                "Volodymyr Mnih",
                "Joshua M Susskind",
                "Geoffrey E Hinton"
            ],
            "title": "Modeling natural images using gated mrfs",
            "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
            "year": 2013
        },
        {
            "authors": [
                "Jost Tobias Springenberg",
                "Alexey Dosovitskiy",
                "Thomas Brox",
                "Martin. Riedmiller"
            ],
            "title": "Striving for simplicity: The all convolutional net",
            "venue": "In http://arxiv.org/abs/1412.6806,",
            "year": 2014
        },
        {
            "authors": [
                "Fisher Yu",
                "Yinda Zhang",
                "Shuran Song",
                "Ari Seff",
                "Jianxiong Xiao"
            ],
            "title": "Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop",
            "year": 2015
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Generating realistic-looking images has been a long-standing goal in machine learning. The early motivation for generating images was mainly as a diagnostic tool, based on the belief that a good generative model can count as evidence for the degree of \u201cunderstanding\u201d that a model has of the visual world (see, example, [6], [7], or [16] and references in these). More recently, due to immense quality improvements over the last two years (for example, [5, 1, 15, 2]), and the successes of discriminative modeling overall, image generation has become a goal on its own, with industrial applications within close reach.\nCurrently, most common image generation models can be roughly categorized into two classes: The first is based on probabilistic generative models, such as the variational autoencoder [11] and a variety of equivalent models introduced at the same time. The idea in these models is to train an autoencoder whose latent representation satisfies certain distributional properties, which makes it easy to sample from the hidden variables, as well as from the data distribution (by plugging samples into the decoder).\nThe second class of generative models is based on adversarial sampling [4]. This approach forgoes the need to encourage a particular latent distribution (and, in fact, the use of an encoder altogether), by training a simple feed-forward neural network to generate \u201cdata-like\u201d examples. \u201cData-likeness\u201d is judged by a simultaneously trained, but otherwise separate, discriminator neural network.\nFor both types of approach, sequential variants were introduced recently, which were shown to work much better in terms of visual quality: The DRAW network [5], for example, is a sequential version of the variational autoencoder, where images are generated by accumulating updates into a canvas using a recurrent network. An example of a sequential adversarial network is the LAPGAN model\nar X\niv :1\n60 2.\n05 11\n0v 4\n[ cs\n.L G\n] 2\n[1], which generates images in a coarse-to-fine fashion, by generating and upsampling in multiple steps.\nMotivated by the successes of sequential generation, in this paper, we propose a new image generation model based on a recurrent network. Similar to [1], our model generates an image in a sequence of structurally identical steps, but in contrast to that work we do not impose a coarse-to-fine (or any other) structure on the generation procedure. Instead we let the recurrent network learn the optimal procedure by itself. In contrast to [5], we obtain very good samples without resorting to an attention mechanism and without variational training criteria (such as a KL-penalty on the hiddens).\nOur model is mainly inspired by a third type of image generation method proposed recently by [2]. In this work, the goal is to change the texture (or \u201cstyle\u201d) of a given reference image by generating a new image that matches image features and texture features within the layers of a pretrained convolutional network. As shown by [2], ignoring the style-cost in this approach and only matching image features, it is possible to render images which are similar to the reference image. As we shall show, unrolling the gradient descent based optimization that generates the target image yields a recurrent computation, in which an \u201cencoder\u201d convolutional network extracts images of the current \u201ccanvas\u201d. The resulting code and the code for the reference image get fed into a \u201cdecoder\u201d which decides on an update to the \u201ccanvas\u201d.\nThis view, along with the successes of trained sequential generation networks, suggests that an iterative convolutional network that is trained to accumulate updates onto a visual canvas should be good at generating images in general, not just those shown as reference images. We show in this paper that this indeed is the case.\nTo evaluate and compare the relative performance of adversarial generative models quantitatively, we also introduce a new evaluation scheme based on a \u201ccross-over\u201d battle between the discriminators and generators of the two models."
        },
        {
            "heading": "2 Background",
            "text": "Generative Adversarial Networks (GAN) are built upon the concept of a non-cooperative game [14], that two networks are trained to play against each other. The two networks are a generative and a discriminative model, G and D. The generative model generates samples that are hard for the discriminator D to distinguish from real data. At the same time, the discriminator tries to avoid getting fooled by the generative model G.\nFormally, the discriminative model is a classifier D : RM \u2192 {0, 1} that tries to determine whether a given point x \u2208 RM is real or generated data. The generative model G : RK \u2192 RM generates samples x \u2208 RM that are similar to the data by mapping a sample z \u2208 RK drawn randomly from some prior distribution p(z) to the data space. These models can be trained by playing a minmax game as follows:\nmin \u03b8G max \u03b8D V (D,G) = min G max D\n[ Ex\u223cpD [ logD(x) ] + Ez\u223cpG [ log ( 1\u2212D(G(z)) )]] . (1)\nwhere \u03b8G and \u03b8D are the parameters of discriminator and generator, respectively. In practice, the second term in Equation 1 is troublesome due to the saturation of log ( 1\u2212D(G(z)) ) . This makes insufficient gradient flow through the generative model G as the magnitude of gradients get smaller and prevent them from learning. To remedy the vanishing gradient problem, the objective function in Equation 1 is reformulated into two separate objectives:\nmax \u03b8D\nEx\u223cpD [ logD(x) ] + Ez\u223cpG [ log ( 1\u2212D(G(z)) )] + max\n\u03b8G Ez\u223cpG\n[ logD ( G(z) )] . (2)\nAlthough Equation 2 is not the same as Equation 1, the underlying intuition is the same. Moreover, the gradient of generators for the two different objectives are always pointing in the same direction and the two objectives have the same fixed points.\nThe generating and discriminating procedure are simple. We consider a Gaussion prior distribution with zero-mean and unit variance. Then, the process of generating an output is simply to pass a sample z \u223c N (\u00b5 = 0,\u03c3 = 1) to the generative model to obtain the sample x \u223c G(z; \u03b8G).\nNote that the generative model G can be a deterministic or a probabilistic model. However, only deterministic models have been deployed in the past, so that x = G(z; \u03b8G). Subsequently, the sample can be passed on to the discriminator to predict D(x; \u03b8D).\nAfter computing the cost in Equation 2, the model parameters can be updated through backpropagation. Due to the two different min-max operators in Equation 2, the update rule is defined as follows:\n{\u03b8\u2032D, \u03b8\u2032G} \u2190  Update \u03b8D if D(x) predicts wrong Update \u03b8D if D(G(z)) predicts wrong Update \u03b8G if D(G(z)) predicts correct\nIdeally, we would like the generative model to learn a distribution such that pG = pD. This requires the generative model to be capable of transforming a simple prior distribution p(z) to more complex distributions. In general, deep neural networks are good candidates as they are capable of modeling complicated functions and they were shown to be effective in previous works [4, 13, 3, 1].\nRecently, [15] showed excellent samples of realistic images using a fully convolutional neural network as the discriminative model and fully deconvolutional neural network [19] as the generative model. The lth convolutional layer in the discriminative network takes the form\nhk (l)\nj = f \u2211 j\u2208Mk hl\u22121j \u2217W k(l) + bk (l) j  , (3) and the lth convolutional transpose layer 1 in the generative network takes the form\ngc (l)\nj = f \u2211 j\u2208Mc gl\u22121j ? W c(l) + bc (l) j  . (4) In these equations, \u2217 is the convolution operator, ? is the convolutional transpose operator, Mj is the selection of inputs from the previous layer (\u201cinput maps\u201d), f is an activation function, and {W k(l) , bk(l)j } and {W c (l) , bc (l)\nj } are the parameters of the discriminator and generator at layer l. The detailed explanation of convolutional transpose is explained in the supplementary materials."
        },
        {
            "heading": "3 Model",
            "text": "We propose sequential modeling using GANs on images. Before introducing our proposed methods, we discuss some of the motivations for our approach. One interesting aspect of models such as the Deep Recurrent Attentive Writer (DRAW) [5] and the Laplacian Generative Adversarial Networks\n1It is more proper to say \u201cconvolutional transpose operation\u201d rather than \u201cdeconvolutional\u201d operation. Hence, we will be using the term \u201cconvolutional transpose\u201d from now on.\n(LAPGAN) [1] is that they generate image samples in a sequential process, rather than generating them in one shot. Both were shown to outperform their ancestor models, which are the variational auto-encoder and GAN, respectively. The obvious advantage of such sequential models is that repeatedly generating outputs conditioned on previous states simplifies the problem of modeling complicated data distributions by mapping them to a sequence of simpler problems.\nThere is a close relationship between sequential generation and Backpropgating to the Input (BI). BI is a well-known technique where the goal is to obtain a neural network input that minimizes a given objective function derived from the network. For example, [2] recently introduced a model for stylistic rendering by optimizing the input image to simultaneously match higher-layer features of a reference content image and a non-linear, texture-sensitive function of the same features of a reference style image. They also showed that in the absence of the style-cost, this optimization yields a rendering of the content image (in a quality that depends on the chosen feature layer).\nInterestingly, rendering by feature matching in this way is itself closely related to DRAW: optimizing a matching cost with respect to the input pixels with backprop amounts to first extracting the current image features fx at the chosen layer using a forward path through the network (up to that layer). Computing the gradient of the feature reconstruction error then amounts to back-propogating the difference fx\u2212fI back to the pixels. This is equivalent to traversing a \u201cdecoder\u201d network, defined as the linearized, inverse network that computes the backward pass. The negative of this derivative is then added into the current version, x, of the generated image. We can thus think of image x as a buffer or \u201ccanvas\u201d onto which updates are accumulated sequentially (see the left of Figure 2). Like in the\nDRAW model, where the updates are computed using a (forward) pass through an encoder network, followed by a (backward) pass through a decoder network. This approach is almost identical to the DRAW network, except for two subtle differences (see, [5]): (i) in DRAW, the difference between the current image and the image to be rendered is used in the forward pass, whereas here this difference is computed in the feature space (after encoding); (ii) DRAW uses a learned, attention-based decoder and encoder rather than (fixed) convolutional network. (see the right of Figure 2). We elaborate on the relationship between the two methods in the supplementary material.\nIn this work, we explore a generative recurrent adversarial network as an intermediate between DRAW and gradient-based optimization based on a generative adversarial objective function.\nD1 G1\nTraining Phase M1\nD2 G2 M2\nFigure 4: Training Phase of Generative Adversarial Networks.\nD1 G2\nTest Phase (a.k.a Battle Phase) M1 versus M2\nD2 G1\nFigure 5: Training Phase and Test Phase of Generative Adversarial Networks."
        },
        {
            "heading": "3.1 Generative Recurrent Adversarial Networks",
            "text": "We propose Generative Recurrent Adversarial Networks (GRAN), whose underlying structure is similar to other GANs. The main difference between GRAN versus other generative adversarial models is that the generator G consists of a recurrent feedback loop that takes a sequence of noise samples drawn from the prior distribution z \u223c p(z) and draws an ouput at multiple time steps \u2206C1,\u2206C2, \u00b7 \u00b7 \u00b7 ,\u2206CT . Accumulating the updates at each time step yields the final sample drawn to the canvas C. Figure 2 delineates the high-level abstraction of GRAN. At each time step, t, a sample z from the prior distribution is passed to a function f(\u00b7) along with the hidden states hc,t. Where hc,t represent the hidden state, or in other words, a current encoded status of the previous drawing \u2206Ct\u22121. Here, \u2206Ct represents the output of the function f(\u00b7). (see Supp. Figure 11.) Henceforth, the function g(\u00b7) can be seen as a way to mimic the inverse of the function f(\u00b7). Ultimately, the function f(\u00b7) acts as a decoder that receives the input from the previous hidden state hc,t and noise sample z, and function g(\u00b7) acts as an encoder that provides a hidden representation of the output \u2206Ct\u22121 for time step t. One interesting aspect of GRAN is that the procedure of GRAN starts with a decoder instead of an encoder. This is in contrast to most auto-encoder like models such as VAE or DRAW, which start by encoding an image (see Figure 2).\nIn the following, we describe the procedure in more detail. We have an initial hidden state hc,0 that is set as a zero vector in the beginning. We then compute the following for each time step t = 1 . . . T :\nzt \u223c p(Z) (5) hc,t = g(\u2206Ct\u22121) (6) hz,t = tanh(Wzt + b). (7) \u2206Ct = f([hz,t,hc,t]), (8)\nwhere [hz,t,hc,t] denotes the concatenation of hz,t and hc,t 2. Finally, we sum the generated images and apply the logistic function in order to scale the final output to be in (0, 1):\nC = \u03c3( T\u2211 t=1 \u2206Ct). (9)\nThe reason for using tanh(\u00b7) in Equation 7 is to rescale z to (\u22121, 1). Hence, rescaling it to the same (bounded) domain as hc,t.\nIn general, one can declare the functions f(\u00b7) and g(\u00b7) to be any type of model. We used a variant of DCGAN [15] in our experiments. Supp. Figure 11 demonstrates the architecture of GRAN at time\n2Note that we explore two scenarios of sampling z in the experiments. The first scenario is where z is sampled once in the beginning, then hz,t = hz as a consequence. In whe other scenario, z is sampled at every time step.\nstep t. The function f(\u00b7) starts with one fully connected layer at the bottom and a deconvolutional layers with fractional-stride convolution at rest of the upper layers. This makes the images gradually upscale as we move up to higher layers. Conversely, the function g(\u00b7) starts from convolutional layers and the fully connected layer at the top. The two functions, f(\u00b7) and g(\u00b7), are symmetric copies of one another, as shown in Figure 2. The overall network is trained via backpropagation through the time."
        },
        {
            "heading": "4 Model Evaluation: Battle between GANs",
            "text": "A problem with generative adversarial models is that there is no obvious way to evaluate them quantitatively. In the past, [4] evaluated GANs by looking at nearest-neighbours in the training data. LAPGAN was evaluated in the same way, and in addition using human inspections [1]. For these, volunteers were asked to judge whether given images are drawn from the dataset or generated by LAPGAN. In that case, the human acts as the discriminator, while the generator is a trained GAN. The problem with this approach is that human inspectors can be subjective to high variance, which makes it necessary to average over a large number of these, and the experimental setup is expensive and cumbersome. A third evaluation scheme, used recently by [15] is based on classification performance. However, this approach is rather indirect and relies heavily on the choice of classifier. For example, in the work by Radford et al, they used nearest neighbor classifiers, which suffers from the problem that Euclidean distance is not a good dissimilarity measure for images.\nHere, we propose an alternative way to evaluate generative adversarial models. Our approach is to directly compare two generative adversarial models by having them engage in a \u201cbattle\u201d against each other. The naive intuition is that, since every generative adversarial models consists of a discriminator and a generator in pairs, we can exchange the pairs and have the models play the generative adversarial game against each other. Figure 5 illustrates this approach3.\nThe training and test stages are as follows. Consider two generative adversarial models,M1 andM2. Each model consists of a generator and a discriminator, M1 = {(G1, D1)} and M2 = {(G2, D2)}. In the training phase, G1 competes with D1 in order to be trained for the battle in the test phase. Likewise for G2 and D2. In the test phase, model M1 plays against model M2 by having G1 try to fool D2 and vice-versa.\nFigure 6: Model Comparison Metric for GANs\nM1 M2 M1 D1(G1(z)) , D1(xtrain) D1(G2(z)) , D1(xtest) M2 D2(G1(z)) , D2(xtest) D2(G2(z)) , D2(xtrain)\nAccordingly, we end up with the combinations shown in Table 6. Each entry in the table contains two scores, one from discriminating training or test\ndata points, and the other from discriminating generated samples. At test time, we can look at the following ratios between the discriminative scores of the two models:\nrtest def = ( D1(xtest) ) ( D2(xtest) ) and rsample def= (D1(G2(z))) ( D2(G1(z))\n) , (10) where (\u00b7) is the classification error rate, and xtest is the predefined test set. These ratios allow us to compare the model performances.\nThe test ratio, rtest, tells us which model generalizes better since it is based on discriminating the test data. Note that when the discriminator is overfitted to the training data, the generator will also be affected by this. This will increase the chance of producing biased samples towards the training data.\nThe sample ratio, rsample, tells us which model can fool the other model more easily, since the discriminators are classifying over the samples generated by their opponents. Strictly speaking, as our goal is to generate good samples, the sample ratio determines which model is better at generating good (\u201cdata like\u201d) samples.\nWe suggest using the sample ratio to determine the winning model, and to use the test ratio to determine the validity of the outcome. The reason for using the latter is due to the occasional possibility of the sample ratio being biased, in which case the battle is not completely fair when\n3A larger figure is shown in the supplementary materials.\nthe winner is solely determined by the sample ratio. It is possible that one of the discriminators is biased towards the training data more so than the other (i.e. overfitted on the training data). In order to address this issue, our proposed evaluation metric qualifies the sample ratio to be judged by the test ratio as follows:\nwinner =  M1 if rsample < 1 and rtest ' 1 M2 if rsample > 1 and rtest ' 1 Tie otherwise\n(11)\nThis imposes a condition where rtest ' 1, which assures that none of the discriminator is overfitted more than the other. If rtest 6= 1, then this implies that rsample is biased, and thus, the sample ratio is no longer applicable.\nWe call this evaluation measure Generative Adversarial Metric (GAM). GAM is not only able to compare generative adversarial models against each other, but also able to partially compare other models, such as the VAE or DRAW. This is done by observing the error rate of GRAN\u2019s discriminators based on the samples of the other generative model as an evaluation criterion. For example, in our experiments we report the error rates of the GRAN\u2019s discriminators with the samples of other generative models, i.e. err(D(z)) where z are the samples of other generative models and D(\u00b7) is the discriminator of GRAN."
        },
        {
            "heading": "5 Experiments",
            "text": "In order to evaluate whether the extension of sequential generation enhances the perfomance, we assessed both quantitatively and qualitatively under three different image datasets. We conducted several empirical studies on GRAN under the model selection metrics discussed in Section 4. See Supplementary Materials for full experimental details.\nThe performance of GRAN is presented in Table 1. We focused on comparing GRANs with 1, 3 and 5 time steps. For all three datasets, GRAN3 and GRAN5 outperformed GRAN1 as shown in\nTable 1. Moreover, we present samples from GRAN for MNIST, cifar10 and LSUN in Figure 7,\nFigure 8, and Supp. Figure 12. Most of the MNIST and cifar10 samples shown in Supp. Figure 12 and Figure 7 appear to be discernible and reasonably classifiable by humans. Additionally, the LSUN samples from Figure 8 seem to cover variety of church buildings and contain fine detailed textures. The \u201cimage statistics\u201d of two real image datasets are embedded into both types of sample.\nIn the following, we analyze the results by answering a set of questions on our experiments."
        },
        {
            "heading": "Q: How do GRAN and other GAN type of models perform compared to non generative adversarial models?",
            "text": "We compared our model to other generative models such as denoising VAE (DVAE) [8] and DRAW on the MNIST dataset. Although this may not be the best way to assess the two models, since the generator of GRAN is not used, Table 2 presents the results of applying GAM as described at the end of Section 4. The error rates were all below 50%, and especially low for DVAE\u2019s samples. Surprisingly, even though samples from DRAW look very nice, the error rate on their samples were also quite low with GRAN3. This illustrates that the discriminators of generative adversarial models are good at discriminating the samples generated by DVAE and DRAW. Our hypothesis is that the samples look nicer due to the smoothing effect of having a mean squared error in their objective, but they do not capture all relevant aspects of the statistics of real handwritten images."
        },
        {
            "heading": "Q: Does GRAN overfit to the training data?",
            "text": "Since it is infeasible to naively examine the training data for similar looking images as GRAN\u2019s output, it is common (albeit somewhat questionable) to look at k-nearest neighbors to do a basic sanity check. As shown in Supp. Figure 13, Supp. Figure 16, and Supp. Figure 17 and 18, one does not find any replicates of training data cases.\nEmpirically speaking, we did notice that GRAN tends to generate samples by interpolating between the training data. For example, Supp. Figure 11 illustrates that the church buildings consist of similar structure of the entrance but the overall structure of the church has a different shape. Based on such examples, we hypothesize that the overfitting for GRAN in the worst case may imply that the model learns to interpolate sensibly between training examples. This is not the typical way of the term overfitting is used for generative models, which usually refers to memorizing the data. In fact, in adversarial training in general, the objective function is not based on mean squared error of the pixels which makes it not obvious how to memorize the training samples. However, this could mean that it is difficult for these models to generate images that are interpolated from training data.\nFigure 11: Drawing at different time steps on cifar10 samples.\nFigure 12: Drawing at different time steps on lsun samples.\nFigure 13: Cifar10 samples generated by GRAN with injecting different noises at every time step\nFigure 14: LSUN samples generated by GRAN with injecting different noises at every time step"
        },
        {
            "heading": "Q: What do the samples look during the intermediate time steps?",
            "text": "Supp. Figure 14, Supp. Figure 21, and Supp. Figure 22 present the intermediate samples when the total number of steps is 3. From the figures, we can observe the gradual development of the samples over time. The common observation from the intermediate samples is that images become more fine-grained and introduce details missing from the previous time step image. Intermediate samples for models with a total number of time steps of 5 can be found in the supplementary materials as well. This behaviour is somewhat similar to [1], as one might expect (although filling-in of color details suggest that the process is more complex than a simple coarse-to-fine generation). Note that this behaviour is not enforced in our case, since we use an identical architecture at every time step.\nQ: What happens when we use a different noises for each step?\nWe sampled a noise vector z \u223c p(Z) and used the same noise for every time step. This is because z acts as a reference frame in [2] as shown in Figure 2. On the other hand, the role of the sample in DRAW is to inject noise at each step, z1, z2, \u00b7 \u00b7 \u00b7 zT ,\u223c p(Z), as prescribed by the variational autoencoding framework. We also experimented with both sampling z once in the beginning versus sampling zi at each time step. Here we describe the advantages and disadvantages to these two approaches.\nThe samples of cifar10 and LSUN generated by injecting different noises at each time step are shown in Supp. Figure 19 and Supp. Figure 20. Figure 7 and Figure 8 are the output samples when injected using the same noise. The samples appear to be discernible and reasonably classifiable by humans as well. However, we observe a few samples that look very alike to one other. During the experiments, we found that when using different noise, it requires more effort to find a set of hyper-parameters that produce good samples. Furthermore, the samples tend to collapse when training for a long time. Hence, we had to carefully select the total number of iterations. This illustrates that the training became much more difficult and it provokes GRAN to \u201ccheat\u201d by putting a lot of probability mass on samples that the discriminator cannot classify, which produce samples that look very similar to each other.\nOn the other hand, when we look at the intermmediate time steps of samples generated using multiple noises, we find that there are more pronounced changes within each time step as demonstrated in Supp. Figure 19 and Supp. Figure 20. For example, the colour of the train in Supp. Figure 19 changes, and a partial church is drawn in Supp. Figure 20."
        },
        {
            "heading": "6 Conclusion",
            "text": "We proposed a new generative model based on adversarial training of a recurrent neural network inspired by [2]. We showed the conditions under which the model performs well and also showed that it can produce higher quality visual samples than an equivalent single-step model. We also introduced a new metric for comparing adversarial networks quantitatively and presented that the recurrent generative model yields a superior performance over existing state-of-the-art generative models under this metric."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank the members of the LISA Lab at Montreal, in particular Mohammed Pezeshki and Donghyun Lee, for helpful discussions.\nThis research was developed with funding from the Defense Advanced Research Projects Agency (DARPA). The views, opinions, and/or findings expressed are those of the author and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government."
        }
    ],
    "title": "Generating images with recurrent adversarial networks",
    "year": 2022
}