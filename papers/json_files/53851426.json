{
    "abstractText": "An accurate model of a patient\u2019s individual survival distribution can help determine the appropriate treatment for terminal patients. Unfortunately, risk scores (e.g., from Cox Proportional Hazard models) do not provide survival probabilities, single-time probability models (e.g., the Gail model, predicting 5 year probability) only provide for a single time point, and standard Kaplan-Meier survival curves provide only population averages for a large class of patients meaning they are not specific to individual patients. This motivates an alternative class of tools that can learn a model which provides an individual survival distribution which gives survival probabilities across all times \u2013 such as extensions to the Cox model, Accelerated Failure Time, an extension to Random Survival Forests, and Multi-Task Logistic Regression. This paper first motivates such \u201cindividual survival distribution\u201d (isd) models, and explains how they differ from standard models. It then discusses ways to evaluate such models \u2013 namely Concordance, 1-Calibration, Brier score, and various versions of L1-loss\u2013 and then motivates and defines a novel approach \u201cD-Calibration\u201d, which determines whether a model\u2019s probability estimates are meaningful. We also discuss how these measures differ, and use them to evaluate several isd prediction tools, over a range of survival datasets.",
    "authors": [
        {
            "affiliations": [],
            "name": "Humza Haider"
        },
        {
            "affiliations": [],
            "name": "Bret Hoehn"
        },
        {
            "affiliations": [],
            "name": "Sarah Davis"
        },
        {
            "affiliations": [],
            "name": "Russell Greiner"
        }
    ],
    "id": "SP:b54a8d46da3ddd2d5902986345ef0805fa27d69e",
    "references": [
        {
            "authors": [
                "O. Aalen",
                "O. Borgan",
                "H. Gjessing"
            ],
            "title": "Survival and event history analysis: a process point of view",
            "venue": "Springer Science & Business Media,",
            "year": 2008
        },
        {
            "authors": [
                "F. Anderson",
                "G.M. Downing",
                "J. Hill",
                "L. Casorso",
                "N. Lerch"
            ],
            "title": "Palliative performance scale (pps): a new tool",
            "venue": "Journal of palliative care,",
            "year": 1995
        },
        {
            "authors": [
                "A. Andres",
                "A. Montano-Loza",
                "R. Greiner",
                "M. Uhlich",
                "P. Jin",
                "B. Hoehn",
                "D. Bigam",
                "J.A.M. Shapiro",
                "N.M. Kneteman"
            ],
            "title": "A novel learning algorithm to predict individual survival after liver transplantation for primary sclerosing cholangitis",
            "venue": "PloS one,",
            "year": 2018
        },
        {
            "authors": [
                "J.E. Angus"
            ],
            "title": "The probability integral transform and related results",
            "venue": "SIAM review,",
            "year": 1994
        },
        {
            "authors": [
                "N. Breslow",
                "J. Crowley"
            ],
            "title": "A large sample study of the life table and product limit estimates under random censorship",
            "venue": "The Annals of Statistics,",
            "year": 1974
        },
        {
            "authors": [
                "G.W. Brier",
                "R.A. Allen"
            ],
            "title": "Verification of weather forecasts",
            "venue": "In Compendium of meteorology,",
            "year": 1951
        },
        {
            "authors": [
                "R.-B. Chuang",
                "W.-Y. Hu",
                "T.-Y. Chiu",
                "C.-Y. Chen"
            ],
            "title": "Prediction of survival in terminal cancer patients in taiwan: constructing a prognostic scale",
            "venue": "Journal of pain and symptom management,",
            "year": 2004
        },
        {
            "authors": [
                "G.A. Colditz",
                "B. Rosner"
            ],
            "title": "Cumulative risk of breast cancer to age 70 years according to risk factor status: data from the nurses\u2019 health study",
            "venue": "American journal of epidemiology,",
            "year": 2000
        },
        {
            "authors": [
                "J.P. Costantino",
                "M.H. Gail",
                "D. Pee",
                "S. Anderson",
                "C.K. Redmond",
                "J. Benichou",
                "H.S. Wieand"
            ],
            "title": "Validation studies for models projecting the risk of invasive and total breast cancer incidence",
            "venue": "Journal of the National Cancer Institute,",
            "year": 1999
        },
        {
            "authors": [
                "D. Cox"
            ],
            "title": "Regression models and life-tables",
            "venue": "Journal of the Royal Statistical Society. Series B (Methodological),",
            "year": 1972
        },
        {
            "authors": [
                "S. Cs\u00f6rg\u0150",
                "L. Horv\u00e1th"
            ],
            "title": "The rate of strong uniform consistency for the product-limit estimator",
            "venue": "Zeitschrift fu\u0308r Wahrscheinlichkeitstheorie und verwandte Gebiete,",
            "year": 1983
        },
        {
            "authors": [
                "R. d\u2019Agostino",
                "B.-H. Nam"
            ],
            "title": "Evaluation of the performance of survival analysis models: discrimination and calibration measures",
            "venue": "Handbook of statistics,",
            "year": 2003
        },
        {
            "authors": [
                "M. DeGroot",
                "S. Fienberg"
            ],
            "title": "The comparison and evaluation of forecasters",
            "venue": "Journal of the Royal Statistical Society. Series D (The Statistician),",
            "year": 1983
        },
        {
            "authors": [
                "L.D. Fisher",
                "D.Y. Lin"
            ],
            "title": "Time-dependent covariates in the cox proportional-hazards regression model",
            "venue": "Annual review of public health,",
            "year": 1999
        },
        {
            "authors": [
                "T.A. Gerds",
                "T. Cai",
                "M. Schumacher"
            ],
            "title": "The performance of risk prediction models",
            "venue": "Biometrical Journal: Journal of Mathematical Methods in Biosciences,",
            "year": 2008
        },
        {
            "authors": [
                "T.A. Gerds",
                "M. Schumacher"
            ],
            "title": "Consistent estimation of the expected brier score in general survival models with right-censored event times",
            "venue": "Biometrical Journal,",
            "year": 2006
        },
        {
            "authors": [
                "E. Graf",
                "C. Schmoor",
                "W. Sauerbrei",
                "M. Schumacher"
            ],
            "title": "Assessment and comparison of prognostic classification schemes for survival data",
            "venue": "Statistics in medicine,",
            "year": 1999
        },
        {
            "authors": [
                "D. Guffey"
            ],
            "title": "Hosmer-Lemeshow goodness-of-fit test: Translations to the Cox Proportional Hazards Model",
            "venue": "PhD thesis,",
            "year": 2013
        },
        {
            "authors": [
                "R.C. Gupta",
                "D.M. Bradley"
            ],
            "title": "On representing the mean residual life in terms of the failure rate",
            "venue": "arXiv preprint math/0411297,",
            "year": 2004
        },
        {
            "authors": [
                "B. Gwilliam",
                "V. Keeley",
                "C. Todd",
                "C. Roberts",
                "M. Gittins",
                "L. Kelly",
                "S. Barclay",
                "P. Stone"
            ],
            "title": "Prognosticating in patients with advanced cancer \u2013 observational study comparing the accuracy of clinicians\u2019 and patients\u2019 estimates of survival",
            "venue": "Annals of oncology,",
            "year": 2012
        },
        {
            "authors": [
                "D. Harrington"
            ],
            "title": "Linear rank tests in survival analysis",
            "venue": "Encyclopedia of Biostatistics,",
            "year": 2005
        },
        {
            "authors": [
                "J. Haybittle",
                "R. Blamey",
                "C. Elston",
                "J. Johnson",
                "P. Doyle",
                "F. Campbell",
                "R. Nicholson",
                "K. Griffiths"
            ],
            "title": "A prognostic index in primary breast cancer",
            "venue": "British journal of cancer,",
            "year": 1982
        },
        {
            "authors": [
                "P.J. Heagerty",
                "Y. Zheng"
            ],
            "title": "Survival model predictive accuracy and roc curves",
            "year": 2005
        },
        {
            "authors": [
                "D.W. Hosmer",
                "S. Lemesbow"
            ],
            "title": "Goodness of fit tests for the multiple logistic regression model",
            "venue": "Communications in statistics-Theory and Methods,",
            "year": 1980
        },
        {
            "authors": [
                "D.W. Hosmer",
                "S. Lemeshow",
                "S. May"
            ],
            "title": "Applied survival analysis",
            "year": 2011
        },
        {
            "authors": [
                "H. Ishwaran",
                "U. Kogalur"
            ],
            "title": "Random Forests for Survival, Regression, and Classification (RF-SRC), 2018",
            "venue": "R package version",
            "year": 2018
        },
        {
            "authors": [
                "H. Ishwaran",
                "U.B. Kogalur",
                "E.H. Blackstone",
                "M.S. Lauer"
            ],
            "title": "Random survival forests",
            "venue": "The Annals of Applied Statistics,",
            "year": 2008
        },
        {
            "authors": [
                "J. Kalbfleisch",
                "R. Prentice"
            ],
            "title": "The statistical analysis of failure time data",
            "venue": "Wiley New York:,",
            "year": 2002
        },
        {
            "authors": [
                "P.S. Kamath",
                "W.R. Kim"
            ],
            "title": "The model for end-stage liver disease",
            "venue": "(meld). Hepatology,",
            "year": 2007
        },
        {
            "authors": [
                "E. Kaplan",
                "P. Meier"
            ],
            "title": "Nonparametric estimation from incomplete observations",
            "venue": "Journal of the American Statistical Association,",
            "year": 1958
        },
        {
            "authors": [
                "J. Katzman",
                "U. Shaham",
                "J. Bates",
                "A. Cloninger",
                "T. Jiang",
                "Y. Kluger"
            ],
            "title": "Deepsurv: Personalized treatment recommender system using a cox proportional hazards deep neural network",
            "venue": "arXiv preprint arXiv:1606.00931,",
            "year": 2016
        },
        {
            "authors": [
                "Y. Li",
                "J. Wang",
                "J. Ye",
                "C.K. Reddy"
            ],
            "title": "A multi-task learning formulation for survival analysis",
            "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
            "year": 2016
        },
        {
            "authors": [
                "M. Luck",
                "T. Sylvain",
                "H. Cardinal",
                "A. Lodi",
                "Y. Bengio"
            ],
            "title": "Deep learning for patient-specific kidney graft survival analysis",
            "venue": "arXiv preprint arXiv:1705.10245,",
            "year": 2017
        },
        {
            "authors": [
                "T. Morita",
                "J. Tsunoda",
                "S. Inoue",
                "S. Chihara"
            ],
            "title": "The palliative prognostic index: a scoring system for survival prediction of terminally ill cancer patients",
            "venue": "Supportive Care in Cancer,",
            "year": 1999
        },
        {
            "authors": [
                "A.H. Murphy"
            ],
            "title": "Scalar and vector partitions of the probability score: Part i. two-state situation",
            "venue": "Journal of Applied Meteorology,",
            "year": 1972
        },
        {
            "authors": [
                "A.H. Murphy"
            ],
            "title": "A new vector partition of the probability score",
            "venue": "Journal of applied Meteorology,",
            "year": 1973
        },
        {
            "authors": [
                "M. Pirovano",
                "M. Maltoni",
                "O. Nanni",
                "M. Marinari",
                "M. Indelli",
                "G. Zaninetta",
                "V. Petrella",
                "S. Barni",
                "E. Zecca",
                "E. Scarpi"
            ],
            "title": "A new palliative prognostic score: a first step for the staging of terminally ill cancer patients",
            "venue": "Journal of pain and symptom management,",
            "year": 1999
        },
        {
            "authors": [
                "R. Ranganath",
                "A. Perotte",
                "N. Elhadad",
                "D. Blei"
            ],
            "title": "Deep survival analysis",
            "venue": "arXiv preprint arXiv:1608.02158,",
            "year": 2016
        },
        {
            "authors": [
                "M.P. Rogers",
                "J. Orav",
                "P.M. Black"
            ],
            "title": "The use of a simple likert scale to measure quality of life in brain tumor patients",
            "venue": "Journal of neuro-oncology,",
            "year": 2001
        },
        {
            "authors": [
                "F. Sanders"
            ],
            "title": "On subjective probability forecasting",
            "venue": "Journal of Applied Meteorology,",
            "year": 1963
        },
        {
            "authors": [
                "P. Shivaswamy",
                "W. Chu",
                "M. Jansche"
            ],
            "title": "A support vector approach to censored targets",
            "venue": "ICDM",
            "year": 2007
        },
        {
            "authors": [
                "R.A. Sparapani",
                "B.R. Logan",
                "R.E. McCulloch",
                "P.W. Laud"
            ],
            "title": "Nonparametric survival analysis using bayesian additive regression trees (BART)",
            "venue": "Statistics in medicine,",
            "year": 2016
        },
        {
            "authors": [
                "H. Steck",
                "B. Krishnapuram",
                "C. Dehing-oberije",
                "P. Lambin",
                "V.C. Raykar"
            ],
            "title": "On ranking in survival analysis: Bounds on the concordance index",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2008
        },
        {
            "authors": [
                "E.W. Steyerberg",
                "A.J. Vickers",
                "N.R. Cook",
                "T. Gerds",
                "M. Gonen",
                "N. Obuchowski",
                "M.J. Pencina",
                "M.W. Kattan"
            ],
            "title": "Assessing the performance of prediction models: a framework for some traditional and novel measures",
            "venue": "Epidemiology (Cambridge, Mass.),",
            "year": 2010
        },
        {
            "authors": [
                "T.M. Therneau"
            ],
            "title": "A Package for Survival",
            "venue": "Analysis in S,",
            "year": 2015
        },
        {
            "authors": [
                "T.M. Therneau",
                "P.M. Grambsch"
            ],
            "title": "Modeling survival data: extending the Cox model",
            "venue": "Springer Science & Business Media,",
            "year": 2013
        },
        {
            "authors": [
                "H.C. van Houwelingen",
                "T. Bruinsma",
                "A.A. Hart",
                "L.J. van\u2019t Veer",
                "L.F. Wessels"
            ],
            "title": "Cross-validated cox regression on microarray gene expression data",
            "venue": "Statistics in medicine,",
            "year": 2006
        },
        {
            "authors": [
                "J. Wang",
                "J. Sareen",
                "S. Patten",
                "J. Bolton",
                "N. Schmitz",
                "A. Birney"
            ],
            "title": "A prediction algorithm for first onset of major depression in the general population: development and validation",
            "venue": "Journal of epidemiology and community health, pages jech\u20132013,",
            "year": 2014
        },
        {
            "authors": [
                "P. Wang",
                "Y. Li",
                "C.K. Reddy"
            ],
            "title": "Machine learning for survival analysis: A survey",
            "venue": "arXiv preprint arXiv:1708.04649,",
            "year": 2017
        },
        {
            "authors": [
                "D.M. Witten",
                "R. Tibshirani"
            ],
            "title": "Survival analysis with high-dimensional covariates",
            "venue": "Statistical Methods in Medical Research,",
            "year": 2010
        },
        {
            "authors": [
                "I. Witten",
                "E. Frank",
                "M. Hall"
            ],
            "title": "Data Mining: Practical Machine Learning Tools and Techniques",
            "year": 2011
        },
        {
            "authors": [
                "G. Yan",
                "T. Greene"
            ],
            "title": "Investigating the effects of ties on measures of concordance",
            "venue": "Statistics in medicine,",
            "year": 2008
        },
        {
            "authors": [
                "Y. Yang",
                "H. Zou"
            ],
            "title": "A cocktail algorithm for solving the elastic net penalized cox\u2019s regression in high dimensions",
            "venue": "Statistics and its Interface,",
            "year": 2013
        },
        {
            "authors": [
                "Y. Yang",
                "H. Zou"
            ],
            "title": "fastcox: Lasso and Elastic-Net Penalized Cox\u2019s Regression in High Dimensions Models using the Cocktail Algorithm, 2017",
            "venue": "R package version 1.1.3",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "Keywords: Survival analysis; risk model; patient specific survival prediction; calibration; discrimination\nar X\niv :1\n81 1.\n11 34\n7v 1\n[ cs\n.L G"
        },
        {
            "heading": "1 Introduction",
            "text": "When diagnosed with a terminal disease, many patients ask about their prognosis [21]: \u201cHow long will I live?\u201d, or \u201cWhat is the chance that I will live for 1 year... and the chance for 5 years?\u201d. Here it would be useful to have a meaningful \u201csurvival distribution\u201d S( t | ~x ) that provides, for each time t \u2265 0, the probability that this specific patient ~x will survive at least an additional t months. Unfortunately, many of the standard survival analysis tools cannot accurately answer such questions: (1) risk scores (e.g., Cox proportional hazard [10]) provide only relative survival measures, but not the calibrated probabilities desired; (2) single-time probability models (e.g., the Gail model [9]) provide a probability value but only for a single time point; and (3) class-based survival curves (like Kaplan-Meier, km [31]) are not specific to the patient, but rather an entire population.\nTo explain the last point, Figure 1[left] shows the km curve for patients with stage4 stomach cancer. Here, we can read off the claim that 50% of the patients will survive 11 months, and 95% will survive at least 2 months.1 While these estimates do apply to the population, on average, they are not designed to be \u201caccurate\u201d for an individual patient since these estimates do not include patient-specific information such as age, treatments administered, or general health conditions. It would be better to directly, and correctly, incorporate these important factors ~x explicitly in the prognostic models.\nThis heterogeneity of patients, coupled with the need to provide probabilistic estimates at several time points, has motivated the creation of several individual survival time distribution (isd) tools, each of which can use this wealth of healthcare information from earlier patients, to learn a more accurate prognostic model, which can then predict the isd of a novel patient based on all available patient-specific attributes. This paper considers several isd models: the Kalbfleisch-Prentice extension of the Cox (cox-kp) [29] and the elastic net Cox (coxen-kp) [55] model, the Accelerated Failure Time (aft) model [29], the Random Survival Forest model with Kaplan-Meier extensions (rsf-km), and the Multi-task Logistic Regression (mtlr) model [57]. Figure 1(middle, right) show survival curves (generated by mtlr) for two of these stage-4 stomach cancer patients, which incorporate other information about these individual patients, such as the patient\u2019s age, gender, blood work, etc. We see that these prognoses are very different; in particular, mtlr predicts that [middle] Patient #1\u2019s median survival time is 20.2 months, while [right] Patient #2\u2019s is only 2.6 months. The blue vertical lines show the actual times of death; we see that each of these patients passed away very close to mtlr\u2019s predictions of their respective median survival times.\nOne could then use such curves to make decisions about the individual patient. Of course, these decisions will only be helpful if the model is giving accurate information \u2013 i.e., only if it is appropriate to tell a patient that s/he has a 50% chance of dying before the median survival time of this predicted curve, and a 25% chance of dying before the time associated\n1 In general, a survival curve is a plot where each [x, y ] point represents (the curve\u2019s claim that) there is a y% chance of surviving at least x time. Hence, in Figure 1[left], the [ 11 months, 50% ] point means this curve predicts a 50% chance of living at least 11 months (and hence a 100\u221250 = 50% chance of dying within the first 11 months). The [ 2 months, 95% ] point means a 95% chance of surviving at least 2 months, and the [ 51 months, 5% ] point means a 5% chance of surviving at least 51 months.\nwith the 25% on the curve, etc. We focus on ways to learn such models from a \u201csurvival dataset\u201d (see below), describing earlier individuals. Survival prediction is similar to regression as both involve learning a model that regresses the covariates of an individual to estimate the value of a dependent real-valued response variable \u2013 here, that variable is \u201ctime to event\u201d (where the standard event is \u201cdeath\u201d). But survival prediction differs from the standard regression task as its response variable is not fully observed in all training instances \u2013 this task allows many of the instances to be \u201cright censored\u201d, in that we only see a lower bound of the response value. This might happen if a subject was alive when the study ended, meaning we only know that she lived at least (say) 5 years after the starting time, but do not know whether she actually lived 5 years and a day, or 30 years. This also happens if a subject drops out of a study, after say 2.3 years, and is then lost to follow-up; etc. Moreover, one cannot simply ignore such instances as it is common for many (or often, most) of the training instances to be right-censored; see Table 4. Such \u201cpartial label information\u201d is problematic for standard regression techniques, which assume the label is completely specified for each training instance. Fortunately, there are survival prediction algorithms that can learn an effective model, from a cohort that includes such censored data. Each such \u201csurvival dataset\u201d contains descriptions of a set of instances (e.g., patients), as well as two \u201clabels\u201d for each: one is the time, corresponding to the time from diagnosis to a final date (either death, or time of last follow-up) and the other is the status bit, which indicates whether the patient was alive at that final date. Section 2 summarizes several popular models for dealing with such survival data.\nThis paper provides three contributions: (1) Section 2 motivates the need for such isd models by showing how they differ from more standard survival analysis systems. (2) Section 3 then discusses several ways to evaluate such models, including standard measures (Concordance, 1-Calibration, Brier score), variants/extensions to familiar measures (L1-loss, Log-L1-loss), and also a novel approach, \u201cD-Calibration\u201d which can be used to assess the quality of the individual survival curves generated by isd models. (3) Section 4 evaluates several isd (and related) models (standard: km, cox-kp, aft and more recent: rsf-km,\ncoxen-kp, mtlr) on 8 diverse survival datasets, in terms of all 5 evaluation measures. We will see that mtlr does well \u2013 typically outperforming the other models in the various measures, and often showing vast improvement in terms of calibration metrics.\nThe appendices provide relevant auxiliary information: Appendix A describes some important nuances about survival curves. Appendix B provides further details concerning all the evaluation metrics and in particular, how each addresses censored observations. It also contains some relevant proofs about our novel D-Calibration metric. Appendix C then explains some additional aspects of the isd models considered in this paper. Lastly, Appendix D gives the detailed results from empirical evaluation \u2013 e.g., providing detailed tables corresponding to the results shown as figures in Section 4.2.\nFor readers who want an introduction to survival analysis and prediction, we recommend Applied Survival Analysis by Hosmer and Lemeshow [26]. Wang et al. [51] surveyed machine learning techniques and evaluation metrics for survival analysis. However, that work primarily overviewed the standard survival analysis models, then briefly discussed some of the evaluation techniques and application areas. Our work, instead, focuses on the isd-based models \u2013 first motivating why they are relevant for survival prediction (with a focus on medical situations) then providing empirical results showing the strengths and weaknesses of each of the models considered."
        },
        {
            "heading": "2 Summary of Various Survival Analysis/Prediction",
            "text": "Systems\nThere are many different survival analysis/prediction tools, designed to deal with various different tasks. We focus on tools that learn the model from a survival dataset,\nD = { [~xi, ti, \u03b4i] }i (1)\nwhich provides the values for features ~xi = [x (1) i , \u00b7 \u00b7 \u00b7 , x (k) i ] for each member of a cohort of historical patients, as well as the actual time of the \u201cevent\u201d ti \u2208 <\u22650 which is either death (uncensored) or the last visit (censored), and a bit \u03b4 \u2208 {0, 1} that serves as the indicator for death.2 See Figure 2, in the context of our isd framework.\nHere, we assume ~x is a vector of feature values describing a patients, using information that are available when that patient entered the study \u2013 e.g., when the patient was first diagnosed with the disease, or started the treatment. Additionally, we assume each patient has a death time, di, and a censoring time, ci, and assign ti := min{di, ci} and \u03b4i = I [ di \u2264 ci ] where I [ \u00b7 ] is the Indicator function \u2013 i.e., \u03b4i := 1 if di \u2264 ci or \u03b4i := 0 if di > ci. We follow the standard convention that di and ci are assumed independent.\nTo help categorize the space of survival prediction systems, we consider 3 independent characteristics:\n2 Throughout this work we focus on only Right-Censored survival data. Additionally, we constrain our work to the standard machine-learning framework, where our predictions are based only on information available at fixed time t0 (e.g., start of treatment). While these descriptions all apply when dealing with the time to an arbitrary event, our descriptions will refer to \u201ctime to death\u201d.\nThis section summarizes 5 (of the 2 \u00d7 3 \u00d7 2 = 12) classes of survival analysis tools (see Figure 3), giving typical uses of each, then discusses how they are interrelated.\n2.1 [R,1\u2200,i]: 1-value Individual Risk Models (cox)\nAn important class of survival analysis tools compute \u201crisk\u201d scores, r(~x) \u2208 < for each patient ~x, with the understanding that r(~xa) > r(~xb) corresponds to predicting that ~xa\nwill die before ~xb. Hence, this is a discriminative tool for comparing pairs of patients, or perhaps for \u201cwhat if\u201d analysis of a single patient (e.g., if he continues smoking, versus if he quits). These systems are typically evaluated using a discriminative measure, such as \u201cConcordance\u201d (discussed in Section 3.1). Notice these tools each return a single real value for each patient.\nOne standard generic tool here is the Cox Proportional Hazard (cox) model [10], which is used in a wide variety of applications. This models the hazard function3 as\nhcox( t, ~x ) = \u03bb0(t) exp(~\u03b2 T~x) (2)\nwhere ~\u03b2 are the learned weights for the features, and \u03bb0(t) is the baseline hazard function. We view this as a Risk Model by ignoring \u03bb0(t) (as \u03bb0(t) is the same for all patients), and focusing on just exp(~\u03b2T~x) \u2208 <+. (But see the cox-kp model below, in [P,\u221e,i].) There are many other tools for predicting an individual\u2019s risk score, typically with respect to some disease; see for example the Colditz-Rosner model [8], and the myriad of others appearing on the Disease Risk Index website4. For all of these models, the value returned is atemporal \u2013 i.e., it does not depend on a specific time. There are also tools that produce [R,\u221e,i] models, that return a risk score associated across all time point; see Section 3.1.\n3 The hazard function (also known as the failure rate, hazard rate, or force of mortality) h(t; ~x) = p(t | ~x)/S( t | ~x ) is essentially the chance that ~x will die at time t, given that s/he has lived until this time, using the survival PDF p(t | ~x). When continuous, h(t; ~x) = \u2212 ddt logS( t | ~x ).\n4http://www.diseaseriskindex.harvard.edu/update/"
        },
        {
            "heading": "2.2 [R,1t\u2217,g]: Single-time Group Risk Predictors: Prognostic Scales (PPI, PaP)",
            "text": "Another class of risk predictions explicitly focus on a single time, leading to prognostic scales, some of which are computed using Likert scales [40]. For example, the Palliative Prognostic Index (PPI) [35] computes a risk score for each terminally ill patient, which is then used to assign that patient into one of three groups. It then uses statistics about each group to predict that patients in one group will do better at this specific time (here, 3 weeks), than those in another group. Similarly, the Palliative Prognostic Score (PaP) [38] uses a patient\u2019s characteristics to assign him/her into one of 3 risk groups, which can be used to estimate the 30-day survival risk. (There are many other such prognostic scales, including [7, 2, 23].) Again, these tools are typically evaluated using Concordance.5"
        },
        {
            "heading": "2.3 [P,1t\u2217,i]: Single-time Individual Probabilistic Predictors (Gail, PredictDepression)",
            "text": "Another class of single-time predictors each produce a survival probability S\u0302( t\u2217 | ~x ) \u2208 [0, 1] for each individual patient ~x, for a single fixed time t\u2217 \u2013 which is the probability \u2208 [0, 1] that ~x will survive to at least time t\u2217. For example, the Gail model [Gail] [9] 6 estimates the probability that a woman will develop breast cancer within 5 years based on her responses to a number of survey questions. Similarly, the PredictDepression system [PredDep] [50] 7 predicts the probability that a patient will develop a major depressive episode in the next 4 years based on a small number of responses. The Apervite8 and R-calc9 websites each include dozens of such tools, each predicting the survival probability for 1 (or perhaps 2) fixed time points, for certain classes of diseases.\nNotice these probability values have semantic content, and are labels for individual patients (rather than risk-scores, which are only meaningful within the context of other patients\u2019 risk scores). These systems should be evaluated using a calibration measure, such as 1-Calibration or Brier score (discussed in Sections 3.3 and 3.4).\n2.4 [P,\u221e,g]: Group Survival Distribution (km) There are many systems that can produce a survival distribution: a graph of [t, S\u0302( t )], showing the survival probability S\u0302( t ) \u2208 [0, 1] for each time t \u2265 0; see Figure 1. The KaplanMeier analytic tool (km) is at the \u201cclass\u201d level, producing a distribution designed to apply to everyone in a sub-population: S\u0302( t | ~x ) = S\u0302( t ), for every ~x in some class \u2013 e.g., the km curve in Figure 1[left] applies to every patient ~x with stage-4 stomach cancer. The SEER\n5 Here, they do not compare pairs of individuals from the same group, but only patients from different groups, whose events are comparable (given censoring); see Section 3.1.\n6http://www.cancer.gov/bcrisktool/ 7http://predictingdepression.com/ 8https://apervita.com/community/clevelandclinic 9http://www.r-calc.com/ExistingFormulas.aspx?filter=CCQHS\nwebsite10 provides a set of Kaplan-Meier curves for various cancers. While patients can use such information to estimate their survival probabilities, the original goal of that analysis is to better understand the disease itself, perhaps by seeing whether some specific feature made a difference, or if a treatment was beneficial. For example, we could produce one curve for all stage-4 stomach cancer patients who had treatment tA, and another for the disjoint subset of patients who had no treatment; then run a log-rank test [22] to determine whether (on average) patients receiving treatment tA survived statistically longer than those who did not. Section 3 below describes various ways to evaluate [P,\u221e,i] models; we will use these measures to evaluate km models as well.\n2.5 [P,\u221e,i]: Individual Survival Distribution, isd (cox-kp, coxenkp, aft, rsf-km, mtlr)\nThe previous two subsections described two frameworks:\n\u2022 [P,1t\u2217 ,i] tools, which produce an individualized probability value S\u0302( t\u2217 | ~xi ) \u2208 [0, 1], but only for a single time t\u2217; and\n\u2022 [P,\u221e,g] tools, which produce the entire survival probability curve [t, S\u0302( t )] for all points t \u2265 0, but are not individuated \u2013 i.e., the same curve for all patients { ~xi }.\nHere, we consider an important extension: a tool that produces the entire survival probability curve { [t, S\u0302( t | ~xi )] }t for all points t \u2265 0, specific to each individual patient, ~xi. As noted in the previous section, this is required by any application that requires knowing meaningful survival probabilities for many time points. This model also allows us to compute other useful statistics, such as a specific patient\u2019s expected survival time.\nWe call each such system an \u201cIndividual Survival Distribution\u201d model, isd. While the Cox model is often used just to produce the risk score, it can be used as an isd, given an appropriate (learned) baseline hazard function \u03bb0(t); see Equation 2. We estimate this using the Kalbfleisch-Prentice estimator [29], and call this combination \u201ccox-kp\u201d; we also consider a regularized Cox model, namely the elastic net Cox with the Kalbfleisch-Prentice extension (coxen-kp). We also explore three other models: Accelerated Failure Time model [29] with the Weibull distribution (aft), Random Survival Forests with the Kaplan-Meier extension (rsf-km, described in Appendix C.3) [28] and Multi-task Logistic Regression system (mtlr) [57]. Figure 4 shows the curves from these various models, each over the same set of individuals.\nAbove, we briefly mentioned three evaluation methods: Concordance, 1-Calibration, and Brier score. We show below that we can use any of these methods to evaluate a isd model. In addition, we can also use variants of \u201cL1-loss\u201d, to see how far a predicted single-time differs from the true time of death; see Section 3.2. Each of these 4 methods considers only a single time point of the distribution, or an average of scores, each based on only a single time, or a single statistic (such as its median value). We also consider a novel evaluation measure,\n10http://seer.cancer.gov/\n\u201cD-Calibration\u201d, which uses the entire distribution of estimated survival probabilities; see Section 3.5."
        },
        {
            "heading": "2.6 Other Issues",
            "text": "The goal of many Survival Analysis tools is to identify relevant variables, which is different from our challenge here, of making a prediction about an individual. Some researchers use km to test whether a variable is relevant \u2013 e.g., they partition the data into two subsets, based on the value of that variable, then run km on each subset, and declare that variable to be relevant if a log-rank test claims these two curves are significantly different [22]. It is also a common use of the basic Cox model \u2013 in essence, by testing if the \u03b2\u0302i coefficient associated with feature xi (in Equation 2) is significantly different from 0 [48]. (We will later use this approach to select features, as a pre-processing step, before running the actual survival prediction model; see Section 4.1.)\nNote this \u201cg vs i \u201d distinction is not always crisp, as it depends on how many variables are involved \u2013 e.g., models that \u201cdescribe\u201d each instance using no variables (like km) are clearly \u201cg \u201d, while models that use dozens or more variables, enough to distinguish each patient from one another, are clearly \u201ci \u201d. But models that involve 2 or 3 variables typically will place each patient into one of a small number of \u201cclusters\u201d, and then assign the same values to each member of a cluster. By convention, we will catalog those models as \u201cg \u201d as the decision is not intended to be at an individual level.\nThe \u201c1t\u2217\u201d vs \u201c\u221e\u201d distinction can be blurry, if considering a system that produces a small number k > 1 of predictions for each individual \u2013 e.g., the Gail model provides a prediction of both 5 year and 25 year survival. We consider this system as a pair of \u201c1t\u2217\u201d-predictors, as those two models are different. (Technically, we could view them as \u201cGail[5year]\u201d versus \u201cGail[25year]\u201d models.)\nFinally, recall there are two types of frameworks that each return a single value for each instance: the single value returned by the [R,1\u2200,i]-model cox is atemporal \u2013 i.e., applies to the overall model \u2013 while each single value returned by the [P,1t\u2217 ,i]-model Gail and the [R,1t\u2217 ,g]-model PaP, is for a specific time, t\n\u2217. (Note there can also be [P,1\u2200,i]- and [R,1\u2200,g]models that are atemporal.)"
        },
        {
            "heading": "2.7 Relationship of Distributional Models to Other Survival Anal-",
            "text": "ysis Systems\nWe will use the term \u201cDistributional Model\u201d to refer to algorithms within the [P,\u221e,g] and [P,\u221e,i] frameworks \u2013 i.e., both km and isd models. Note that such models can match the functionality of the first 3 \u201cpersonalized\u201d approaches. First, to emulate [P,1t\u2217 ,i], we just need to evaluate the distribution at the specified single time t\u2217 \u2013 i.e., S\u0302( t\u2217 | ~x ). So for Patient #1 (from Figure 1), for t\u2217 = \u201c48 months\u201d, this would be 20%. Second, to emulate [R,1t\u2217 ,i], we can just use the negative of this value as the time-dependent risk score \u2013 so the 4-year risk for Patient #1 would be -0.20. Third, to deal with [R,1\u2200,i], we need to reduce the distribution to\na single real number, where larger values indicate shorter survival times. A simple candidate is the individual distribution\u2019s median value, which is where the survival curve crosses 50%.11 So for Patient #1 in Figure 1, the median is t\u0302 (0.5) 1 = 16 months. We can then view (the negative of) this scalar as the risk score for that patient. So for Patient #1, the \u201crisk\u201d would be r(~x1) = \u221216 . Fourth, to view the isd model in the [R,1\u2200,g] framework, we need to place the patients into a small number of \u201crelatively homogeneous\u201d bins. Here, we could quantize the (predicted) mean value \u2013 e.g., mapping a patient to Bin#1 if that mean is in [0, 15), Bin#2 if in [15, 27), and Bin#3 if in [27, 70]. (Here, this patient would be assigned to Bin#2.) Fifth, to view the isd model in the [R,1t\u2217 ,g] framework, associated with a time t\n\u2217, we could quantize the t\u2217-probability \u2013 e.g., quantize the S\u0302( t\u2217 = 48 months | ~x ) into 4 bins corresponding to the intervals [0, 0.20), [0.20, 0.57), [0.57, 0.83], and [0.83, 1.0].\nThese simple arguments show that a distributional model can produce the scalars used by five other frameworks [P,1t\u2217 ,i], [R,1t\u2217 ,i], [R,1\u2200,i], [R,1\u2200,g], and [R,1t\u2217 ,g]. Of course, a distributional model can also provide other information about the patient \u2013 not just the probability associated with one or two time points, but at essentially any time in the future, as well as the mean/median value. Another advantage of having such survival curves is visualization (see Figure 1): it allows the user (patient or clinician) to see the shape of the curve, which provides more information than simply knowing the median, or the chance of surviving 5 years, etc.\nThere are some subtle issued related to producing meaningful survival curves \u2013 e.g., many curves end at a non-zero value: note the km curve in Figure 4(top left) stops at (83, 0.12), rather than continue to intersect the x-axis at, perhaps (103, 0.0). This is true for many of the curves produced by the isds. Indeed, some of the curves do not even cross y = 0.5, which means the median time is not well-defined; cf. the top orange line on the aft curve (top right), which stops at (83, 0.65), as well as many of the other curves throughout that figure. This causes many problems, in both interpreting and evaluating isd models. Appendix A shows how we address this."
        },
        {
            "heading": "3 Measures for Evaluating Survival Analysis/Prediction",
            "text": "Models\nThe previous section mentioned 5 ways to evaluate a survival analysis/prediction model: Concordance, 1-Calibration, Brier score, L1-loss, and D-Calibration. This section will describe these \u2013 quickly summarizing the first four (standard) evaluation measures (and leaving the details, including discussion of censoring, for Appendix B) then providing a more thorough motivation and description of the fifth, D-Calibration. The next section shows how the 6 distribution-learning models perform with respect to these evaluations.\nFor notation, we will assume models were trained on a training dataset, formed from the same triples as shown in Equation 1, that is D = DU \u222aDC where DU = { [~xj, dj, \u03b4j = 1] }j\n11 Another candidate is the mean value of the distribution, which corresponds to the area under the survival curve; see Theorem B.1.\nis the set of uncensored instances (notice the event time, tj, here is written as dj), and DC = { [~xk, ck, \u03b4k = 0] }k is the set of censored instances (tk, here is written as ck). Note also that this training dataset D is disjoint from the validation dataset, V . Since models are evaluated on V and we save discussion of censoring for Appendix B, we assume here that all of V is uncensored \u2013 i.e., V = VU = { [~xj, dj, \u03b4j = 1] }j \u2248 { [~xj, dj] }j (to simplify notation)."
        },
        {
            "heading": "3.1 Concordance",
            "text": "As noted above, each individual risk model [R,1\u00b7,-] (i.e., [R,1\u00b7,i] or [R,1\u00b7,g], where 1\u00b7 can be either 1t\u2217 or 1\u2200) assigns to each individual ~x, a \u201crisk score\u201d r(~x) \u2208 <, where r(~xa) > r(~xb) means the model is predicting that ~xa will die before ~xb. Concordance (a.k.a. C-statistic, C-index) is commonly used to validate such risk models. Specifically, Concordance considers each pair of patients, and asks whether the predictor\u2019s values for those patients matches what actually happened to them. In particular, if the model gives ~xa a higher score than ~xb, then the model gets 1 point if ~xa dies before ~xb. If instead ~xb died before ~xa, the model gets 0 points for this pair. Concordance computes this for all pairs of comparable patients, and returns the average.\nWhen considering only uncensored patients, every pair is comparable, which means there are ( n 2 ) = n\u00b7(n\u22121) 2 pairs from n = |VU | elements. Given these comparable pairs, Concordance is calculated as,\nC \u2227 (VU , r(\u00b7) ) = 1\n|VU | \u00b7 (|VU |\u22121) 2\n\u00b7 \u2211\n[~xi,di]\u2208VU \u2211 [~xj ,dj ]\u2208VU : di<dj I [ r(~xi) > r(~xj) ] . (3)\nAs an example, consider the table of death times di and risk scores, for 5 patients, shown in Table 1[left]. Table 1[right] shows that these risk scores are correct in 7 of the ( 5 2 ) = 10 pairs, so the Concordance here is 7/10 = 0.7. This Concordance measure is very similar to the area under the receiver operating curve (AUC) and equivalent when di is constrained to values {0, 1} [33]. This Concordance measure is relevant when the goal is to rank or discriminate between patients \u2013 e.g., when one wants to know who will live longer between a pair of patients. (For\nexample, if we want to transplant an available liver to the patient who will die first \u2013 this corresponds to \u201curgency\u201d.) Concordance is the desired metric here due to it\u2019s interpretation, i.e. given two randomly selected patients, ~xa and ~xb, if a model with Concordance of 0.9 assigns a higher risk score to ~xa than ~xb, then there is a 90% chance that ~xa will die before ~xb.\nWhile [R,1\u2200,i] models (such as cox) provide a risk score that is independent of time, there are also [R,\u221e,i] models that produces a risk score r(~x, t) for an instance ~x that depends on time t; such as Aalen\u2019s additive regression model [1] or time-dependent Cox (td-Cox) [14], which uses time-dependent features. These models can be evaluated using time-dependent Concordance (aka, \u201ctime-dependent ROC curve analysis\u201d) [24].\nFinally, the [R,\u2212,g] systems compute a risk score, but then bin these scores into a small set of intervals. When computing Concordance, they then only consider patients in different bins. For example, if Bin1 = [0, 10] and Bin2 = [11, 20], then this evaluation would only consider pairs of patients (~xa, ~xb) where one is in Bin1 and the other is in Bin2 \u2013 e.g., r(~xa) \u2208 [0, 10] and r(~xb) \u2208 [11, 20]. (Hence, it will not consider the pair (~xc, ~xd) if both r(~xc), r(~xd) \u2208 [11, 20].)\nSee Appendix B.1 for more details, including a discussion of how this measure deals with censored instances and tied risk scores/death times."
        },
        {
            "heading": "3.2 L1-loss",
            "text": "Survival prediction is very similar to regression: given a description of a patient, predict a real number (his/her time of death). With this similarity in mind, one can evaluate a survival model using the techniques used to evaluate regression tasks, such as L1-loss \u2013 the average absolute value of the difference between the true time of death, di, and the predicted time d \u2227\ni: 1 n \u2211 i |di\u2212d \u2227\ni|. (We consider the L1-loss, rather than L2-loss which squares the differences, as the distribution of survival times is often right skewed, and L1-loss is less swayed by outliers than L2-loss.)\nOne challenge in applying this measure to our [P,\u221e,-] models is identifying the predicted time, d \u2227 i. Here, we will use the predicted median survival time, that is d \u2227 i = t\u0302 (0.5) i , leading to the following measure:\nL1( VU , { S\u0302( \u00b7 | ~xi ) }i ) = 1 |VU | \u2211\n[~xi,di]\u2208VU \u2223\u2223\u2223di \u2212 t\u0302(0.5)i \u2223\u2223\u2223 . (4) While we would like this value to be small, we should not expect it to be 0: if the distribution is meaningful, there should be a non-zero chance of dying at other times as well. For example, while the L1-loss is 0 for the Heaviside distribution at the time of death (shown in green in Figure 5), this is unrealistic.\nAppendix B.2 discusses many issues with the L1-loss measure, related to censored data, and reasons to consider using the log of survival time."
        },
        {
            "heading": "3.3 1-Calibration",
            "text": "The [P,1t\u2217 ,i] tools estimate the survival probability S\u0302( t \u2217 | ~x ) \u2208 [0, 1] for each instance ~x, at a single time point t\u2217. For example, the PredictDepression system [50] predicts the chance that a patient will have a major depression episode within the next 4 years, based on their current characteristics \u2013 i.e., this tool produces a single probability value S\u0302( 4yr | ~xi ) \u2208 [0, 1] for each patient described as ~xi. We can use 1-Calibration to measure the effectiveness of such predictors. To help explain this measure, consider the \u201cweatherman task\u201d of predicting, on day t, whether it will rain on day t+ 1. Given the uncertainty, forecasters provide probabilities. Imagine, for example, there were 10 times that the weatherman, Mr.W, predicted that there was a 30% chance that it would rain tomorrow. Here, if Mr.W was calibrated, we expect that it would rain 3 of these 10 times \u2013 i.e., 30%. Similarly, of the 20 times Mr.W claims that there is an 80% chance of rain tomorrow, we expect rain to occur 16 = 20 \u00d7 0.8 of the 20 times.\nHere, we have described a binary probabilistic prediction problem \u2013 i.e., predicting the chance that it will rain the next day. One of the most common calibration measures for such binary prediction problems is the Hosmer-Lemeshow goodness-of-fit test [25]. First, we sort the predicted probabilities for this time t\u2217 for all patients { S\u0302( t\u2217 | ~xi ) }i and group them into a number (B) of \u201cbins\u201d; commonly into deciles, i.e., B = 10 bins. Suppose there are 200 patients; the first bin would include the 20 patients with the largest S\u0302( t\u2217 | ~xi ) values, the second bin would contain the patients with the next highest set of values, and so on, for all 10 bins. Next, within each bin, we calculate the expected number of events, p\u0304j = 1 |Bj | \u2211 ~xi\u2208Bj(1 \u2212 S\u0302( t \u2217 | ~xi )). We also let nj = |Bj| be the size of the jth bin (here, n1 = n2 = \u00b7 \u00b7 \u00b7 = n10 = 200/10 = 20), and Oj be the number of patients (in the jth bin) who died before t\u2217. Recalling that di denotes Patient #i\u2019s time of death and letting oi = I [ di \u2264 t\u2217 ] denote the event status of the ith patient at t\u2217: for the jth bin, Bj, we have Oj = \u2211 ~xi\u2208Bj oi. Figure 6 graphs the 10 values of observed Oj and expected nj p\u0304j for the deciles, for two different tests (corresponding to two different isd-models, on the same dataset and t\u2217 time). Additionally, see Appendix B.3 for an example walking through 1-Calibration.\nFor each test, we can then compute the Hosmer-Lemeshow test statistic:\nHL \u2227 (VU , S\u0302( t \u2217 | \u00b7 ) ) =\nB\u2211 j=1 (Oj \u2212 nj p\u0304j)2 nj p\u0304j (1\u2212 p\u0304j) , (5)\nIf the model is 1-Calibrated, then this statistic follows a \u03c72B\u22122 distribution, which then can be used to find a p-value. For a given time t\u2217, finding p < 0.05 suggests the survival model is not well calibrated at t\u2217 \u2013 i.e., the predicted probabilities of survival at t\u2217 may not be representative of patient\u2019s true survival probability at t\u2217.\nReturning to Figure 6, the HL statistics are 5.99 and 321.44, for the left and right, leading to the p-values p =0.741 and p < 0.001 \u2013 meaning the left one passes but the right one does not. (This is not surprising, given that each pair of bars on the left are roughly the same height, while the pairs of the right are not.)\nNote that a [P,\u221e,i] model, which gives probabilities for multiple time points, may be calibrated at one time t1, but not be calibrated at another time t2, since Oj, and p\u0304j are dependent on the chosen time point. This issue motivated us to define a notion of calibration across a distribution of time points, D-Calibration, in Section 3.5. Appendix B.3 provides further details about 1-Calibration including ways to handle censored patients."
        },
        {
            "heading": "3.4 Brier Score",
            "text": "We often want a model to be both discriminative (high Concordance) and calibrated (passes the 1-Calibration test). While one can rank Concordance scores to compare two models\u2019\ndiscriminative abilities, 1-Calibration cannot rank models besides suggesting one model is calibrated (p \u2265 0.05) and one is not (p < 0.05) (as p-values are not intended to be ranked). The Brier score [6] is a commonly used metric that measures both calibration and discrimination; see Appendix B.4.1. Mathematically, the Brier score is the mean squared error between the {0, 1} event status at time t\u2217 and the predicted survival probability at t\u2217. Given a fully uncensored validation set VU , the Brier score, at time t \u2217, is\nBSt\u2217 ( VU , S\u0302( t \u2217 | \u00b7 ) )\n= 1 |VU | \u2211\n[~xi,di]\u2208VU\n( I [ di \u2264 t\u2217 ] \u2212 S\u0302( t\u2217 | ~xi ) )2 . (6)\nHere, a perfect model (that only predicts 1s and 0s as survival probabilities and is correct in every case) will get the perfect score of 0, whereas a reference model that gives S\u0302( t\u2217 | \u00b7 ) = 0.5 for all patients will get a score of 0.25.\nAn extension of the Brier score to an interval of time points is the Integrated Brier score, which will give an average Brier score across a time interval,\nIBS( \u03c4, VU , S\u0302( \u00b7 | \u00b7 ) ) = 1\n\u03c4 \u222b \u03c4 0 BSt ( VU , S\u0302( t | \u00b7 ) ) dt . (7)\nWe will use this measure for our analysis, where \u03c4 is the maximum event time of the combined training and validation datasets \u2013 this way, the interval evaluated is equivalent across crossvalidation folds.\nAs noted above, the Brier score measures both calibration and discrimination, implying it should be used when seeking a model that must perform well on both calibration and discrimination, or when one is investigating the overall performance of survival models. Appendix B.4 shows how to incorporate censoring into the Brier score, and discusses the decomposition of the Brier score into calibration and discriminative components."
        },
        {
            "heading": "3.5 D-Calibration",
            "text": "The previous sections summarized several common ways to evaluate standard survival prediction models, that produce only a single value for each patient \u2013 e.g., the patient\u2019s risk score, perhaps with respect to a single time, or the mean survival time. (Each is a [-,1\u00b7,-] model.) However, the [P,\u221e,-] tools produce a distribution \u2013 i.e., each is a function that maps [0,\u221e] to [0, 1] (with some constraints of course), such as the ones shown in Figure 4; see Footnote 1. It would be useful to have a measure that examines the entire distribution as a distribution.12\nA distributional calibration (D-Calibration) [3] measure addresses the critical question:\nShould the patient believe the predictions implied by the survival curve? (8)\nFirst, consider population-based models [P,\u221e,g], like Kaplan-Meier curves \u2013 e.g., Figure 1[left], for patients with stage-4 stomach cancer. If a patient has stage-4 stomach cancer, should\n12While the Integrated Brier score does consider all the points across the distribution, it simply views that distribution as a set of (x, y) points; see Appendix B.4.2 for further explanation.\ns/he believe that his/her median survival time is 11 months, and that s/he has a 75% chance of surviving more than 4 months? To test this, we could take 1000 new patients (with stage-4 stomach cancer) and ask whether \u2248500 of these patients lived at least 11 months, and if \u2248750 lived more than 4 months.\nFor notation, given a dataset, D, and [P,\u221e,g]-model \u0398, and any interval [a, b] \u2282 [0, 1], let\nD\u0398( [a, b] ) = { [~xi, di, \u03b4 = 1] \u2208 D | S\u0302\u0398( di ) \u2208 [a, b] } (9)\nbe the subset of (uncensored) patients in D whose time of death is assigned a probability (by \u0398) in the interval [a, b]. For example, D\u0398( [0.5, 1.0] ) is the subset of patients who lived at least the median survival time (using S\u0302\u0398( \u00b7 )\u2019s median), and D\u0398( [0.25, 1.0] ) is the subset who died after the 25th percentile of S\u0302\u0398( \u00b7 ). By the argument above, we expect D\u0398( [0, 0.5] ) to contain about 1/2 of D, and D\u0398( [0.25, 1.0] ) to contain about 3/4 of D. Indeed, for any interval [a, 1.0], we expect\n|D\u0398( [a, 1.0] )| |D|\n= 1\u2212 a (10)\nor in general |D\u0398( [a, b] )| |D|\n= b\u2212 a (11)\nThis leads to the idea of a survival distribution [P,\u221e,g] model, \u0398, being D-Calibrated: For each uncensored patient ~xi, we can observe when s/he died di, and also determine the percentile for that time, based on \u0398: S\u0302\u0398( di ). If \u0398 is D-Calibrated, we expect roughly 10% of the patients to die in the [90%, 100%] interval \u2013 i.e., |D\u0398( [0.9, 1.0] )||D| \u2248 1 \u2212 0.9 = 0.1 \u2013 and another 10% to die in the [80%, 90%) interval, and so forth for each of the 10 different 10%-intervals. More precisely, the set { S\u0302i( di ) } over all of the patients should be distributed uniformly on [0, 1], which means that each of the 10 bins would contain 10% of D.\nThis suggests a measure to evaluate a distributional model: see how close each of these 10 bins is to the expected 10%. We therefore use Pearson\u2019s \u03c72 test: compute the \u03c72-statistic with respect to the ten 10% intervals, and ask whether the bins appear uniform, at (say) the p > 0.05 level. Theorem B.2 (in Appendix B.5) states and proves the appropriateness of the Pearson\u2019s \u03c72 goodness-of-fit test.\nThis addresses the question posed at the start of this subsection (Equation 8):\nYes, a patient should believe the prediction from the survival curve\nwhenever this goodness-of-fit test reports p > 0.05.\n3.5.1 Dealing with Individual Survival Distributions, isd\nEverything above was for a population-based distributional model [P,\u221e,g]. These specific results do not apply to individual survival distributions [P,\u221e,i]: For example, consider a single patient, Patient #1, whose curve is shown in Figure 1[middle]. Should he believe this plot, which implies that his median survival time is 18 months, and that he has a 75% chance of surviving more than 13 months?\nIf we could observe 1000 patients exactly identical to this Patient #1, we could verify this claim by seeing their actual survival times: this survival curve is meaningful if its predictions matched the outcomes of those copies \u2013 e.g., if around 250 died in the first 13 months, another \u2248250 in months 13 to 18, etc.\nUnfortunately, however, we do not have 1000 \u201ccopies\u201d of Patient #1. But here we do have many other patients, each with his/her own characteristic survival curve, including the 4 curves shown in Figure 7. Notice each patient has his/her own distribution, and hence his/her own quartiles \u2013 e.g., the predicted median survival times for Patient A (resp., B, C and D), are 28.6 (resp., 65.7, 11.4, and 13.9) months; see Table 2. For these historical patients, we know the actual event time for each.13 Here, if our predictor is working correctly, we would expect that 2 of these 4 would pass away before respective median times, and the other 2 after their median times. Indeed, we would actually expect 1 to die in each of the 4 quartiles; the blue vertical lines (the actual times of death) show that, in fact, this does happen. See also Table 2.\nWith a slight extension to the earlier notation (Equation 9), for a dataset D and [P,\u221e,i]model \u0398, and any interval [a, b] \u2282 [0, 1], let\nD\u0398( [a, b] ) = { [~xi, di, \u03b4 = 1] \u2208 D | S\u0302\u0398( di | ~xi ) \u2208 [a, b] } (12)\nbe the subset of (uncensored) patients in the dataset D whose time of death is assigned a probability (based on its individual distribution, computed by \u0398) in the interval [a, b].\nAs above, we could put these S\u0302\u0398( di | ~xi ) into \u201c10%-bins\u201d, and then ask if each bin holds about 10% of the patients. The right-side of Figure 8 plots that information, for the isd \u0398\n13 Here we just consider uncensored patients; Appendix B.5 extends this to deal with censoring.\nlearned by mtlr from the NACD dataset (described in Section 4.1), as a sideways histogram. We see that each of these intervals is very close to 10%. In fact, the \u03c72 goodness-of-fit test yields p = 0.433, which suggests that this isd is sufficiently uniform that we can believe that these survival curves are D-calibrated.\nNote that Figure 8 is actually showing 5-fold cross-validation results: the survival curve for each patient was computed based on the model learned from the other 4/5 of the data, which is then applied to this patient [53]. Also, the rust-colored intervals correspond to the censored patients; see Appendix B.5 for an explanation."
        },
        {
            "heading": "3.5.2 Relating D-Calibration to 1-Calibration",
            "text": "This standard notion of 1-Calibration is very similar to D-Calibration, as both involve binning probability values and applying a goodness-of-fit test. However, 1-Calibration involves a single prediction time \u2013 here S\u0302( t\u2217 | ~xi ), which is the probability that the patient ~xi will survive at least to the specified time, t\u2217. Patients are then sorted by these probabilities, partitioned into equal-size bins, and assessed as to whether the observed survival rates for each bin match the predicted rates using a Hosmer-Lemeshow test. By contrast, D-Calibration considers the entire curve, S\u0302( t | ~xi ) over all times t \u2013 producing curves like the ones shown in Figures 1, 4, and 7. Each curve corresponds to a patient, who has an associated time of death, di. Here, we are considering the model\u2019s (estimated) probability of the patient\u2019s survival at his/her time of death, given by S\u0302i( di | ~xi ). These patients are then placed into B = 10 bins,14 based on the values of their associated probabilities, S\u0302i( di | ~xi ). Here the\n14Note the number of bins does not have to be 10 \u2013 we chose 10 to match the typical value chosen for the 1-Calibration test.\ngoodness-of-fit test measures whether the resulting bins are approximately equal-sized, as would be expected if \u0398 accurately estimated the true survival curves (argued further in Appendix B.5).\nNote D-Calibration tests the proportion of instances in bins across the entire [0, 1] interval, but this is not required for the \u201csingle probability\u201d 1-Calibration. For example, the single probability estimates for the rsf-km curve in Figure 3, at time 20, range only from 0.05 to 0.62. That is, the distribution calibration { S\u0302i( di | ~xi ) } should match the uniform distribution over [0,1], while the single probability calibration { S\u0302i( t\u2217 | ~xi ) } is instead expected to match the empirical percentage of deaths.\nTable 3 summarizes the differences between D-Calibration and 1-Calibration.15 To see that they are different, Proposition B.3, in Appendix B.5, gives a simple example of a model that is perfectly D-Calibrated but clearly not 1-Calibrated, and another example that is perfectly 1-Calibrated but clearly not D-Calibrated. In addition, we will see below several examples of this \u2013 e.g., coxen-kp is D-Calibrated for the GLI dataset, but it is not 1- Calibrated at any of the 5 time points considered, and aft is 1-Calibrated for the 50th and 75th percentiles of GBM but is not D-Calibrated.\n4 Evaluating isd Models\nSections 2.4 and 2.5 listed several distributional models (km, and the isds: cox-kp, coxenkp, aft, mtlr, and rsf-km), and Section 3 provided 5 different evaluation measures: Concordance, L1-loss, 1-Calibration, Integrated Brier score, and D-Calibration. This section provides an empirical comparison of these 6 models, with respect to all 5 of these measures, over 8 datasets.\nOf course, these 6 models do not include all possible survival models; they instead serve as a sample of the types of models available. The km, cox-kp, and aft model are all very common \u2013 these are standard approaches used throughout survival analysis and represent non-parametric, semi-parametric, and parametric models, respectively. As our preliminary studies with cox-kp suggested it was overfitting, we also included a regularized extension, using elastic net, coxen-kp. Since Random Survival Forests (rsf) were introduced in 2008, they have had a large impact on the survival analysis community. However, as the\n15Further differences occur when considering how censored patients are handled; see Appendices B.3 and B.5.\nKaplan-Meier extension to transform rsf into an isd is not well known, it is summarized in Appendix C.3. More recent still is the mtlr technique [57] that directly learns a survival distribution, by essentially learning the associated probability mass function (whose sequential right-to-left sum, when smoothed, is the survival distribution). We found some subsequent similar models, including \u201cMulti-Task Learning for Survival Analysis\u201d (MTLSA) [33], some deep learning variants [39, 32, 34], and a computationally demanding Bayesian regression trees model [44], but for brevity, we focused on just the first such model, mtlr.\nNote the distribution class D chosen for aft certainly influences its performance \u2013 e.g., it is possible that aft[Weibull] on a dataset may fail D-Calibration whereas aft[Log-Logistic] may pass; similarly for 1-Calibration at some time t\u2217, and the scores for Concordance, L1loss and Integrated Brier score will depend on that distribution class. This paper will focus on aft[Weibull] because, while still being parametric, the Weibull distribution is versatile enough to fit many datasets."
        },
        {
            "heading": "4.1 Datasets and Evaluation Methodology",
            "text": "There are many different survival datasets; here, we selected 8 publicly available medical datasets in order to cover a wide range of sample sizes, number of features, and proportions of censored patients. We excluded small datasets (with fewer than 150 instances) to reduce the variance in the evaluation metrics. Our datasets ranged from 170 to 2402 patients, from 12 to 7401 features, and percentage of censoring from 17.23% to 86.21%; see Table 4. Note that we have not included extremely high-dimensional data (with tens of thousands of features, often found in genomic datasets), as such data raises additional challenges beyond the scope of standard survival analysis; see [52] for methods to handle extremely high-dimensional data.\nThe Northern Alberta Cancer Dataset (NACD), with 2402 patients and 53 features, is a conglomerate of many different cancer patients, including lung, colorectal, head and neck, esophagus, stomach, and other cancers. In addition to using the complete NACD dataset, we considered the subset of 950 patients with colorectal cancer (Nacd-Col), with the same 53 features.\nAnother four datasets were retrieved from data generated by The Cancer Genome Atlas (TCGA) Research Network [15]: Glioblastoma multiforme (GBM; 592 patients, 12 features), Glioma (GLI; 1105 patients, 13 features), Rectum adenocarcinoma (READ; 170 patients, 18 features), and Breast invasive carcinoma (BRCA; 1095 patients, 61 features). To ensure a variety of feature/sample-size ratios, we consider only the clinical features in our experiments.\nLastly, we included two high-dimensional datasets: the Dutch Breast Cancer Dataset (DBCD) [49] contains 4919 microarray gene expression levels for 295 women with breast cancer, and the Diffuse Large B-Cell Lymphoma (DLBCL) [33] dataset contains 7401 features focusing on Lymphochip DNA microarrays for 240 biopsy samples.\nWe applied the following pre-processing steps to each dataset: We first removed any feature that was missing over 25% of its values, as well as any features containing only 1 unique value. For the remaining features, we \u201cone-hot encoded\u201d each nominal feature and then passed each feature to a univariate Cox filter, and removed any feature that was not\nsignificant at the p \u2264 0.10 level. Following feature selection, we replaced any missing value with the respective feature\u2019s mean value. (Note this feature selection was found to benefit all isd models across all performance metrics; data not shown.) Table 4 provides the dataset statistics and a full breakdown of feature numbers in each step.\nFollowing feature selection, features were normalized (transformed to zero mean with unit variance) and passed to models for five-fold cross validation (5CV). We compute the folds by sorting the instances by time and censorship, then placing each censored (resp., uncensored) instance sequentially into the folds \u2013 meaning all folds had roughly the same distribution of times, and censorships.\nFor coxen-kp, rsf-km, and mtlr we used an internal 5CV for hyper-parameter selection. There were no hyper-parameters to tune for the remaining models: cox, km, and aft.\nAs 1-Calibration required specific time points, and as models might perform well on some survival times but poorly on others, we chose five times to assess the calibration results of each model: the 10th, 25th, 50th, 75th, and 90th percentiles of survival times for each dataset. Here, we used the D\u2019Agostino-Nam translation to include censored patients for these evaluation results \u2013 see Appendix B.3. Appendix D.4 presents all 240 values (6 models \u00d7 8 datasets \u00d7 5 time-points); here we instead summarize the number of datasets that each model passed as 1-Calibrated (at p \u22650.05) for each percentile.\nFor all evaluations, we report the averaged 5CV results for Concordance, Integrated Brier score, and L1-loss. As Concordance requires a risk score, we use the negative of the median survival time and similarly use the median survival time for predictions for the L1-loss. To adjust for presence of censored data, we used the L1-Margin loss, given in Appendix B.2, which extends the \u201cUncensored L1-loss\u201d given in Section 3.2 (which considers only uncensored patients). Additionally, as 1-Calibration (resp., D-Calibration) results are reported as p-values, and it is not appropriate to average over the folds, we combined the predicted survival curves from all cross-validation folds for a single evaluation, and report the resulting p-value.\nEmpirical evaluations were completed in R version 3.4.4. The implementations of km, aft, and cox-kp can all be found in the survival package [47] whereas coxen-kp uses the\ncocktail function found in the fastcox package [56]. Both rsf and rsf-km come from the randomForestSRC package [27]. An implementation of mtlr (and of all the code used in this analysis) is publicly available on the GitHub account16 of the lead author."
        },
        {
            "heading": "4.2 Empirical Results",
            "text": "Below, we consider a dataset to be \u201cNice\u201d if its feature-to-sample-size ratio was less than 0.05 (for the final feature set) and its censoring was less than 55%; this includes four of the 8 datasets: GBM, Nacd-Col, GLI, NACD \u2013 which are shown first in all of our empirical studies. We let \u201cHigh-Censor\u201d datasets refer to READ and BRCA and \u201cHigh-Dimensional\u201d datasets refer to the other two (DLBCL and DBCD)."
        },
        {
            "heading": "4.2.1 Concordance, Integrated Brier score, and L1-loss Results",
            "text": "Figures 9, 10 and 11 give the empirical results for Concordance, Integrated Brier score, and L1-Margin loss respectively, where each circle is the score of the associated model on the dataset, and lines correspond to one standard deviation (over the 5 cross-validation folds). Appendix D provides the exact empirical results for these measures.\nBest Performance: The blue circles represent the best performing models, for each dataset; here we find that mtlr performs best on a majority of datasets: six of eight for Concordance and L1-loss, and seven of eight for the Integrated Brier score.\nNice Datasets: Recall that the first 4 datasets are Nice. Here, we find that most models performed comparably \u2013 and in particular, aft and cox-kp perform nearly as well as the other, more complex, models. aft even performs best in terms of L1-loss on GBM. The only exception was rsf-km, which did much worse on GBM and GLI, in all three measures.\nkm was worse than the various isd-models for all 3 measures. (The only exception was rsf-km, which was worse on for the datasets GLI and GBM for Integrated Brier score, and for those datasets and also Nacd-Col for L1-loss.)\nHigh-Censor Datasets \u2013 READ and BRCA: Note first that the variance in the evaluation metrics is generally higher on READ for all models (except km) due to the small number of uncensored patients within each test fold \u2013 this is not present in BRCA due to the larger sample size (1095). Again we find that coxen-kpand mtlr are similar for all three measures, but rsf-km performs consistently worse across all three metrics for both READ and BRCA. aft and cox-kp are either comparable (or inferior) to the other three isd-models: Concordance: worse performance but within error-bars for READ and BRCA; Integrated Brier score: similar for both READ and BRCA; L1-loss: slightly worse for READ and BRCA. Additionally, aft and cox-kp tend to show higher variance in evaluation estimates on READ than other models for all three measures.\nkm is worse than all 5 isd-models for Concordance, but comparable to the best for Integrated Brier score and L1-loss (actually scoring better than cox-kp and aft for L1-loss on READ and beating cox-kp on BRCA).\n16https://github.com/haiderstats/ISDEvaluation\nHigh-Dimensional Datasets \u2013 DBCD and DLBCL: There are no entries for cox-kp for these two datasets as it failed to run on them, likely due to the large number of features. As aft is unregularized, it is not surprising that it does poorly across all measures for these high-dimensional datasets \u2013 indeed, even worse than km, which did not use any features! We see that the other three isd-models \u2013 coxen-kp, mtlr and rsf-km \u2013 perform similarly to one another here, and km also achieves similar results (ignoring Concordance where km always achieves 0.5, as it gives identical predictions for all patients)."
        },
        {
            "heading": "4.2.2 1-Calibration Results",
            "text": "Table 5 gives the number of datasets each model passed for 1-Calibration for each time of interest. We see that mtlr is typically 1-Calibrated across the percentiles of survival times. Specifically, mtlr is 1-Calibrated for at a minimum of four of eight datasets for the 10th, 25th, 50th, and 90th percentiles, outperforming all other models considered. The 90th percentile appear to be the most challenging in general, as some models (aft, cox-kp, rsfkm) are not 1-Calibrated for any datasets, coxen-kp is 1-Calibrated for two, and mtlr is 1-Calibrated for four. The 75th percentile also showed to be challenging, however aft, coxkp, and rsf-km were 1-Calibrated for one, coxen-kp is 1-Calibrated for two, and mtlr is 1-Calibrated for three. The most challenging datasets for rsf-km once again were GBM, GLI, BRCA, and READ, for which rsf-km was 1-Calibrated only at the 10th percentile for READ \u2013 see Appendix D.4. Additional challenging datasets include the complete NACD and DBCD which were challenging for all models. As km assigns an identical prediction for all patients, it cannot partition patients into different bins, meaning it cannot be evaluated by 1-Calibration."
        },
        {
            "heading": "4.2.3 D-Calibration Results",
            "text": "Table 6, which gives the D-Calibration p-values for each model and dataset, shows that both km and mtlr pass D-Calibration for every dataset, with km receiving the highest possible p-value, p =1.000, for each. (In fact, Lemma 2 in Appendix B.5 proves that km is asymptotically D-Calibrated). While km will tend to be D-Calibrated, it is also the least informative model, since it assigns all patients the same survival curve. mtlr is also D-\nCalibrated for all datasets, but in addition, it also provides each patients with his/her own survival curve.\nFollowing km and mtlr, coxen-kp performed next best, only failing to be D-Calibrated for one dataset: NACD. rsf-km followed closely behind, being D-Calibrated for five of eight datasets, failing on GBM, GLI, and NACD. aft performed similarly to cox-kp, each of which being D-Calibrated on three of eight datasets.\nFigure 12 provides (sideways) histograms, to help visualize D-calibration. For each subfigure, each of the 10 horizontal bars should be 10%; we see a great deal of variance for the not-D-Calibrated cox-kp [left], a small (but acceptable) variability for the D-Calibrated mtlr [middle], and essentially perfect alignment for the D-Calibrated km [right]. See also Figure 8."
        },
        {
            "heading": "5 Discussion",
            "text": "Comparing different isd-models: Steyerberg et al. [46] noted two different types of performance measures of a survival analysis model \u2013 calibration and discrimination \u2013 each of which can be assessed separately:\nCalibration: \u201cOf 100 patients with a risk prediction of x%, do close to x experience the event?\u201d\nDiscrimination: \u201cDo patients with higher risk predictions experience the event sooner than those who have lower risk predictions?\u201d\nDiscrimination is a very important measure for some situations \u2013 e.g., if we have 2 patients who each need a kidney transplant, but there is only a single kidney donor, then we want to know which patient will die faster without the transplant [30]. As discussed in Section 3.1, Concordance measures how well a predictor does, in terms of this discrimination task.\nThis paper, however, motivates and studies models that produce an individual survival curve for a specific patient. Such isd tools may not be optimal for maximizing discrimination (and therefore Concordance); and even tools like cox and rsf, that were originally developed for discrimination, were then extended to produce these individual survival curves. Given this qualifier, we see (over the set of isd tools tested), mtlr scored best on Concordance for six of the eight datasets tested and rsf-km scored the best on the other two. (The relatively low performance of cox-kp is unexpected given the claim that \u201ca method designed to maximize the Cox\u2019s partial likelihood also ends up (approximately) maximizing the [concordance]\u201d [45].) However, when we look at the Nice datasets, 4 of the 5 isdmodels give nearly identical results (rsf-km differs by giving noticeably lower performance on GBM and GLI). These findings suggest that, for Nice datasets, more complex models (mtlr, rsf-km, and coxen-kp) do not offer large benefits in terms of Concordance. For the High-Dimensional datasets, mtlr and coxen-kp performed only marginally better than rsf-km for DBCD but noticeably better than rsf-km on DLBCL. Although these are only two datasets, this suggests that rsf-km may not be optimal for these high-dimensional datasets, in terms of Concordance. For the High-Censor datasets rsf-km saw much worse performance for Concordance (among other metrics) suggesting rsf-km may not be suitable for datasets with a high proportion of censored data.\nAs noted above, Concordance is only one measure for an isd tool. Given that an isd tool can produce a survival curve for each patient (and not just a single real-valued score), it can be used for various tasks, with various associated evaluations. For example, consider patients who are deciding whether to undergo an intensive medical procedure. Using the plots from Figure 7, note that Patient C has a very steep survival curve with a low median survival time, while Patient A has a shallow survival curve with a large median survival time. If we were to use this to predict the outcome of a procedure, we might expect Patient C to opt-out of the procedure, but Patient A to go through with it. Note the decision for Patient C is completely independent of Patient A, in that we could give the procedure to one, both, or neither of them. As these patients are not being ranked for a limited procedure, Concordance is not\nan appropriate metric and instead we need to evaluate such predictors using a calibration score \u2013 perhaps 1-Calibration or D-Calibration, as discussed in Sections 3.3 and 3.5.\nAs discussed in Section 3.3, 1-Calibration is particularly relevant for [P,1t\u2217 ,i] models \u2013 i.e., models that produce a probability score for only 1 time point (for each patient). We also noted that isd models, that produce individual survival curves, can also be evaluated using 1-Calibration, once the evaluator has identified the relevant specific time t\u2217. Here, we evaluated a variety of time points: the 10th, 25th, 50th, 75th and 90th percentiles of survival times for each dataset. We found mtlr to be superior to all the models considered here for all percentiles. The observation that mtlr was 1-Calibrated for a range of time points, across a large number of diverse datasets, suggests that the probabilities assigned by mtlr\u2019s survival curves are representative of the patients\u2019 true survival probabilities; the observation that the other models were not 1-Calibrated as often, calls into question their effectiveness here.\nOf course, our analysis is performing the 1-Calibration test for 5 models (km is excluded) across 8 datasets and 5 percentiles, meaning we are performing 200 statistical tests. We considered applying some p-value corrections \u2013 e.g., the Bonferroni correction \u2013 to reduce the chance of \u201cfalse-positives\u201d, which here would mean declaring a model that was truly calibrated, as not. However, the actual p-values (see Appendix D.4) show that including these corrections would actually benefit mtlr the most, further strengthening the claim that mtlr has excellent 1-Calibration performance.\nOur D-Calibration results further support the use of mtlr\u2019s individual survival curves over other isd-models, by showing that mtlr was the only isd-model to be D-Calibrated for all datasets. (Recall that km is technically not an isd since it gives one curve for all patients.) We see that different isd-models are quite different for this measure \u2013 e.g., aft and cox-kp produce significantly worse performance for D-Calibration, being D-Calibrated for only three datasets. As discussed in Section 4.2, aft is a completely parametric model, which means it cannot produce different shapes (see Figure 4[top-right]), likely impacting its ability to be D-Calibrated. (Our analysis showed only that aft[Weibull] is here not D-Calibrated; aft[\u03c7] for some other distribution class \u03c7, might be D-Calibrated for more datasets.)\nIn addition to discussing discrimination (Concordance) and calibration (1-Calibration, DCalibration) separately, we can also consider a hybrid evaluation metric \u2013 the Integrated Brier score \u2013 which measures a combination of both calibration and discrimination \u2013 see Section 3.4 and Appendix B.4. We see mtlr performing the best for seven of the eight datasets, however, mtlr is no longer superior for DBCD, one of the high-dimensional datasets, even though it was superior for Concordance. Instead, coxen-kp, rsf-km, and mtlr all perform nearly identical for these High-Dimensional datasets.\nThe Integrated Brier scores, along with 1-Calibration and D-Calibration results, collectively show mtlr outperforms other models (for calibration), and is followed by coxenkp and rsf-km. Specifically, coxen-kp and rsf-km are competitive to mtlr for HighDimensional datasets \u2013 the 1-Calibration metric shows that both coxen-kp and rsf-km match the performance of mtlr for DLBCL (coxen-kp and mtlr are 1-Calibrated across\nall percentiles and rsf-km is 1-Calibrated across three of five, though p-values are very close to the 0.05 threshold for the other two). DBCD appeared to be the more challenging HighDimensional dataset \u2013 mtlr and coxen-kp were 1-Calibrated for two of five percentiles and rsf-km was 1-Calibrated for one. This, coupled with the findings for Integrated Brier Score and D-Calibration, suggest that coxen-kp, rsf-km and mtlr are equally competitive for modeling individual patients\u2019 survival probabilities when dealing with a large number of features. However, this does not apply to smaller-dimensional datasets.\nrsf-km was not 1-Calibrated across any percentiles for GBM, GLI, BRCA, and only 1-Calibrated at the 10th percentile for READ, and was not D-Calibrated for GBM and GLI. This, along with the poor performance of rsf-km for all measures of GBM, GLI, READ, and BRCA suggests that rsf-km does not produce effective individual survival curves for low-dimensional datasets. Other experiments (not shown) suggest that rsf-km tends to overfit to the training set when given too few features. Additional meta-parameter tuning in these experiments was unable to correct for overfitting.\nGiven that survival prediction looks very similar to regression, it is tempting to evaluate such models using measures like L1-loss (which can lead to models like censored support vector regression [43]). A small L1-loss shows that a model can help with many important tasks, such as decisions about hospice, and for deciding about various treatments, based on their predicted survival times. However, simply because a model has the best performance for L1-loss does not mean the estimates are useful \u2013 consider the complete NACD dataset where mtlr has the best performance with an average L1-loss of 43.97 months. While this is the lowest average error, predicting the time of death up to an error of 43.97 months (\u22483.7 years) is likely not helpful to a patient, especially as the maximum follow-up time was 84.3 months.\nWhile the best model may not represent a \u201cgood\u201d model, our empirical results still showed mtlr had the lowest L1-loss on six of eight datasets, although all isd models performed comparably for the four Nice datasets (with the exception of rsf-km). We see that km is also competitive for the High-Censor datasets, but given the construction of the L1Margin loss, this is not surprising; see Appendix B.2. Moreover, the three complex models (coxen-kp, rsf-km, mtlr) appear comparable for the High-Dimensional datasets.\nWe also compared the models in terms of \u201cUncensored L1-loss\u201d, which just considers the loss on the uncensored instances; see Table 11 in Appendix D.3. We see km is no longer competitive for the High-Censor datasets, showing how influential this effect is. Instead, at least one of the complex models {coxen-kp, rsf-km, mtlr} outperforms aft and cox-kp for every dataset.\nThat appendix also motivates and defines the Log L1-loss, and its Table 12 shows that mtlr performs best in 4 of the datasets, and is either second or third best in the others.\nWhich isd-Model to Use?: As shown above, which isd-model works best depends on properties of the dataset, and on what we mean by \u201cbest\u201d. Table 7 summarizes our results here.\nIn general, for Nice datasets, mtlr was superior for calibration but for discrimination, all isd-models were equivalent, leading us to recommend using the simplest models: (cox-\nkp, aft). As we found that rsf-km would overfit the training data when the number of features was small (here, less than 34), we recommend avoiding rsf-km when there are so few features.\nFor High-Censor datasets, we recommend mtlr or coxen-kp when there are not many features (e.g., READ, BRCA) for both calibration and discrimination. Typically cox-kp and aft had poor performance and high variability for High-Censor datasets. For HighDimensional datasets with low censoring (less than 70% i.e., DLBCL), mtlr, coxen-kp, and rsf-km had the best performance for calibration. For discrimination, rsf-km seemed slightly worse for Concordance and Brier score, suggesting it may be a weaker model.\nTo explore whether examine if these findings hold in general, we examined 33 other public datasets \u2013 16 (Low Dimension, Low Censoring), 12 (Low Dimension, High Censoring), 4 (High Dimension, Low Censoring) and 1 (High Dimension, High Censoring) where High Censoring is \u2265 70%. Note that all Low Dimensional datasets were taken from the TCGA website whereas the other (High Dimensional) datasets arise from a variety of sources. The results from these 33 datasets are consistent with the findings reported here; specific results can be found on the lead author\u2019s RPubs site17. Given the low overall number of High-Dimensional datasets, these findings should be examined on further datasets.\nWhy use isd-Models?: As noted above, this paper considers only models that generate isds (i.e., [P,\u221e,i]). This is significantly different from models that only generate risk scores ([R,1\u2200,i]), as those models can only be evaluated using a discriminatory metric. While this discrimination task (and hence evaluation) is helpful for some situations (e.g., when deciding which patients should receive a limited resource), it is not helpful for others (e.g., deciding whether a patient should go to a hospice, or terminate a treatment). A patient\u2019s primary focus will be on his/her own survival, not how they rank among others \u2013 hence the risk score such models produce do not meaningfully inform individual patients.\nThe single point probability models, [P,1t\u2217 ,i], are a step in the direction for benefiting patients, but they are still often inadequate, as they apply only to a single time-point. While hospital administrators may want to know about specific time intervals (e.g., t\u2217 =\u201c30- day readmission\u201d probabilities), medical conditions seldom, if ever, are so precise. This is problematic as these probabilities can change dramatically over a short time interval \u2013 i.e., whenever a survival curve has a very steep drop. For example, consider Patient #5 (P5) in Figure 4 for the mtlr model. Here, we would optimistic about this patient if we considered\n17See http://rpubs.com/haiderstats/ISDEvaluationSupplement\nthe single point probability model at t\u2217= 6months, as S\u0302MTLR(P5 | 6months ) = 0.8, but very concerned if we instead used t\u2217 = 12months, as S\u0302MTLR(P5 | 12months ) = 0.3. Note this trend holds for the other isd-models shown; and also for many of the patients, including P6, P7, P10.\nThis suggests a model based on only a single time point may lead to inappropriate decisions for a patient. Note also that such a model might not even provide consistent relative rankings over a pair of patients \u2013 i.e., it might provide different discriminative conclusions. Consider patients P2 and P9 in Figure 4[mtlr]. Here, at t\u2217 = 20months, we would conclude that the purple P9 is doing worse (and so should get the available liver), but at t\u2217= 30months, that the orange P2 is more needy. (We see similar inversions for a few other pairs of patients in mtlr, and also for several pairs in the rsf model.)\nOf course, one could argue that we just need to use multiple single-time models. Even here, we would need to a priori specific the set of time points \u2013 should we use 6 months and 12 months, and perhaps also 30 months? And maybe 20 months?\nThis becomes a non-issue if we use individual survival distribution (isd; [P,\u221e,i]) models, which produce an entire survival curve, specifying a probability values for every future time point. Moreover, while risk score models can only be evaluated using a discrimination metric, these isd models can be evaluated using all metrics, making them an overall more versatile method for survival analysis.\nBottom line: In general, a survival task is based on both a dataset, and an objective, corresponding to the associated evaluation measure. Our isd framework is an all-around more flexible approach, as it can be evaluated using any of the 5 measures discussed here (Section 3) \u2013 both commonly-used and alternative. Importantly, when evaluating isd models discriminatively (using Concordance), the risk scores we advocate (mean/median survival time) have meaning to clinicians and patients, whereas a general risk score, in isolation, has no clinical relevance. Moreover, the resulting survival curves are easy to visualize, which adds further appeal."
        },
        {
            "heading": "6 Conclusion",
            "text": ""
        },
        {
            "heading": "Future Work:",
            "text": "This paper has focused on the most common situation for survival analysis: where all instances in the training data are described using a fixed number of features (see the matrix in Figure 2), there is no missing values, and each instance either has a specified time of death, or is right-censored \u2013 i.e., we have a lower bound on that patient\u2019s time of death. There are many techniques for addressing the first two issues \u2013 such as ways to \u201cencode\u201d a time series of EMRs as a fixed number of features, or using mean imputations. There are also relatively easy extensions to some of the models (e.g., mtlr) to handle left-censored instances (where the dataset specifies an upper-bound on the patient\u2019s time of death), or interval censored. These extensions, however, are beyond the scope of the current paper.\nContributions:\nThis paper has surveyed several different approaches to survival analysis, including assigning individualized risk scores [R,1\u2200,i], assigning individualized survival probabilities for a single time point [P,1t\u2217 ,i], modeling a population level survival distribution, [P,\u221e,g], and primarily isd (individual survival distribution; [P,\u221e,i]) models. We discussed the advantages of having an individual survival distribution for each patient, as this can help patients and clinicians to make informed decisions about treatments, lifestyle changes, and end-oflife care. We discussed how isd models can be used to compute Concordance measures for discrimination and L1-loss, but should primarily be evaluated using calibration metrics (Sections 3.3, and 3.5) as these measure the extent to which the individual survival curves represent the \u201ctrue\u201d survival of patients.\nNext, we identified various types of isd-models, and empirically evaluated them over a wide range of survival datasets \u2013 over a range of #features, #instance and %censoring. This analysis showed that mtlr was typically superior for the L1-loss, Integrated Brier score, and Concordance, but most importantly, showed it outperformed or matched all other models for the calibration metrics.\nIn conclusion, this paper explains why we encourage researchers, and practioners, to use isd-models (and especially ones similar to mtlr) to produce meaningful survival analysis tools, by showing how this can help patients and clinicians make informed healthcare decisions."
        },
        {
            "heading": "Acknowledgements",
            "text": "We gratefully acknowledge funding from NSERC, Amii, and Borealis AI (of RBC). We also thank Adam Kashlak for his insightful discussions regarding D-Calibration."
        },
        {
            "heading": "A Extending Survival Curves to 0",
            "text": "In practice, survival curves often stop at a non-zero probability \u2013 see Figure 4 and Figure 13[left] below. This is problematic as it means they do not correspond to complete distribution (recall a survival curve should be \u201c1\u2212CDF(t)\u201d, where CDF is the Cumulative Distribution Function) which leads to problems for many of the metrics, as it is not clear how to compute the mean, or the median, value of the distribution. One approach is to extend each of the curves, horizontally, to some arbitrary time and then drop each to zero (the degenerate case being dropping the survival probability to zero at the last observed time point). This approach has downsides: Dropping the curve to zero at the last observed time point produces curves whose mean survival times are actually a lower bound on the patient\u2019s mean survival time, which is likely too small. In the event that the last survival probability is above 0.5 (as is often the case for highly censored datasets) this may bias our estimate of the L1-loss, which is based on the median value. Alternatively, if we instead extend each curve to some arbitrary time and then drop the curve to zero, we need to decide on that extension, which also could bias the L1-loss.\nSince both standard approaches have clear downsides (and there is no way of knowing how the survival curves act beyond the sampled survival times), we chose to simply extrapolate survival curves using a simple linear fit: for each patient ~xi, draw a line from (0, 1) \u2013 i.e., time is zero and survival probability is 1 \u2013 to the last calculated survival probability, (tmax, S\u0302( tmax | ~xi )), then extend this line to the time for which survival probability equals 0 \u2013 i.e., (t0(~xi), 0) \u2013 see Figure 13[right]. Note that curves cannot cross within the extended interval, which means this extension will not change the discriminatory criteria.\nThere are extreme cases where a survival model will predict a survival curve with survival probabilities of 1 (up to machine precision) for all survival times (think \u201ca horizontal line, at p = 1\u201d) \u2013 this occurred for unregularized models on high-dimensional datasets. In these cases, this linear extrapolation will never reach 0. To address this, we fit the Kaplan-\nMeier curve with the linear extension described above to compute t0KM ; we then replace any infinite prediction with this value. Additionally, as the Kaplan-Meier curve is to represent the survival curve on a population level, we also truncated any patient\u2019s median survival time by t0KM ."
        },
        {
            "heading": "B Evaluation Measures Supplementary Information",
            "text": "This appendix provides additional information about the various evaluation measures."
        },
        {
            "heading": "B.1 Concordance",
            "text": "As discussed in Section 3.1, Concordance is designed to measure the discriminative ability of a model. This is challenging for censored data. For example, suppose we have two patients who were censored at t1 and t2. Since both patients were censored, there is no way of knowing which patient died first and hence the risk scores for these patients are incomparable. However, if one patient\u2019s censored time is later than the death time of another patient, we do know the true survival order of this pair: the second patient died before the first.\nTo be precise, we first need to define the set of comparable pairs, which is the subset of pairs of indices (here using the validation dataset (V ) and recalling that \u03b4 = 1 indicates a patient who died (uncensored)) containing all pair of instances when we know which patient died first:\nCP(V ) = {[i, j] \u2208 V \u00d7 V | ti < tj and \u03b4i = 1 } (13)\nNotice when the earlier event is uncensored (a death), we know the ordering of the deaths (whether the second time is censored or not) \u2013 see Figure 14. The ti < tj condition is to prevent double-counting such that |CP(V )| \u2264 (|V |\n2\n) .\nWe then consider how many of the possible pairs our predictor put in the correct order: That is, of all [i, j] pairs in CP(V ), we want to know how often r(~xi) > r(~xj) given that ti < tj. Hence, the Concordance index of V , with respect to the risk scores, r(\u00b7), is\nC\u0302(V, r(\u00b7) ) = 1 |CP(V )| \u2211 i:\u03b4i=1 \u2211 j: ti<tj I [ r(~xi) > r(~xj) ] . (14)\nOne issue is how to handle ties, in either risk scores or death times \u2013 i.e., for two patients, Patient A and Patient B, consider either r(~xA) = r(~xB) or dA = dB. The two standard approaches are (1) to give the model a score of 0.5 for ties (of either risk scores or death times), or (2) to remove tied pairs entirely [54]. The first option is equivalent to Kendall\u2019s tau, while the second leads to the Goodman-Kruskal gamma. The empirical evaluations (given in Section 4.2) use the first, as this gives Kaplan-Meier a Concordance index of 0.5 for all models. If we use the second option (excluding ties), then the Concordance for the Kaplan-Meier model is not well-defined."
        },
        {
            "heading": "B.2 L1-loss, and variants",
            "text": "As discussed in Section 3.2, survival analysis can be viewed as a regression problem that is attempting to minimize the difference between an estimated time of death and the true time of death. However, typical regression problems require having precise target values for each instance; here, many instances are censored \u2013 i.e., providing only lower bounds for the target values. One option is to simply remove all the censored patients and use the L1-loss given by Equation 4 (which we call \u201cUncensored L1-Loss\u201d); however, this will likely bias the true loss. Table 11 in Appendix D.3 provides the results for this Uncensored L1-loss over the 8 datasets. (We see that mtlr is best for 6 of these datasets.)\nOne way to incorporate censoring is to use the Hinge loss for censored patients, which assigns 0 loss to any patient whose censoring time ck is prior to the estimated median survival time, t\u0302\n(0.5) k \u2013 i.e., a loss of 0 if ck < t\u0302 (0.5) k \u2013 and a loss of ck \u2212 t\u0302 (0.5) k if the censoring time is\ngreater than t\u0302 (0.5) k . That is:\nL1hinge(V, {t\u0302(0.5)j }j ) = 1\n|V | [ \u2211 j\u2208VU |dj \u2212 t\u0302(0.5)j | + \u2211 k\u2208VC [ck \u2212 t\u0302(0.5)k ]+ ] . (15)\nwhere VU is the subset of the validation dataset that is uncensored, and VC is the censored subset, and [a]+ is the positive part of a, i.e.,\n[a]+ = max{a, 0} = { a if a \u2265 0 0 otherwise .\nThis formulation is an optimistic lower bound on the L1-loss for two reasons: (1) it gives a loss of 0 if the censoring occurs prior to the estimated survival time, implying that dk = t\u0302 (0.5) k ,\nand (2) it gives a loss of ck \u2212 t\u0302(0.5)k if the censoring time occurs after the estimated survival time, which assumes that dk = ck. Both are the best possible values for the unknown dk, given the constraints..\nOne weakness of the L1-Hinge loss is that if a model predicts very large survival times for all patients (both censored and observed), the hinge loss will give 0 loss for the censored patients; in datasets with a large proportion of censored patients, this leads to an optimistic score overall. Thus the hinge loss will favor models that tend to largely overestimate survival times as opposed to those models underestimating survival time.\nA third variant of L1-loss, the L1-Margin loss, assigns a \u201cBest-Guess\u201d value to the death time corresponding to ck, which is the patient\u2019s conditional expected survival time given they have survived up to ck \u2013 given by\nBG(ck) = ck +\n\u222b\u221e ck S(t) dt\nS(ck) (16)\nwhere S(\u00b7) is the survival function; Theorem B.1 proves this value corresponds to the conditional expectation. In practice we use Kaplan-Meier estimate, S\u0302KM( \u00b7 ), generated from the training dataset (disjoint from the validation dataset) as our estimate of S(\u00b7) in Equation 16.\nWe also realized that these BG(ck) estimates are more accurate for some patients, than for others. If ck \u2248 0 \u2013 that is, if the patient was censored near the beginning time \u2013 then we know very little about the true timing of when the death occurred, so the estimate BG(ck) is quite vague, which suggests we should give very little weight to the associated loss, |BG(ck)\u2212 t\u0302(0.5)k |. Letting \u03b1k be the weight associated with these terms, we would like \u03b1k \u2248 0. On the other hand, if cr is large \u2013 towards the longest survival time observed (call it dmax) \u2013 then there is a relatively narrow gap of time where this ~xr could have died (probably within the small interval (cr, dmax)); here, we should give a large weight to loss associated with this estimate.\nThis motivates us to define\nL1margin(V, {t\u0302(0.5)j } ) = 1 |VU |+ \u2211\nk\u2208VC \u03b1k [ \u2211 j\u2208VU |dj \u2212 t\u0302(0.5)j | + \u2211 k\u2208VC \u03b1k|BG(ck)\u2212 t\u0302(0.5)k | ] (17)\nwhere \u03b1k reflects the confidence in each Best-Guess estimate. To implement this, we set \u03b1k = 1\u2212S\u0302KM(ck), which gives little weight to instances with early censor times but considers late censor times to be almost equivalent to an observed death time. Note this is the version of L1-loss we presented in Figure 11, with details in Table 10.\nFor completeness, we prove Equation 16. (This claim is also proven by Gupta and Bradley [20], which uses mean residual life rather than expected total life.)\nTheorem B.1. The conditional expectation of time of death, D, given that a patient was censored at time c, is given by: E[D |D > c] = c+ \u222b\u221e c S(x) dx\nS(c) .\nProof. Let D be the r.v. for the time when a patient dies, and define\nS(c) = P (D > c) =\n\u222b \u221e c P (D = t) dt\nas the survival function \u2013 i.e., the probability that the patient dies after time c. Given this, the conditional probability is\nP (D = t |D > c ) = P (D = t, D > c ) P (D > c )\n= P (D = t, D > c )\nS( c ) =\n{ 0 if t < c\nP (D=t ) S( c )\notherwise .\nE[D |D > c ] = \u222b \u221e c t P (D = t ) S( c ) dt\n= 1\nS( c ) [\u222b \u221e c c P (D = t ) dt + \u222b \u221e c (t\u2212 c)P (D = t ) dt ]\n= 1\nS( c )\n[ c S( c ) + \u222b \u221e c (\u222b t c dx ) P (D = t) dt ] = c + 1\nS( c ) [\u222b \u221e c (\u222b \u221e x P (D = t) dt ) dx ] (18)\n= c +\n\u222b\u221e c S(x ) dx\nS( c ) .\nStep 18 is an application of Tonelli\u2019s theorem [41], which lets us swap the order of integration for a non-negative function. As desired, this quantity, E[D |D > c ], is always at least c. Moreover, when c = 0, this is\n0 +\n\u222b\u221e 0 S( t ) dt\n1 =\n\u222b \u221e 0 S( t ) dt = E[D ]\nwhich is the expected value of the distribution for this survival curve (and exactly the claim of the Theorem)."
        },
        {
            "heading": "B.2.1 Log L1-loss",
            "text": "The L1-loss measure implicitly assumes that the quality of a prediction, t\u0302 (0.5) j , depends only on how close it is to the truth dj\u2013 i.e., on |dj\u2212 t\u0302(0.5)j |. But this does not always match how we think of the error: if we predict Patient A will live for 120 months then found that he actually lived 117 months, we would consider our prediction very accurate. By contrast, if we predict Patient B will live 1 month, but then find she lived 4 months, we would consider this to be a poor prediction. Notice, however, the L1-loss for Patient A is |dA\u2212 t\u0302(0.5)A | = |120\u2212 117| = 3 months, which is the same as the L1-loss for Patient B: |dB \u2212 t\u0302(0.5)B | = |1\u2212 4| = 3 months!\nThis motivates us to consider the relative error, rather than an absolute error: here, as our prediction for Patient A is off by only 3 / 120 = 2.5%, we consider it good, whereas our\nprediction for Patient B is off by 3 / 1 = 300%. The Log-L1-loss reflects this:18\n`LogL1( di, t\u0302 (0.5) i ) = | log(di)\u2212 log(t\u0302 (0.5) i )| (19)\nTo compute the average Log-L1-loss over the dataset VU , we can use Equation 4 but using log(dj) rather than dj, etc. To avoid taking log 0, we replace 0 with half the minimum, positive death time (see Section B.6). Table 12 in Appendix D.3 provides the results here, over the 8 datasets. (We see that mtlr is best for 4 of these datasets.)\nB.3 1-Calibration\nTo demonstrate the description from Section 3.3, consider the following example: If there are n = 50 patients, then 50/10 = 5 will be in each bin, and the first bin B#1 will contain the 5 with lowest predicted probability values, and the second bin B#2 will contain the next smallest 5 values, and so forth \u2013 e.g.,\nB#1 = {0.32, 0.34, 0.43, 0.43, 0.48} B#2 = {0.55, 0.56, 0.61, 0.61, 0.72}\n...\nB#10 = {0.85, 0.85, 0.86, 0.87, 0.87}\nNow consider the 5 patients who belong to B#1. As the average of their probabilities is 0.32+0.34+0.43+0.43+0.48\n5 = 0.4, we should expect 40% of these 5 individuals to die in the next\n5 years \u2013 that is, 2 should die. We can then compare this prediction (0.40\u00d7 5 = 2) with the actual number of these B#1 patients who died. We can similarly compare the number of B#2 patients who actually died to the number predicted (based on the average of these 5 probability values, which here is 0.61\u00d7 5 = 3.05), and so forth.\nIn general, we say that the predictor is 1-Calibrated if these B predictions, for the B = 10 bins, are sufficiently close to the actual number of deaths with respect to these bins. Here, we use the Hosmer\u2013Lemeshow statistical test (given in Section 3.3) to see if the observed results were significant; repeating Equation 5:\nHL \u2227 (VU , S\u0302( t \u2217 | \u00b7 ) ) =\nB\u2211 j=1 (Oj \u2212 nj p\u0304j)2 nj p\u0304j (1\u2212 p\u0304j) ,\nwhere Oj is the number of observed events, nj is the number of patients, p\u0304j is the average predicted probability, and subscript j refers to within the jth of B bins.\n18 Note that the times mentioned in \u201cDoc, do I have a day, a week, a month or a year?\u201d are basically in a log-scale.\nB.3.1 Incorporating Censoring into 1-Calibration\nSurvival data typically contains some amount of censoring, making the exact number of deaths for the jth bin, Oj, unobservable when the bin contains patients censored before t\n\u2217. That is, given a censored patient whose censoring time occurred before the time of interest (ci < t\n\u2217) the patient may or may not have died by t\u2217. There are many standard techniques for incorporating censoring [19]; we use the D\u2019Agostino-Nam translation [12], which uses the within bin Kaplan-Meier curve in place of Oj. Specifically, the test statistic is given by,\nHL \u2227 DN (V, S\u0302( t \u2217 | \u00b7 ) ) =\nB\u2211 j=1 ( nj (1\u2212KMj(t\u2217)) \u2212 nj p\u0304j)2 nj p\u0304j (1\u2212 p\u0304j) , (20)\nwhere KMj(t \u2217) is the height of the Kaplan-Meier curve generated by the patients in the jth bin, evaluated at t\u2217. We use 1\u2212KMj(t\u2217) as we are predicting the number of deaths and not KMj(t \u2217) which instead gives the probability of survival at t\u2217. Note also that HL \u2227\nDN follows a \u03c72B\u22121 distribution, as opposed to the \u03c7 2 B\u22122 distribution for Equation 5."
        },
        {
            "heading": "B.4 Brier Score Details",
            "text": "This section supplements the description of the Brier score given in Section 3.4, discussing (1) the decomposition of the Brier score into calibration and discrimination components, (2) the failure of the Integrated Brier score to incorporate the full distribution of probabilities in survival curves, and (3) how to incorporate censoring into the Brier score."
        },
        {
            "heading": "B.4.1 Brier Score Decomposition",
            "text": "As mentioned in Section 3.4, the Brier score can be separated into calibration and discriminatory components. The original separations were the the work of Sanders [42] and Murphy [36, 37] and later put into the context of calibration and discrimination (also known as refinement) by DeGroot and Fineberg [13].\nRecall the notation and mathematical expression of the Brier score for a set of uncensored instances, VU ,\nBS ( S\u0302( t\u2217 | \u00b7 ), {~xi} ) =\n1 |VU | \u2211 i\u2208VU ( I [ di \u2264 t\u2217 ]\u2212 S\u0302(t\u2217|~xi) )2 .\nTo simplify notation, let pi = S\u0302( t \u2217 | ~xi ). The separation of the Brier score requires that a discrete, distinct number of predictions exist; here, assume there are K distinct values for pk for k = 1, . . . K.\nFurther, let nk be the total number of patients with pk as their prediction and hence |VU | = \u2211K k=1 nk. Finally, let \u03bbk be the observed proportion of patients who have died by t \u2217 and thus (1 \u2212 \u03bbk) is the proportion still alive. The separation theorem of the Brier score\nstates that BS = C + D, where C and D are nonnegative calibration and discriminatory scores where\nC = 1\n|VU | K\u2211 k=1 nk(\u03bbk \u2212 pk)2 (21)\nD = 1\n|VU | K\u2211 k=1 nk\u03bbk(1\u2212 \u03bbk). (22)\nNote the calibration score, C, is nearly equivalent (up to a factor of nk) to the numerator of the Hosmer-Lemeshow test (Equation 5). However, the Hosmer-Lemeshow test subscript refers to bins whereas here the subscript refers to a distinct value of pk. One can see that C represents a calibration score as the estimated probabilities, pk, must be close to the true proportion of deaths, \u03bbk in order to have a small score (lower is better). In fact, to satisfy C = 0, all predictions, pk must be equal to \u03bbk (Equation 21).\nThere are also similarities between D and the denominator of the Hosmer-Lemeshow test. However, note Equation 22 uses the the true proportion of deaths \u03bbk, whereas the Hosmer-Lemeshow test uses an estimated value, p\u0304. Note that D has a \u201cgood\u201d (low) score if all patients associated with a prediction probability pk have the same status \u2013 i.e., they either all die or are all still alive. To understand why this means D is a discriminatory measure, consider the extreme case where BS(\u00b7, \u00b7) = 0, which means both D = 0 and C = 0. For D = 0, all patients associated with each probability value must either be dead by t\u2217 or all be alive at t\u2217 \u2013 i.e., \u03bbk \u2208 {0, 1} for k = 1, 2; note only K = 2 is possible here. In turn, for C = 0, we require pk = \u03bbk for k = 1, 2, that is pk \u2208 {0, 1} \u2013 all predictions will be 1 or 0. Here we are discriminating perfectly between the patients who have died and the patients who are still alive, with a model that predicts only 1\u2019s or 0\u2019s. Of course, we should not require a model to estimate survival probabilities to be precisely 1 or 0, for the same reason that we do not expect the learned distribution to correspond to the Heaviside distribution shown in Figure 5.\nB.4.2 Integrated Brier score does not involve the Entire Distribution\nAt the beginning of Section 3.5, we claimed the Integrated Brier score (IBS) does not utilize the survival curves\u2019 full distribution of probabilities over all times. For example, on a km curve, we expect that 10% of patients will die in every 10% interval \u2013 e.g., 10% of all patients will die in the [0.5, 0.6) interval. While D-Calibration will debit a model that fails to do this, this Integrated Brier score does not require this. The most obvious example is the perfect model, where each patient is given the appropriate Heaviside distribution (Figure 5) at his/her time-of-death: here the only probabilities are {0,1} \u2013 here IBS(\u00b7, \u00b7) = 0, even though no patient\u2019s S\u0302Heaviside( di | ~xi ) is ever in [0.5, 0.6). However, as we have previously noted, the inherent stochasticity of the world means that meaningful distributions should include non-zero probabilities in other places as well, rather than placing all weight on a single time point.\nSince the Integrated Brier score fails to account for this, there is no guarantee that probabilities are meaningful across individual survival curves. This motivated us to introduce DCalibration, to determine whether a proposed isd-model produces meaningful distributions, with probabilities that reflect the number of deaths that have occurred in the population. To see that these two metrics are measuring different aspects, note that the Integrated Brier scores for the (aft, cox-kp, coxen-kp, and mtlr) models are all well within 1 standard error of one another for the GBM dataset, but only coxen-kp and mtlr are D-Calibrated. (This is also true for the GLI dataset.)\nB.4.3 Incorporating Censoring into the Brier score\nIn 1999, Graf et al. [18] proposed a way to compute the Brier Score for censored data, by using inverse probability of censoring weights (IPCW), which requires estimating the censoring survival function, denoted as G\u0302(t) over time points t. We can estimate G\u0302(t) by the Kaplan-Meier curve of the censoring distribution \u2013 i.e., swapping those who are censored with those who are not, (\u03b4Censi = 1 \u2212 \u03b4i) and building the standard Kaplan-Meier model. Intuitively, this IPCW weighting counteracts the sparsity of later observations \u2013 if a patient dies early, there is a good chance that di < ci meaning the event is observed, but if the patient survives for a long time, it becomes more likely that ci < di meaning this patient will be censored. Gerds et al. [16, 17] formalizes and proves this intuition.\nThe censored version of the Brier score for a given time, t\u2217, is calculated as\nBSt\u2217 ( V, S\u0302(t\u2217|\u00b7) ) = 1\n|V | |V |\u2211 i=1\nI [ ti \u2264 t\u2217, \u03b4i = 1 ] ( 0\u2212 S\u0302(t\u2217|~xi) )2\nG\u0302(ti) + I [ ti > t\u2217 ]\n( 1\u2212 S\u0302(t\u2217|~xi) )2 G\u0302(t\u2217)  , (23) where ti = min{di, ci}. The first part of Equation 23 considers only uncensored patients whereas the second part counts all patients whose event time is greater than t\u2217. The patients who were censored prior to t\u2217 are not explicitly included, but contribute based on their influence in G\u0302(\u00b7).\nAs G\u0302(t) is a decreasing step function of t, 1 G\u0302(t) is increasing, which means that patients who survive longer than t\u2217 have larger weights than patients that died earlier, since the longer surviving patients were more likely to become censored."
        },
        {
            "heading": "B.5 D-Calibration",
            "text": "We begin this section by justifying why, in the case of all uncensored patients, (1) the distribution of the survival function, {S(t)}t, should follow a uniform distribution, then (2) Following this discussion, we show how to incorporate censored patients into the DCalibration estimate, and finally, (3) that this combination of censored and uncensored patients will produce a uniform distribution for the goodness-of-fit test to test against.\nFor this analysis, we assume each patient ~xi has a true survival function, S( t | ~xi ), which is the probability that this patient will die after time t. Assume each patient has a time of death, di and a censoring time, ci, and ti = min {di, ci} is the observed event time. We also\nassume that censoring time is independent of death time, ci \u22a5 di. Given a validation set |V |, we first examine the case of all uncensored patients \u2013 i.e., ti = di for i = 1, . . . , |V |.\nLemma 1. The distribution of a patient\u2019s survival probability at the time of death S( di | ~xi ) is uniformly distributed on [0,1].\nProof. The probability integral transform [4] states that, for any random continuous variable, X, with cumulative distribution function given by Fx(\u00b7), the random variable Y = Fx(X) will follow a uniform distribution on [0,1], denoted as U(0, 1). Thus, given randomly sampled event times, t, we have F (t) \u223c U(0, 1). As the survival function is simply S(t) = 1 \u2212 F (t), its distribution is 1\u2212 U(0, 1), which also follows U(0, 1) and hence S(t) \u223c U(0, 1).\nThis Lemma shows that, given the true survival model, producing S( \u00b7 | ~xi ) curves, the distribution of S( di | ~xi ) should be uniform over event times. Thus if a learned model accurately learns the true survival function, S\u0302\u0398( \u00b7 | \u00b7 ) \u2248 S(\u00b7|\u00b7), we will expect the distribution across event times to be uniform. This is then tested using the goodness-of-fit test assuming each bin contains an equal proportions of patients.\nOf course, conditions become more complicated when considering censored patients. Suppose we have a censored patient \u2013 i.e., ti = ci \u2013 such that S( ci | ~xi ) = 0.25. Since the censoring time is a lower bound on the true death time, we know that S( di | ~xi ) \u2264 0.25, since ci < di and survival functions are monotonically decreasing as event time increases. If we are using deciles, we would like to know the probability that the time of death occurred in the [0.2,0.3) bin \u2013 i.e., P ( S(di|~xi) \u2208 [0.2, 0.3) | S(di|~xi) \u2264 0.25). Using the rules of conditional probability, this is computationally straightforward19:\nP (S(di) \u2208 [0.2, 0.3) |S(di) < 0.25 ) = P (S(di) \u2208 [0.2, 0.3), S(di) < 0.25 )\nP (S(di) < 0.25 )\n= P (S(di) \u2208 [0.2, 0.25)) P (S(di) < 0.25)\n= 0.05\n0.25 (as S(\u00b7) \u223c U(0, 1))\n= 0.2\nSimilarly, we can use the same logic as above to compute these probabilities for the other two bins, [0.1, 0.2) and [0.0, 0.1):\nP (S(di) \u2208 [0.1, 0.2) |S(di) < 0.25 ) = P (S(di) \u2208 [0.1, 0.2), S(di) < 0.25)\nP (S(di) < 0.25)\n= P (S(di) \u2208 [0.1, 0.2)) P (S(di) < 0.25)\n19To simplify notation, we drop the conditioning on ~xi of S(\u00b7|\u00b7).\n= 0.1\n0.25 (as S(\u00b7) \u223c U(0, 1))\n= 0.4\nand similarly for the [0.0, 0.1) bin. Note that these probabilities sum to one, (0.2+0.4+0.4) = 1, as desired.\nThis example motivates the following procedure to incorporate censored patients into the D-Calibration process: Given B bins that equally divide [0,1] into intervals of width BW = 1/B, suppose a patient is censored at time c with associated survival probability S(c). Let b1 be the infimum probability of the bin that contains S(c) \u2013 e.g., 0.2 for the example above where S(ci) = 0.25 \u2208 [0.2, 0.3). Then we assign the following weights to bins:\n(A) Bin [b1, b2) (which contains S(c)): S(c)\u2212b1 S(c) = 1\u2212 b1 S(c)\n(B) All following bins (i.e., the bins whose survival probabilities are all less than b1): BW S(c) = 1 B\u00b7S(c) ,\nNote this formulation follow directly from the example above. This weight assignment effectively \u201cblurs\u201d censored patients across the bins following the bin where the patient\u2019s learned survival curve, S\u0302\u0398( ci | i ) placed the censored patient.\nTo further illustrate this concept of blurring a patient across bins, consider a patient who is censored at t = 0 with S(ci) = 1. This patient is then blurred across all (B = 10) bins, adding a weight of 0.1 to all 10 bins. Alternatively, if a patient is censored very late, with S(ci) \u2264 0.1 then the patient is not blurred at all \u2013 only a weight of 1 is added to the last bin.\nThis identifies a weakness of D-Calibration: if a validation set contains N0 patients censored at time 0, then all bins are given an equal weight of N0/B; if N0 is large relative to the total number of patients, then the bins may appear uniform, no matter how the other patients are distributed, which means any model based on such heavily \u201ctime 0 censored\u201d data would be considered to be D-Calibrated.\nTo perform the goodness-of-fit test, we must first calculate the observed proportion of patients within each bin. Let Nk represent the observed proportion of patients within the interval [pk, pk+1) \u2013e.g., [pk, pk+1) = [0.2, 0.3) in the example above. We can formally calculate:\nNk = 1\n|V | |V |\u2211 i=1 [ I [S(di) \u2208 [pk, pk+1) \u2227 di \u2264 ci ] (24)\n+ S(ci)\u2212 pk S(ci) \u00b7 I [S(ci) \u2208 [pk, pk+1) \u2227 ci < di ] (25) + (pk+1 \u2212 pk)\nS(ci) \u00b7 I [S(ci) \u2265 pk+1 \u2227 ci < di ]\n] . (26)\nAbove, (24) refers to the weight that the patients with observed events contribute to the kth bin \u2013 i.e., each uncensored patient whose survival probability at time of death lands in [pk, pk+1) contribute a value of 1. Here, we consider di = ci to be an uncensored event. Next, (25) gives the weight from the censored patients whose survival probability at time of censoring is within the kth bin (item (A) above). Lastly, (26) gives the weights from censored patients whose survival probability was contained in a previous bin (item (B) above).\nTheorem B.2 below proves that the expected value ofNk is equal for all bins \u2013 i.e., E[Nk] = pk+1 \u2212 pk \u2013 which allows us to apply the goodness-of-fit test with uniform proportions.\nWe assume that all survival curves are strictly monotonically decreasing meaning we have the equality, di \u2264 ci \u21d0\u21d2 S(di) \u2265 S(ci)). This equivalence lets us replace di \u2264 ci with S(di) \u2265 S(ci), within the indicator functions in Nk. To simplify notation, we define Ik := [pk, pk+1), Sc := S( c | ~x ), and Sd := S( d | ~x ). The proof below shows that the expected value of the summand within Equations (24) \u2013 (26) above is equal to pk+1 \u2212 pk \u2013 i.e., we ignore 1|V | \u2211|V | i=1[\u00b7] and take the expected value of the term inside the summation.\nTheorem B.2. Given the formula for Nk (Equations (24) - (26)), if the true survival function S(\u00b7|\u00b7) is strictly monotonically decreasing then proportions are equal across all bins \u2013 i.e., E[Nk] = pk+1 \u2212 pk."
        },
        {
            "heading": "Proof.",
            "text": "E[Nk] =E [ I [Sd \u2208 Ik \u2227 Sd \u2265 Sc ]\n+ Sc \u2212 pk Sc \u00b7 I [Sc \u2208 Ik \u2227 Sc > Sd ] + (pk+1 \u2212 pk)\nSc \u00b7 I [Sc > Sd \u2227 Sc \u2208 [pk+1, 1] ]\n]\n= E [ I [Sd \u2208 Ik \u2227 Sd \u2265 Sc ] ] + E\n[ Sc \u2212 pk Sc \u00b7 I [Sc \u2208 Ik \u2227 Sc > Sd ] ]\n+ E [\n(pk+1 \u2212 pk) Sc\n\u00b7 I [Sc > Sd \u2227 Sc \u2265 pk+1 ] ]\n= Pr[Sd \u2208 Ik \u2227 Sd \u2265 Sc ] + Pr[Sc \u2208 Ik \u2227 Sc > Sd] \u2212 pk E [ 1\nSc \u00b7 I [Sc > Sd \u2227 Sc \u2208 Ik ] ] + (pk+1 \u2212 pk)E [ 1\nSc \u00b7 I [Sc > Sd \u2227 Sc \u2265 pk+1 ]\n]\n= Pr[Sd \u2208 Ik \u2227 Sd \u2265 Sc] + Pr[Sc \u2208 Ik \u2227 Sc > Sd] (I)\n\u2212 pk E [ 1\nSc \u00b7 I [Sc > Sd \u2227 Sc \u2265 pk ]\n] (II)\n+ pk+1E [ 1\nSc \u00b7 I [Sc > Sd \u2227 Sc \u2265 pk+1 ]\n] (III)\nFocusing on the second probability in line (I), note Sc \u2208 Ik = [pk, pk+1) and Sc > Sd imply that Sd \u2208 [0, pk+1) which can be expanded to the cases for Sd < pk and Sd \u2208 Ik. Using this, we reformulate the probability by noting the equivalence of the event space,\nPr[Sc \u2208 Ik \u2227 Sc > Sd] = Pr[Sc \u2208 Ik \u2227 Sd < pk] + Pr[(Sc \u2227 Sd) \u2208 Ik \u2227 Sc > Sd].\nCombining the second piece above with the first probability in line (I), we again simplify by noting these probabilities bound Sc < pk+1,\nPr[Sd \u2208 Ik \u2227 Sd \u2265 Sc] + Pr[(Sc \u2227 Sd) \u2208 Ik \u2227 Sc > Sd] = Pr[Sd \u2208 Ik \u2227 Sc < pk+1].\nUsing this simplification we can rewrite the entirety of line (1),\nPr[ Sd \u2208 Ik \u2227 Sd \u2265 Sc ] + Pr[ Sc \u2208 Ik \u2227 Sc > Sd ] = Pr[ Sd \u2208 Ik \u2227 Sc < pk+1 ] + Pr[ Sc \u2208 Ik \u2227 Sd < pk ]\nRecalling the independence assumption, c \u22a5 d, we have the following equalities:\nPr[Sd \u2208 Ik \u2227 Sc < pk+1] = Pr[Sd \u2208 Ik] \u00b7 Pr[Sc < pk+1] = (pk+1 \u2212 pk) Pr[Sc < pk+1], Pr[Sc \u2208 Ik \u2227 Sd < pk] = Pr[Sc \u2208 Ik] \u00b7 Pr[Sd < pk] = pk Pr[Sc \u2208 Ik],\nwhere the final equalities are due to the uniformity of the survival function on d, S(d) \u223c U(0, 1). This then leaves the final simplification of line (I) as,\nPr[Sd \u2208 Ik \u2227 Sd \u2265 Sc] + Pr[Sc \u2208 Ik \u2227 Sc > Sd] = (pk+1 \u2212 pk) Pr[Sc < pk+1] + pk Pr[Sc \u2208 Ik].\nNow we address line (II) and analagously line (III): \u2212pk E [ 1\nSc \u00b7 I [Sc > Sd \u2227 Sc > pk ]\n] = \u2212pk (\u222b 1 pk \u222b Sc 0 1 Sc f(Sc) dSd dSc ) (Def. of E[\u00b7])\n= \u2212pk (\u222b 1\npk\nSc Sc f(Sc) dSc\n)\n= \u2212pk Pr[Sc > pk]\nHere f is the probability distribution function (PDF) for the distribution generated by the survival function applied to a censored observation. As the censoring distribution is unknown f(Sc) is also unknown whereas f(Sd) would be the PDF of the uniform distribution.\nFollowing the steps above for line (III) analogously gives us pk+1 E [ 1\nSc \u00b7 I [Sc > Sd \u2227 Sc > pk+1 ]\n] = pk+1 Pr[Sc > pk+1]\nCombining the simplifications of lines (I), (II) and (III), we have the following,\nE[Nk] = (pk+1 \u2212 pk) Pr[Sc < pk+1] + pk Pr[Sc \u2208 Ik] (I) \u2212 pk Pr[Sc > pk] (II) + pk+1 Pr[Sc > pk+1] (III)\n= pk+1 (Pr[Sc < pk+1] + Pr[Sc > pk+1])\n\u2212 pk (Pr[Sc < pk+1]\u2212 Pr[Sc \u2208 [pk, pk+1) + Pr[Sc > pk])\n= pk+1 \u2212 pk\nThis proof requires the assumption that survival curves are strictly monotonically decreasing on [0,1]. This means survival curves will not contain any large flat areas, which means there will not be non-zero probability mass for S(ci) = S(di) when ci 6= di, which means certain terms in the proof below would fail to cancel with one another, leaving us with non-equivalent proportions within each bin (specifically higher proportions within bins that contain these flat lines).\nA natural corollary of Theorem B.2 is that all consistent estimators of the true survival distribution will be D-Calibrated (if the true survival distribution is strictly monotonic). Further, if survival time is independent and identically distributed (i.i.d.) across patients then there will only be one true survival curve for all patients, and thus, as Kaplan-Meier is uniformly consistent [5, 11]:\nLemma 2. The Kaplan-Meier distribution is asymptotically D-Calibrated.\nThis is consistent with the results given in Section 4.2, which showed that km always passed the D-Calibration test with a p-value 1.000, in all 8 datasets. Under all uncensored data, we would expect the typical 5% Type I error rate for claiming p <0.05 as significant, however in the presence of censored data a correct estimate of the survival distribution the proportion within bins become smoothed, boosting the p-value.\nProposition B.3. It is possible for a isd model to be perfectly D-calibrated but not 1- calibrated at a time t\u2217; and for (another) isd model to be perfectly 1-calibrated at time t\u2217 but not D-calibrated.\nProof. \u201c1-Calibration 6\u21d2 D-Calibration\u201d: Consider the model shown in Figure 15[left]. Here, the green curve corresponds to 4 apparently-identical patients {~xg,1, . . . , ~xg,4}, and the red curve, to apparently-identical {~xr,1, . . . , ~xr,4}. The \u201c\u2217\u201ds mark the time when each patient died, denoted as d~x for ~x. We intentionally use simple examples, with no censored patients, with curves that go to 0. Note this model assigns S\u0302(T1 | ~xg,i ) = 0.75 for each of the 4 green patients, and S\u0302(T1 | ~xr,j ) = 0.25 for each of the 4 red patients\nTo show that this model is 1-Calibrated, with respect to T1: Recall we first sort the S\u0302(T1 | ~x ) values, then partition them into k sets. Here, we consider k = 2, rather than the deciles earlier. The first set contains the 4 patients with S\u0302(T1 | ~x ) = 0.75 (i.e., the green patients); and the second, the 4 patients with S\u0302(T1 | ~x ) = 0.25. Now note that 3 of the 4 \u201cS\u0302(T1 | ~x ) = 0.75 patients\u201d are alive at T1; and 1 of the 4 \u201cS\u0302(T1 | ~x ) = 0.25 patients\u201d are alive at T1 \u2013 which means this model is perfectly 1-Calibrated at T1.\nHowever, this model is not D-Calibrated: To be consistent with the earlier 1-Calibration analysis, we partition the time intervals into 2 sets (not 10), as shown in Figure 15. Here, S\u0302( d~x | ~x ) \u2208 [0.5, 1] holds for only 1 patient, and S\u0302( d~x | ~x ) \u2208 [0, 0.5] holds for 7; if the model was D-Calibrated, each of these sets should contain 4 patients. \u201cD-Calibration 6\u21d2 1-Calibration\u201d: See Figure 15[right], where again, each line represent 4 different patients; notice the outcomes are different from those on the left. To see that this model is D-Calibrated, note there are 4 patients with S\u0302( d~x | ~x ) \u2208 [0.5, 1] (the green patients), and 4 with S\u0302( d~x | ~x ) \u2208 [0, 0.5] (for the red patients). However, the model is not 1-Calibrated, at T1: Of the 4 patients with S\u0302(T1 | ~x ) = 0.75, 2 are alive at T1; and of the 4 patients with S\u0302(T1 | ~x ) = 0.25, 2 are alive at T1. To be 1-Calibrated, there should be 3 living patients in the first set, and 1 in the second; hence this model is not 1-Calibrated at T1."
        },
        {
            "heading": "B.6 Other Subtle Points",
            "text": "All of these tools for producing survival curves are able to deal with \u201cright censored\u201d events: where the censored event time is a lower bound of the time of death. (This corresponds to, perhaps, the termination of a study, or when a participant left the study early.) There are other types of censoring, including \u201cleft censoring\u201d, which provides an upper-bound on the time of death (e.g., when a survey finds that the patient is currently dead, but does not know when previously this happened), and \u201cinterval censoring\u201d, when we can constrain the time of death to some interval. While there are extensions of each of these tools that can accommodate these alternate types of censoring, here we considered the most common case of having right-censored instances, and included only datasets that had only such instances.\nAs a second subtle issue: some of the methods involve taking the log of a predicted value, or of a true value; see Appendix B.2.1. This is clearly problematic if that value is 0 \u2013 e.g., if a patient died during a transplantation surgery. To avoid these errors, we replace any such 0 with the \u03b7 for a database, which is defined as 1/2 of the minimum observed positive time of any event, in that dataset. That is, we ignore all time=0 events, and then consider the smallest remaining value. If that value is, say, 1.0 day, then we set \u03b7 =0.5 days. Note that all other times are left unchanged."
        },
        {
            "heading": "C Comments about Various isd\u2019s",
            "text": "C.1 Comments about cox-kp\nNotice Equation 2 embodies two strong assumptions: (1) that the individual features are independent of one another (e.g., the outcome does not depend on a non-linear combination of the features), and (2) that these covariates are independent of time \u2013 which means that a blood test is as important just after an operation, as it is a year later, or a decade later. These assumptions mean the survival curves for different patients will have the same basic shape, and will not cross; see Figure 4 (middle-left). These simplifications allow the Cox model to suggest important information about individual features by examining the single coefficient \u03b2j associated with the j\nth feature, e.g., does \u201cbeing male\u201d increase the risk of dying from this specific cancer, or does it protect against this outcome (or neither). This \u201cneither\u201d case suggests that a given feature is not relevant to the prediction; for this reason, we used univariate Cox is a feature selection technique for our results.\nBy contrast, mtlr and rsf-km do not make these extreme assumptions, which means that a given feature can have different levels of importance at different times. Moreover, the curves for different patients can cross; see Figure 4. More relevant, however, we found that mtlr is more often D-Calibrated, and hence more meaningful for individual patients, than this \u201cpredictive Cox\u201d system; see Table 6. While this Cox analysis of survival may not be directly relevant for individual patients, there are still extreme benefits in being able to identify important features. By observing how different features impact survival, clinicians can be made aware of treatments or lifestyle changes that best help patients survival.\nC.2 Overview of mtlr\nConsider20 modeling the probability of survival of patients at each of a vector of time points \u03c4 = [t1, t2, . . . , tm] \u2013 e.g., \u03c4 could be the 60 monthly intervals from 1 month up to 60 months. We can set up a series of logistic regression models: For each patient, represented as ~x,\nS~\u03b8i(T \u2265 ti | ~x ) = ( 1 + exp(~\u03b8i \u00b7 ~x) )\u22121 , 1 \u2264 i \u2264 m, (27)\nwhere ~\u03b8i are the time-specific parameter vectors. While the input features ~x stay the same for all these classification tasks, the binary labels yi = [T \u2265 ti] can change depending on the threshold ti. We encode the survival time d of a patient as a sequence of binary values: y = y(d) = [y1, y2, . . . , ym], where yi = yi(d) \u2208 {0, 1} denotes the survival status of the patient at time ti, so that yi = 0 (no death event yet) for all i with ti < d, and yi = 1 (death) for all i with ti \u2265 d. Here there are m + 1 possible legal sequences of the form21 [0, 0, . . . , 1, 1, . . . , 1], including the sequence of all \u20180\u2019s and the sequence of all \u20181\u2019s. Our mtlr model computes the probability of observing the survival status sequence y = [y1, y2, . . . , ym] as:\nS\u0398(Y=[y1, y2, . . . , ym] | ~x ) = exp( \u2211m i=1 yi \u00d7 ~\u03b8i \u00b7 ~x)\u2211m\nk=0 exp(f\u0398(~x, k)) ,\nwhere \u0398 = [~\u03b81, . . . , ~\u03b8m], and f\u0398(~x, k) = \u2211m i=k+1( ~\u03b8i \u00b7 ~x) for 0 \u2264 k \u2264 m is the score of the sequence with the event occurring in the interval [tk, tk+1) before taking the logistic transform, with the boundary case f\u0398(~x,m) = 0 being the score for the sequence of all \u20180\u2019s. Given a dataset of n patients {~xr} with associated time of deaths {dr}, we find the optimal parameters (for the mtlr model) \u0398\u2217 as\n\u0398\u2217 = arg max \u0398 n\u2211 r=1 [ m\u2211 i=1 yj(dr)(~\u03b8i \u00b7~xr)\u2212 log m\u2211 k=0 exp f\u0398(~xr, k) ] \u2212 C 2 m\u2211 j=1 \u2016~\u03b8j\u20162 (28)\nwhere the C (for the regularizer) is found by an internal cross-validation process. There are many details here \u2013 e.g., to insure that the survival function starts at 1.0, and decreases monotonically and smoothly until reaching 0.0 for the final time point; to deal appropriately with censored patients; to decide how many time points to consider (m); and to minimize the risk of overfitting (by regularizing), and by selecting the relevant features. The paper by Yu et al. [57] provides the details.\nAfterwards, the isd-Predictor can use the learned mtlr-model \u0398\u2217 = [~\u03b81, . . . , ~\u03b8m] to produce a curve for a novel patient, who is represented as the vector of his/her covariates ~xj. This involves computing the m values, [f1(~xj, ~\u03b81), . . . , fr(~xj, ~\u03b8m)]; the running sum of these values is essentially the survival curve. We then use splines to produce a smooth monotonically decreasing curve \u2013 such as the 10 such curves shown in Figure 4 (bottomright).\n20 This paragraph is paraphrased from [57]; reprinted with permission of publisher/author. 21 Notice there are no \u20180\u2019s after a \u20181\u2019. This is the \u2018no zombie\u2019 rule: once someone dies, that person stays\ndead.\nC.3 Extension to Random Survival Forests (rsf-km)\nGiven a labeled dataset, a random survival forest learner will produce a set of T decision trees from a bootstrapped sample of the training data. It grows each tree recursively, starting from the root \u2013 identifying each position with the set of patients who arrive there. For each position, the growth stops if there are fewer than d0 deaths (where d0 is chosen via crossvalidation). Otherwise, it identifies the feature for this node: it first randomly draws a small random subset of the features to consider, then selects the feature (from that subset) that maximizes the difference in survival between two daughter nodes, based on the logrank test statistic (or some other chosen splitting rule). This becomes the rule of that node; and the learner then considers its two daughters, by splitting on the node\u2019s feature.\nEach leaf node in each tree corresponds to the set of training instances that reached that node. Given these learned trees, to classify a novel instance ~x, the random forest performance system will drop ~x into each of the trees, which will lead to T different leaf nodes, then use the T subsets of training instances to make a decision. Since each terminal node in the random survival forest contains a set of instances, we can use these instances to produce a Kaplan-Meier curve.22\nOnce the survival forest has been learned (with T trees), a patient is dropped into each of the T survival trees, leading to T leaf nodes, which produces T Kaplan-Meier curves. The rsf-km implementation then \u201caverages\u201d these curves, by taking a point-wise average across the curve for all time points \u2013 see Figure 16.23\nNote that the risk score generated by the median of the individual survival curves (produced here) does not necessarily result in the same ordering of patients as the risk scores of the original rsf implementation, which uses averaged cumulative hazards as a risk score. For this reason, we also applied the original rsf process to the datasets presented in the paper. We found that the Concordance scores were similar to that of rsf-km; mtlr still outperformed rsf on the datasets where mtlr outperformed rsf-km (data not shown).\n22 While the original paper does not consider survival curves, documentation https://kogalur.github. io/randomForestSRC/theory.html#section8.1 describing the inner workings of the R package states that survival curves in terminal nodes are created via the Kaplan-Meier estimator.\n23 The method for generating individual survival curves could not be found in any of the literature by the authors of random survival forests. Survival curves were reverse-engineered by the authors of this paper \u2013 all survival curves tested matched the methodology explained here."
        },
        {
            "heading": "D Detailed Empirical Results",
            "text": "This sub-appendix includes the tables that correspond to the figures given in Section 4.2. Further, Appendix D.4 provides the all p-values for the 1-Calibration tests."
        },
        {
            "heading": "D.1 Concordance",
            "text": "See Table 8 for the results corresponding to Figure 9."
        },
        {
            "heading": "D.2 Brier Score",
            "text": "See Table 9 for the results corresponding to Figure 10."
        },
        {
            "heading": "D.3 Empirical Values of L1-Loss, and Variants",
            "text": "Here we give the the results for the Margin-L1-loss (Table 10) as given in Figure 11. Additionally, we give results for the Uncensored L1-loss (Table 11) and the Log-Margin-L1-loss (Table 12).\nD.4 1-Calibration\nEach table corresponds to a different percentile of event times for each dataset. Moving down the 10th, 25th, 50th, 75th, and 90th percentiles are given. Bolded values indicate models which passed 1-Calibration (p > 0.05). The \u201cTotal\u201d column of each table gives the total number of datasets passed by each model \u2013 that is, the values in that columns correspond to Table 5."
        }
    ],
    "title": "Effective Ways to Build and Evaluate Individual Survival Distributions",
    "year": 2018
}