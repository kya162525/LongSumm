{
    "abstractText": "Convolutional neural networks (CNNs) have proven highly effective at image synthesis and style transfer. For most users, however, using them as tools can be a challenging task due to their unpredictable behavior that goes against common intuitions. This paper introduces a novel concept to augment such generative architectures with semantic annotations, either by manually authoring pixel labels or using existing solutions for semantic segmentation. The result is a content-aware generative algorithm that offers meaningful control over the outcome. Thus, we increase the quality of images generated by avoiding common glitches, make the results look significantly more plausible, and extend the functional range of these algorithms\u2014whether for portraits or landscapes, etc. Applications include semantic style transfer and turning doodles with few colors into masterful paintings!",
    "authors": [
        {
            "affiliations": [],
            "name": "Alex J. Champandard"
        }
    ],
    "id": "SP:3e837ba2bfcb028e2331cd3b0e5fefddc5b1c2a4",
    "references": [
        {
            "authors": [
                "Ecker Gatys",
                "L.A. Bethge 2015] Gatys",
                "A.S. Ecker",
                "M. Bethge"
            ],
            "title": "A neural algorithm of artistic style",
            "venue": "CoRR abs/1508.06576",
            "year": 2015
        },
        {
            "authors": [
                "Li",
                "C. Wand 2016] Li",
                "M. Wand"
            ],
            "title": "Combining markov random fields and convolutional neural networks for image synthesis",
            "year": 2016
        },
        {
            "authors": [
                "Mahendran",
                "A. Vedaldi 2014] Mahendran",
                "A. Vedaldi"
            ],
            "title": "Understanding deep image representations by inverting them.CoRR abs/1412.0035",
            "year": 2014
        },
        {
            "authors": [
                "Nikulin",
                "Y. Novak 2016] Nikulin",
                "R. Novak"
            ],
            "title": "Exploring the neural algorithm of artistic style.CoRR abs/1602.07188",
            "year": 2016
        },
        {
            "authors": [
                "Simonyan",
                "K. Zisserman 2014] Simonyan",
                "A. Zisserman"
            ],
            "title": "Very deep convolutional networks for largescale image recognition",
            "year": 2014
        },
        {
            "authors": [
                "Yang"
            ],
            "title": "Semantic portrait color transfer with internet",
            "year": 2015
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :1\n60 3.\n01 76\n8v 1\n[c s.\nC V\n] 5\nM ar\nConvolutional neural networks (CNNs) have proven highly effective at image synthesis and style transfer. For most users, however, using them as tools can be a challenging task due to their unpredictable behavior that goes against common intuitions. This paper introduces a novel concept to augment such generative architectures with semantic annotations, either by manually authoring pixel labels or using existing solutions for semantic segmentation. The result is a content-aware generative algorithm that offers meaningful control over the outcome. Thus, we increase the quality of images generated by avoiding common glitches, make the results look significantly more plausible, and extend the functional range of these algorithms\u2014whether for portraits or landscapes, etc. Applications include semantic style transfer and turning doodles with few colors into masterful paintings!\nIntroduction Image processing algorithms have improved dramatically thanks to CNNs trained on image classification problems to extract underlying patterns from large datasets (Simonyan and Zisserman 2014). As a result, deep convolution layers in these networks provide a more expressive feature space compared to raw pixel layers, which proves useful not only for classification but also generation (Mahendran and Vedaldi 2014). For transferring style between two images in particular, results are astonishing\u2014especially with painterly, sketch or abstract styles (Gatys, Ecker, and Bethge 2015).\nHowever, to achieve good results using neural style transfer in practice today, users must pay particular attention to the composition and/or style image selection, or risk seeing unpredictable and incorrect patterns. For portraits, facial features can be ruined by incursions of background colors or clothing texture, and for landscapes pieces of vegetation may be found in the sky or other incoherent places. There\u2019s certainly a place for this kind of glitch art, but many users become discouraged not being able to get results they want.\nThrough our social media bot that first provided these algorithms as a service (Champandard 2015), we observe that users have clear expectations how style transfer should\n\u2217This research was funded out of the marketing budget.\noccur: most often this matches semantic labels, e.g. hair style and skin tones should transfer respectively regardless of color. Unfortunately, while CNNs routinely extract semantic information during classification, such information is poorly exploited by generative algorithms\u2014as evidenced by frequent glitches.\nWe attribute these problems to two underlying causes:\n1. While CNNs used for classification can be re-purposed to extract style features (e.g. textures, grain, strokes), they were not architected or trained for correct synthesis.\n2. Higher-level layers contain the most meaningful information, but this is not exploited by the lower-level layers used in generative architectures: only error backpropagation indirectly connects layers from top to bottom.\nTo remedy this, we introduce an architecture that bridges the gap between generative algorithms and pixel labeling neural networks. The architecture commonly used for image synthesis (Simonyan and Zisserman 2014) is augmented with semantic information that can be used during generation. Then we explain how existing algorithms can be adapted to include such annotations, and finally we show-\ncase some applications in style transfer as well as image synthesis by analogy (e.g. Figure 1).\nRelated Work The image analogy algorithm (Hertzmann et al. 2001) is able to transfer artistic style using pixel features and their local neighborhoods. While more recent algorithms using deep neural networks generate better quality results from a stylistic perspective, this technique allows users to synthesize new images based on simple annotations. As for recent work on style transfer, it can be split into two categories: specialized algorithms or more general neural approaches.\nThe first neural network approach to style transfer is gram-based (Gatys, Ecker, and Bethge 2015), using socalled \u201cGram Matrices\u201d to represent global statistics about the image based on output from convolution layers. These statistics are computed by taking the inner product of intermediate activations\u2014a tensor operation that results ina N \u00d7 N matrix for each layer ofN channels. During this operation, all local information about pixels is lost, and only correlations between the different channel activations remain. When glitches occur, it\u2019s most often due to these global statistics being imposed onto the target image regardless of its own statistical distribution, and without any understanding of local pixel context.\nA more recent alternative involves apatch-based approach (Li and Wand 2016), which also operates on the output of convolution layers. For layers ofN channels, neural patches of3 \u00d7 3 are matched between the style and content\nimage using a nearest neighbor calculation. Operating on patches in such a way gives the algorithm local understanding of the patterns in the image, which overall improves the precision of the style transfer since fewer errors are introduced by globally enforcing statistical distributions.\nBoth gram- and patch-based approaches struggle to provide reliable user controls to help address glitches. The primary parameter exposed is a weighting factor between style and content; adjusting this results in either an abstract-styled mashup that mostly ignores the input content image, or the content appears clearly but its texture looks washed out (see Figure 2, second column). Finding a compromise where content is replicated precisely and the style is faithful remains a challenge\u2014in particular because the algorithm lacks semantic understanding of the input.\nThankfully, recent CNN architectures are capable of providing such semantic context, typically by performing pixel labeling and segmentation (Thoma 2016). These models rely primarily on convolutional layers to extract high-level patterns, then use deconvolution to label the individual pixels. However, such insights are not yet used for synthesis\u2014 despite benefits shown by non-neural approaches.\nThe state-of-the-artspecialized approaches to style transfer exploit semantic information to great effect, performing color transfer on photo portraits using specifically crafted image segmentation (Yang et al. 2015). In particular, facial features are extracted to create masks for the image, then masked segments are processed independently and colors can be transferred between each corresponding part (e.g.\nbackground, clothes, mouth, eyes, etc.) Thanks to the additional semantic information, even simpler histogram matching algorithms may be used to transfer colors successfully.\nModel Our contribution builds on a patch-based approach (Li and Wand 2016) to style transfer, using optimization to minimize content reconstruction errorEc (weighted by\u03b1) and style remapping errorEs (weight\u03b2). See (Gatys, Ecker, and Bethge 2015) for details aboutEc.\nE = \u03b1Ec + \u03b2Es (1)\nFirst we introduce an augmented CNN (Figure 6) that incorporates semantic information, then we define the input semantic map and its representation, and finally show how the algorithm is able to exploit this additional information."
        },
        {
            "heading": "Architecture",
            "text": "The most commonly used CNNs for image synthesis is VGG (2014), which combines pooling and convolution layers l with 3 \u00d7 3 filters (e.g. the first layer after third pool is namedconv4 1). Intermediate post-activation results are labeledxl and consist ofN channels, which capture patterns from the images for each region of the image: grain, colors, texture, strokes, etc. Other architectures tend to skip pixels regularly, compress data, or optimized for classification\u2014resulting in low-quality synthesis (Nikulin and Novak 2016).\nOur augmented network concatenates additional semantic channelsml of sizeM at the same resolution, computed by down-sampling a static semantic map specified as input. The\nresult is a new output withN +M channels, denotedsl and labeled accordingly for each layer (e.g.sem4 1).\nBefore concatenation, the semantic channels are weighted by parameter\u03b3 to provide an additional user control point:\ns l = xl\u2016\u03b3ml (2)\nFor style images, the activations for the input image and its semantic map are concatenated together assls. For the output image, the current activationsxl and the input content\u2019s semantic map are concatenated assl. Note that the semantic part of this vector is, therefore, static during the optimization process (implemented using L-BFGS).\nThis architecture allows specifying manually authored semantic maps, which proves to be a very convenient tool for user control\u2014addressing the unpredictability of current generative algorithms. It also lets us transparently integrate ecent pixel labeling CNNs (Thoma 2016), and leverage any advances in this field to apply them to image synthesis."
        },
        {
            "heading": "Representation",
            "text": "The input semantic map can contain an arbitrary number of channelsM . Whether doing image synthesis or style transfer, there are only two requirements: \u2022 Each image has its own semantic map of the same aspect\nratio, though it can be lower resolution (e.g. 4x smaller) since it\u2019ll be downsampled anyway.\n\u2022 The semantic maps may use an arbitrary number of channels and representation, as long as they are consistent for the current style and content (soM must be the same). Common representations include single greyscale channels or RGB+A colors\u2014both of which are very easy to author. The semantic map can also be a collection of layer\nmask per label as output by existing CNNs, or even some kind of \u201csemantic embedding\u201d that compactly describes image pixels (i.e. the representation for hair, beards, and eyebrows in portraits would be in close proximity)."
        },
        {
            "heading": "Algorithm",
            "text": "Patches ofk \u00d7 k are extracted from the semantic layers and denoted by the function\u03a8, respectively\u03a8(sls) for the input style patches and\u03a8(sl) for the current image patches. For any patchi in the current image and layerl, its nearest neighborNN(i) is computed using normalized crosscorrelation\u2014taking into account weighted semantic map:\nNN(i) := arg min j\n\u03a8i(s) \u00b7\u03a8j(ss)\n|\u03a8i(s)| \u00b7 |\u03a8j(ss)| (3)\nThe style errorEs between all the patchesi of layer l in the current image to the closest style patch is defined as the sum of the Euclidean distances:\nEs(s, ss) = \u2211\ni\n||\u03a8i(s)\u2212\u03a8NN(i)(ss)|| 2 (4)\nNote that the information from the semantic map inml is used to compute the best matching patches and contributes to the loss value, but is not part of the derivative of the loss relative to the current pixels; only the differences in activationxl compared to the style patches cause an adjustment of the image itself via the L-BFGS algorithm.\nBy using an augmented CNN that\u2019s compatible with the original, existing patch-based implementations can use the additional semantic information without changes. If the semantic map andml is zero, the original algorithm (2016) is intact. In fact, the introduction of the\u03b3 parameter from Equation 2 provides a convenient way to introduce semantic style transfer incrementally.\nExperiments The following experiments were generated from VGG19 network using augmented layerssem3 1 andsem4 1, with 3 \u00d7 3 patches and no additional rotated or scaled versions of the style images. The semantic maps used were manually edited as RGB images, thus channels are in the range [0..255]. The seed for the optimization was random, and rendering completed in multiple increasing resolutions\u2014a usual for patch-based approaches (Li and Wand 2016). On a GTX970 with 4Gb of GPU RAM, rendering takes from 3 to 8 minutes depending on quality and resolution."
        },
        {
            "heading": "Precise Control via Annotations",
            "text": "Transferring style in faces is arguably the most challenging task to meet expectations\u2014and particularly if the colors in the corresponding segments of the image are opposed. Typical results from our solution are shown in portraits from Figure 2, which contains both success cases (top row) and sub-optimal results (bottom row). The input images were chosen once upfront and not curated to showcase representative results; the only iteration was in using the semantic map as a tool to improve the quality of the output.\nIn the portraits, the semantic map includes four main labels for background, clothing, skin and hair\u2014with minor color variations for the eyes, mouth, nose and ears. (The semantic maps in this paper are shown as greyscale, but contain three channels.)\nIn practice, using semantic maps as annotations helps alleviate issues with patch- or gram-based style transfer. Often, repeating patches appear when setting style weight\u03b2 too high (Figure 2, second row). When style weight is low, pat-\nterns are not transferred but lightly blended (Figure 2, first row). The semantics map prevents these issues by allowing the style weight to vary relative to the content without suffering from such artifacts; note in particular that the skin tones and background colors are transferred more faithfully."
        },
        {
            "heading": "Parameter Ranges",
            "text": "Given a fixed weight for the content loss\u03b1 = 10, the style loss\u03b2 for images in this paper ranges from 25 to 250 depending on image pairs. Figure 5 shows a grid with visualizations of results as\u03b2 and\u03b3 vary; we note the following:\n\u2022 The quality and variety of the style degenerates as\u03b3 increases too far, without noticeably improving the precision wrt. annotations.\n\u2022 As \u03b3 decreases, the algorithm reverts to its semantically unaware version that ignores the annotations provided, but also indirectly causes an increase in style weight.\n\u2022 The default value of\u03b3 is chosen to equalize the value range of the semantic channelsml and convolution activationsxl, in this case\u03b3 = 50.\n\u2022 Lowering \u03b3 from its default allows style to be reused across semantic borders, which may be useful for certain applications if used carefully.\nIn general, with the recommended default value of\u03b3, adjusting style weight\u03b2 now allows meaningful interpolation that does not degenerate into abstract patchworks.\nAnalysis Here we report observations from working with the algorithm, and provide our interpretations.\nSemantic Map Values Since the semantic channelsml are integrated into the same patch-based calculation, it affects how the normalized cross-correlation takes place. If the channel range is large, the values from convolutionxl will be scaled very differently depending on the location in the map. This may be desired, but in most cases it seems sensible to make sure values inml have similar magnitude.\nAuthored Representations We noticed that when users are asked to annotate images, after a bit of experience with the system, they implicitly create \u201csemantic embeddings\u201d that compactly describe pixel classes. For example, the representation of a stubble would be a blend between hair and skin, jewelry is similar but not identical to clothing, etc. Such representations seem better suited to semantic style transfer than plain layer masks.\nContent Accuracy vs. Style Quality When using semantic maps, only the style patches from the appropriate segment can be used for the target image. When the number of source patches is small, this causes repetitive patterns, as witnessed in parts of Figure 2. This can be addressed by loosening the style constraint and lowering\u03b3, at the cost of precision.\nBlending Segments In the examples shown and others, the algorithm does a great job of painting the borders between image segments, often using appropriate styles from the corresponding image. Smoothing the semantic map can help in some cases, however, crisp edges still generate surprising results.\nWeight Sensitivity The algorithm is less fragile to adjustments in style weight; typically as the weight increases, the image degenerates and becomes a patchwork of the style content. The semantic map helps maintain the results more consistent for a wider range of the parameter space.\nPerformance Due to the additional channels in the model, our algorithm requires more memory as well as extra computation compared to its predecessor. When using only RGB images this is acceptable: around 1% extra memory for all patches and all convolution output, approximately 5% extra computation. However with pixels labeled using individual classes this grows quickly. This is a concern, although\npatch-based solutions in general would benefit from significant optimization that would apply here too.\nConclusion Existing style transfer techniques perform well when colors and/or accuracy don\u2019t matter too much for the output image (painterly, abstract or sketch styles, glitch art) or when both image patterns are already similar\u2014which obviously reduces the appeal and applicability of such algorithms. In this paper, we resolved these issues by annotating input images with a semantic map, either manually authored or from pixel labeling algorithms. We introduced an augmented CNN architecture to leverage this information at runtime, while further tying advances in image segmentation to image synthesis. We showed that existing patch-based algorithms require minor adjustments and perform very well using this additional information.\nThe examples shown for style transfer show how\nthis technique helps deal with completely opposite patterns/colors in corresponding image regions, and we analyzed how it helps users control the output of these algorithms better. Reducing the unpredictability of neural networks certainly is a step forward towards making them more useful as a tool to enhance creativity and productivity.\nAcknowledgments This work was possible thanks to thenucl.ai Conference 2016\u2019s marketing budget that was repurposed for this research. As long as you visit us in Vienna on July 18-20, it\u2019s better this way!http://nucl.ai\nThanks to Petra Champandard-Pail, Roelof Pieters, Sander Dieleman, Jamie Blondin, Samim Winiger, Timmy Gilbert, Kyle McDonald, Mia Bergeron, Seth Johnson.\nReferences [Champandard 2015] Champandard, A. 2015. Deep forger: Paint photos in the style of famous artists. https://deepforger.com/.\n[Gatys, Ecker, and Bethge 2015] Gatys, L. A.; Ecker, A. S.; and Bethge, M. 2015. A neural algorithm of artistic style. CoRR abs/1508.06576.\n[Hertzmann et al. 2001] Hertzmann, A.; Jacobs, C.; Oliver, N.; Curless, B.; and Salesin, D. 2001. Image analogies. SIGGRAPH Conference Proceedings.\n[Li and Wand 2016] Li, C., and Wand, M. 2016. Combining markov random fields and convolutional neural networks for image synthesis. abs/1601.04589.\n[Mahendran and Vedaldi 2014] Mahendran, A., and Vedaldi, A. 2014. Understanding deep image representations by inverting them.CoRR abs/1412.0035.\n[Nikulin and Novak 2016] Nikulin, Y., and Novak, R. 2016. Exploring the neural algorithm of artistic style.CoRR abs/1602.07188.\n[Simonyan and Zisserman 2014] Simonyan, K., and Zisserman, A. 2014. Very deep convolutional networks for largescale image recognition.CoRR abs/1409.1556.\n[Thoma 2016] Thoma, M. 2016. A survey of semantic segmentation.CoRR abs/1602.06541.\n[Yang et al. 2015] Yang, Y.; Zhao, H.; You, L.; Tu, R.; Wu, X.; and Jin, X. 2015. Semantic portrait color transfer with internet images.Multimedia Tools and Applications 1\u201319."
        }
    ],
    "title": "Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artwork",
    "year": 2016
}