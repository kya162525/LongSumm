{
    "abstractText": "While deep learning has shown a great potential in various domains, the lack of transparency has limited its application in security or safety-critical areas. Existing research has attempted to develop explanation techniques to provide interpretable explanations for each classification decision. Unfortunately, current methods are optimized for non-security tasks (e.g., image analysis). Their key assumptions are often violated in security applications, leading to a poor explanation fidelity. In this paper, we propose LEMNA, a high-fidelity explanation method dedicated for security applications. Given an input data sample, LEMNA generates a small set of interpretable features to explain how the input sample is classified. The core idea is to approximate a local area of the complex deep learning decision boundary using a simple interpretable model. The local interpretable model is specially designed to (1) handle feature dependency to better work with security applications (e.g., binary code analysis); and (2) handle nonlinear local boundaries to boost explanation fidelity. We evaluate our system using two popular deep learning applications in security (a malware classifier, and a function start detector for binary reverse-engineering). Extensive evaluations show that LEMNA\u2019s explanation has a much higher fidelity level compared to existing methods. In addition, we demonstrate practical use cases of LEMNA to help machine learning developers to validate model behavior, troubleshoot classification errors, and automatically patch the errors of the target models.",
    "authors": [
        {
            "affiliations": [],
            "name": "Wenbo Guo"
        },
        {
            "affiliations": [],
            "name": "Dongliang Mu"
        },
        {
            "affiliations": [],
            "name": "Jun Xu"
        },
        {
            "affiliations": [],
            "name": "Purui Su"
        },
        {
            "affiliations": [],
            "name": "Gang Wang"
        },
        {
            "affiliations": [],
            "name": "Xinyu Xing"
        }
    ],
    "id": "SP:83b2860bc11a4c5eeebf2ed279f91f5dfad99fa4",
    "references": [
        {
            "authors": [
                "Daniel Arp",
                "Michael Spreitzenbarth",
                "Malte Hubner",
                "Hugo Gascon",
                "Konrad Rieck"
            ],
            "title": "DREBIN: Effective and Explainable Detection of Android Malware in Your Pocket",
            "venue": "In Proceedings of the 20th Network and Distributed System Security Symposium (NDSS)",
            "year": 2014
        },
        {
            "authors": [
                "Sebastian Bach",
                "Alexander Binder",
                "Gr\u00e9goire Montavon",
                "Frederick Klauschen",
                "Klaus-Robert M\u00fcller",
                "Wojciech Samek"
            ],
            "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation",
            "year": 2015
        },
        {
            "authors": [
                "Dzmitry Bahdanau",
                "Kyunghyun Cho",
                "Yoshua Bengio"
            ],
            "title": "Neural machine translation by jointly learning to align and translate",
            "year": 2014
        },
        {
            "authors": [
                "Tiffany Bao",
                "Johnathon Burket",
                "MaverickWoo",
                "Rafael Turner",
                "David Brumley"
            ],
            "title": "Byteweight: Learning to recognize functions in binary code",
            "venue": "In Proceedings of the 23rd USENIX Security Symposium (USENIX Security)",
            "year": 2014
        },
        {
            "authors": [
                "Osbert Bastani",
                "Carolyn Kim",
                "Hamsa Bastani"
            ],
            "title": "Interpreting blackbox models via model extraction",
            "venue": "arXiv preprint arXiv:1705.08504",
            "year": 2017
        },
        {
            "authors": [
                "Konstantin Berlin",
                "David Slater",
                "Joshua Saxe"
            ],
            "title": "Malicious behavior detection usingwindows audit logs",
            "venue": "In Proceedings of the 8thWorkshop onArtificial Intelligence and Security (AISec)",
            "year": 2015
        },
        {
            "authors": [
                "Arjun Nitin Bhagoji",
                "Daniel Cullina",
                "Prateek Mittal"
            ],
            "title": "Dimensionality reduction as a defense against evasion attacks on machine learning classifiers",
            "year": 2017
        },
        {
            "authors": [
                "Xiaoyu Cao",
                "Neil Zhenqiang Gong"
            ],
            "title": "Mitigating evasion attacks to deep neural networks via region-based classification",
            "venue": "In Proceedings of the 33rd Annual Computer Security Applications Conference (ACSAC)",
            "year": 2017
        },
        {
            "authors": [
                "Yinzhi Cao",
                "Junfeng Yang"
            ],
            "title": "Towardsmaking systems forget withmachine unlearning",
            "venue": "In Proceedings of the 36th IEEE Symposium on Security and Privacy (S&P)",
            "year": 2015
        },
        {
            "authors": [
                "Nicholas Carlini",
                "David Wagner"
            ],
            "title": "Towards evaluating the robustness of neural networks",
            "venue": "In Proceedings of the 38th IEEE Symposium on Security and Privacy (S&P)",
            "year": 2017
        },
        {
            "authors": [
                "Gert Cauwenberghs",
                "Tomaso Poggio"
            ],
            "title": "Incremental and decremental support vector machine learning",
            "venue": "In Proceedings of the 13th Conference on Neural Information Processing Systems (NIPS)",
            "year": 2000
        },
        {
            "authors": [
                "Peng Chen",
                "Hao Chen"
            ],
            "title": "Angora: Efficient Fuzzing by Principled Search",
            "venue": "In Proceedings of the 39th IEEE Symposium on Security and Privacy (S&P)",
            "year": 2018
        },
        {
            "authors": [
                "Zheng Leong Chua",
                "Shiqi Shen",
                "Prateek Saxena",
                "Zhenkai Liang"
            ],
            "title": "Neural Nets Can Learn Function Type Signatures From Binaries",
            "venue": "In Proceedings of the 26th USENIX Security Symposium (USENIX Security)",
            "year": 2017
        },
        {
            "authors": [
                "George E Dahl",
                "Jack W Stokes",
                "Li Deng",
                "Dong Yu"
            ],
            "title": "Large-scale malware classification using random projections and neural networks",
            "venue": "In Proceedings of the 38th International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "year": 2013
        },
        {
            "authors": [
                "R.C. Fong",
                "A. Vedaldi"
            ],
            "title": "Interpretable Explanations of Black Boxes by Meaningful Perturbation",
            "venue": "In Proceedings of the 16th International Conference on Computer Vision (ICCV)",
            "year": 2017
        },
        {
            "authors": [
                "Chuang Gan",
                "Naiyan Wang",
                "Yi Yang",
                "Dit-Yan Yeung",
                "Alex G Hauptmann"
            ],
            "title": "Devnet: A deep event network for multimedia event detection and evidence recounting",
            "venue": "In Proceedings of the 28th Conference on Computer Vision and Pattern Recognition. (CVPR)",
            "year": 2015
        },
        {
            "authors": [
                "Timon Gehr",
                "Matthew Mirman",
                "Dana Drachsler-Cohen",
                "Petar Tsankov",
                "Swarat Chaudhuri",
                "Martin Vechev"
            ],
            "title": "Safety and Robustness Certification of Neural Networks with Abstract Interpretation",
            "venue": "In Proceedings of the 39th IEEE Symposium on Security and Privacy (S&P)",
            "year": 2018
        },
        {
            "authors": [
                "Ian J. Goodfellow",
                "Jonathon Shlens",
                "Christian Szegedy"
            ],
            "title": "Explaining and harnessing adversarial examples",
            "venue": "In Proceedings of the 3rd International Conference on Learning Representations (ICLR)",
            "year": 2015
        },
        {
            "authors": [
                "Kathrin Grosse",
                "Nicolas Papernot",
                "Praveen Manoharan",
                "Michael Backes",
                "Patrick McDaniel"
            ],
            "title": "Adversarial perturbations against deep neural networks for malware classification",
            "year": 2016
        },
        {
            "authors": [
                "Anil K. Jain",
                "B. Chandrasekaran"
            ],
            "title": "Dimensionality and Sample Size Considerations in Pattern Recognition Practice",
            "venue": "Handbook of Statistics",
            "year": 1982
        },
        {
            "authors": [
                "Ahmad Javaid",
                "Quamar Niyaz",
                "Weiqing Sun",
                "Mansoor Alam"
            ],
            "title": "A deep learning approach for network intrusion detection system",
            "venue": "In Proceedings of the 9th International Conference on Bio-inspired Information and Communications Technologies (BIONETICS)",
            "year": 2016
        },
        {
            "authors": [
                "Justin Johnson",
                "Andrej Karpathy",
                "Li Fei-Fei"
            ],
            "title": "Densecap: Fully convolutional localization networks for dense captioning",
            "venue": "In Proceedings of the 29th Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2016
        },
        {
            "authors": [
                "Ian T Jolliffe"
            ],
            "title": "Principal component analysis and factor analysis",
            "venue": "In Principal component analysis",
            "year": 1986
        },
        {
            "authors": [
                "Michael I Jordan",
                "Robert A Jacobs"
            ],
            "title": "Hierarchical mixtures of experts and the EM algorithm",
            "venue": "Neural computation",
            "year": 1994
        },
        {
            "authors": [
                "Abbas Khalili",
                "Jiahua Chen"
            ],
            "title": "Variable selection in finite mixture of regression models",
            "venue": "Journal of the american Statistical association",
            "year": 2007
        },
        {
            "authors": [
                "Pang Wei Koh",
                "Percy Liang"
            ],
            "title": "Understanding Black-box Predictions via Influence Functions",
            "venue": "In Proceedings of the 34th International Conference on Machine Learning (ICML)",
            "year": 2017
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Geoffrey E Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "In Proceedings of the 25th Conference on Neural Information Processing Systems (NIPS)",
            "year": 2012
        },
        {
            "authors": [
                "Himabindu Lakkaraju",
                "Stephen H Bach",
                "Jure Leskovec"
            ],
            "title": "Interpretable decision sets: A joint framework for description and prediction",
            "venue": "In Proceedings of the 22nd International Conference on Knowledge Discovery and Data Mining (KDD)",
            "year": 2016
        },
        {
            "authors": [
                "Jiwei Li",
                "Will Monroe",
                "Dan Jurafsky"
            ],
            "title": "Understanding Neural Networks through Representation Erasure",
            "venue": "arXiv preprint arXiv:1612.08220",
            "year": 2016
        },
        {
            "authors": [
                "Yanpei Liu",
                "Xinyun Chen",
                "Chang Liu",
                "Dawn Song"
            ],
            "title": "Delving into transferable adversarial examples and black-box attacks",
            "venue": "In Proceedings of the 5th International Conference on Learning Representations (ICLR)",
            "year": 2017
        },
        {
            "authors": [
                "Scott M Lundberg",
                "Su-In Lee"
            ],
            "title": "A unified approach to interpreting model predictions",
            "venue": "In Proceedings of the 30th Conference on Neural Information Processing Systems (NIPS)",
            "year": 2017
        },
        {
            "authors": [
                "J.M. Mengersen K.Marin",
                "C.P. Robert"
            ],
            "title": "Bayesianmodelling and inference on mixtures of distributions",
            "venue": "Handbook of statistics",
            "year": 2005
        },
        {
            "authors": [
                "Dongyu Meng",
                "Hao Chen"
            ],
            "title": "Magnet: a two-pronged defense against adversarial examples",
            "venue": "In Proceedings of the 24th ACM Conference on Computer and Communications Security (CCS)",
            "year": 2017
        },
        {
            "authors": [
                "Bengt Muth\u00e9n",
                "Kerby Shedden"
            ],
            "title": "Finite mixture modeling with mixture outcomes using the EM algorithm",
            "venue": "Biometrics",
            "year": 1999
        },
        {
            "authors": [
                "In Jae Myung"
            ],
            "title": "Tutorial on maximum likelihood estimation",
            "venue": "Journal of mathematical Psychology",
            "year": 2003
        },
        {
            "authors": [
                "Bruno A Olshausen",
                "David J Field"
            ],
            "title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images",
            "venue": "Nature",
            "year": 1996
        },
        {
            "authors": [
                "Nicolas Papernot",
                "Patrick McDaniel",
                "Xi Wu",
                "Somesh Jha",
                "Ananthram Swami"
            ],
            "title": "Distillation as a defense to adversarial perturbations against deep neural networks",
            "venue": "In Proceedings of the 37th IEEE Symposium on Security and Privacy (S&P)",
            "year": 2016
        },
        {
            "authors": [
                "Fabian Pedregosa",
                "Ga\u00ebl Varoquaux",
                "Alexandre Gramfort",
                "Vincent Michel",
                "Bertrand Thirion",
                "Olivier Grisel",
                "Mathieu Blondel",
                "Peter Prettenhofer",
                "Ron Weiss",
                "Vincent Dubourg"
            ],
            "title": "Scikit-learn: Machine learning in Python",
            "venue": "Journal of machine learning research",
            "year": 2011
        },
        {
            "authors": [
                "Kexin Pei",
                "Yinzhi Cao",
                "Junfeng Yang",
                "Suman Jana"
            ],
            "title": "Deepxplore: Automated whitebox testing of deep learning systems",
            "venue": "In Proceedings of the 26th Symposium on Operating Systems Principles (SOSP)",
            "year": 2017
        },
        {
            "authors": [
                "Sarunas J. Raudys",
                "Anil K. Jain"
            ],
            "title": "Small Sample Size Effects in Statistical Pattern Recognition: Recommendations for Practitioners",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "year": 1991
        },
        {
            "authors": [
                "Marco Tulio Ribeiro",
                "Sameer Singh",
                "Carlos Guestrin"
            ],
            "title": "Why should i trust you?: Explaining the predictions of any classifier",
            "venue": "In Proceedings of the 22nd International Conference on Knowledge Discovery and Data Mining (KDD)",
            "year": 2016
        },
        {
            "authors": [
                "Enrique Romero",
                "Ignacio Barrio",
                "Llu\u00eds Belanche"
            ],
            "title": "Incremental and decremental learning for linear support vector machines",
            "venue": "In Proceedings of the 17th International Conference on Artificial Neural Networks (ICANN)",
            "year": 2007
        },
        {
            "authors": [
                "Sherif Saad",
                "Issa Traore",
                "Ali Ghorbani",
                "Bassam Sayed",
                "David Zhao",
                "Wei Lu",
                "John Felix",
                "Payman Hakimian"
            ],
            "title": "Detecting P2P botnets through network behavior analysis and machine learning",
            "venue": "In Proceedings of the 9th International Conference on Privacy, Security and Trust (PST)",
            "year": 2011
        },
        {
            "authors": [
                "Joshua Saxe",
                "Konstantin Berlin"
            ],
            "title": "Deep neural network based malware detection using two dimensional binary program features",
            "venue": "In Proceedings of the 10th International Conference on Malicious and Unwanted Software (MALWARE)",
            "year": 2015
        },
        {
            "authors": [
                "Henry Scheffe"
            ],
            "title": "The relation of control charts to analysis of variance and chi-square tests",
            "venue": "J. Amer. Statist. Assoc",
            "year": 1947
        },
        {
            "authors": [
                "Ramprasaath R Selvaraju",
                "Michael Cogswell",
                "Abhishek Das",
                "Ramakrishna Vedantam",
                "Devi Parikh",
                "Dhruv Batra"
            ],
            "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
            "venue": "arxiv. org/abs/1610.02391",
            "year": 2016
        },
        {
            "authors": [
                "Monirul Sharif",
                "Andrea Lanzi",
                "Jonathon Giffin",
                "Wenke Lee"
            ],
            "title": "Automatic reverse engineering of malware emulators",
            "venue": "In Proceedings of the 30th IEEE Symposium on Security and Privacy (S&P)",
            "year": 2009
        },
        {
            "authors": [
                "Eui Chul Richard Shin",
                "Dawn Song",
                "Reza Moazzezi"
            ],
            "title": "Recognizing Functions in Binaries with Neural Networks",
            "venue": "In Proceedings of the 24th USENIX Security Symposium (USENIX Security)",
            "year": 2015
        },
        {
            "authors": [
                "Avanti Shrikumar",
                "Peyton Greenside",
                "Anshul Kundaje"
            ],
            "title": "Learning Important Features Through Propagating Activation Differences",
            "venue": "In Proceedings of the 34th International Conference on Machine Learning (ICML)",
            "year": 2017
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrea Vedaldi",
                "Andrew Zisserman"
            ],
            "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
            "year": 2013
        },
        {
            "authors": [
                "Charles Smutz",
                "Angelos Stavrou"
            ],
            "title": "Malicious PDF detection using metadata and structural features",
            "venue": "In Proceedings of the 28th Annual Computer Security Applications Conference (ACSAC)",
            "year": 2012
        },
        {
            "authors": [
                "Dawn Song",
                "David Brumley",
                "Heng Yin",
                "Juan Caballero",
                "Ivan Jager",
                "Min Gyung Kang",
                "Zhenkai Liang",
                "James Newsome",
                "Pongsin Poosankam",
                "Prateek Saxena"
            ],
            "title": "BitBlaze: A new approach to computer security via binary analysis",
            "venue": "In Proceedings of 4th International Conference on Information Systems Security (ICISS)",
            "year": 2008
        },
        {
            "authors": [
                "Jost Tobias Springenberg",
                "Alexey Dosovitskiy",
                "Thomas Brox",
                "Martin Riedmiller"
            ],
            "title": "Striving for simplicity: The all convolutional net",
            "year": 2014
        },
        {
            "authors": [
                "Nedim Srndic",
                "Pavel Laskov"
            ],
            "title": "Practical evasion of a learning-based classifier: A case study",
            "venue": "In Proceedings of the 35th IEEE Symposium on Security and Privacy (S&P)",
            "year": 2014
        },
        {
            "authors": [
                "Mukund Sundararajan",
                "Ankur Taly",
                "Qiqi Yan"
            ],
            "title": "Gradients of counterfactuals",
            "venue": "arXiv preprint arXiv:1611.02639",
            "year": 2016
        },
        {
            "authors": [
                "Ilya Sutskever",
                "Oriol Vinyals",
                "Quoc V Le"
            ],
            "title": "Sequence to sequence learning with neural networks",
            "venue": "In Proceedings of the 27th Conference on Neural Information Processing Systems (NIPS)",
            "year": 2014
        },
        {
            "authors": [
                "Christian Szegedy",
                "Wojciech Zaremba",
                "Ilya Sutskever",
                "Joan Bruna",
                "Dumitru Erhan",
                "Ian Goodfellow",
                "Rob Fergus"
            ],
            "title": "Intriguing properties of neural networks",
            "year": 2013
        },
        {
            "authors": [
                "Tuan A Tang",
                "Lotfi Mhamdi",
                "Des McLernon",
                "Syed Ali Raza Zaidi",
                "Mounir Ghogho"
            ],
            "title": "Deep learning approach for network intrusion detection in software defined networking",
            "venue": "In Proceedings of the 12th International Conference on Wireless Networks and Mobile Communications (WINCOM)",
            "year": 2016
        },
        {
            "authors": [
                "Robert Tibshirani",
                "Michael Saunders",
                "Saharon Rosset",
                "Ji Zhu",
                "Keith Knight"
            ],
            "title": "Sparsity and smoothness via the fused lasso",
            "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology)",
            "year": 2005
        },
        {
            "authors": [
                "Cheng-Hao Tsai",
                "Chieh-Yen Lin",
                "Chih-Jen Lin"
            ],
            "title": "Incremental and decremental training for linear classification",
            "venue": "In Proceedings of the 20th International Conference on Knowledge Discovery and Data Mining (KDD)",
            "year": 2014
        },
        {
            "authors": [
                "Grigorios Tzortzis",
                "Aristidis Likas"
            ],
            "title": "Deep belief networks for spam filtering",
            "venue": "In Proceedings of the 19th International Conference on Tools with Artificial Intelligence (ICTAI)",
            "year": 2007
        },
        {
            "authors": [
                "Qinglong Wang",
                "Wenbo Guo",
                "Kaixuan Zhang",
                "II Ororbia",
                "G Alexander",
                "Xinyu Xing",
                "Xue Liu",
                "C Lee Giles"
            ],
            "title": "Learning adversary-resistant deep neural networks",
            "year": 2016
        },
        {
            "authors": [
                "Qinglong Wang",
                "Wenbo Guo",
                "Kaixuan Zhang",
                "Alexander G Ororbia II",
                "Xinyu Xing",
                "Xue Liu",
                "C Lee Giles"
            ],
            "title": "Adversary resistant deep neural networks with an application to malware detection",
            "venue": "In Proceedings of the 23rd International Conference on Knowledge Discovery and Data Mining (KDD)",
            "year": 2017
        },
        {
            "authors": [
                "Han Xiao",
                "Kashif Rasul",
                "Roland Vollgraf"
            ],
            "title": "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms",
            "year": 2017
        },
        {
            "authors": [
                "Cihang Xie",
                "Jianyu Wang",
                "Zhishuai Zhang",
                "Zhou Ren",
                "Alan Yuille"
            ],
            "title": "Mitigating adversarial effects through randomization",
            "venue": "In Proceedings of the 6th International Conference on Learning Representations (ICLR)",
            "year": 2017
        },
        {
            "authors": [
                "Xiaojun Xu",
                "Chang Liu",
                "Qian Feng",
                "Heng Yin",
                "Le Song",
                "Dawn Song"
            ],
            "title": "Neural Network-based Graph Embedding for Cross-Platform Binary Code Similarity Detection",
            "venue": "In Proceedings of the 24th Conference on Computer and Communications Security (CCS)",
            "year": 2017
        },
        {
            "authors": [
                "Michal Zalewski"
            ],
            "title": "American fuzzy lop",
            "year": 2007
        },
        {
            "authors": [
                "Matthew D Zeiler",
                "Rob Fergus"
            ],
            "title": "Visualizing and understanding convolutional networks",
            "venue": "In Proceedings of the 13th European Conference on Computer Vision (ECCV)",
            "year": 2014
        },
        {
            "authors": [
                "Mingwei Zhang",
                "R. Sekar"
            ],
            "title": "Control Flow Integrity for COTS Binaries",
            "venue": "In Proceedings of the 22nd USENIX Conference on Security (USENIX Security)",
            "year": 2013
        }
    ],
    "sections": [
        {
            "text": "In this paper, we propose LEMNA, a high-fidelity explanation method dedicated for security applications. Given an input data sample, LEMNA generates a small set of interpretable features to explain how the input sample is classified. The core idea is to approximate a local area of the complex deep learning decision boundary using a simple interpretable model. The local interpretable model is specially designed to (1) handle feature dependency to better work with security applications (e.g., binary code analysis); and (2) handle nonlinear local boundaries to boost explanation fidelity. We evaluate our system using two popular deep learning applications in security (a malware classifier, and a function start detector for binary reverse-engineering). Extensive evaluations show that LEMNA\u2019s explanation has a much higher fidelity level compared to existing methods. In addition, we demonstrate practical use cases of LEMNA to help machine learning developers to validate model behavior, troubleshoot classification errors, and automatically patch the errors of the target models.\nCCS CONCEPTS \u2022 Security and privacy\u2192 Software reverse engineering;\nKEYWORDS Explainable AI, Binary Analysis, Deep Recurrent Neural Networks\nACM Reference Format: Wenbo Guo, Dongliang Mu, Jun Xu, Purui Su, Gang Wang, Xinyu Xing. 2018. LEMNA: Explaining Deep Learning based Security Applications. In CCS \u00e2\u0102\u017918: 2018 ACM SIGSAC Conference on Computer & Communications Security, Oct. 15\u201319, 2018, Toronto, ON, Canada. ACM, New York, NY, USA, 16 pages. https://doi.org/10.1145/3243734.3243792\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CCS \u201918, October 15\u201319, 2018, Toronto, ON, Canada \u00a9 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5693-0/18/10. . . $15.00 https://doi.org/10.1145/3243734.3243792"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "In recent years, Deep Neural Networks have shown a great potential to build security applications. So far, researchers have successfully applied deep neural networks to train classifiers for malware classification [2, 16, 21, 48, 68], binary reverse-engineering [15, 52, 71] and network intrusion detection [24, 62], which all achieved an exceptionally high accuracy.\nWhile intrigued by the high-accuracy, security practitioners are concerned about the lack of transparency of the deep learning models and thus hesitated to widely adopt deep learning classifiers in security and safety-critical areas. More specifically, deep neural networks could easily contain hundreds of thousands or even millions of neurons. This network, once trained with massive datasets, can provide a high classification accuracy. However, the high complexity of the network also leads to a low \u201cinterpretability\u201d of the model. It is very difficult to understand how deep neural networks make certain decisions. The lack of transparency creates key barriers to establishing trusts to the model or effectively troubleshooting classification errors.\nTo improve the transparency of deep neural networks, researchers start to work on explanation methods to interpret the classification results. Most existing works focus on non-security applications such as image analysis or natural language processing (NLP). Figure 1a shows an example. Given an input image, the explanation method explains the classification result by pinpointing the most impactful features to the final decision. Common approaches involve running forward propagation [17, 19, 32, 76] or backward propagation [3, 50, 53] in the network to infer important features. More advanced methods [34, 45] produce explanations under a \u201cblackbox\u201d setting where no knowledge of classifier details is available. The basic idea is to approximate the local decision boundary using a linear model to infer the important features.\nUnfortunately, existing explanation methods are not directly applicable to security applications. First, most existing methods are designed for image analysis, which prefers using Convolutional Neural Networks (CNN). However, CNN model is not very popular in security domains. Security applications such as binary reverseengineering and malware analysis either have a high-level feature dependency (e.g, binary code sequences), or require high scalability. As a result, Recurrent Neural Networks (RNN) or Multilayer Perceptron Model (MLP) are more widely used [15, 21, 52, 68]. So far, there is no explanation method working well on RNN. Second, existing methods still suffer from a low explanation fidelity, as validated by our experiments in \u00a75. This might be acceptable for image analysis, but can cause serious troubles in security applications. For example, in Figure 1a, the highlighted pixels are not entirely accurate (in particular at the edge areas) but are sufficient to provide an intuitive understanding. However, for security applications such as\nbinary analysis, incorrectly highlighting one byte of code may lead to serious misunderstandings or interpretation errors. OurDesigns. In this paper, we seek to develop a novel, high-fidelity explanation method dedicated for security applications. Our method works under a black-box setting and introduces specialized designs to address the above challenges. Given an input data instance x and a classifier such as an RNN, our method aims to identify a small set of features that have key contributions to the classification of x. This is done by generating a local approximation of the target classifier\u2019s decision boundary near x. To significantly improve the fidelity of the approximation, our method no longer assumes the local detection boundary is linear, nor does it assume the features are independent. These are two key assumptions made by existing models [34, 45] which are often violated in security applications, causing a poor explanation fidelity. Instead, we introduce a new approach to approximate the non-linear local boundaries based on a mixture regression model [27] enhanced by fused lasso [64].\nOur design is based on two key insights. First, a mixture regression model, in theory, can approximate both linear and non-linear decision boundaries given enough data [35]. This gives us the flexibility to optimize the local approximation for a non-linear boundary and avoid big fitting errors. Second, \u201cfused lasso\u201d is a penalty term commonly used for capturing feature dependency. By adding fused lasso to the learning process, the mixture regression model can take features as a group and thus capture the dependency between adjacent features. In this way, our method produces high-fidelity explanation results by simultaneously preserving the local nonlinearity and feature dependency of the deep learning model. For convenience, we refer to our method as \u201cLocal Explanation Method using Nonlinear Approximation\u201d or LEMNA. Evaluations. To demonstrate the effectiveness of our explanation model, we apply LEMNA to two promising security applications: classifying PDF malware [55], and detecting the function start to reverse-engineer binary code [52]. The classifiers are trained on 10,000 PDF files and 2,200 binaries respectively, and both achieve an accuracy of 98.6% or higher. We apply LEMNA to explain their classification results and develop a series of fidelity metrics to assess the correctness of the explanations. The fidelity metrics are computed either by directly comparing the approximated detection boundary with the real one, or running end-to-end feature tests. The results show that LEMNA significantly outperforms existing methods across all different classifiers and application settings.\nGoing beyond the effectiveness assessment, we demonstrate how security analysts and machine learning developers can benefit from the explanation results. First, we show that LEMNA could help to establish trusts by explaining how classifiers make the correct decisions. In particular, for both binary and malware analyses, we demonstrate the classifiers have successfully learned a number of well-known heuristics and \u201cgolden rules\u201d in the respective domain. Second, we illustrate that LEMNA could extract \u201cnew knowledge\u201d from classifiers. These new heuristics are difficult to be manually summarized in a direct way, but make intuitive sense to domain experts once they are extracted by LEMNA. Finally, with LEMNA\u2019s capability, an analyst could explain why the classifiers produce errors. This allows the analyst to automatically generate targeted patches\nby augmenting training samples for each of the explainable errors, and improve the classifier performance via targeted re-training. Contributions. Our paper makes three key contributions. \u2022 We design and develop LEMNA, a specialized explanation method for deep learning based security applications. Using a mixture regression model enhanced by fused lasso, LEMNA generates high-fidelity explanation results for a range of deep learning models including RNN. \u2022 We evaluate LEMNA using two popular security applications, including PDF malware classification and function start detection in binary reverse-engineering. We propose a series of \u201cfidelity\u201d metrics to quantify the accuracy of the explanation results. Our experiments show that LEMNA outperforms existing explanation methods by a significant margin. \u2022 We demonstrate the practical applications of the explanation method. For both binary analysis and malware detection, LEMNA sheds lights on why the classifier makes correct and incorrect decisions. We present a simple method to automatically convert the insights into actionable steps to patch the targeted errors of the classifiers.\nTo the best our knowledge, this is the first explanation system specially customized for security applications and RNN. Our work is only the initial step towards improving the model transparency for more effective testing and debugging of deep learning models. By making the decision-making process interpretable, our efforts can make a positive contribution to building reliable deep learning systems for critical applications."
        },
        {
            "heading": "2 EXPLAINABLE MACHINE LEARNING",
            "text": "In this section, we start with introducing the background of explainable machine learning, and then discuss existing explanation techniques. Following that, in Section \u00a73, we introduce key security applications using deep learning models and discuss why existing explanation techniques are not applicable to security applications."
        },
        {
            "heading": "2.1 Problem Definition",
            "text": "Explainable machine learning seeks to provide interpretable explanations for the classification results. More specifically, given an input instance x and a classifier C , the classifier will assign a label y for x during the testing time. Explanation techniques then aim to illustrate why instance x is classified as y. This often involves identifying a set of important features that make key contributions to the classification process (or result). If the selected features are\ninterpretable to human analysts, then these features can offer an \u201cexplanation\u201d. Figure 1 shows examples for image classification and sentiment analysis. The classifier decision can be explained by selected features (e.g., highlighted pixels and keywords).\nIn this paper, we focus on the deep neural networks to develop explanation methods for security applications. Up to the present, most existing explanation methods are designed for image analysis or NLP. We categorize them into \u201cwhitebox\u201d and \u201cblackbox\u201d methods and describe how they work."
        },
        {
            "heading": "2.2 Whitebox Explanation Methods",
            "text": "Most existing explanation techniques work under the whitebox setting where the model architecture, parameters, and training data are known. These techniques are also referred as Deep Explanation Methods and mainly designed for CNN. They leverage two major strategies to infer feature importance: (1) forward propagation based input or structure occlusion; and (2) gradient-based backpropagation. We discuss those techniques in the following. Forward Propagation based Methods. Given an input sample, the key idea is to perturb the input (or hidden network layers) and observe the corresponding changes. The intuition behind is that perturbing important features is more likely to cause major changes to the network structure and the classification output. Existing methods either nullify a subset of features or removing intermediate parts of the network [17, 32, 74, 76]. A recent work [19] extends this idea to detecting adversarial examples (i.e., malicious inputs aiming to cause classification errors). BackwardPropagation basedMethods.Back-propagation based methods leverage the gradients of the deep neural network to infer feature importance. The gradients can be the partial derivatives of classifier output with respect to the input or hidden layers. By propagating the output back to the input, these methods directly calculate the weights of input features. For image classifiers, the basic method is to compute a feature \u201csaliency map\u201d using the gradients of output with respect to the input pixels in images [54, 57] or video frames [18]. Later works improve this idea by applying saliency map layer by layer [3] or mapping groups of pixels [50].\nBackward propagation based methods face the challenge of \u201czero gradient\u201d. Inside a neural network, the activation functions often have saturated parts, and the corresponding gradients will become zero. Zero gradients make it difficult (if not impossible) for the \u201csaliency map\u201d to back-track the important features. Recent works [53, 59] attempted to address this problem through approximation. However, this sacrifices the fidelity of the explanation [34]."
        },
        {
            "heading": "2.3 Blackbox Explanation Methods",
            "text": "Blackbox explanation methods require no knowledge about the classifier internals such as network architecture and parameters. Instead, they treat the classifier as a \u201cblackbox\u201d and analyze it by sending inputs and observing the outputs (i.e., Model Induction Methods).\nThe most representative system in this category is LIME [45]. Given an input x (e.g., an image), LIME systematically perturbs x to obtain a set of artificial images from the nearby areas of x in the feature space (see x\u2032 and x\u2032\u2032 in Figure 2). Then, LIME feeds the\nartificial images to the target classifier f (x) to obtain labels, and uses the labeled data to fit a linear regression model \u0434(x). This \u0434(x) aims to approximate the small part of f (x) near the input image in the feature space. LIME assumes that the local area of the classification boundary near the input instance is linear, and thus it is reasonable to use a linear regression model to locally represent the classification decision made by f (x). Linear regression is selfexplanatory, and thus LIME can pinpoint important features based on the regression coefficients. A recent work SHAP [34] tries to extend LIME by adding weights to the artificially generated data samples. Other works propose to use other linear models (e.g., decision tree [6] and decision set [31]) to incrementally approximate the target detection boundaries.\nAs a side note, we want to clarify that machine learning explanation is completely different from feature selection methods such as Principal Component Analysis (PCA) [26], Sparse Coding [39] or Chi-square Statistics [49]. Explanation methods aim to identify the key features of a specific input instance x to specifically explain how an instance x is classified. On the other hand, feature selection methods such as PCA are typically applied before training on the whole training data to reduce the feature dimension (to speed up the training or reduce overfitting), which cannot explain how a specific classification decision is made."
        },
        {
            "heading": "3 EXPLAINING SECURITY APPLICATIONS",
            "text": "While deep learning has shown a great potential to build security applications, the corresponding explanation methods are largely falling behind. As a result, the lack of transparency reduces the trust. First, security practitioners may not trust the deep learning model if they don\u2019t understand how critical decisions are made. Second, if security practitioners cannot troubleshoot classification errors (e.g., errors introduced by biased training data), the concern is that these errors may be amplified later in practice. In the following, we introduce two key security applications where deep learning has recently achieved success. Then we discuss why existing explanation methods are not applicable to the security applications."
        },
        {
            "heading": "3.1 Deep Learning in Security Applications",
            "text": "In this paper, we focus on two important classes of security applications: binary reverse engineering and malware classification. Binary Reverse-Engineering. The applications of deep learning in binary analysis include identifying function boundaries [52], pinpointing the function type signatures [15] and tracking down similar binary code [71]. More specifically, using a bi-directional RNN, Shin et al. improve the function boundary identification and achieve a nearly perfect performance [52]. Chua et al. also use RNN to accurately track down the arguments and types of functions in binaries [15]. More recently, Xu et al. employ an MLP to encode a control flow graph to pinpoint vulnerable code fragments [71]. Malware Classification. Existing works mainly use MLP models for large-scale malware classifications. For example, researchers have trained MLP to detect malware at the binary code level [48] and classify Android malware [2, 21]. More recently, Wang et al. [68] propose an adversarial resistant neural network for detecting malware based on audit logs [7].\nA key observation is that RNN and MLP are more widely adopted by these security applications compared to CNN. The reason is that RNN is designed to handle sequential data, which performs exceptionally well in processing the long sequences of binary code. Particularly, Bi-directional RNN can capture the bi-directional dependencies in the input sequences between each hex [52]. For malware classification, MLP is widely used for its high efficiency. On the other hand, CNN performs well on images since it can take advantage of the grouping effect of features on the 2D images [30]. These security applications do not have such \u201cmatrix-like\u201d data structures to benefit from using CNN."
        },
        {
            "heading": "3.2 Why Not Existing Explanation Methods",
            "text": "There are key challenges to directly apply existing explanation methods to the security applications. In Table 1, we summarize the desired properties, and why existing methods fail to deliver them. Supporting RNN and MLP. There is a clear mismatch between the model choices of the above security applications and existing explanationmethods. Most existing explanationmethods are designed for CNN to work with image classifiers. However, as mentioned in \u00a73.1, security applications of our interests primarily adopt RNN or MLP. Due to model mismatches, existing explanation methods are not quite applicable. For example, the back-propagation methods including \u201csaliency map\u201d [3, 18, 54, 57] and activation difference propagation [53] require special operations on the convolutional layers and pooling layers of CNN, which do not exist in RNN or MLP 1.\n1[15] presents some case studies using saliency map to explain RNN, but is forced to ignore the feature dependency of RNN, leading to a low explanation fidelity.\nBlackbox methods such as LIME do not support RNN well either (validated by our experiments later). Methods like LIME assume features are independent, but this assumption is violated by RNN which explicitly models the dependencies of sequential data. Supporting Locally Non-linear Decision Boundary. Most existing methods (e.g., LIME) assume the local linearity of the decision boundary. However, when the local decision boundary is non-linear, which is true for most complex networks, those explanation methods would produce serious errors. Figure 3a shows an example where the decision boundary around x is highly non-linear. In other words, the linear part is heavily restricted to a very small region. The typical sampling methods can easily hit the artificial data points beyond the linear region, making it difficult for a linear model to approximate the decision boundary near x. Later in our experiments (\u00a7 5), we confirm that a simple linear approximation will significantly degrade the explanation fidelity. SupportingBlackbox Setting.Although bothwhitebox and blackboxmethods have their application scenarios, blackboxmethods are still more desirable for security applications. Noticeably, it is not uncommon for people to use pre-trainedmodels (e.g., \u201cBi-directional RNN\u201d [52], \u201cprefix tree\u201d in Dyninst [5]) where the detailed network architecture, parameters or training data are not all available. Even though a few forward propagation methods can be forced to work under a blackbox setting (by giving up the observations of intermediate layers), it would inevitably lead to performance degradation. Summary. In this paper, we aim to bridge the gaps by developing dedicated explanation methods for security applications. Our method aims to work under a blackbox setting and efficiently support popular deep learning models such as RNN, MLP, and CNN. More importantly, the method need to achieve a much higher explanation fidelity to support security applications."
        },
        {
            "heading": "4 OUR EXPLANATION METHOD",
            "text": "To achieve the above goals, we design and develop LEMNA. At the high-level, we treat a target deep learning classifier as a blackbox and derive explanation through model approximation. In order to provide a high fidelity explanation, LEMNA needs to take a very different design path from existing methods. First, we introduce fused lasso [64] to handle the feature dependency problems that are often encountered in security applications and RNN (e.g., time series analysis, binary code sequence analysis). Then, we integrate fused lasso into amixture regressionmodel [28] to approximate locally nonlinear decision boundaries to support complex security applications. In the following, we first discuss the insights behind the design choices of using fused lasso and mixture regression model. Then, we describe the technical details to integrate them into a single model to handle feature dependencies and locally nonlinearity at the same time. Finally, we introduce additional steps to utilize LEMNA to derive high-fidelity explanations."
        },
        {
            "heading": "4.1 Insights behind Our Designs",
            "text": "Fused Lasso. Fused lasso is a penalty term commonly used for capturing feature dependencies, and is useful to handle the dependent features in deep learning models such as RNN. At the high-level, \u201cfused lasso\u201d forces LEMNA to group relevant/adjacent features together to generate meaningful explanations. Below, we introduce the technical details of this intuition.\nTo learn a model from a set of data samples, a machine learning algorithm needs to minimize a loss function L( f (x),y) that defines the dissimilarity between the true label and the predicted label by the model. For example, to learn a linear regression model f (x) = \u03b2x + \u03f5 from a data set with N samples, a learning algorithm needs to minimize the following equation with respect to the parameter \u03b2 using Maximum Likelihood Estimation (MLE) [38].\nL( f (x),y) = N\u2211 i=1 \u2225\u03b2xi \u2212 yi \u2225 . (1)\nHere, xi is a training sample, represented by anM-dimensionality feature vector (x1,x2, \u00b7 \u00b7 \u00b7 ,xM )T . The label of xi is denoted as yi . The vector \u03b2 = (\u03b21, \u03b22, \u00b7 \u00b7 \u00b7 \u03b2M ) contains the coefficients of the linear model. \u2225 \u00b7 \u2225 is the L2-norm measuring the dissimilarity between the model prediction and the true label.\nFused lasso is a penalty term that can be introduced into any loss functions used by a learning algorithm. Take linear regression for example. Fused lasso manifests as a constraint imposed upon coefficients, i.e.,\nL( f (x),y) = N\u2211 i=1 \u2225\u03b2xi \u2212 yi \u2225 ,\nsubject to M\u2211 j=2 \u2225\u03b2j \u2212 \u03b2j\u22121\u2225 \u2264 S .\n(2)\nFused lasso restricts the dissimilarity of coefficients assigned to adjacent features within a small threshold S (i.e., a hyper-parameter) when a learning algorithm minimizes the loss function. As a result, the penalty term forces a learning algorithm to assign equal weights to the adjacent features. Intuitively, this can be interpreted\nas forcing a learning algorithm to take features as groups and then learn a target model based on feature groups.\nSecurity applications, such as time series analysis and code sequence analysis, often need to explicitly model the feature dependency of sequential data using RNN. The resulting classifier makes a classification decision based on the co-occurrence of features. If we use a standard linear regression model (e.g., LIME) to derive an explanation, we cannot approximate a local decision boundary correctly. This is because a linear regression model cannot capture feature dependency and treat them independently.\nBy introducing fused lasso in the process of approximating local decision boundary, we expect the resulting linear model to have the following form:\nf (x) = \u03b21x1 + \u03b22 (x2 + x3) + \u03b23 (x4 + x5) + \u00b7 \u00b7 \u00b7 + \u03b2kxM , (3) where features are grouped together and thus important features are likely to be selected as a group or multiple groups. Explicitly modeling this process in LEMNA helps to derive a more accurate explanation, particularly for the decision made by an RNN. We further explain this idea using an example of sentiment analysis in Figure 1b. With the help of fused lasso, a regression model would collectively consider adjacent features (e.g., words next to each other in a sentence). When deriving the explanations, our model does not simply yield a single word \u201cnot\u201d2, but can accurately capture the phrase \u201cnot worth the price\u201d as the explanation for the sentiment analysis result. Mixture Regression Model. A mixture regression model allows us to approximate locally nonlinear decision boundaries more accurately. As shown in Figure 3b, a mixture regression model is a combination of multiple linear regression models, which makes it more expressive to perform the approximation:\ny = K\u2211 k=1 \u03c0k (\u03b2kx + \u03f5k ) , (4)\nwhere K is a hyper-parameter indicating the total number of linear components combined in the mixture model; \u03c0k indicates the weight assigned to that corresponding component.\nGiven sufficient data samples, whether the classifier has a linear or non-linear decision boundary, the mixture regression model can nearly perfectly approximate the decision boundary (using a finite set of linear models) [35]. As such, in the context of deep learning explanation, the mixture regression model can help avoid the aforementioned non-linearity issues and derive more accurate explanations.\nTo illustrate this idea, we use the example in Figure 3. As shown in Figure 3a, a standard linear approximation cannot guarantee the data sampled around the input x still remain in the locally linear region. This can easily lead to imprecise approximation and lowfidelity explanations. Our method in Figure 3b approximates the local decision boundary with a polygon boundary, in which each blue line represents an independent linear regression model. The best linear model for producing the explanation should be the red line passing through the data point x. In this way, the approximation process can yield an optimal linear regressionmodel for pinpointing important features as the explanation. 2In sentiment analysis, \u201cnot\u201d does not always carry negative sentiment, e.g., \u201cnot bad\u201d."
        },
        {
            "heading": "4.2 Model Development",
            "text": "Next, we convert these design insights into a functional explanation system. We introduce the technical steps to integrate fused lasso in the learning process of a mixture regression model so that we can handle feature dependency and decision boundary non-linearity at the same time. Technically speaking, we need to derive a mixture regression model by minimizing the following equation\nL( f (x),y) = N\u2211 i=1 \u2225 f (xi ) \u2212 yi \u2225 ,\nsubject to M\u2211 j=2 \u2225\u03b2k j \u2212 \u03b2k (j\u22121) \u2225 \u2264 S , k = 1, . . . ,K .\n(5)\nwhere f (\u00b7) represents the mixture regression model shown in Equation (4), and \u03b2k j indicates the parameter in the k th linear regression model tied to its jth feature.\nDifferent from a standard linear regression, our optimization objective is intractable andwe cannot simply utilizeMLE to perform minimization. To effectively estimate parameters for the mixture regression model, we utilize an alternative approach.\nFirst, we represent the mixture regression model in the form of probability distributions\nyi \u223c K\u2211 k=1 \u03c0kN (\u03b2kxi ,\u03c3 2k ) . (6)\nThen, we treat \u03c01:K , \u03b21:K and\u03c3 21:K as parameters 3. By taking a guess at these parameters, we initialize their values and thus perform parameter estimation by using ExpectationMaximization (EM) [37], an algorithm which estimates parameters by repeatedly performing two steps \u2013 E-Step and M-Step. In the following, we briefly describe how this EM algorithm is used in our problem. More details can be found in Appendix-A.\nIn the Equation (6), yi follows a distribution which combines K Gaussian distributions, and each of these distributions has the mean \u03b2kxi and the variance \u03c3 2k . In the E-Step, we assign each of the data samples to one of the Gaussian distributions by following the standard procedure applied in learning an ordinary mixture regression model. Based on the data samples assigned in the previous E-Step, we then re-compute the parameters \u03c01:K , \u03b21:K and \u03c3 21:K . For the parameters \u03c01:K and \u03c3 2 1:K , the re-computation still follows the standard procedure used by ordinary mixture model learning. But, for each parameter in \u03b21:K , re-computation follows a customized procedure. That is to compute \u03b2k by minimizing the following equation with respect to \u03b2k\nL(x ,y) = Nk\u2211 i=1 \u2225\u03b2kxi \u2212 yi \u2225 ,\nsubject to M\u2211 j=2 \u2225\u03b2k j \u2212 \u03b2k (j\u22121) \u2225 \u2264 S ,\n(7)\nwhere Nk refers to the number of samples assigned to the k th component. Here, the reason behind this re-computation customization 3\u03c01:K indicates parameters \u03c01, \u00b7 \u00b7 \u00b7 , \u03c0K . \u03b2 1:K represents parameters \u03b2 1, \u00b7 \u00b7 \u00b7 , \u03b2K . \u03c3 21:K are the parameters \u03c3 2 1 , \u00b7 \u00b7 \u00b7 , \u03c3K1 , each of which describes the variance of the normal distribution that \u03f5k follows, i.e., \u03f5k \u223c N (0, \u03c3 2k ).\nis that fused lasso has to be imposed to parameters \u03b21:K in order to grant a mixture regression model the ability to handle feature dependency. As we can observe, the equation above shares the same form with that shown in Equation (2). Therefore, we can minimize the equation through MLE and thus compute the values for parameters \u03b21:K .\nFollowing the standard procedure of EM algorithm, we repeatedly perform the E-step and M-Step. Until stability is reached (i.e., the Gaussian distributions do not vary much from the E-step to the M-step), we output the mixture regression model. Note that we convert \u03c3 21:K into the model parameter \u03f51:K by following the standard approach applied in ordinary mixture model learning."
        },
        {
            "heading": "4.3 Applying the Model for Explanation",
            "text": "With the enhanced mixture regression model, we now discuss how to derive high-fidelity explanations for deep learning classifiers. Approximating Local Decision Boundary. Given an input instance x, the key to generate the explanation is to approximate the local decision boundary of the target classifier. The end product is an \u201cinterpretable\u201d linear model that allows us to select a small set of top features as the explanation. To do so, we first synthesize a set of data samples locally (around x) following the approach described in [45]. The idea is to randomly nullify a subset of features of x.\nUsing the corpus of synthesized data samples, we then approximate the local decision boundary. There are two possible schemes: one is to train a single mixture regression model to perform multiclass classification; the other scheme is to train multiple mixture regression models, each of which performs binary classification. For efficiency considerations, we choose the second scheme and put more rigorous analysis to the Appendix-B. Deriving Explanations. Given the input data instance x, and its classification result y, we now can generate explanations as a small set of important features to x\u2019s classification. More specifically, we obtain a mixture regression model enhanced by fused lasso. From this mixture model, we then identify the linear component that has the best approximation of the local decision boundary. The weights (or coefficients) in the linear model can be used to rank features. A small set of top features is selected as the explanation result.\nNote that LEMNA is designed to simultaneously handle non-linearity and feature dependency, but this does not mean that LEMNA cannot work on deep learning models using relatively independent features (e.g., MLP or CNN). In fact, the design of LEMNA provides the flexibility to adjust the explanation method according to the target deep learning model. For example, by increasing the hyper-parameter S (which is a threshold for fused lasso), we can relax the constraint imposed upon parameter \u03b21:K and allow LEMNA to better handle less dependent features. In Section \u00a75, we demonstrate the level of generalizability by applying LEMNA to security applications built on both RNN and MLP."
        },
        {
            "heading": "5 EVALUATION",
            "text": "In this section, we evaluate the effectiveness of our explanation method on two security applications: malware classification and binary reverse engineering. This current section focuses evaluating\non the accuracy of the explanation through a series of fidelity metrics. In the next section (\u00a76), we will present practical use cases of LEMNA to understand classifier behavior, troubleshoot classification errors, and patch the errors of the classifiers."
        },
        {
            "heading": "5.1 Experimental Setup",
            "text": "We apply LEMNA to two security applications: detecting the \u201cfunction start\u201d for reverse-engineering binary code using RNN, and classifying PDF malware based on MLP. Below, we introduce details about the two security applications, the implementation of LEMNA, and the comparison baselines. Binary Reverse-Engineering. Binary code reverse-engineering, which transfers binary code to assembly code, is a crucial step in (1) examining and detecting malware [51], (2) hardening the security of software [75], and (3) generating security patches [56]. For years, binary analysis is primarily done manually by experienced security analysts. Recently, researchers show that well-trained RNN can help handle critical reverse-engineering steps such as detecting the function start [52], which can significantly save human efforts. Considering the importance of detecting function start (i.e., all binary code reverse-engineering requires knowing the function start), we choose this application to test LEMNA.\nWe follow [52] to build a RNN based on a widely used dataset that contains 2200 binaries [5]. We compile these binaries under x86 architecture and gcc compiler with four different optimization levels O0, O1, O2, and O3 respectively. This produces 4 training datasets, one for each optimization level. Like [52], we use the bi-directional RNN and train 4 different classifiers.\nEach binary in the dataset is presented as a sequence of hex code. As shown in Figure 4, we first transfer the hex code to their decimal values, and treat each element in the sequence as a feature. For training, each element in the sequence has a label of either \u201ca function start\u201d or \u201cnot a function start\u201d. As shown in Figure 4, suppose the original binary code is \u201c90 90 90 83 ec 4c\u201d and the function start is at \u201c83\u201d, then the label vector is (0, 0, 0, 0, 1, 0, 0). We follow [52] to truncate very long binary sequences and set the maximum length to 200. Then we feed the sequences into the RNN. We used Keras [14] to train the model, with Theano [63] as a backend. We split the dataset randomly using 70% of the samples for training, and the rest 30% for testing.\nAs shown in Table 2, the detection accuracy is extremely high, with a 98.57% or higher precision and recall for all cases. The results are comparable to those reported in [52]. The hyper-parameters of the RNNs can be found in the Appendix-C. PDF Malware Classifier. We follow [21, 48] to construct a MLPbased malware classifier based on a widely used dataset (4999 malicious PDF files and 5000 benign files) [55]. We follow [55, 58] to extract 135 features for each file. The features were manually crafted by researchers based on the meta-data and the structure of the PDF, such as number of object markers and number of javascript markers. The full feature list can be found in the Mimicus [1]. We follow the standard method to transform the feature values into a binary representation [41] (i.e., nonzero feature values are converted to 1), which helps avoid certain high-value features skewing the training process. Like before, we randomly select 70% of the datasets (malware and benign 1:1) as the training data, and use the remaining 30% as the testing data. As shown in Table 2, our precision and recall are both above 98.13%, which are similar to [55]. LEMNA Implementation. We treat the above RNN and MLP as the target classifiers to run LEMNA. Given an input instance, LEMNA approximates the target classifier and explain the classification result. \u201cExplanations\u201d are presented as the most important features for the given input. For the malware classifier, LEMNA outputs a small set of top features that explains why a file is (not) a malware. For the \u201cfunction start\u201d detector, an example is shown in Figure 4. Given an input hex sequence and the detected function start (i.e., \u201c83\u201d), LEMNA marks out a small set of hex code in the sequence that has the biggest contribution. Here, \u201c83\u201d is the function start, and LEMNA points out that the hex code \u201c90\u201d before the function start is the most important reason of the detection.\nLEMNA has 3 hyper-parameters that are configurable. First, to approximate the local decision boundary, we set to craft N data samples for the model fitting (see \u00a74). The second and third parameters are the number of mixture components K , and the threshold of the fused lasso S . For binary function start detection, we set parameters as: N=500, K=6, S=1e \u2212 4. For malware classification, we set parameters as: N=500, K=6, S=1e4. Note that the parameter S is set very differently because malware analysis features are relatively independent, while the binary analysis features have a high dependency level. We fix these parameters to run most of our experiments. Later, we have a dedicated section to perform sensitivity tests on the parameter settings (which shows LEMNA is not sensitive to these hyper-parameters). LEMNA\u2019s Computational Costs. The computational costs of LEMNA are relatively low. For both security applications, the time to generate the explanation for a given instance is about 10 seconds. This computation task further benefits from parallelization. For example, using a server with Intel Xeon CPU E5-2630, one Nvidia Tesla K40c\nGPU and 256G RAM, it takes about 2.5 hours to explain all 25, 040 binary testing sequences for O0 with 30 threads. Comparison Baselines. We use two baselines for comparison. First, we use the state-of-the-art blackbox method LIME [45] as our comparison baseline. LIME [45] has been used to explain image classifiers and NLP applications. Its performance on security applications and RNN is not yet clear4. For a fair comparison, we also configure LIME with N=500 which is the number of artificial samples used to fit the linear regression model. Second, we use a random feature selection method as the baseline. Given an input, the Random method selects features randomly as the explanation for the classification result."
        },
        {
            "heading": "5.2 Fidelity Evaluation",
            "text": "To validate the correctness (fidelity) of the explanation, we conduct a two-stage experiment. In the first stage, we directly examine the accuracy of our local approximation with respect to the original decision boundary. This is likely to give an initial estimation of the explanation accuracy. In the second stage, we perform end-toend evaluation on the explanation fidelity. We design three fidelity tests to show whether the selected features are indeed the main contributors to the classification results. Evaluation 1: Local Approximation Accuracy. This metric is directly computed by comparing the approximated decision boundary and the original one. We measure Root Mean Square Error\n(RMSE): RMSE = \u221a\u2211n\ni=1 (pi\u2212p\u0302i ) n , where pi represents a single pre-\ndiction obtained from a target deep learning classifier, p\u0302i denotes the approximated prediction obtained from the explanation method, and n is the total number of testing data samples. More specifically, we start from a given classifier and a set of testing data samples. For each testing data sample xi , we first obtain a prediction probability pi using the classifier. Then for xi , we follow Equation (6) to generate a regression model, which can produce an estimated prediction probability p\u0302i . After running these steps for all n testing samples, we obtain a prediction vector P = (p1,p2, ...,pn ) and the corresponding approximation vector P\u0302 = (p\u03021, p\u03022, ..., p\u0302n ). Finally, we computer RMSE based on the two vectors. A lower RMSEmeans the approximated decision boundary (P\u0302) is closer to the true boundary (P), indicating a higher fidelity of explanation. Evaluation 2: End-to-end Fidelity Tests. To validate the correctness of the selected features, we design three end-to-end fidelity tests. To help readers to understand the testing process, we use 4We have tested SHAP [34], which is an extension of LIME. We find that SHAP is very slow and its performance is worse than LIME for our applications.\n\u201cimage classifier\u201d as a toy example5. The procedure works in the same way for other classifiers. As shown in Figure 5, the image classifier is trained to classify \u201cshoe\u201d from \u201csweater\u201d. Figure 5a is the input image (x) with the label as \u201csweater\u201d. In Figure 5b, the explanation method explains the reasons for the classification by highlighting important pixels (features) in red. We denote the selected features as Fx. To test the fidelity of the explanation, we have three intuitions: \u2022 If features Fx are accurately selected, then removing Fx from the input x will lead to classifying this image to a different label, i.e., \u201cshoe\u201d (Figure 5c). \u2022 If features Fx are accurately selected, then adding the feature values of Fx to an image of \u201cshoe\u201d is likely to lead to a misclassification, i.e., classifying it as a \u201csweater\u201d (Figure 5d). \u2022 If features Fx are accurately selected, we can craft a synthetic images that only contains the features in Fx, and this synthetic image is likely to be classified as \u201csweater\u201d (Figure 5e).\nUsing these intuitions, we construct 3 different fidelity tests to validate the selected features. More formally, given an input instance x and its classification label y, LEMNA identifies a small set of important features (Fx) as the \u201cexplanation\u201d. We then follow the steps below to generate 3 testing samples t(x)1, t(x)2 and t(x)3 for feature validation: \u2022 Feature Deduction Test: we construct a sample t(x)1 by nullifying the selected features Fx from the instance x. \u2022 Feature Augmentation Test: we first select one random instance r from the opposite class (i.e., as long as r \u2019s label is not y). Then we construct t(x)2 by replacing the feature values of the instance r with those of Fx. \u2022 Synthetic Test: we construct t(x)3 as a synthetic instance. We preserve the feature values of the selected features Fx while randomly assigning values for the remaining features.\nThe key variable in this experiment is the number of important features selected as the \u201cexplanation\u201d (i.e., |Fx |). Intuitively, a larger |Fx | may yield a better explanation fidelity, but hurts the interpretability of results. We want to keep |Fx | small so that human analysts are able to comprehend.\nFor each classifier, we run the fidelity tests on the testing dataset (30% of the whole data). Given an instance x in the testing dataset, we generate 3 samples, one for each fidelity test. We feed the 3 samples into the classifier, and examine the positive classification rate (PCR). PCR measures the ratio of the samples still classified as x\u2019s original label. Note that \u201cpositive\u201d here does not mean \u201cmalware\u201d or \u201cfunction start\u201d. It simply means the new sample is still classified as the x\u2019s original label. If the feature selection is accurate, we expect the feature deduction samples return a low PCR, the feature augmentation samples return a high PCR, and the synthetic testing samples return a high PCR."
        },
        {
            "heading": "5.3 Experimental Results",
            "text": "Our experiments show that LEMNA outperforms LIME and the random baseline by a significant margin across all fidelity metrics. Local Approximation Accuracy. As shown in Table 3, LEMNA has a RMSE an order of magnitude smaller than that of LIME. This 5The image is selected from the Fashion-mnist dataset [69].\nMethod Binary Function Start PDF malware O0 O1 O2 O3\nLIME 0.1784 0.1532 0.1527 0.1750 0.1178 LEMNA 0.0102 0.0196 0.0113 0.0110 0.0264\n(a) Feature Deduction test. A lower PCR reflects a higher explanation fidelity.\n5 15 25 35 Nfeatures\n0\n25\n50\n75\n100\nP C\nR (%\n)\nBinary O0\n5 15 25 35 Nfeatures\n0\n25\n50\n75\n100\nP C\nR (%\n)\nBinary O3\n5 15 30 45 60 75 Nfeatures\n0\n25\n50\n75\n100\nP C\nR (%\n)\nGMM-FL LIME Random\nPDF Malware\n(b) Feature Augmentation test. A higher PCR reflects a higher explanation fidelity.\n5 15 25 35 Nfeatures\n0\n25\n50\n75\n100\nP C\nR (%\n)\nBinary O0\n5 15 25 35 Nfeatures\n0\n25\n50\n75\n100\nP C\nR (%\n)\nBinary O3\n5 15 30 45 60 75 Nfeatures\n0\n25\n50\n75\n100\nP C\nR (%\n)\nGMM-FL LIME Random\nPDF Malware\n(c) Synthetic test. A higher PCR reflects a higher explanation fidelity.\nFigure 6: Fidelity test results. y-axis denotes the positive classification rate PCR and y-axis denote the number of selected features NFeature by the explanation method. Due to the space limit, the results of Binary-O1 and O2 are shown in Appendix-D.\nobservation holds for both the malware classifier and the function start detection. The best performing result of LIME has a RMSE of 0.1532, which is still almost 10 times higher than the worse performing result of LEMNA ( 0.0196). This result confirms that our mixture regression model is able to build a much more accurate approximation than a simple linear model. Note that this metric is not applicable to the random baseline since the random baseline does not construct a decision boundary. Fidelity Tests. Figure 6a shows the results from feature deduction test. Recall feature deduction test is to remove important features from the input instances. A lower PCR indicates that selected features are more important to the classification decision. By only nullifying the top 5 features produced by LEMNA, the function start detector drops the PCR to 25% or lower. Considering the extremely high accuracy of the classifier (99.5%+, see Table 2), this drastic decrease of PCR indicates the small set of features are highly important to the classification. Note that the feature nullification is consider minor since the top 5 features only count of 2.5% of the\n200 total features in the input sequence. If we nullify the top 35 features, the PCR is dropped to almost 0.\nFigure 6b shows the results of the feature augmentation test. Recall that feature augmentation is to add the selected features of input x to an instance of the opposite class, expecting the classifier to produce a label of x. A higher PCR indicates the selected features are more important to x. The results are relatively consistent with the previous test: (1) adding a small number of top features can flip the label of the instance in the opposite class; (2) our method outperforms both baselines by a big margin. Noticeably, for the PDF malware classifier, by replacing the top 5 features, 75% of the testing cases flip their labels.\nFigure 6c shows a similar trend for the synthetic test. Using our selected features from a given x, the synthetic instances are more likely to be labeled as x\u2019s label. Using only 5 top features, the synthetic instances have a 85%\u201390% of the chance to take x\u2019s label, indicating that the core patterns have been successfully captured.\nAcross all three tests, our LEMNA outperforms LIME and the random baseline by a big margin. Interestingly, for the malware classifier, LIME performs as poor as random feature selection. This is because the feature vectors are sparse, which hurts the \u201csmoothness\u201d of the decision boundary. LIME has a hard time to accurately approximate the non-smooth boundary, which again validates our design intuition. Our system is more suitable for security applications, considering that security applications require a much higher explanation precision compared to image analysis tasks. Sensitivity of Hyper-parameters. Finally, we test how our results would change if the parameters are set differently. We tested a large number of parameter configurations, and find that our conclusions remain consistent. Due to the space limit, we summarize key results in Table 4. The three hyper-parameters are the \u201cnumber of crafted data samples\u201d for model fitting (N ), the \u201ctotal number of mixture components\u201d (K), and the \u201cthreshold for fused lasso\u201d (S). Table 4 presents the results of the binary function start detector on the O0 dataset. We show 4 groups of configurations where we change one parameter at a time. For the fidelity tests, we fix the number of selected features as 25 to calculate the PCR. The results confirm that changing the hyper-parameters do not significantly influence the performance of LEMNA."
        },
        {
            "heading": "6 APPLICATIONS OF ML EXPLANATION",
            "text": "So far, we have validated the fidelity of the explanation results. In this section, we present practical applications of LEMNA. We use case studies to show how the explanation results can help security analysts to 1) establish trusts to the trained classifiers, 2) troubleshoot classification errors, 3) and systematically patch the targeted errors. In the following, we primarily focus on the binary reverse-engineering application since this application domain of deep learning is relatively new and not well-understood. We have\nfeatures as red , followed by orange , gold , yellow . We also translate the hex code to assembling code for the ease of understanding. Note that the F. start refers to the function start detected by the deep learning classifier. The function start is also marked by a black square in the hex sequence. *For false negatives under R.F.N., we present the real function start that the classifier failed to detect, and explain why the function start is missed.\nperformed the same analysis for the PDF malware classifier, and the results are in Appendix-E."
        },
        {
            "heading": "6.1 Understanding Classifier Behavior",
            "text": "The primary application of our explanation method is to assess the reliability of the classifiers and help to establish the \u201ctrust\u201d. We argue that classifier reliability and trusts do not necessarily come from a high classification accuracy on the training data. Often cases, the training data is not complete enough to capture all the possible variances. Instead, trusts are more likely to be established by understanding the model behavior. In this section, we examine two key directions to understand how classifier makes decisions: (1) capturing and validating \u201cgolden rules\u201d and well-established heuristics; and (2) discovering new knowledge. Capturing Well-known Heuristics (C.W.H.). A reliable classifier should at least capture the well-known heuristics in the respective application domain. For example, in the area of binary reverse-engineering, security practitioners have accumulated a set of useful heuristics to identify the function start, some of which are even treated as \u201cgolden rules\u201d. Certain \u201cgolden rules\u201d are derived from the specifications of the Application Binary Interface (ABI) standards [22]. For example, the ABI requires a function to store the old frame pointer (ebp) at the start if this function maintains a new frame pointer. This leads to the most commonly seen prologue [push ebp; mov ebp, esp]. Another set of well-established rules come from mainstream compilers. For example, GNU GCC often inserts nop instructions before a function start, which aligns the function for architectural optimization [43].\nBy analyzing the explanation results, we observed strong evidence that deep learning classifiers have successfully captured well-known heuristics. In Table 5, we show 4 most representative\ncases, one for each classifier (or optimization level). In Case-1, the classifier correctly detected the function start at \u201c55\u201d. Then our LEMNA shows why 55 is marked as the function start by highlighting the importance of features (i.e., the hex code nearby). The result matches the well-known golden rule, namely [push ebp; mov ebp,esp]. This suggests the classifiers are making decisions in a reasonable way. Similarly, Case-2 captures the function start \u201c53\u201d right after a \u201cc3\u201d. This corresponds to a popular heuristic introduced by compilers as compilers often make a function exit in the end through a \u201cret\u201d instruction (particularly at the O0 and O1 level).\nIn Case-4, \u201c83\u201d is the function start and LEMNA highlighted the \u201c90\u201d in red. This indicates that the classifier follows the \u201c nop right before a function start\u201d rule, which is caused by compilers padding \u201cnop\u201ds prior to aligned functions. Similarly, in Case-3, LEMNA highlighted padding instruction [lea esi,[esi+eiz*1+0]], which is another pattern introduced by compilers. Overall, LEMNA shows that well-known heuristics are successfully captured by the classifiers.\nDuring our analysis, we observe that well-known heuristics are widely applicable at the lower optimization levels (O0, O1), but do not cover as many binaries at the higher levels (O2, O3). For example, 95% of the functions at O0-level start with [55 89 E5], matching the heuristics of Case-1. 74% of the O1-optimized functions have ret as the ending instruction (Case-2). On the contrary, only 30% of the binary functions at the O2 or O3 level match the well-known heuristics, e.g., padding instructions at the function end (\u201c[90 90 90 90]\u201d, \u201c[8d b4 26 00 00 00 00]\u201d. This makes intuitive sense because the higher-level optimization would significantly diversify the code structure, making golden rules less effective. Discovering New Knowledge (D.N.K.). In addition to matching well-known heuristics, we also examine if the classifiers have picked\nup new heuristics beyond existing knowledge. For security applications, we argue that the new heuristics need to be interpretable by domain experts. In the domain of binary analysis, many potentially useful heuristics are specific to individual functions, and it is hard to summarize all of them manually. For example, the utility functions inserted by the linker often have unique beginning code segments and those segments rarely appear elsewhere (e.g., the _start function always start with [xor ebp, ebp; pop esi]). Manually organizing such rules are not practical. However, these rules, once derived by LEMNA, would make intuitive sense to domain experts.\nAs shown in Table 5, we analyze the explanation results and find that classifiers indeed learned new knowledge. We select five representative cases (ID 5\u20139). Case-5 shows that \u201c31\u201d is detected as the function start because of the subsequent [ed 5e]. \u201c [31 ed 5e]\u201d corresponds to the start of utility function _start (namely [xor ebp, ebp; pop esi]). This illustrates that our explanation method can help summarize unique prologues pertaining to special functions. Note that the function start \u201c31\u201d itself is not necessarily an important indicator. In fact, \u201c31\u201d represents an opcode (xor) that often appears in the middle of the functions. It is \u201c[ed 5e]\u201d that leads to the correct detection.\nCase-6 illustrates another interesting pattern where \u201c2b\u201d is the most important feature to detect the function start at \u201cb8\u201d. \u201c2b\u201d resides in instruction following the pattern [mov eax, CONS1; sub eax, CONS2]where CONS1 and CONS2 are constant values and CONS1 - CONS2 = 0 or 3. This pattern appears only in the prologues of \u201cregister_tm_clones\u201d and \u201cderegister_tm_clones\u201d, which are utility functions for transactional memory. Again this is a function-specific pattern to detect function start.\nCase-7, Case-8 and Case-9 all have some types of \u201cpreparations\u201d at the function start. In Case-7, \u201c[83, ec]\u201d is marked as the most important feature, which corresponds to the instruction [sub esp, 0x1c]. Instructions of this form are frequently used at function start to prepare the stack frame. For Case-8, [mov eax, DWORD PTR [esp+0x4]] is marked as the most indicative feature. This instruction is usually inserted to fetch the first argument of a function. Note that \u201c04\u201d has the red color, which is because \u201c04\u201d is used as the offset for [esp+0x4] to fetch the argument of the function. If this offset is of a different value, this instruction would not necessarily be an indicator of the function start. For Case-9, it starts with preserving the registers that are later modified ([push ebp; push edi; push esi]). Preservation of those registers, which is required by the calling convention (a common ABI standard), also frequently appears at the function start.\nOverall, LEMNA validates that the classifiers\u2019 decision-making has largely followed explainable logics, which helps to establish the trust to these classifiers."
        },
        {
            "heading": "6.2 Troubleshooting Classification Errors",
            "text": "The deep neural networks, although highly accurate, still have errors. These errors should not be simply ignored since they often indicate insufficient training, which may be amplified in practice (due to the biased training). Our explanation method seeks to provide insights into \u201cwhat caused the error\u201d for a given misclassification.\nBy inspecting the reason of errors, we seek to provide actionable guidelines for targeted error correction. Reasons for False Negatives (R.F.N.). For the binary analysis application, the classifiers would occasionally miss the real function start. As shown in Table 5 (under \u201cR.F.N.\u201d), given a false negative, we explain \u201cwhy the real function start is not classified as a function start\u201d. Specifically, we feed the tuple (Code-sequence, Real-function-start) into LEMNA, and the red-colored features are the reasons for not recognizing the function start. For example, in Case-10, \u201c[50 fd]\u201d is marked as the main reason, which correspond to \u201c[jmp 0xfffffd50]\u201d. This instruction almost always appears in the middle of routines or functions, which misleads the classifier to think the substantial 31 is not a function start. This is an outlier case because this \u201c[50 fd]\u201d happens to be the last instruction of a special region .plt, which is followed by the _start function. Case-11 and Case-12 are mis-classified due to instructions \u201c[mov edx,eax]\u201d and \u201c[mov eax,ds:0x82014d0]\u201d, which often appear in the middle of functions. Reasons for False Positives (R.F.P.). Table 5 also show examples where the classifier picked the wrong function start. Here, we feed the tuple (Code-Sequence, Wrong-function-start) into LEMNA to explain why the wrong function start is picked. For example, Case-13 highlighted \u201cc3\u201d in red which represents the \u201cret\u201d instruction. Typically, \u201cret\u201d is located at the end of a function to make the exit, which makes the next byte \u201c83\u201d a strong candidate for the function start. However, Case-13 is special because \u201cret\u201d is actually placed in the middle of a function for optimization purposes. Case-14 and Case-15 are both misled by the padding instruction [lea esi,[esi+eiz*1+0x0]] which is often used to align functions. However, in both cases, this padding instruction is actually used to align the basic blocks inside of the function.\nOverall, LEMNA shows that the errors are largely caused by the fact that the misleading patterns are dominating over the real indicators. To mitigate such errors, we need to pinpoint the corresponding areas in the feature space and suppress the misleading patterns."
        },
        {
            "heading": "6.3 Targeted Patching of ML Classifiers",
            "text": "Based on the above results, we now develop automatic procedures to convert the\u201cinsights\u201d into actions to patch the classifiers. Patching Method. To patch a specific classification error, our idea is to identify the corresponding parts of the classifier that are undertrained. Then we craft targeted training samples to augment the original training data. Specifically, given a misclassified instance, we apply LEMNA to pinpoint the small set of features (Fx ) that cause the errors. Often cases, such instances are outliers in the training data, and do not have enough \u201ccounter examples\u201d. To this end, our strategy is to augment the training data by adding related \u201ccounter examples\u201d, by replacing the feature values of Fx with random values.\nWe use an example (Case-10 in Table 5) to describe the patching procedure. The classifier missed the function start due to \u201c[50 fd]\u201d, a hex pattern that often exists in the middle of a function. Ideally, the classifier should have picked up the other pattern \u201c[31 ed 5e]\u201d to locate the function start. Unfortunately, the impact of the wrong pattern is too dominating. To this end, we can add new samples to reduce the impact of the misleading features (\u201c[50 fd]\u201d) and\npromote the right indicator (\u201c[31 ed 5e]\u201d). The new samples are generated by replacing the hex value of \u201c[50 fd]\u201d with random hex values. By adding the new samples to the training data, we seek to reduce the errors in the retrained classifier. Evaluation Results. To demonstrate the effectiveness of patching, we perform the above procedure on all 5 classifiers. For each false positive and false negative, we generate kp and kn new samples respectively. Note that kp and kn are not necessarily the same, but they both need to be small. After all, we want to patch the targeted errors without hurting the already high accuracy of the classifiers. Consistently for all the classifiers, we replace the top 5 misleading features and retrain the models with 40 epochs.\nTable 6 shows the classifier performance before and after the patching. We have tested the sensitivity of the parameters and find the results remain relatively consistent as long as we set kp and kn between 2 to 10 (Appendix-F). Due to the space limit, Table 6 only presents one set of the results for each classifier. Our experiment shows that both false positives and false negatives can be reduced after retraining for all five classifiers. These results demonstrate that by understanding the model behavior, we can identify the weaknesses of the model and enhance the model accordingly."
        },
        {
            "heading": "7 DISCUSSION",
            "text": "Benefits v.s. Risks. LEMNA is designed to assist security analysts to understand, scrutinize and even patch a deep learning based security system. While designed from the defense perspective, it might be used by an attacker to seek theweakness of a deep learning classifier. However, we argue that this should not dilute the value of LEMNA, and should not be a reason for not developing explanation tools. The analogy is the software fuzzing techniques [13, 73]: while fuzzing tools can be used by hackers to seek vulnerabilities to exploit, the fuzzing techniques have significantly benefited the software industry by facilitating software testing to find and fix vulnerabilities before the software release. Guidelines for Analyzing LEMNA\u2019s Outputs. LEMNA outputs an \u201cexplanation\u201d to each testing case. To thoroughly examine a classifier, developers might need to run a large number of testing cases through LEMNA. Manually reading each case\u2019s explanation is timeconsuming, and thus we suggest a more efficient method, which is to group similar explanations first. In \u00a76, we grouped explanations that are exactly the same before picking the \u201cmost representative\u201d\ncases. In practice, developers can use any other clustering techniques to group explanations as needed. Broader Security Applications. LEMNA is evaluated using two popular security applications. There are many other security applications such as detecting the \u201cfunction end\u201d for binary code, pinpointing the function types and detecting vulnerable code [15, 24, 47, 52, 66]. They can also potentially benefit from LEMNA, given that their deep learning architectures are RNN or MLP. Note that models like CNN share some similarities with MLP, and thus LEMNA can potentially help with related applications (e.g., image analysis). Future work will explore the applicability of LEMNA in broader application domains. Other Deep Learning Architectures. In addition to MLP and RNN, there are other deep learning architectures such as sequence-tosequence networks [4, 60], and hybrid networks [25, 36, 71]. Although, these architectures primarily find success in fields such as machine translation [4] and image captioning [25], initial evidence shows that they have the potential to play a bigger role in security [36, 71]. Once concrete security applications are built in the future, we plan to test LEMNA on these new architectures. Feature Obfuscation. LEMNA is useful when features are interpretable, but this may not be true for all applications. In particular, researchers recently proposed various methods [8, 67, 70] to obfuscate input features to increase the difficulty of running adversarial attacks. Possibly because feature obfuscation often degrades classifier accuracy, these techniques haven\u2019t received a wide usage yet. LEMNA is not directly applicable to classifiers trained on obfuscated features. However, if the model developer has a mapping between the raw and obfuscated features, the developer can still translate LEMNA\u2019s output to the interpretable features."
        },
        {
            "heading": "8 OTHER RELATEDWORK",
            "text": "Since most related works have been discussed in \u00a72 and \u00a73, we briefly discuss other related works here. ImprovingMachine LearningRobustness.Adeep learningmodel can be deceived by an adversarial sample (i.e., a malicious input crafted to cause misclassification) [61]. To improve the model resistance, researchers have proposed various defense methods [9, 20, 36, 40, 67]. The most relevant work is adversarial training [20]. Adversarial training seeks to add adversarial examples to the training dataset to retrain a more robust model. Various techniques are available to craft adversarial examples for adversarial training [11, 33, 42, 72]. A key difference between our patching method and the standard adversarial training is that our patching is based on the understanding of the errors. We try to avoid blindly retraining the model which may introduce new vulnerabilities. Mitigating the Influence of Contaminated Data. Recent research has explored ways to mitigate misclassifications introduced by contaminated training data [10, 12, 46, 65]. A representative method is \u201cmachine unlearning\u201d [10], which is to remove the influence of certain training data by transforming the standard training algorithms into a summation form. A more recent work [29] proposes to utilize an influence function to identify data points that contribute to misclassification. Our approach is complementary to\nexisting works: we propose to augment training data to fix undertrained components (instead of removing bad training data). More importantly, LEMNA helps the human analysts to understand these errors before patching them."
        },
        {
            "heading": "9 CONCLUSION",
            "text": "This paper introduces LEMNA, a new method to derive high-fidelity explanations for individual classification results for security applications. LEMNA treats a target deep learning model as a blackbox and approximates its decision boundary through a mixture regression model enhanced by fused lasso. By evaluating it on two popular deep learning based security applications, we show that the proposed method produces highly accurate explanations. In addition, we demonstrate howmachine learning developers and security analysts can benefit from LEMNA to better understand classifier behavior, troubleshoot misclassification errors, and even perform automated patches to enhance the original deep learning model."
        },
        {
            "heading": "10 ACKNOWLEDGMENTS",
            "text": "We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Tesla K40 GPU used for this research. We also would like to thank the anonymous reviewers and Jia Yan for their helpful feedback. This project was supported in part by NSF grants CNS-1718459, CNS-1750101 and CNS-1717028. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of any funding agencies."
        },
        {
            "heading": "APPENDIX - A. DETAIL OF EM ALGORITHM",
            "text": "As is specified in Section \u00a74, we utilize EM algorithm to estimate parameters while learning a mixture regression model enhanced by fused lasso. Here, we provide more detail about this process.\nRecall that a mixture regression model contains K components, each of which indicates an individual linear regression model. In the E-Step, we assign each data sample xi to a Gaussian distribution corresponding to one of the components. To achieve this, we introduce a set of latent variables {zi1, zi2, ..., ziK }, and use it to indicate to which distribution a data sample is assigned. Note that we use zik = 1 to represent that the data sample xi is assigned to the k th distribution.\nTo compute values for latent variables, we define p (zik = 1) = \u03c0k , (8)\nand thus have the following\np (yi |xi , zi1:K ) = K\u220f k=1 [N (yi |\u03b2kxi ,\u03c3 2k )] zik , (9)\nwhereN (yi |\u03b2kxi ,\u03c3 2k ) indicates the k th Gaussian distribution with the mean and variance equal to \u03b2kxi and \u03c3 2k respectively. From the Equation (9), we can derive a likelihood function below\np (y, z |x,\u0398) = N\u220f i=1 p (yi , zi1, . . . , zin |\u03b2xi ,\u03c3 2)\n= K\u220f k=1 N\u220f i=1 [\u03c0kN (xi |\u03b2kxi ,\u03c3 2k )] zik\n= K\u220f k=1 \u03c0 nk k N\u220f i=1 [N (xi |\u03b2kxi ,\u03c3 2k )] zik\n(10)\nfrom which we can further compute the expectation of this loglikelihood function (i.e., Q function) as follow: Q (\u0398,\u0398(t ) ) =E[logp (y, z |x,\u0398) |y, x,\u0398(t )]\n= K\u2211 k=1 {nk log\u03c0k + N\u2211 i=1 z\u0302ik \u00b7\n[log( 1\u221a 2\u03c0 ) \u2212 log\u03c0k \u2212 1 \u03c3 2k (yi \u2212 \u03b2kxi )2]} .\n(11)\nHere,nk = \u2211N k=1 Ezik .\u0398 indicates all of the parameters. z\u0302ik = Ezik which can be further represented as\n\u02c6zik = \u03c0kN (yi |\u03b2kxi ,\u03c3 2k )\u2211K k=1 \u03c0kN (yi |\u03b2kxi ,\u03c3 2 k ) , i = 1, . . . ,N ,k = 1, . . . ,K ,\n(12) With the latent variables computed through the Equation (12), we can assign each data sample to a corresponding Gaussian distribution. Then, in the M-step, we re-compute the parameters by maximizing the aforementioned Q function with respect to each parameter. More specifically, we can compute parameter \u03c3 2k and \u03c0k by using the following equations\n\u03c3 2k =\n\u2211N i=1 z\u0302ik (yi \u2212 \u03b2kxi ) 2\nnk ,k = 1, 2, . . . ,K ,\n\u03c0k = nk N ,k = 1, 2, . . . ,K .\n(13)\nRecall that we re-compute parameter \u03b21:K by minimizing the Equation (7) shown in Section \u00a74. While it can be resolved by using MLE, in order to improve the efficiency of resolving this equation, we can also an alternative algorithm introduced in [64]. As is depicted\nin Figure 7, we can repeatedly perform E-step and then M-step until the parameters converge, and thus output the mixture regression model enhanced by fused lasso."
        },
        {
            "heading": "APPENDIX - B. MULTI-CLASS VS MULTIPLE SINGLE-CLASS APPROXIMATION",
            "text": "As is mentioned in Section 4.3, we choose to perform model approximation with multiple single-class approximation rather than a single muti-class approximation. Here, we discuss the rationale behind our choice.\nAs is stated in Section 4.1, the Equation (4) represents a practice that estimates parameters for a binary classifier, in which there are K \u00d7 (2 +M ) parameters involved in the process of model learning. For a single mixture regression model that classifies a data sample xi into one of L categories (L > 2), the parameter \u03b2k and \u03c32k no longer represent a vector and a singular value. Rather, they denote matrices with the dimensionality of L \u00d7M and L \u00d7 L respectively. In the process of learning a mixture regression model, this means that, in addition to \u03c01:K which still represents K parameters, the learning algorithm needs to estimate \u03b21:K and \u03c321:K , which denote L \u00d7 K \u00d7M and L2 \u00d7 K parameters respectively.\nAccording to learning heuristics [23, 44], the more parameters a learning algorithm needs to estimate, the more data samples it would typically need. Technically speaking, following the data point sampling approach commonly used by other model induction explanation techniques, we have no difficulty in synthesizing sufficient data samples to perform model learning (i.e., parameter estimation) reasonably well. However, the practice shows that learning a model with a large amount of data samples typically requires substantial amount of computation resources. Recall that for each data sample we have to train an individual mixture regression model in order to derive an explanation. Therefore, we select the single-class approximation scheme that can yield an explanation in a more efficient fashion, even though both of the approximation schemes could yield model(s) representing the equally good approximation for the corresponding local decision boundary."
        },
        {
            "heading": "APPENDIX - C. HYPER-PARAMETERS OF TARGET DEEP LEARNING MODEL",
            "text": "In Table 7, we show the hyper-parameters used for training corresponding deep learning models. Regarding function start detector, we utilized a recurrent neural network in which its first, second and output layers are an embedding layer with 256 units, a bi-directional RNN with 8 hidden units and a softmax classifier respectively. With respect to the application of PDF malware classification, we used a standard MLP which contains one input layer, three hidden layers and one output layer. The number of hidden units tied to each layer is presented in Table 7.\nfiles. The feature values have been normalized to 0 or 1. We mark the most important features as red , followed by orange ,\ngold , yellow ."
        },
        {
            "heading": "APPENDIX - D. FIDELITY TEST FOR O1 AND O2 COMPILATION OPTIONS",
            "text": "Figure 8 shows the results of fidelity tests for O1 and O2 datasets. The results are consistent with those of other classifiers."
        },
        {
            "heading": "APPENDIX - E. MALWARE CLASSIFIER CASES",
            "text": "Table 8 shows 4 cases studies on the PDF Malware classifier\u2019s decisions, which correspond to true positives, true negatives, false positives and fale negatives respectively. We also present the labels assigned by the classifier. Catching Well-known Heuristics (C.W.H.). Case-16 classified as a malware primarily because feature F31 and F33 are set to nonzero values. As is shown in Table 8, these features are related to javascript objects, which match well-known heuristics and indicators of malicious PDF files. In the contrary, Case-17 has a benign\nfile and features related to javascripts have zero values (e.g., no javascript code in the file). Reasons for False Positives/Negative (R.F.P., R.F.N). Case-18 and Case-19 represents false positives and negatives. Our explanation results show that the two instances are mis-classified because they violated the well-known heuristics learned by the classifier. For example, Case-18 is a malware that contains \u201cng\u201d injected in the javascript. As a result, the Features F31 and F33 both have a zero value, and the classifier cannot detect this type of malware. On the contrary, if the benign file somehow contains some javascript code (e.g., Case-19), the classifier will incorrectly label them as malware."
        },
        {
            "heading": "APPENDIX - F. SENSITIVITY OF kn AND kp",
            "text": "In section \u00a76.3, the patching method has two hyper-parameters kn and kp . Here, we show the results of the sensitivity tests on these two these parameters. We select the classifier trained for binary function start detection using the O2 dataset. Our experiment mythology is to fix one parameter and swap the other one. Then we observe the changes of the re-trained classifier\u2019s false positives and false negatives. In Figure 9a, we fix kn = 4 and then set Kp = 1, 3, 5, 7, 9. In Figure 9b, we fix kp = 5 and set kn = 2, 4, 6, 8, 10. The results show that increasing kp will reduce false positives but may increase false negatives. On the contrary, increasing kn will reduce false negatives but may increase false positives. The results confirm our statements in \u00a76.3. Targeted patching should limit to using small kp and kn to patch the target errors while avoiding introducing new errors. By adjusting kp and kn , security analysts can reduce false positives and false negatives at the same time. In \u00a76.3 we present the selected results where the false positives and the false negatives are relatively balanced (kn = 4 and kp = 5 for this classifier)."
        }
    ],
    "title": "LEMNA: Explaining Deep Learning based Security Applications",
    "year": 2018
}