{
    "abstractText": "As deep learning models are applied to increasingly diverse problems, a key bottleneck is gathering enough high-quality training labels tailored to each task. Users therefore turn to weak supervision, relying on imperfect sources of labels like pattern matching and user-defined heuristics. Unfortunately, users have to design these sources for each task. This process can be time consuming and expensive: domain experts often perform repetitive steps like guessing optimal numerical thresholds and developing informative text patterns. To address these challenges, we present Snuba, a system to automatically generate heuristics using a small labeled dataset to assign training labels to a large, unlabeled dataset in the weak supervision setting. Snuba generates heuristics that each labels the subset of the data it is accurate for, and iteratively repeats this process until the heuristics together label a large portion of the unlabeled data. We develop a statistical measure that guarantees the iterative process will automatically terminate before it degrades training label quality. Snuba automatically generates heuristics in under five minutes and performs up to 9.74 F1 points better than the best known user-defined heuristics developed over many days. In collaborations with users at research labs, Stanford Hospital, and on open source datasets, Snuba outperforms other automated approaches like semisupervised learning by up to 14.35 F1 points. PVLDB Reference Format: Paroma Varma, Christopher R\u00e9. Snuba: Automating Weak Supervision to Label Training Data. PVLDB, 12(3): 223-236, 2018. DOI: https://doi.org/10.14778/3291264.3291268",
    "authors": [
        {
            "affiliations": [],
            "name": "Paroma Varma"
        },
        {
            "affiliations": [],
            "name": "Christopher R\u00e9"
        }
    ],
    "id": "SP:290941887c37547462285152ff82796ad71dd757",
    "references": [
        {
            "authors": [
                "E. Alfonseca",
                "K. Filippova",
                "J.-Y. Delort",
                "G. Garrido"
            ],
            "title": "Pattern learning for relation extraction with a hierarchical topic model",
            "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2, pages 54\u201359. Association for Computational Linguistics,",
            "year": 2012
        },
        {
            "authors": [
                "R. Alur",
                "R. Bodik",
                "G. Juniwal",
                "M.M. Martin",
                "M. Raghothaman",
                "S.A. Seshia",
                "R. Singh",
                "A. Solar-Lezama",
                "E. Torlak",
                "A. Udupa"
            ],
            "title": "Syntax-guided synthesis",
            "venue": "234 Formal Methods in Computer-Aided Design (FMCAD), 2013, pages 1\u20138. IEEE,",
            "year": 2013
        },
        {
            "authors": [
                "S.H. Bach",
                "B. He",
                "A. Ratner",
                "C. R\u00e9"
            ],
            "title": "Learning the structure of generative models without labeled data",
            "venue": "International Conference on Machine Learning, pages 273\u2013282,",
            "year": 2017
        },
        {
            "authors": [
                "A. Blum",
                "T. Mitchell"
            ],
            "title": "Combining labeled and unlabeled data with co-training",
            "venue": "Proceedings of the eleventh annual conference on Computational learning theory, pages 92\u2013100. ACM,",
            "year": 1998
        },
        {
            "authors": [
                "R. Bunescu",
                "R. Mooney"
            ],
            "title": "Learning to extract relations from the web using minimal supervision",
            "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 576\u2013583,",
            "year": 2007
        },
        {
            "authors": [
                "O. Chapelle",
                "B. Scholkopf",
                "A. Zien"
            ],
            "title": "Semi-supervised learning (Chapelle, O",
            "venue": "et al., eds.; 2006)[book reviews]. IEEE Transactions on Neural Networks, 20(3):542\u2013542,",
            "year": 2009
        },
        {
            "authors": [
                "M. Craven",
                "J. Kumlien"
            ],
            "title": "Constructing biological knowledge bases by extracting information from text sources",
            "venue": "In ISMB,",
            "year": 1999
        },
        {
            "authors": [
                "N. Dalvi",
                "A. Dasgupta",
                "R. Kumar",
                "V. Rastogi"
            ],
            "title": "Aggregating crowdsourced binary ratings",
            "venue": "Proceedings of the 22nd international conference on World Wide Web, pages 285\u2013294. ACM,",
            "year": 2013
        },
        {
            "authors": [
                "A.P. Davis",
                "C.J. Grondin",
                "R.J. Johnson",
                "D. Sciaky",
                "B.L. King",
                "R. McMorran",
                "J. Wiegers",
                "T.C. Wiegers",
                "C.J. Mattingly"
            ],
            "title": "The comparative toxicogenomics database: update 2017",
            "venue": "Nucleic acids research, 45(D1):D972\u2013D978,",
            "year": 2016
        },
        {
            "authors": [
                "A.P. Dawid",
                "A.M. Skene"
            ],
            "title": "Maximum likelihood estimation of observer error-rates using the EM algorithm",
            "venue": "Applied statistics, pages 20\u201328,",
            "year": 1979
        },
        {
            "authors": [
                "J. Deng",
                "W. Dong",
                "R. Socher",
                "L.-J. Li",
                "K. Li",
                "L. Fei-Fei"
            ],
            "title": "ImageNet: A large-scale hierarchical image database",
            "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248\u2013255. IEEE,",
            "year": 2009
        },
        {
            "authors": [
                "X.L. Dong",
                "D. Srivastava"
            ],
            "title": "Big data integration",
            "venue": "Synthesis Lectures on Data Management, 7(1):1\u2013198,",
            "year": 2015
        },
        {
            "authors": [
                "A. Graves",
                "J. Schmidhuber"
            ],
            "title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures",
            "venue": "Neural Networks, 18(5-6):602\u2013610,",
            "year": 2005
        },
        {
            "authors": [
                "S. Gulwani"
            ],
            "title": "Synthesis from examples: Interaction models and algorithms",
            "venue": "Symbolic and Numeric Algorithms for Scientific Computing (SYNASC), 2012 14th International Symposium on, pages 8\u201314. IEEE,",
            "year": 2012
        },
        {
            "authors": [
                "T. Hastie",
                "S. Rosset",
                "J. Zhu",
                "H. Zou"
            ],
            "title": "Multi-class AdaBoost",
            "venue": "Statistics and its Interface, 2(3):349\u2013360,",
            "year": 2009
        },
        {
            "authors": [
                "M.A. Hearst"
            ],
            "title": "Automatic acquisition of hyponyms from large text corpora",
            "venue": "Proceedings of the 14th conference on Computational linguistics-Volume 2, pages 539\u2013545. Association for Computational Linguistics,",
            "year": 1992
        },
        {
            "authors": [
                "G.E. Hinton"
            ],
            "title": "Training products of experts by minimizing contrastive divergence",
            "venue": "Neural computation, 14(8):1771\u20131800,",
            "year": 2002
        },
        {
            "authors": [
                "S. Hochreiter",
                "J. Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural computation, 9(8):1735\u20131780,",
            "year": 1997
        },
        {
            "authors": [
                "P. Jaccard"
            ],
            "title": "Lois de distribution florale dans la zone alpine",
            "venue": "Bull Soc Vaudoise Sci Nat, 38:69\u2013130,",
            "year": 1902
        },
        {
            "authors": [
                "S. Jha",
                "S. Gulwani",
                "S.A. Seshia",
                "A. Tiwari"
            ],
            "title": "Oracle-guided component-based program synthesis",
            "venue": "Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering-Volume 1, pages 215\u2013224. ACM,",
            "year": 2010
        },
        {
            "authors": [
                "M. Joglekar",
                "H. Garcia-Molina",
                "A. Parameswaran"
            ],
            "title": "Comprehensive and reliable crowd assessment algorithms",
            "venue": "Data Engineering (ICDE), 2015 IEEE 31st International Conference on, pages 195\u2013206. IEEE,",
            "year": 2015
        },
        {
            "authors": [
                "J. Johnson",
                "B. Hariharan",
                "L. van der Maaten",
                "L. Fei-Fei",
                "C.L. Zitnick",
                "R. Girshick"
            ],
            "title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning",
            "venue": "In Computer Vision and Pattern Recognition (CVPR),",
            "year": 2017
        },
        {
            "authors": [
                "S. Kok",
                "P. Domingos"
            ],
            "title": "Statistical predicate invention",
            "venue": "Proceedings of the 24th international conference on Machine learning, pages 433\u2013440. ACM,",
            "year": 2007
        },
        {
            "authors": [
                "R. Krishna",
                "Y. Zhu",
                "O. Groth",
                "J. Johnson",
                "K. Hata",
                "J. Kravitz",
                "S. Chen",
                "Y. Kalantidis",
                "L.-J. Li",
                "D.A. Shamma"
            ],
            "title": "Visual Genome: Connecting language and vision using crowdsourced dense image annotations",
            "venue": "International Journal of Computer Vision,",
            "year": 2017
        },
        {
            "authors": [
                "A. Krizhevsky",
                "I. Sutskever",
                "G.E. Hinton"
            ],
            "title": "ImageNet classification with deep convolutional neural networks",
            "venue": "Advances in neural information processing systems, pages 1097\u20131105,",
            "year": 2012
        },
        {
            "authors": [
                "N. Lao",
                "W.W. Cohen"
            ],
            "title": "Relational retrieval using a combination of path-constrained random walks",
            "venue": "Machine learning, 81(1):53\u201367,",
            "year": 2010
        },
        {
            "authors": [
                "H. Li",
                "B. Yu",
                "D. Zhou"
            ],
            "title": "Error rate analysis of labeling by crowdsourcing",
            "venue": "ICML Workshop: Machine Learning Meets Crowdsourcing. Atalanta, Georgia, USA. Citeseer,",
            "year": 2013
        },
        {
            "authors": [
                "Y. Li",
                "J. Gao",
                "C. Meng",
                "Q. Li",
                "L. Su",
                "B. Zhao",
                "W. Fan",
                "J. Han"
            ],
            "title": "A survey on truth discovery",
            "venue": "ACM Sigkdd Explorations Newsletter, 17(2):1\u201316,",
            "year": 2016
        },
        {
            "authors": [
                "T.-Y. Lin",
                "M. Maire",
                "S. Belongie",
                "J. Hays",
                "P. Perona",
                "D. Ramanan",
                "P. Doll\u00e1r",
                "C.L. Zitnick"
            ],
            "title": "Microsoft COCO: Common objects in context",
            "venue": "European conference on computer vision, pages 740\u2013755. Springer,",
            "year": 2014
        },
        {
            "authors": [
                "C. Metz"
            ],
            "title": "https://www.crowdflower.com/data/ airline-twitter-sentiment",
            "venue": "Crowdflower dataset: Airline Twitter sentiment,",
            "year": 2015
        },
        {
            "authors": [
                "M. Mintz",
                "S. Bills",
                "R. Snow",
                "D. Jurafsky"
            ],
            "title": "Distant supervision for relation extraction without labeled data",
            "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 1003\u20131011. Association for Computational Linguistics,",
            "year": 2009
        },
        {
            "authors": [
                "R. Mooney"
            ],
            "title": "Relational learning of pattern-match rules for information extraction",
            "venue": "Proceedings of the Sixteenth National Conference on Artificial Intelligence, volume 334,",
            "year": 1999
        },
        {
            "authors": [
                "S.J. Pan",
                "Q. Yang"
            ],
            "title": "A survey on transfer learning",
            "venue": "IEEE Transactions on knowledge and data engineering, 22(10):1345\u20131359,",
            "year": 2010
        },
        {
            "authors": [
                "F. Pedregosa",
                "G. Varoquaux",
                "A. Gramfort",
                "V. Michel",
                "B. Thirion",
                "O. Grisel",
                "M. Blondel",
                "P. Prettenhofer",
                "R. Weiss",
                "V. Dubourg"
            ],
            "title": "Scikit-learn: Machine learning in Python",
            "venue": "Journal of machine learning research,",
            "year": 2011
        },
        {
            "authors": [
                "J. Pennington",
                "R. Socher",
                "C. Manning"
            ],
            "title": "GloVe: Global vectors for word representation",
            "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532\u20131543,",
            "year": 2014
        },
        {
            "authors": [
                "R. Pochampally",
                "A. Das Sarma",
                "X.L. Dong",
                "A. Meliou",
                "D. Srivastava"
            ],
            "title": "Fusing data with correlations",
            "venue": "Proceedings of the 2014 ACM SIGMOD international conference on Management of data, pages 433\u2013444. ACM,",
            "year": 2014
        },
        {
            "authors": [
                "A. Ratner",
                "S.H. Bach",
                "H. Ehrenberg",
                "J. Fries",
                "S. Wu",
                "C. R\u00e9"
            ],
            "title": "Snorkel: Rapid training data creation with weak supervision",
            "venue": "PVLDB, 11(3):269\u2013282,",
            "year": 2017
        },
        {
            "authors": [
                "A.J. Ratner",
                "C.M. De Sa",
                "S. Wu",
                "D. Selsam",
                "C. R\u00e9"
            ],
            "title": "Data programming: Creating large training sets, quickly",
            "venue": "Advances in Neural Information Processing Systems, pages 3567\u20133575,",
            "year": 2016
        },
        {
            "authors": [
                "T. Rekatsinas",
                "M. Joglekar",
                "H. Garcia-Molina",
                "A. Parameswaran",
                "C. R\u00e9"
            ],
            "title": "SLiMFast: Guaranteed results for data fusion and source reliability",
            "venue": "Proceedings of the 2017 ACM International Conference on Management of Data, pages 1399\u20131414. ACM,",
            "year": 2017
        },
        {
            "authors": [
                "B. Roth",
                "D. Klakow"
            ],
            "title": "Combining generative and discriminative model scores for distant supervision",
            "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 24\u201329,",
            "year": 2013
        },
        {
            "authors": [
                "S.R. Safavian",
                "D. Landgrebe"
            ],
            "title": "A survey of decision tree classifier methodology",
            "venue": "IEEE transactions on systems, man, and cybernetics, 21(3):660\u2013674,",
            "year": 1991
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv preprint arXiv:1409.1556,",
            "year": 2014
        },
        {
            "authors": [
                "R. Singh",
                "S. Gulwani"
            ],
            "title": "Synthesizing number transformations from input-output examples",
            "venue": "International Conference on Computer Aided Verification, pages 634\u2013651. Springer,",
            "year": 2012
        },
        {
            "authors": [
                "S. Soderland"
            ],
            "title": "Learning information extraction rules for semi-structured and free text",
            "venue": "Machine learning, 34(1-3):233\u2013272,",
            "year": 1999
        },
        {
            "authors": [
                "A. Solar-Lezama",
                "L. Tancau",
                "R. Bodik",
                "S. Seshia",
                "V. Saraswat"
            ],
            "title": "Combinatorial sketching for finite programs",
            "venue": "ACM Sigplan Notices, 41(11):404\u2013415,",
            "year": 2006
        },
        {
            "authors": [
                "S. Takamatsu",
                "I. Sato",
                "H. Nakagawa"
            ],
            "title": "Reducing wrong labels in distant supervision for relation extraction",
            "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 721\u2013729. Association for Computational Linguistics,",
            "year": 2012
        },
        {
            "authors": [
                "S. Van der Walt",
                "J.L. Sch\u00f6nberger",
                "J. Nunez-Iglesias",
                "F. Boulogne",
                "J.D. Warner",
                "N. Yager",
                "E. Gouillart",
                "T. Yu"
            ],
            "title": "scikit-image: image processing",
            "venue": "in Python. PeerJ,",
            "year": 2014
        },
        {
            "authors": [
                "P. Varma",
                "B.D. He",
                "P. Bajaj",
                "N. Khandwala",
                "I. Banerjee",
                "D. Rubin",
                "C. R\u00e9"
            ],
            "title": "Inferring generative model structure with static analysis",
            "venue": "Advances in Neural Information Processing Systems, pages 239\u2013249,",
            "year": 2017
        },
        {
            "authors": [
                "P. Varma",
                "D. Iter",
                "C. De Sa",
                "C. R\u00e9"
            ],
            "title": "Flipper: A systematic approach to debugging training sets",
            "venue": "Proceedings of the 2nd Workshop on Human-In-the-Loop Data Analytics, page 5. ACM,",
            "year": 2017
        },
        {
            "authors": [
                "P. Varma",
                "C. R\u00e9"
            ],
            "title": "Snuba: Automating weak supervision to label training data, 2018",
            "year": 2018
        },
        {
            "authors": [
                "P. Varma",
                "R. Yu",
                "D. Iter",
                "C. De Sa",
                "C. R\u00e9"
            ],
            "title": "Socratic learning: Correcting misspecified generative models using discriminative models",
            "venue": "arXiv preprint arXiv:1610.08123,",
            "year": 2017
        },
        {
            "authors": [
                "F. Wang",
                "C. Rudin"
            ],
            "title": "Falling rule lists",
            "venue": "Artificial Intelligence and Statistics, pages 1013\u20131022,",
            "year": 2015
        },
        {
            "authors": [
                "T. Wang",
                "C. Rudin",
                "F. Doshi-Velez",
                "Y. Liu",
                "E. Klampfl",
                "P. MacNeille"
            ],
            "title": "Or\u2019s of and\u2019s for interpretable classification, with application to context-aware recommender systems",
            "venue": "arXiv preprint arXiv:1504.07614,",
            "year": 2015
        },
        {
            "authors": [
                "W.Y. Wang",
                "K. Mazaitis",
                "W.W. Cohen"
            ],
            "title": "Structure learning via parameter learning",
            "venue": "Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, pages 1199\u20131208. ACM,",
            "year": 2014
        },
        {
            "authors": [
                "C.-H. Wei",
                "Y. Peng",
                "R. Leaman",
                "A.P. Davis",
                "C.J. Mattingly",
                "J. Li",
                "T.C. Wiegers",
                "Z. Lu"
            ],
            "title": "Overview of the biocreative v chemical disease relation (CDR) task",
            "venue": "Proceedings of the fifth BioCreative challenge evaluation workshop, pages 154\u2013166. Sevilla Spain,",
            "year": 2015
        },
        {
            "authors": [
                "S. Wu",
                "L. Hsiao",
                "X. Cheng",
                "B. Hancock",
                "T. Rekatsinas",
                "P. Levis",
                "C. R\u00e9"
            ],
            "title": "Fonduer: Knowledge base construction from richly formatted data",
            "venue": "Proceedings of the 2018 International Conference on Management of Data, pages 1301\u20131316. ACM,",
            "year": 2018
        },
        {
            "authors": [
                "F. Yang",
                "Z. Yang",
                "W.W. Cohen"
            ],
            "title": "Differentiable learning of logical rules for knowledge base reasoning",
            "venue": "Advances in Neural Information Processing Systems, pages 2319\u20132328,",
            "year": 2017
        },
        {
            "authors": [
                "Y. Zhang",
                "X. Chen",
                "D. Zhou",
                "M.I. Jordan"
            ],
            "title": "Spectral methods meet EM: A provably optimal algorithm for crowdsourcing",
            "venue": "Advances in neural information processing systems, pages 1260\u20131268,",
            "year": 2014
        },
        {
            "authors": [
                "D. Zhou",
                "O. Bousquet",
                "T.N. Lal",
                "J. Weston",
                "B. Sch\u00f6lkopf"
            ],
            "title": "Learning with local and global consistency",
            "venue": "Advances in neural information processing systems, pages 321\u2013328,",
            "year": 2004
        }
    ],
    "sections": [
        {
            "text": "PVLDB Reference Format: Paroma Varma, Christopher Re\u0301. Snuba: Automating Weak Supervision to Label Training Data. PVLDB, 12(3): 223-236, 2018. DOI: https://doi.org/10.14778/3291264.3291268"
        },
        {
            "heading": "1. INTRODUCTION",
            "text": "The success of machine learning for tasks like image recognition and natural language processing [12, 14] has ignited interest in using similar techniques for a variety of tasks. However, gathering enough training labels is a major bottleneck in applying machine learning to new tasks. In response, there has been a shift towards relying on weak su-\nThis work is licensed under the Creative Commons AttributionNonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. For any use beyond those covered by this license, obtain permission by emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights licensed to the VLDB Endowment. Proceedings of the VLDB Endowment, Vol. 12, No. 3 ISSN 2150-8097. DOI: https://doi.org/10.14778/3291264.3291268\npervision, or methods that can assign noisy training labels to unlabeled data, like crowdsourcing [9, 22, 60], distant supervision [8,32], and user-defined heuristics [38,39,50]. Over the past few years, we have been part of the broader effort to enhance methods based on user-defined heuristics to extend their applicability to text, image, and video data for tasks in computer vision, medical imaging, bioinformatics and knowledge base construction [4, 39,50].\nThrough our engagements with users at large companies, we find that experts spend a significant amount of time designing these weak supervision sources. As deep learning techniques are adopted for unconventional tasks like analyzing codebases and now commodity tasks like driving marketing campaigns, the few domain experts with required knowledge to write heuristics cannot reasonably keep up with the demand for several specialized, labeled training datasets. Even machine learning experts, such as researchers at the computer vision lab at Stanford, are impeded by the need to crowdsource labels before even starting to build models for novel visual prediction tasks [23, 25]. This raises an important question: can we make weak supervision techniques easier to adopt by automating the process of generating heuristics that assign training labels to unlabeled data?\nThe key challenge in automating weak supervision lies in replacing the human reasoning that drives heuristic development. In our collaborations with users with varying levels of machine learning expertise, we noticed that the process to develop these weak supervision sources can be fairly repetitive. For example, radiologists at the Stanford Hospital and Clinics have to guess the correct threshold for each heuristic that uses a geometric property of a tumor to determine if it is malignant (example shown in Figure 1). We instead take advantage of a small, labeled dataset to automatically generate noisy heuristics. Though the labeled dataset is too small to train an end model, it has enough information to\ngenerate heuristics that can assign noisy labels to a large, unlabeled dataset and improve end model performance by up to 12.12 F1 points. To aggregate labels from these heuristics, we improve over majority vote by relying on existing factor graph-based statistical techniques in weak supervision that can model the noise in and correlation among these heuristics [2,4,39,41,48,50]. However, these techniques were intended to work with user-designed labeling sources and therefore have limits on how robust they are. Automatically generated heuristics can be noisier than what these models can account for and introduce the following challenges:\nAccuracy. Users tend to develop heuristics that assign accurate labels to a subset of the unlabeled data. An automated method has to properly model this trade-off between accuracy and coverage for each heuristic based only on the small, labeled dataset. Empirically, we find that generating heuristics that each labels all the datapoints can degrade end model performance by up to 20.69 F1 points.\nDiversity. Since each heuristic has limited coverage, users develop multiple heuristics that each labels a different subset to ensure a large portion of the unlabeled data receives a label. In an automated approach, we could mimic this by maximizing the number of unlabeled datapoints the heuristics label as a set. However, this approach can select heuristics that cover a large portion of the data but have poor performance. There is a need to account for both the diversity and performance of the heuristics as a set. Empirically, balancing both aspects improves end model performance by up to 18.20 F1 points compared to selecting the heuristic set that labels the most datapoints.\nTermination Condition. Users stop generating heuristics when they have exhausted their domain knowledge. An automated method, however, can continue to generate heuristics that deteriorate the overall quality of the training labels assigned to the unlabeled data, such as heuristics that are worse than random for the unlabeled data. Not accounting for performance on the unlabeled dataset can affect end model performance by up to 7.09 F1 points.\nOur Approach. To address the challenges above, we introduce Snuba, an automated system that takes as input a small labeled and a large unlabeled dataset and outputs probabilistic training labels for the unlabeled data, as shown in Figure 1. These labels can be used to train a downstream machine learning model of choice, which can operate over the raw data and generalize beyond the heuristics Snuba generates to label any datapoint. Users from research labs, hospitals and industry helped us design Snuba such that it outperforms user-defined heuristics and crowdsourced labels by up to 9.74 F1 points and 13.80 F1 points in terms of end model performance. Snuba maintains a set of heuristics that is used to assign labels to the unlabeled dataset. At each iteration, Snuba appends a new heuristic to this set after going through the following components:\nSynthesizer for Accuracy. To address the trade-off between the accuracy and coverage of each heuristic, the synthesizer (Section 3.1) generates heuristics based on the labeled set and adjusts its labeling pattern to abstain if the heuristic has low confidence. The synthesizer relies on a small number of primitives, or features of the data, to generate multiple, simple models like decision trees, which improves over fitting a single model over primitives by 12.12 F1 points. These primitives are user-defined and part of open\nsource libraries [35, 49] and data models in existing weak supervision frameworks [38,58]. Primitives examples in our evaluation include bag-of-words for text and bounding box attributes for images.\nPruner for Diversity. To ensure that the set of heuristics is diverse and assigns high-quality labels to a large portion of the unlabeled data, the pruner (Section 3.2) ranks the heuristics the synthesizer generates by the weighted average of their performance on the labeled set and coverage on the unlabeled set. It selects the best heuristic at each iteration and adds it to the collection of existing heuristics. This method performs up to 6.57 F1 points better than ranking heuristics by performance only.\nVerifier to Determine Termination Condition. The verifier uses existing statistical techniques to aggregate labels from the heuristics into probabilistic labels for the unlabeled datapoints [4,39,50]. However, the automated heuristic generation process can surpass the noise levels to which these techniques are robust to and degrade end model performance by up to 7.09 F1 points. We develop a statistical measure that uses the small, labeled set to determine whether the noise in the generated heuristics is below the threshold these techniques can handle (Section 4).\nContribution Summary. We describe Snuba, a system to automatically generate heuristics using a small labeled dataset to assign training labels to a large, unlabeled dataset in the weak supervision setting. A summary of our contributions are as follows:\n\u2022 We describe the system architecture, the iterative process of generating heuristics, and the optimizers used in the three components (Section 3). We also show that our automated optimizers can affect end model performance by up to 20.69 F1 points (Section 5).\n\u2022 We present a theoretical guarantee that Snuba will terminate the iterative process before the noise in heuristics surpasses the threshold to which statistical techniques are robust (Section 4). This theoretical result translates to improving end model performance by up to 7.09 F1 points compared to generating as many heuristics as possible (Section 5).\n\u2022 We evaluate our system in Section 5 by using Snuba labels to train downstream models, which generalize beyond the heuristics Snuba generates. We report on collaborations with Stanford Hospital and Stanford Computer Vision Lab, analyzing text, image, and multimodal data. We show that heuristics from Snuba can improve over hand-crafted heuristics developed over several days by up to 9.74 F1 points. We compare to automated methods like semi-supervised learning, which Snuba outperforms by up to 14.35 F1 points."
        },
        {
            "heading": "2. SYSTEM OVERVIEW",
            "text": "We describe the input and output for Snuba, introduce notation used in the rest of paper, and summarize statistical techniques Snuba relies on to learn heuristic accuracies."
        },
        {
            "heading": "2.1 Input and Output Data",
            "text": "Input Data. The input to Snuba is a labeled dataset OL with NL datapoints and an unlabeled dataset OU with NU\ndatapoints. Each datapoint is defined by its associated primitives, or characteristics of the data, and a label. The inputs to the system can be represented as\n{xi, y\u2217i }NLi=1, (for the labeled set OL), and\n{xi}NUi=1, (for the unlabeled set OU )\nwhere xi \u2208 RD, y\u2217 represent the primitives for a particular object and the true label, respectively. For convenience, we focus on the binary classification setting, in which y\u2217 \u2208 {\u22121, 1} and discuss the multi-class setting in Section 3.4.\nThe primitives for each datapoint xi \u2208 RD can be viewed as features of the data \u2014 examples include numerical features such as area or perimeter of a tumor for image data. or one-hot vectors for the bag of words representation for text data. For our collaborators using Snuba, these primitives are usually part of data models in existing weak supervision systems and open source libraries [35, 38, 49, 58]. For example, Scikit-image includes functions to extract geometric properties from segmented images [49]. In our evaluation, we do not allow users to extend the set of primitives beyond those present in these data models and libraries, though they could be extended in principle.\nOutput Data. Snuba outputs a probabilistic training label y\u0303 = P [y\u2217 = 1] \u2208 [0, 1] for each datapoint in the unlabeled set, a weighted combination of labels from different heuristics. Since Snuba only relies on information about the data encoded in the primitives and does not take advantage of a complete representation of the data, it is advantageous to train a downstream model that has access to the entire input data space using probabilistic labels from Snuba as training labels. These downstream models, such as a convolutional neural network (CNN) [26] for image classification or a long-short term memory (LSTM) architecture [19] for natural language processing tasks, can operate over the raw data (e.g., the radiology image of a tumor from Figure 1 or complete sentences). We discuss specific end models and show that the end model generalizes beyond the heuristics by improving recall by up to 61.54 points in Section 5."
        },
        {
            "heading": "2.2 Learning Heuristic Accuracies",
            "text": "Each heuristic Snuba generates relies on one or more primitives and outputs a binary label or abstains for each datapoint in the unlabeled dataset (Section 3.1). A single bad (but prolific) voter can compromise majority vote, which weights all heuristics equally [39]. Snuba instead relies on existing statistical techniques (Section 4) that can learn the\naccuracies of these heuristics without using ground truth labels and assign probabilistic labels to the unlabeled dataset accordingly [2, 4, 39, 41, 48, 50]. We treat these statistical techniques as black-box methods that learns heuristic accuracies and refer to them as label aggregators since they combine the labels the heuristics assign to generate a single probabilistic label per datapoint. However, since Snuba can generate heuristics that are much noisier than the label aggregator can handle, it has to determine the conditions under which the aggregator operates successfully (Section 4)."
        },
        {
            "heading": "3. THE SNUBA ARCHITECTURE",
            "text": "The Snuba process is iterative and generates a new heuristic specialized to the subset of the data that did not receive high confidence labels from the existing set of heuristics at each iteration. As shown in Figure 2, the three components of Snuba are the synthesizer (Section 3.1) that generates a candidate set of heuristics, a pruner (Section 3.2) that selects a heuristic to add to an existing committed set of heuristics, and a verifier (Section 3.3) that assigns probabilistic labels to the data and passes the subset of the labeled data that received low confidence labels to the synthesizer for the next iteration. This process is repeated until the subset the verifier passes to the synthesizer is empty, or the verifier determines that the conditions for the label aggregator to operate successfully are violated (Section 4)."
        },
        {
            "heading": "3.1 Synthesizer",
            "text": "The Snuba synthesizer takes as input the labeled set, or a subset of the labeled set after the first iteration, and outputs a candidate set of heuristics (Figure 2). First, we describe how the heuristics are generated using the labeled dataset and the different models the heuristic can be based on. Then, we describe how the labeling pattern of the heuristics are adjusted to assign labels to only a subset of the unlabeled dataset. Finally, we explore the trade-offs between accuracy and coverage by comparing heuristics Snuba generated to other automated methods.\n3.1.1 Heuristic Generation In Snuba, users can select the model they want to base\ntheir heuristics on given the heuristic h follows the inputoutput form: h(x\u2032i) \u2192 P [y\u2217i = 1] \u2208 [0, 1] where x\u2032i \u2208 RD \u2032\nis a subset of primitives, D\u2032 \u2264 D is the number of primitives in this subset, and P [y\u2217i = 1] is a probabilistic label.\nChoosing subsets of size D\u2032 from the primitives translates to selecting D\u2032 rows from X, as shown in function GenComb in Algorithm 1. For D primitives, there will be a total of( D D\u2032 ) distinct primitive subsets of size D\u2032. These subsets of primitives can be representative of a few specific words if primitives are generated using a bag-of words model while a subset of bounding box attribute primitives could represent the x,y-coordinates of the bounding box. The synthesizer generates a heuristic for each possible combination of 1 to D primitives, resulting in \u2211D D\u2032=1 ( D D\u2032 ) = 2D\u22121 total heuristics per iteration of Snuba. We find that D\u2032 < 4 for most real-world tasks, resulting in a maximum runtime of 14.45 minutes for Snuba on a single thread (Section 5).\nHeuristic Models. In this paper, we focus on heuristics that are based on classification models that take as input one or more primitives and assign probabilistic labels P [y\u2217i = 1] \u2208 [0, 1] to the unlabeled datapoints. We consider three different ways of generating heuristics given a subset of the labeled data and a subset of primitives (Figure 3).\n\u2022 Decision Stumps mimic the nested threshold-based heuristics that users commonly write. To maintain the simplicity of the heuristic, we limit the depth of each tree to the number of primitives the heuristic depends on. The confidence each unlabeled datapoint receives is the fraction of labeled datapoints that belong to the same leaf.\n\u2022 Logistic Regressor allows the heuristic to learn a single linear decision boundary. As shown in Figure 3, it does not have to be parallel to the primitive axes, unlike decision trees. The confidence for an unlabeled datapoint is determined by the sigmoid function, whose parameters are learned using the labeled datapoints.\n\u2022 K-Nearest Neighbor is based on a kd-tree implementation of nearest neighbor and can lead to complex decision boundaries that neither decision trees nor logistic regressors can capture. Unlike the previous heuristic models, it does not learn a parameter per primitive, but instead relies on the distribution of the labeled datapoints to decide the decision boundaries. The confidence for a unlabeled datapoint is a function of its distance from labeled datapoints.\nThe user can replace the heuristic model with another function of choice as long as it follows the input-output criteria described earlier in this section. For example, decision trees that rely on bag-of-words primitives represent heuristics that check whether a particular word, represented as a primitive, exists or not.\n3.1.2 Tuning Threshold for Abstains We can improve performance of heuristics by modeling the\ntrade-off between heuristic accuracy and coverage. Snuba\nAlgorithm 1: Snuba Synthesis Procedure\n1 function GenerateHeuristics (f,X, y\u2217) Input: Heuristic model f \u2208 {DT,LR,NN}, Primitive\nmatrix X \u2208 RD\u00d7NL , Labels y\u2217 \u2208 {\u22121, 1}NL Output: Candidate set of heuristics H, Primitive\ncombinations Xcomb 2 H,Xcomb = [ ] for D\n\u2032 = 1...D do 3 //generate primitive combinations of size D\u2032 4 idxcomb = GenComb(X,D \u2032) 5 for i = 1...len(idxcomb) do 6 X \u2032 = X[idxcomb, :] 7 h = f(X \u2032, y\u2217) 8 yprob = predictProb(h,X\n\u2032) 9 \u03b2 = FindBeta(yprob, y\n\u2217) 10 H = H \u222a {(h, \u03b2)} 11 Xcomb = Xcomb \u222aX \u2032 12 end\n13 end 14 return H,Xcomb 15 function FindBeta (yprob, y \u2217) 16 betaList = [0, 0.05, . . . 0.5] 17 for j in len(betaList) do 18 beta = betaList[j] 19 F1[j] = calcF1(y\u2217, yprob, beta) 20 end 21 return betaList[argmax(F1)]\n22 function GenComb (X,D\u2032) 23 //get all D\u2019 length subsequences from range(D) 24 return all subsets of size D\u2032 from D\nforces heuristics to only assign labels to datapoints they have high confidence for and abstain for the rest. To measure confidences, Snuba relies on the probabilistic label P [y\u2217i = 1] that each heuristic model assigns to a datapoint. We define datapoints that heuristics have low confidence for as the points where |P [y\u2217i = 1] \u2212 0.5| \u2264 \u03b2, \u03b2 \u2208 (0, 0.5). For each heuristic, Snuba selects a threshold \u03b2 that determines when a heuristic assigns a label, y\u0302 \u2208 {\u22121, 1} and when it abstains, y\u0302 = 0. The relation between \u03b2 and y\u0302 can be defined as:\ny\u0302i =  1 if P [y\u0302i = 1] \u2265 0.5 + \u03b2 0 if |P [y\u0302i = 1]\u2212 0.5| < \u03b2 \u22121 if P [y\u0302i = 1] \u2264 0.5\u2212 \u03b2\nTo choose the best threshold \u03b2, we need a metric that models the trade-offs between coverage and accuracy. We calculate the precision and recall of the heuristics on the labeled set with NL datapoints as a proxy for their performance on the unlabeled dataset. We define these metrics below:\n\u2022 Precision (P) the fraction of correctly labeled points over the total points labeled, \u2211NL i=1 1(y\u0302i=y\n\u2217 i )\u2211NL\ni=1 1(y\u0302i 6=0)\n\u2022 Recall (R) the fraction of correctly labeled points over the total number of points, \u2211NL i=1 1(y\u0302i=y \u2217 i )\nNL\n\u2022 F1 Score the harmonic mean of P and R, 2P\u00d7R P+R\nTo balance precision and recall, the Snuba synthesizer selects \u03b2 for each heuristic that maximizes the F1 score on the\nlabeled dataset OL (Algorithm 1). The synthesizer iterates through (default 10) equally spaced values in \u03b2 \u2208 (0, 0.5), calculates the F1 score the heuristic achieves, and selects the \u03b2 that maximizes F1 score. In case of ties, the synthesizer chooses the lower \u03b2 value for higher coverage. We find selecting \u03b2 based on F1 score outperforms a constant \u03b2 by up to 5.30 F1 points (Section 5).\nAs an example, if the synthesizer uses a decision tree as the heuristic model, it trains a normal decision tree on the small labeled dataset and learns appropriate parameters for a specific subset of primitives (e.g., D = 2 means two primitives, or two rows of X in Algorithm 1) to decide on a label. Then, the synthesizer learns \u03b2, which adjusts these decision tree thresholds to abstain for low-confidence datapoints. This adjusted decision tree is then added as a heuristic to the candidate set, and the process is repeated for different subsets of primitives as inputs to the decision tree.\n3.1.3 Synthesizer Tradeoffs We explore the trade-offs that result from allowing the\nheuristics to abstain in terms of the effect on end model performance. We compare to automated baseline methods (more details in Section 5.1) that assign labels to the entire unlabeled dataset. We generate a synthetic experiment (Figure 4) using one of the datasets from our evaluation, the Visual Genome dataset [25] (more details in Section 5.1). To study how Snuba performs given varying amounts of unlabeled data, we set up the following simulation: given NL = 100 labeled datapoints, we varied the amount of unlabeled data available to Snuba from NU = 100 to NU = 500. Each of the methods assigned training labels to the unlabeled dataset, and this dataset was used to fine-tune the last layer of GoogLeNet [47]. NL \u2248 NU Case: Since Snuba only labels a portion of the unlabeled data, the end model has fewer training labels to learn from compared to the other methods that do not abstain. Since the unlabeled set is small in this situation (NL = NU = 100), the baseline methods have better end model performance. NL <<NU Case: Heuristics Snuba generates continue to only assign labels with high confidence, leading to a smaller labeled training set than other methods, but high quality training labels for that portion. This is promising for machine learning applications in which the bottleneck lies in gathering enough training labels, while unlabeled data is readily available. Semi-supervised learning also performs better as the amount of unlabeled data increases; however, it still performs worse than Snuba when the amount of unlabeled data is more than 3\u00d7 larger than labeled data since semi-supervised methods do not abstain. Snuba also outperforms these baseline methods when the unlabeled data is between 2\u00d7 to 1000\u00d7 as much as labeled data (Section 5)."
        },
        {
            "heading": "3.2 Pruner",
            "text": "The pruner takes as input the candidate heuristics from the synthesizer and selects a heuristic to add to the committed set of heuristics (Figure 2). We want the heuristics in the committed set to be diverse in terms of the datapoints in the unlabeled set they label, but also ensure that it performs well for the datapoints it labels in the labeled dataset.\nA diverse heuristic is defined as one that labels points that have never received a label from any other heuristic. Therefore, we want to be able to maximize the dissimilarity\nAlgorithm 2: Snuba Pruning Procedure\n1 function SelectBestHeuristic (H,HC , y \u2217, XL, XU , n, w)\nInput: Candidate (H) and committed set (HC) of heuristics, Labels y\u2217 \u2208 {\u22121, 1}NL , Primitives XL \u2208 RD\u00d7NL , XU \u2208 RD\u00d7NU , If label assigned n \u2208 {0, 1}NU , Weight for F1 w \u2208 [0, 0.5]\nOutput: Best heuristic in candidate set, hbest \u2208 H 2 hbest = None 3 bestScore = 0 4 for hi \u2208 H do 5 y\u0302iL = applyHeuristic(hi, XL) 6 fscore = calcF1(y\u0302 i L, y \u2217)\n7 y\u0302iU = applyHeuristic(hi, XU ) 8 jscore = calcJaccard(y\u0302 i U , n)\n9 if w(jscore + fscore) \u2265 bestScore then 10 hbest = hi 11 //default w=0.5 12 bestScore = (1\u2212 w) \u2217 jscore + w \u2217 fscore 13 end\n14 end 15 return hbest\nbetween the set of datapoints a heuristic labels and the set of datapoints that previous heuristics in the committed set have already labeled. Let nj \u2208 {0, 1}NU represent whether heuristic j from the candidate set has assigned labels to the datapoints in the unlabeled set. Let n \u2208 {0, 1}NU represent whether any heuristic from the committed set has assigned a label to the datapoints in the unlabeled set. To measure the distance between these two vectors, we rely on the Jaccard distance metric [20], the complement of Jaccard similarity, as a standard measure of similarity between sets. For a particular heuristic hj in the candidate set, the generalized Jaccard distance is defined as:\nJj = 1\u2212 nj \u2229 n nj \u222a n\nTo measure performance on the labeled dataset, Snuba uses the F1 score of each heuristic in the candidate set, as defined in the previous section. As the final metric to rank heuristics, the pruner uses a weighted average of the Jaccard distance and F1 score and selects the highest ranking heuristic from the candidate set and adds it to the committed set of heuristics. This process is described in Algorithm 2. For our experiments, we use both w = 0.5 for a simple average and w = 1 Tn NU (percentage of unlabeled set with at least one\nlabel). The latter weights the F1 score more as coverage of the unlabeled dataset increases. We find that considering both performance on the labeled set and diversity on the unlabeled set improves over only considering diversity by up to 18.20 F1 points and over only considering performance by up to 6.57 F1 points in Section 5."
        },
        {
            "heading": "3.3 Verifier",
            "text": "The verifier uses the label aggregator (Section 4) to learn accuracies of the heuristics in the committed set without any ground truth labels to produce a single, probabilistic training label for each datapoint in the unlabeled dataset.\nAlgorithm 3: Snuba Verifier Procedure\n1 function FindFeedback (HC , y \u2217, XL, XU )\nInput: Committed set of heuristics HC , H[i] = h(X \u2032 i),\nLabels y\u2217 \u2208 {\u22121, 1}NL , Primitives XL \u2208 RD\u00d7NL , XU \u2208 RD\u00d7NU\nOutput: Subset of labeled set, O\u2032L, Prob. labels, y\u0303U 2 \u03b1\u0303 = learnAcc (HC , XU ) 3 \u03b1\u0302 = calcAcc (HC , XL, y \u2217)\n4 y\u0303U = calcLabels (\u03b1\u0303,XU ) 5 y\u0303L = calcLabels (\u03b1\u0303,XL)\n6 = findEps(NU ,M) 7 \u03bd = findNu(M) 8 if ||\u03b1\u0303\u2212 \u03b1\u0302||\u221e \u2265 then 9 return O\u2032L = \u2205, y\u0303U\n10 end 11 else 12 return oi \u2208 O\u2032L if |y\u0303iL \u2212 0.5| \u2264 \u03bd, y\u0303U 13 end\nThese probabilistic labels also represent how confident the label aggregator is about the assigned label. Datapoints that have not received a single label from heuristics in the committed set will have a probabilistic label P [y\u2217 = 1] = 0.5, equal chance of belonging to either class. P [y\u2217 = 1] close to 0.5 represent datapoints with low confidence, which can result from scenarios with low accuracy heuristics labeling that datapoint, or multiple heuristics with similar accuracies disagreeing on the label for that datapoint. Since Snuba generates a new heuristic at each iteration, we want the new heuristic to assign labels to the subset that currently has low confidence labels. Snuba identifies datapoints in the labeled set that receive low confidence labels from the label aggregator. It passes this subset to the synthesizer with the assumption that similar datapoints in the unlabeled dataset would have also received low confidence labels (Algorithm 3).\nFormally, we define low confidence labels as |y\u0303i \u2212 0.5| \u2264 \u03bd where y\u0303 is the probabilistic label assigned by the label aggregator and \u03bd = 1\n2 \u2212 1 (M+1)\u03b7 where the \u03b7 > 1 parameter\n(default \u03b7 = 3 2 ) controls the rate at which the definition of low confidence changes with number of heuristics in the committed set (M). As the number of heuristics increases, we expect that fewer datapoints will have confidences near 0.5 and adjust what is considered low confidence accordingly. We also compare to a weighted feedback approach in which the weights are the inverse of the label confidence (wv = 1 2 \u2212 |y\u0303 \u2212 1 2 |) normalized across all datapoints.\nThe iterative process terminates if: (1) the statistical measure discussed in Section 4 suggests the generative model in the synthesizer is not learning the accuracies of the heuristics\nproperly, or (2) there are no low confidence datapoints, as defined by \u03bd, in the small, labeled dataset. Empirically, we find that (1) is a more popular termination condition than (2). In both cases, it is likely for some datapoints in the large, unlabeled set to not receive a label from any heuristic in the committed set; however, since Snuba generates training labels, the downstream end model can generalize to assign labels to these datapoints."
        },
        {
            "heading": "3.4 Discussion",
            "text": "We discuss the extension of the Snuba architecture to the multi-class setting, intuition behind the greedy approach, alternative heuristic models, and limitations of the system.\nMulti-Class Setting. While we focus on the binary setting, Snuba can be extended to the multi-class setting without additional changes. We include an example of a threeclass classification task in [52]. Statistics like F1 and Jaccard score in the synthesizer and pruner are calculated using only overall accuracy and coverage, which apply to the multi-class setting. The label aggregator in the verifier can operate over multi-class labels [38,39] and pass feedback using the probabilistic label of the most likely class.\nGreedy Approach. Our intuition behind generating heuristics greedily was to mimic the the user process of manually developing heuristics. The iterative approach tries to ensure each heuristic labels a subset of the data that does not have labels from existing heuristics and ensure a large portion of the datapoints receive high confidence labels. We use a statistical method to determine the optimal stopping condition for the iterative approach (Section 4, Figure 5).\nAlternative Heuristic Models. While we only discuss three possible heuristic models in this paper, Snuba can handle any heuristic model that follows the input-output schema described in Section 3.1. The user can therefore design different heuristic models that are specialized for their classification task. For example, the user can use a regex heuristic model that can perform more complex operations over bag-of-words primitives than a decision tree.\nLimitations. First, the performance of the Snuba heuristics is bounded by the quality of the input primitives. For example, if the primitives for the tumor classification task only contained age, which was a poor signal of tumor malignancy, then the heuristics Snuba generated would not assign high quality training labels. Second, Snuba heuristics can only rely on the input primitives and no external knowledge about the task, such as knowledge bases, which is a limitation compared to user-defined heuristics (more details in Section 5.2.3). Finally, Snuba is likely to overfit and not perform well on the unlabeled dataset if the small, labeled dataset is not representative of the unlabeled dataset. For the tumor classification task, the images in the small, labeled set could be taken from one perspective while the ones in the larger, unlabeled dataset are from a different perspective. This can lead the distribution of the primitives to be significantly different across the two datasets and prevent Snuba from generating high quality heuristics."
        },
        {
            "heading": "4. SNUBA SYSTEM GUARANTEES",
            "text": "We provide an overview of generative models [4,39,50,53] that serve as the label aggregator for Snuba. As discussed in Section 2, these models can learn the accuracies of the noisy\nheuristics without using any ground truth data and can assign probabilistic labels to the unlabeled data accordingly. However, these generative models are designed to model the noise in user-defined heuristics, which are much more accurate than automatically generated heuristics. Specifically, the generative model assumes that heuristics always have accuracies better than 50%; however, Snuba-generated heuristics can easily violate this assumption as described in Section 4.2. Therefore, a key challenge in Snuba is recognizing whether the committed set includes heuristics that are worse than random for the unlabeled dataset without access to ground truth labels. We introduce a statistical measure in Section 4.3 that relies on the accuracies the generative model learns and the small labeled dataset. In Section 4.4, we formally define this statistical measure and provide a theoretical guarantee that it will recognize when the generative model is not learning heuristic accuracies successfully."
        },
        {
            "heading": "4.1 Generative Model",
            "text": "Generative models are a popular approach to learn and model the accuracies of different labeling sources like userdefined heuristics and knowledge bases when data is labeled by a variety of sources [11, 39]. In Snuba, we could also rely on the accuracies of the heuristics on the small, labeled dataset, \u03b1\u0302; however, this could degrade end model performance by up to 8.43 F1 points (Section 5). Formally, the goal of the generative model is to estimate the true accuracies of the heuristics, \u03b1\u2217 \u2208 RM , using the labels the heuristics assign to the unlabeled data, Y\u0302 \u2208 { \u2212 1, 0, 1}M\u00d7NU . It models the true class label Y \u2217 \u2208 {\u22121, 1}NU for a datapoint as a latent variable in a probabilistic model and in the simplest case, assumes that each labeling source is independent. The generative model is expressed as a factor graph:\n\u03c0\u03c6(Y\u0302 , Y \u2217) =\n1\nZ\u03c6 exp\n( \u03c6T Y\u0302 Y \u2217 ) (1)\nwhere Z\u03c6 is a partition function to ensure \u03c0 is a normalized distribution. The parameter \u03c6 \u2208 RM is used to calculate the learned accuracies \u03b1\u0303 = exp(\u03c6)\n1+exp(\u03c6) \u2208 RM (defined point-\nwise). It is estimated by maximizing the marginal likelihood\nof the observed heuristics Y\u0302 , using a method similar to contrastive divergence [18], alternating between using stochastic gradient descent and Gibbs sampling [4, 39]. The generative model assigns probabilistic training labels by computing Y\u0303 = \u03c0\u03c6(Y\n\u2217 | Y\u0302 ) for each datapoint. These probabilistic training labels can be used to train any end model with\nnoise-aware loss [38,39]\nmin NU\u2211 i=1 Ey\u223cY\u0303 [l(h\u03b8 (oi) , y)]\nwhere oi \u2208 OU is an object in the unlabeled dataset and Y\u0303 are the probabilistic training labels. In our experiments, we adjust the loss functions of several popular machine learning models to the use the noise-aware variant."
        },
        {
            "heading": "4.2 Assumption Violation",
            "text": "Since the generative model requires no ground truth labels to learn heuristic accuracies \u03b1\u0303, it has to solve an underdetermined problem where the heuristics could have accuracies \u03b1\u0303 or 1 \u2212 \u03b1\u0303. The generative model assumes that the labeling sources always perform better than random (\u03b1\u2217 > 0.5), which is a reasonable assumption for user-defined heuristics [4, 39, 50]. Since Snuba generates these heuristics automatically, it is possible for the heuristics to be accurate for the labeled set but violate the generative model\u2019s assumption that \u03b1\u2217 > 0.5. An example of such a situation is shown in Figure 5(a),(c) for two real datasets. The 8th and 12th heuristics, respectively, have an accuracy worse than 50% on the unlabeled dataset. However, since the generative model does not know that this assumption has been violated, it learns an accuracy much greater than 50% in both cases. If these heuristics are included in the generative model, the generated probabilistic training labels degrade end model performance by 5.15 F1 and 4.05 F1 points, respectively."
        },
        {
            "heading": "4.3 Statistical Measure",
            "text": "Snuba can take advantage of the small, labeled dataset to indirectly determine whether the generated heuristics are worse than random for the unlabeled dataset. We define the empirical accuracies of the heuristics as\n\u03b1\u0302i = 1\nNi NL\u2211 j=1 1(Y\u0302ij = Y \u2217 j ),\nfor i = 1 . . .M. Y\u0302ij \u2208 {\u22121, 0, 1} is the label heuristic i assigned to the j-th datapoint in the labeled set OL, and Ni is the number of datapoints where Y\u0302i \u2208 {1,\u22121}. Our goal is to use the empirical accuracies, \u03b1\u0302 to estimate whether the learned accuracies, \u03b1\u0303 are close to the true accuracies, \u03b1\u2217, defined as ||\u03b1\u2217 \u2212 \u03b1\u0303||\u221e < \u03b3, the maximum absolute difference between the learned and true accuracies being less than \u03b3, a positive constant to be set. Toward this end, we define the\nmeasured error between the learned and empirical accuracies as ||\u03b1\u0302\u2212 \u03b1\u0303||\u221e. To guarantee with high probability that the generative model learns accuracies within \u03b3, we want to find , the largest allowed error between the learned and empirical accuracies, ||\u03b1\u0302 \u2212 \u03b1\u0303||\u221e \u2264 at each iteration. We discuss the exact form of in Section 4.4.\nWe compare the measured error ||\u03b1\u0302 \u2212 \u03b1\u0303||\u221e to the maximum allowable value of at each iteration, as shown in Figure 5(b),(d). If the measured error is greater than , then we stop the iterative process of generating heuristics and use the probabilistic training labels generated at the previous iteration (since the heuristic generated at the current iteration led to measured error being greater than ). As shown in Figure 5, this stopping point maps to the iteration at which the new heuristic generated has a true accuracy \u03b1\u2217 worse than 50% for the unlabeled dataset (we only calculate \u03b1\u2217 for demonstration since we would not have access to ground truth labels for real-world tasks). Intuitively, we expect that once the synthesizer generates a heuristic that is worse than random for the unlabeled dataset, it will never generate heuristics that will be helpful in labeling the data anymore. Empirically, we observe that this is indeed the case as shown for two real tasks in Figure 5(a) and (c)."
        },
        {
            "heading": "4.4 Theoretical Guarantees",
            "text": "Assuming that the objects in the labeled set OL are independent and identically distributed, we provide the following guarantee on the probability of the generative model learning the accuracies successfully:\nProposition 1: Suppose we have M heuristics with empirical accuracies \u03b1\u0302, accuracies learned by the generative model \u03b1\u0303, and measured error ||\u03b1\u0302\u2212 \u03b1\u0303||\u221e \u2264 for all M iterations. Then, if each heuristic labels a minimum of\nN \u2265 1 2(\u03b3 \u2212 )2 log\n( 2M2\n\u03b4 ) datapoints at each iteration, the generative model will succeed in learning accuracies within ||\u03b1\u2217\u2212 \u03b1\u0303||\u221e < \u03b3 across all iterations with probability 1\u2212 \u03b4. We provide a formal proof for this proposition in [52]. We require each heuristic to assign labels to at least N datapoints to guarantee that the generative model will learn accuracies within \u03b3 of the true accuracies, given the measured error is less than for all iterations. We solve for the maximum allowed error at each iteration:\n= \u03b3 \u2212\n\u221a 1\n2N log\n( 2M\n\u03b4\n) .\nThis value is plotted against the value of the measured error ||\u03b1\u0302 \u2212 \u03b1\u0303||\u221e in Figure 5(b,d). Snuba stops generating new heuristics when the measured error surpasses the allowed error. The above proposition relies only on the measured error to guarantee whether the generative model is learning accuracies successfully."
        },
        {
            "heading": "5. EVALUATION",
            "text": "We compare the performance of end models trained on labels generated by Snuba and other baseline methods. We seek to experimentally validate the following claims:\n\u2022 Training labels from Snuba outperform labels from automated baseline methods We compare Snuba to models that generate heuristics using only the labeled data, such as boosting and decision trees, and semi-supervised methods, which utilize both labeled and unlabeled datasets. Snuba outperforms these methods by up to 14.35 F1 points. We also compare to transfer learning using only the labeled dataset, which Snuba outperforms by up to 5.74 F1 points.\n\u2022 Training labels from Snuba outperform those from user-developed heuristics We compare the performance of heuristics generated by Snuba to heuristics developed by users. Snuba can use the same amount of labeled data as users to generate heuristics and improve end model performance by up to 9.74 F1 points.\n\u2022 Each component of Snuba boosts overall system performance We evaluate separate components of the Snuba system by changing how the \u03b2 parameter is chosen in the synthesizer, how the pruner selects a heuristic to add to the committed set, and different label aggregation methods in the verifier. Compared to the complete Snuba system, we observe that performance can degrade by up to 20.69 F1 points by removing these components."
        },
        {
            "heading": "5.1 Experiment Setup",
            "text": "We describe the datasets, baseline methods, performance metrics, and implementation details for Snuba.\n5.1.1 Datasets We consider real-world applications and tasks over open\nsource datasets for image, text, and multi-modal classification. For each of the tasks, previous techniques to assign training labels included using crowdsourcing, user-defined functions, and decision trees based on a small, labeled dataset.\nSummary statistics are provided in Table 1 and additional details are in [52].\nImage Classification. We focus on two real-world medical image classification tasks that we collaborated on with radiologists at Stanford Hospital and Clinics. The Bone Tumor and Mammogram tumor classification tasks demonstrate how Snuba-generated heuristics compare to those developed by domain experts. The first dataset uses domainspecific primitives while the second relies on simple geometric primitives. Working with graduate students in the Stanford Computer Vision lab, we identify images of \u201cperson riding bike\u201d. We use the Visual Genome database [25] with bounding box characteristics as primitives and study how Snuba performs with severe class imbalance.\nText and Multi-Modal Classification. We applied Snuba to text and multi-modal datasets to study how well Snuba operated in domains where humans could easily interpret and write rules over the raw data. We generate primitives by featurizing the text using a bag-of-words representation. The MS-COCO dataset [30] had heuristics generated over captions and classification performed over associated images, and the IMDb plot summary classification [1] is purely text-based. The Twitter sentiment analysis dataset relied on crowdworkers for labels [31] while the chemical-disease relation extraction task (CDR) [57] relies on external sources of information like knowledge bases. The Hardware relation extraction task over richly formatted data classifies part numbers and electrical characteristics from specification datasheets1 as valid or not. We use visual, tabular, and structural primitives extracted using Fonduer [58].\n5.1.2 Baseline Methods We compare to pruned decision tree [42] and boost-\ning [16] (AdaBoost), which use the labeled dataset to generate one complex or multiple, simple decision trees, respectively. We compare to semi-supervised learning [61], which uses both the labeled and unlabeled dataset to assign training labels and represents a single \u2018heuristic\u2019 in the form of a black-box model. For select tasks, we perform transfer learning using pre-trained models. We use GLoVE embeddings [36] for IMBd and Twitter only tune the last layer of a VGGNet [43] for MS-COCO, and tune the weights of a GoogLeNet [47] pre-trained on ImageNet [12] for Visual Genome and Mammogram (more details in [52]).\nAs shown in Table 1, training labels for all tasks were previously generated by some user-driven labeling method,\n1https://www.digikey.com\nsuch as user-defined heuristics, distant supervision, or crowdsourcing. These were developed by users, ranging from domain experts to machine learning practitioners and input to label aggregators we developed [38, 50, 51]. For tasks like CDR, Bone Tumor, and Mammogram that required specific domain knowledge, the time taken for bioinformatics experts and radiologists to manually develop heuristics ranged from a few days to a few weeks. For tasks that did not require domain expertise, such as IMDb and Visual Genome, graduate students wrote a small number of heuristics over a period of a few hours. In all cases, users encoded their domain knowledge in heuristics and evaluated their performance on a small, held-out labeled set in an iterative manner.\n5.1.3 Implementation Details Primitives for Snuba. Since text-based tasks used a bagof-words representation, the primitives are sparse and number in the hundreds of thousands. We filter bag-of-words primitives by only considering primitives that are active for both the labeled and unlabeled dataset, and for at least 5% of the unlabeled dataset to ensure a minimim coverage for generated heuristics. The 5% threshold had the best performance for our text datasets but this threshold can be user-defined in practice.\nFor our image-based tasks, we found that Snuba never generated heuristics that relied on more than 4 primitives as input, while for text-based tasks, it only generated heuristics that relied on a single primitive (Figure 6). Heuristics rely on a small number of primitives since this limits their complexity and prevents them from overfitting to the small, labeled dataset. Moreover, relying on multiple primitives can also lower the coverage of the heuristics, and a fairly accurate heuristic that relies on several primitives being present is filtered by the pruner, which relies on both coverage and performance. The relatively small number of\nTable 3: Precision (P), Recall (R) and F1 scores for user-defined heuristics, Snuba-generated heuristics, and end model trained on labels from Snuba-generated heuristics. Lift reported is from user to Snuba heuristics, then Snuba heuristics to end model. Snuba heuristics have lower precision than users\u2019 and end model improves recall.\nApplication User Heuristics Snuba Heuristics Snuba + End Model F1 P R F1 P R Lift(F1) F1 P R Lift(F1)\nBone Tumor 30.91 89.47 18.68 31.58 33.75 29.67 +0.67 71.55 58.86 91.21 +39.97 Visual Genome 34.76 98.28 21.11 46.06 48.10 44.19 +11.30 56.83 41.34 90.91 +10.77 MS-COCO 21.43 63.66 12.88 24.41 29.40 41.49 +12.98 69.52 55.80 92.16 +35.11 IMDb 20.65 76.19 11.94 46.47 48.03 45.52 +25.82 62.47 45.42 100. +16.00\nprimitives heuristics used as input leads to a maximum single threaded runtime of 14.45 mins for the Hardware task on a Xeon E7-4850 v3 CPU.\nPerformance Metrics. To measure performance, we report the F1 score of an end model trained on labels from Snuba and the baseline methods on a test set. We report F1 score instead of accuracy since some datasets have class imbalance that can lead to high accuracy by naively predicting the majority class for all datapoints. The F1 scores for the end model are defined in terms of true positives instead of correctly classified datapoints (this is different Section 3.1.2, since the end models never abstain).\nEnd Models. While Snuba can generate training labels efficiently, they rely only on the user-defined primitives. The end model trained on these labels can use the raw data or representations of the data based on pre-trained models. For example, the end model can operate over the entire raw image, sentence or representation from a pre-trained model as opposed to measurements of the tumor, bag-of-words representation, or bounding box coordinates. For image classification tasks, we use popular deep learning models like GoogLeNet and VGGNet that take the raw image as input, while for text tasks we use a model composed of a single embedding and a single LSTM layer that take the raw text sentence(s) as input. These models take as input the probabilistic or binary training labels from Snuba or the baseline methods and minimize the noise-aware loss, as defined in Section 4. While the tasks explored in this section are all binary classification, the system can be easily generalized to the multi-class case (Section 3.4)."
        },
        {
            "heading": "5.2 End to End System Comparison",
            "text": "We demonstrate that a downstream model trained on the labels from Snuba generalizes beyond the Snuba heuristics, improving recall by up to 61.54 points (Section 5.2.1), outperforms automated baseline methods by up to 12.12 F1 points (Section 5.2.2) and user-driven labeling by up to 9.74 F1 points (Section 5.2.3).\n5.2.1 Generalization beyond Heuristics One of the motivations for designing Snuba is to efficiently\nlabel enough training data for training powerful, downstream machine learning models like neural networks. Heuristics from Snuba are not used directly for the classification task at hand because (1) they may not label the entire dataset due to abstentions, and (2) they are based only on the userdefined primitives and fail to take advantage of the raw data representation. For datasets like MS-COCO, the end model also operates over a different modality than the heuristics. To demonstrate the advantage of training an end model, we compare the performance of Snuba heuristics to standard\nend models trained on labels from Snuba on a test set in Table 3. The end model improves over the heuristics\u2019 performance by up to 39.97 F1 points. The end model helps generalize beyond the heuristics, as a result of more powerful underlying models and access to raw data, and improves recall by up to 61.54 points.\n5.2.2 Automated Methods Table 2 shows that Snuba can outperform automated base-\nline methods by up to 14.35 F1 points. Snuba outperforms decision trees, which fit a single model to the labeled dataset, by 7.38 F1 points on average, the largest improvement compared to other baselines. The method that performs the closest to Snuba for most tasks is semi-supervised learning, which takes advantage of both the unlabeled and unlabeled dataset, but fails to account for diversity, performing worse than Snuba by 6.21 F1 points on average. Finally, compared to transfer learning which does not have to learn a representation of the data from scratch, Snuba performs up to 5.74 F1 points better using the same amount of labeled data. This demonstrates how for many tasks, using a larger training set with noisy labels is able to train a better end model from scratch than fine tuning a pre-trained model with a small labeled dataset.\n5.2.3 User-Driven Labeling Methods We compare end model performance trained on labels\nSnuba generates to labels from manually generated labeling sources in Table 2 and report the precision, recall, and F1 score of Snuba-generated and user-defined heuristics in Table 3. The labels from the heuristics are combined using the Snuba label aggregator, the generative model in Section 4. Overall, Snuba generates heuristics that perform up to 25.82 F1 points better than user-defined heuristics. Note that users develop heuristics that are very high precision, up to 98.28 points. Snuba-generated heuristics, on the other hand, balance both precision and recall. This supports the design of the system since the synthesizer optimizes for F1 score, which relies on both precision and recall, and the pruner optimizes for both accuracy and coverage, which are related to both precision and recall.\nFor image domains, Snuba generates fewer heuristics (Figure 7) that depend on more primitives than user-defined heuristics. Primitives for image domains are numerical and require guessing the correct threshold for heuristics, a process Snuba automates while users guess manually. For the Bone Tumor classification task, the user-defined heuristics were manually tuned versions of decision trees fit to the labeled set. Therefore, Snuba only improves 0.67 F1 points over this partially automated approach. For text datasets (MS-COCO and IMDb), Snuba generates almost 5\u00d7 as many heuristics as users since each heuristic relies only on a single primitive and improves F1 score by up to 25.82 points (Table 3). For CDR, users relied on distant supervision through the Comparative Toxicogenomics Database [10]. Snuba only relies on the primitives it has access to and cannot incorporate any external information, leading to 12.24 F1 points lower performance than user-defined heuristics using distant supervision. Finally, for Hardware, Snuba uses only 100 labeled datapoints to generate heuristics while users had access to 47, 413, which leads to Snuba performing 4.75 F1 points worse in terms of end model performance."
        },
        {
            "heading": "5.3 Micro-Benchmarking Results",
            "text": "We evaluate the individual components of the Snuba system and show how adjustments to each component can affect end model performance by up to 20.69 F1 points.\n5.3.1 Synthesizer First, we compare how different heuristic models perform\nfor select tasks in Table 4 and show how much better the best heuristic type (marked as 0) performs compares to alternate heuristic types. For text-based tasks, decision tree and logistic regressor based heuristics perform the same since they both rely on a single primitive and learn the same threshold to make a binary decision. These heuristic models essentially check whether a word exists in a sentence.\nNext, we set \u03b2 = 0 to prevent heuristics from abstaining and set it to a constant \u03b2 = 0.25, the midpoint of possible values \u03b2 \u2208 (0, 0.5) (Table 4). Allowing heuristics to abstain can improve end model performance by up to 20.69 F1 points and choosing the correct \u03b2 value can improve end model performance by up to 5.30 F1 points.\n5.3.2 Pruner We show the performance of the pruner compared to only\noptimizing for either performance (with F1 score) or diversity (with Jaccard distance) in Table 5. For text tasks, only optimizing for performance comes within 2.15 F1 points of the Snuba pruner since each heuristic selecting a different\nword automatically accounts for diversity. On the other hand, only optimizing for diversity in text domains can affect performance by up to 18.20 F1 points since it could result in a large portion of the unlabeled dataset receiving low-quality labels. We also compare to weighting the F1 score by how much of the unlabeled dataset is covered, which performs closest to the simple average case for text-based tasks. This suggests that other domain-specific weighting schemes, like weighting coverage more than accuracy given sparse primitives can further improve performance.\n5.3.3 Verifier Finally, we look at how learning heuristic accuracies for\nlabel aggregation compares to majority vote in Table 6. Text domains in which the number of heuristics generated is more than 15, the majority vote score comes within 1.80 F1 points of the Snuba verifier. With a large number of heuristics, each datapoint receives enough labels that learning accuracies has little effect on the assigned labels [28].\nWe compare to using the empirical accuracies of the heuristics \u03b1\u0302 rather than learning accuracies based on labels assigned to the unlabeled data. This method performs worse than the Snuba verifier by up to 8.43 F1 points. We also generate heuristics till there are no more datapoints in the small, labeled dataset with low confidence labels and find that this can degrade end model performance by up to 7.09 F1 points as shown in Table 6.\nWe compare to passing a weighted version of the small, labeled dataset as feedback to the synthesizer instead of a subset and find it performs up to 4.42 F1 points worse than passing a subset. We posit that heuristics fit to a weighted set can lead to more low confidence labels and eventually a higher rate of abstentions for the unlabeled dataset."
        },
        {
            "heading": "6. RELATED WORK",
            "text": "We provide an overview of methods that label data automatically based on heuristics, use both labeled an unlabeled data, and aggregate noisy sources of labels.\nRule Learning. The inspiration for Snuba comes from program synthesis, where programs are generated given access to a set of input-output pairs [15,44], reference implementations [3], or demonstrations [21]. The design is based loosely on counter-example guided inductive synthesis (CEGIS) in which a synthesizer generates programs, passes it to the verifier that decides whether the candidate program satisfies the given specifications, and passes relevant feedback to the synthesizer [15, 21, 44, 46]. However, unlike Snuba, such models only synthesize programs that match all the specified input-output pairs. Other works also generate heuristics to help interpret the underlying data labels [54, 55], but neither methods use unlabeled data since the programs generated either mimic the desired program perfectly or provide interpretations for existing labels. While Snuba focuses on generating training labels for various domains, rule learning has been widely studied in the context of information extraction [33, 45]. Recent works can learn logical rules for knowledge base reasoning [59], interleave beam search with parameter learning [24], select rules from a restricted set using lasso regression [27], and use alternate gradient-based search to find parameters for probailistic logic [56]. While these methods are more sophisticated than Snuba, they use a large amount of training data and rely directly on the generated rules for prediction. Incorporating these methods into the Snuba synthesizer could be interesting for future work, especially for text-based tasks.\nTraining Label Generation. Focusing on the problem of generating training data, Snorkel [38] is a system that relies on domain experts manually developing heuristics, patterns, or distant supervision rules to label data noisily. While users in Snorkel rely on a small, labeled dataset to evaluate and refine their heuristics, Snuba automatically generates heuristics using the labeled and unlabeled data it has access to. Snorkel and Snuba both use the generative model to aggregate heuristic labels, but Snuba can generate heuristics that are noisier than the generative model can account for. Therefore, it uses a statistical measure to determine when the generative model can be used (Section 4). Other methods that rely on imperfect sources of labels that are partially user-defined include heuristic patterns [6, 17] and distant supervision [8, 32], which relies on information present in knowledge bases.\nUtilizing Labeled and Unlabeled Data. To train a deep learning model with a small, labeled dataset, a common approach is using transfer learning, or retraining models that have been trained for different tasks that have abundant training data in the same domain [34]. However, this approach does not take advantage of any unlabeled data available. Semi-supervised learning leverages both labeled and unlabeled data, along with assumptions about low-dimensional structure and smoothness of the data to automatically assign labels to the unlabeled data [7,61]. Unlike semi-supervised learning, which generates a single black-box model, Snuba generates multiple, diverse heuristics to label the unlabeled data. Moreover, as demonstrated in Section 5, Snuba performs better than a specific semi-supervised model, label spreading [61], when the amount of unlabeled data is larger than than the amount of labeled data. Co-training [5] also takes advantage of both labeled and unlabeled data and trains two independent models on two separate views of the data. Snuba does not require access to separate feature sets\nas views and can generate more than two heuristics (classifiers) that can be correlated with each other (Section 4).\nCombining Noisy Labels. Combining labels from multiple sources like heuristics is well-studied problem [11], especially in the context of crowdsourcing [9, 22, 60]. However, these methods assume the labeling sources are not generated automatically and requires a labeled dataset to learn the accuracies of the different sources. Other methods, including our previous work [39, 50, 53], rely on generative models to learn accuracies and dependencies among labeling sources [2,41,48]. Areas like data fusion [13,37,40] and truth discovery [29] also look at the problem of estimating how reliable different data sources are while utilizing probabilistic graphical models like Snuba."
        },
        {
            "heading": "7. CONCLUSION",
            "text": "Snuba is a system to automatically generate heuristics using a small labeled dataset to assign training labels to a large, unlabeled dataset, which can be used to train a downstream model of choice. It iteratively generates heuristics that are accurate and diverse for the unlabeled dataset using the small, labeled dataset. Snuba relies on a statistical measure to determine when generated heuristics are too noisy and therefore when to terminate the iterative process. We demonstrate how training labels from Snuba outperform labels from semi-supervised learning by up to 14.35 F1 points and from user-defined heuristics by up to 9.74 F1 points in terms of end model performance for tasks across various domains. Our work suggests that there is potential to use a small amount of labeled data to make the process of generating training labels much more efficient."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank Christopher Aberger, Jared Dunnmon, Avner May, Shoumik Palkar, Theodoros Rekatsinas, Sahaana Suri, and Sandeep Tata for their valuable feedback, and Sen Wu for help with the Hardware dataset. We gratefully acknowledge the support of DARPA under Nos. FA87501720095 (D3M) and FA86501827865 (SDH), NIH under No. N000141712266 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity) and CCF1563078 (Volume to Velocity), ONR under No. N000141712266 (Unifying Weak Supervision), the Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, Google, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, and American Family Insurance, the National Science Foundation Graduate Research Fellowship under Grant No. DGE114747, the Joseph W. and Hon Mai Goodman Stanford Graduate Fellowship,and members of the Stanford DAWN project: Intel, Microsoft, Teradata, Facebook, Google, Ant Financial, NEC, SAP, and VMWare. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of DARPA, NIH, ONR, or the U.S. Government."
        },
        {
            "heading": "8. REFERENCES",
            "text": "[1] IMDb Dataset. https://www.imdb.com/interfaces/.\n[2] E. Alfonseca, K. Filippova, J.-Y. Delort, and G. Garrido. Pattern learning for relation extraction with a hierarchical topic model. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2, pages 54\u201359. Association for Computational Linguistics, 2012. [3] R. Alur, R. Bodik, G. Juniwal, M. M. Martin, M. Raghothaman, S. A. Seshia, R. Singh, A. Solar-Lezama, E. Torlak, and A. Udupa. Syntax-guided synthesis. In\nFormal Methods in Computer-Aided Design (FMCAD), 2013, pages 1\u20138. IEEE, 2013.\n[4] S. H. Bach, B. He, A. Ratner, and C. Re\u0301. Learning the structure of generative models without labeled data. In International Conference on Machine Learning, pages 273\u2013282, 2017. [5] A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training. In Proceedings of the eleventh annual conference on Computational learning theory, pages 92\u2013100. ACM, 1998. [6] R. Bunescu and R. Mooney. Learning to extract relations from the web using minimal supervision. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 576\u2013583, 2007. [7] O. Chapelle, B. Scholkopf, and A. Zien. Semi-supervised learning (Chapelle, O. et al., eds.; 2006)[book reviews]. IEEE Transactions on Neural Networks, 20(3):542\u2013542, 2009. [8] M. Craven, J. Kumlien, et al. Constructing biological knowledge bases by extracting information from text sources. In ISMB, volume 1999, pages 77\u201386, 1999. [9] N. Dalvi, A. Dasgupta, R. Kumar, and V. Rastogi. Aggregating crowdsourced binary ratings. In Proceedings of the 22nd international conference on World Wide Web, pages 285\u2013294. ACM, 2013. [10] A. P. Davis, C. J. Grondin, R. J. Johnson, D. Sciaky, B. L. King, R. McMorran, J. Wiegers, T. C. Wiegers, and C. J. Mattingly. The comparative toxicogenomics database: update 2017. Nucleic acids research, 45(D1):D972\u2013D978, 2016. [11] A. P. Dawid and A. M. Skene. Maximum likelihood estimation of observer error-rates using the EM algorithm. Applied statistics, pages 20\u201328, 1979. [12] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248\u2013255. IEEE, 2009. [13] X. L. Dong and D. Srivastava. Big data integration. Synthesis Lectures on Data Management, 7(1):1\u2013198, 2015. [14] A. Graves and J. Schmidhuber. Framewise phoneme classification with bidirectional LSTM and other neural network architectures. Neural Networks, 18(5-6):602\u2013610, 2005. [15] S. Gulwani. Synthesis from examples: Interaction models and algorithms. In Symbolic and Numeric Algorithms for Scientific Computing (SYNASC), 2012 14th International Symposium on, pages 8\u201314. IEEE, 2012. [16] T. Hastie, S. Rosset, J. Zhu, and H. Zou. Multi-class AdaBoost. Statistics and its Interface, 2(3):349\u2013360, 2009. [17] M. A. Hearst. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th conference on Computational linguistics-Volume 2, pages 539\u2013545. Association for Computational Linguistics, 1992. [18] G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural computation, 14(8):1771\u20131800, 2002. [19] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997. [20] P. Jaccard. Lois de distribution florale dans la zone alpine. Bull Soc Vaudoise Sci Nat, 38:69\u2013130, 1902. [21] S. Jha, S. Gulwani, S. A. Seshia, and A. Tiwari. Oracle-guided component-based program synthesis. In Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering-Volume 1, pages 215\u2013224. ACM, 2010. [22] M. Joglekar, H. Garcia-Molina, and A. Parameswaran. Comprehensive and reliable crowd assessment algorithms. In Data Engineering (ICDE), 2015 IEEE 31st International Conference on, pages 195\u2013206. IEEE, 2015. [23] J. Johnson, B. Hariharan, L. van der Maaten, L. Fei-Fei, C. L. Zitnick, and R. Girshick. Clevr: A diagnostic dataset\nfor compositional language and elementary visual reasoning. In Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on, pages 1988\u20131997. IEEE, 2017. [24] S. Kok and P. Domingos. Statistical predicate invention. In Proceedings of the 24th international conference on Machine learning, pages 433\u2013440. ACM, 2007. [25] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma, et al. Visual Genome: Connecting language and vision using crowdsourced dense image annotations. International Journal of Computer Vision, 123(1):32\u201373, 2017. [26] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012. [27] N. Lao and W. W. Cohen. Relational retrieval using a combination of path-constrained random walks. Machine learning, 81(1):53\u201367, 2010. [28] H. Li, B. Yu, and D. Zhou. Error rate analysis of labeling by crowdsourcing. In ICML Workshop: Machine Learning Meets Crowdsourcing. Atalanta, Georgia, USA. Citeseer, 2013. [29] Y. Li, J. Gao, C. Meng, Q. Li, L. Su, B. Zhao, W. Fan, and J. Han. A survey on truth discovery. ACM Sigkdd Explorations Newsletter, 17(2):1\u201316, 2016. [30] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dolla\u0301r, and C. L. Zitnick. Microsoft COCO: Common objects in context. In European conference on computer vision, pages 740\u2013755. Springer, 2014. [31] C. Metz. Crowdflower dataset: Airline Twitter sentiment, 2015. https://www.crowdflower.com/data/ airline-twitter-sentiment/. [32] M. Mintz, S. Bills, R. Snow, and D. Jurafsky. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 1003\u20131011. Association for Computational Linguistics, 2009. [33] R. Mooney. Relational learning of pattern-match rules for information extraction. In Proceedings of the Sixteenth National Conference on Artificial Intelligence, volume 334, 1999. [34] S. J. Pan and Q. Yang. A survey on transfer learning. IEEE Transactions on knowledge and data engineering, 22(10):1345\u20131359, 2010. [35] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, et al. Scikit-learn: Machine learning in Python. Journal of machine learning research, 12(Oct):2825\u20132830, 2011. [36] J. Pennington, R. Socher, and C. Manning. GloVe: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532\u20131543, 2014. [37] R. Pochampally, A. Das Sarma, X. L. Dong, A. Meliou, and D. Srivastava. Fusing data with correlations. In Proceedings of the 2014 ACM SIGMOD international conference on Management of data, pages 433\u2013444. ACM, 2014. [38] A. Ratner, S. H. Bach, H. Ehrenberg, J. Fries, S. Wu, and C. Re\u0301. Snorkel: Rapid training data creation with weak supervision. PVLDB, 11(3):269\u2013282, 2017. [39] A. J. Ratner, C. M. De Sa, S. Wu, D. Selsam, and C. Re\u0301. Data programming: Creating large training sets, quickly. In Advances in Neural Information Processing Systems, pages 3567\u20133575, 2016. [40] T. Rekatsinas, M. Joglekar, H. Garcia-Molina, A. Parameswaran, and C. Re\u0301. SLiMFast: Guaranteed results for data fusion and source reliability. In Proceedings of the 2017 ACM International Conference on Management of Data, pages 1399\u20131414. ACM, 2017.\n[41] B. Roth and D. Klakow. Combining generative and discriminative model scores for distant supervision. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 24\u201329, 2013. [42] S. R. Safavian and D. Landgrebe. A survey of decision tree classifier methodology. IEEE transactions on systems, man, and cybernetics, 21(3):660\u2013674, 1991. [43] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. [44] R. Singh and S. Gulwani. Synthesizing number transformations from input-output examples. In International Conference on Computer Aided Verification, pages 634\u2013651. Springer, 2012. [45] S. Soderland. Learning information extraction rules for semi-structured and free text. Machine learning, 34(1-3):233\u2013272, 1999. [46] A. Solar-Lezama, L. Tancau, R. Bodik, S. Seshia, and V. Saraswat. Combinatorial sketching for finite programs. ACM Sigplan Notices, 41(11):404\u2013415, 2006. [47] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, A. Rabinovich, et al. Going deeper with convolutions. Cvpr, 2015. [48] S. Takamatsu, I. Sato, and H. Nakagawa. Reducing wrong labels in distant supervision for relation extraction. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 721\u2013729. Association for Computational Linguistics, 2012. [49] S. Van der Walt, J. L. Scho\u0308nberger, J. Nunez-Iglesias, F. Boulogne, J. D. Warner, N. Yager, E. Gouillart, and T. Yu. scikit-image: image processing in Python. PeerJ, 2:e453, 2014. [50] P. Varma, B. D. He, P. Bajaj, N. Khandwala, I. Banerjee, D. Rubin, and C. Re\u0301. Inferring generative model structure with static analysis. In Advances in Neural Information Processing Systems, pages 239\u2013249, 2017. [51] P. Varma, D. Iter, C. De Sa, and C. Re\u0301. Flipper: A systematic approach to debugging training sets. In Proceedings of the 2nd Workshop on Human-In-the-Loop Data Analytics, page 5. ACM, 2017. [52] P. Varma and C. Re\u0301. Snuba: Automating weak supervision to label training data, 2018. https://paroma.github.io/tech_report_snuba.pdf. [53] P. Varma, R. Yu, D. Iter, C. De Sa, and C. Re\u0301. Socratic learning: Correcting misspecified generative models using discriminative models. arXiv preprint arXiv:1610.08123, 2017. [54] F. Wang and C. Rudin. Falling rule lists. In Artificial Intelligence and Statistics, pages 1013\u20131022, 2015. [55] T. Wang, C. Rudin, F. Doshi-Velez, Y. Liu, E. Klampfl, and P. MacNeille. Or\u2019s of and\u2019s for interpretable classification, with application to context-aware recommender systems. arXiv preprint arXiv:1504.07614, 2015. [56] W. Y. Wang, K. Mazaitis, and W. W. Cohen. Structure learning via parameter learning. In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, pages 1199\u20131208. ACM, 2014. [57] C.-H. Wei, Y. Peng, R. Leaman, A. P. Davis, C. J. Mattingly, J. Li, T. C. Wiegers, and Z. Lu. Overview of the biocreative v chemical disease relation (CDR) task. In Proceedings of the fifth BioCreative challenge evaluation workshop, pages 154\u2013166. Sevilla Spain, 2015. [58] S. Wu, L. Hsiao, X. Cheng, B. Hancock, T. Rekatsinas, P. Levis, and C. Re\u0301. Fonduer: Knowledge base construction from richly formatted data. In Proceedings of the 2018 International Conference on Management of Data, pages 1301\u20131316. ACM, 2018. [59] F. Yang, Z. Yang, and W. W. Cohen. Differentiable learning of logical rules for knowledge base reasoning. In\nAdvances in Neural Information Processing Systems, pages 2319\u20132328, 2017.\n[60] Y. Zhang, X. Chen, D. Zhou, and M. I. Jordan. Spectral methods meet EM: A provably optimal algorithm for crowdsourcing. In Advances in neural information processing systems, pages 1260\u20131268, 2014. [61] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and B. Scho\u0308lkopf. Learning with local and global consistency. In Advances in neural information processing systems, pages 321\u2013328, 2004."
        }
    ],
    "title": "Snuba: Automating Weak Supervision to Label Training Data",
    "year": 2018
}