{
    "abstractText": "A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality bylearning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yoshua Bengio"
        },
        {
            "affiliations": [],
            "name": "R\u00e9jean Ducharme"
        },
        {
            "affiliations": [],
            "name": "Pascal Vincent"
        },
        {
            "affiliations": [],
            "name": "Christian Jauvin"
        },
        {
            "affiliations": [],
            "name": "John Shawe-Taylor"
        }
    ],
    "id": "SP:82becf5ac59b91ac8a8312ff8ebf849571d752eb",
    "references": [
        {
            "authors": [
                "S. Bengio",
                "Y. Bengio"
            ],
            "title": "Taking on the curse of dimensionality in joint distributions using neural networks.IEEE Transactions on Neural Networks, special issue on Data",
            "venue": "Mining and Knowledge Discovery,",
            "year": 2000
        },
        {
            "authors": [
                "Y. Bengio"
            ],
            "title": "New distributed probabilistic language models",
            "venue": "Technical Report 1215, Dept. IRO, Universite\u0301 de Montre\u0301al,",
            "year": 2002
        },
        {
            "authors": [
                "AISTATS",
                "2003. A. Berger",
                "S. Della Pietra",
                "V. Della Pietra"
            ],
            "title": "A maximum entropy approach to natural language",
            "year": 2003
        },
        {
            "authors": [
                "A. Brown",
                "G.E. Hinton"
            ],
            "title": "Products of hidden markov models",
            "venue": "Technical Report GCNU TR 2000-004,",
            "year": 1996
        },
        {
            "authors": [
                "G.E. Hinton"
            ],
            "title": "Training products of experts by minimizing contrastive divergence",
            "venue": "Technical Report GCNU TR 2000-004,",
            "year": 2000
        },
        {
            "authors": [
                "Kaufmann. H. Schwenk",
                "J-L. Gauvain"
            ],
            "title": "Connectionist language modeling for large vocabulary continuous speech recognition",
            "venue": "InInternational Conference on Acoustics, Speech and Signal Processing",
            "year": 1993
        }
    ],
    "sections": [
        {
            "text": "A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of thecurse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality bylearning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.\nKeywords: Statistical language modeling, artificial neural networks, distributed representation, curse of dimensionality"
        },
        {
            "heading": "1. Introduction",
            "text": "A fundamental problem that makes language modeling and other learning problems difficult is the curse of dimensionality. It is particularly obvious in the case when one wants to model the joint distribution between many discrete random variables (such as words in a sentence, or discrete attributes in a data-mining task). For example, if one wants to model the joint distribution of 10 consecutive words in a natural language with a vocabularyV of size 100,000, there are potentially 10000010\u2212 1 = 1050\u2212 1 free parameters. When modeling continuous variables, we obtain generalization more easily (e.g. with smooth classes of functions like multi-layer neural networks or Gaussian mixture models) because the function to be learned can be expected to have some local smoothness properties. For discrete spaces, the generalization structure is not as obvious: any change of these discrete variables may have a drastic impact on the value of the function to be esti-\nc\u00a92003 Yoshua Bengio, R\u00e9jean Ducharme, Pascal Vincent, Christian Jauvin.\nmated, and when the number of values that each discrete variable can take is large, most observed objects are almost maximally far from each other in hamming distance.\nA useful way to visualize how different learning algorithms generalize, inspired from the view of non-parametric density estimation, is to think of how probability mass that is initially concentrated on the training points (e.g., training sentences) is distributed in a larger volume, usually in some form of neighborhood around the training points. In high dimensions, it is crucial to distribute probability mass where it matters rather than uniformly in all directions around each training point. We will show in this paper that the way in which the approach proposed here generalizes is fundamentally different from the way in which previous state-of-the-art statistical language modeling approaches are generalizing.\nA statistical model of language can be represented by the conditional probability of the next word given all the previous ones, since\nP\u0302(wT1 ) = T\n\u220f t=1\nP\u0302(wt |wt\u221211 ),\nwherewt is the t-th word, and writing sub-sequencew j i = (wi,wi+1, \u00b7 \u00b7 \u00b7 ,wj\u22121,wj). Such statistical language models have already been found useful in many technological applications involving natural language, such as speech recognition, language translation, and information retrieval. Improvements in statistical language models could thus have a significant impact on such applications.\nWhen building statistical models of natural language, one considerably reduces the difficulty of this modeling problem by taking advantage of word order, and the fact that temporally closer words in the word sequence are statistically more dependent. Thus,n-grammodels construct tables of conditional probabilities for the next word, for each one of a large number ofcontexts, i.e. combinations of the lastn\u22121 words:\nP\u0302(wt |wt\u221211 )\u2248 P\u0302(wt |wt\u22121t\u2212n+1).\nWe only consider those combinations of successive words that actually occur in the training corpus, or that occur frequently enough. What happens when a new combination ofn words appears that was not seen in the training corpus? We do not want to assign zero probability to such cases, because such new combinations are likely to occur, and they will occur even more frequently for larger context sizes. A simple answer is to look at the probability predicted using a smaller context size, as done in back-off trigram models (Katz, 1987) or in smoothed (or interpolated) trigram models (Jelinek and Mercer, 1980). So, in such models, how is generalization basically obtained from sequences of words seen in the training corpus to new sequences of words? A way to understand how this happens is to think about a generative model corresponding to these interpolated or backoff n-gram models. Essentially, a new sequence of words is generated by \u201cgluing\u201d very short and overlapping pieces of length 1, 2 ... or up ton words that have been seen frequently in the training data. The rules for obtaining the probability of the next piece are implicit in the particulars of the back-off or interpolated n-gram algorithm. Typically researchers have usedn = 3, i.e. trigrams, and obtained state-of-the-art results, but see Goodman (2001) for how combining many tricks can yield to substantial improvements. Obviously there is much more information in the sequence that immediately precedes the word to predict than just the identity of the previous couple of words. There are at least two characteristics in this approach which beg to be improved upon, and that we\nwill focus on in this paper. First, it is not taking into account contexts farther than 1 or 2 words,1 second it is not taking into account the \u201csimilarity\u201d between words. For example, having seen the sentence \u201cThe cat is walking in the bedroom \u201d in the training corpus should help us generalize to make the sentence \u201cA dog was running in a room \u201d almost as likely, simply because \u201cdog \u201d and \u201ccat \u201d (resp. \u201cthe \u201d and \u201ca\u201d, \u201c room\u201d and \u201cbedroom \u201d, etc...) have similar semantic and grammatical roles.\nThere are many approaches that have been proposed to address these two issues, and we will briefly explain in Section 1.2 the relations between the approach proposed here and some of these earlier approaches. We will first discuss what is the basic idea of the proposed approach. A more formal presentation will follow in Section 2, using an implementation of these ideas that relies on shared-parameter multi-layer neural networks. Another contribution of this paper concerns the challenge of training such very large neural networks (with millions of parameters) for very large data sets (with millions or tens of millions of examples). Finally, an important contribution of this paper is to show that training such large-scale model is expensive but feasible, scales to large contexts, and yields good comparative results (Section 4).\nMany operations in this paper are in matrix notation, with lower casev denoting a column vector andv\u2032 its transpose,Aj the j-th row of a matrixA, andx.y = x\u2032y."
        },
        {
            "heading": "1.1 Fighting the Curse of Dimensionality with Distributed Representations",
            "text": "In a nutshell, the idea of the proposed approach can be summarized as follows:\n1. associate with each word in the vocabulary a distributedword feature vector(a realvalued vector inRm),\n2. express the jointprobability functionof word sequences in terms of the feature vectors of these words in the sequence, and\n3. learn simultaneously theword feature vectorsand the parameters of thatprobability function.\nThe feature vector represents different aspects of the word: each word is associated with a point in a vector space. The number of features (e.g.m =30, 60 or 100 in the experiments) is much smaller than the size of the vocabulary (e.g. 17,000). The probability function is expressed as a product of conditional probabilities of the next word given the previous ones, (e.g. using a multilayer neural network to predict the next word given the previous ones, in the experiments). This function has parameters that can be iteratively tuned in order tomaximize the log-likelihood of the training data or a regularized criterion, e.g. by adding a weight decay penalty.2 The feature vectors associated with each word are learned, but they could be initialized using prior knowledge of semantic features.\nWhy does it work? In the previous example, if we knew thatdog and cat played similar roles (semantically and syntactically), and similarly for (the ,a), (bedroom ,room ), (is ,was),\n1. n-grams withn up to 5 (i.e. 4 words of context) have been reported, though, but due to data scarcity, most predictions are made with a much shorter context. 2. Like in ridge regression, the squared norm of the parameters is penalized.\n(running ,walking ), we could naturally generalize (i.e. transfer probability mass) from The cat is walking in the bedroom to A dog was running in a room and likewise to The cat is running in a room\nA dog is walking in a bedroom The dog was walking in the room\n... and many other combinations. In the proposed model, it will so generalize because \u201csimilar\u201d words are expected to have a similar feature vector, and because the probability function is asmooth function of these feature values, a small change in the features will induce a small change in the probability. Therefore, the presence of only one of the above sentences in the training data will increase the probability, not only of that sentence, but also of its combinatorial number of \u201cneighbors\u201d in sentence space (as represented by sequences of feature vectors)."
        },
        {
            "heading": "1.2 Relation to Previous Work",
            "text": "The idea of using neural networks to model high-dimensional discrete distributions has already been found useful to learn the joint probability ofZ1 \u00b7 \u00b7 \u00b7Zn, a set of random variables where each is possibly of a different nature (Bengio and Bengio, 2000a,b). In that model, the joint probability is decomposed as a product of conditional probabilities\nP\u0302(Z1 = z1, \u00b7 \u00b7 \u00b7 ,Zn = zn) = \u220f i P\u0302(Zi = zi |gi(Zi\u22121 = zi\u22121,Zi\u22122 = zi\u22122, \u00b7 \u00b7 \u00b7 ,Z1 = z1)),\nwhereg(.) is a function represented by a neural network with a special left-to-right architecture, with the i-th output blockgi() computing parameters for expressing the conditional distribution of Zi given the value of the previousZ\u2019s, in some arbitrary order. Experiments on four UCI data sets show this approach to work comparatively very well (Bengio and Bengio, 2000a,b). Here we must deal with data of variable length, like sentences, so the above approach must be adapted. Another important difference is that here, all theZi (word ati-th position), refer to the same type of object (a word). The model proposed here therefore introduces a sharing of parameters across time \u2013 the same gi is used across time \u2013 that is, and across input words at different positions. It is a successful largescale application of the same idea, along with the (old) idea of learning a distributed representation for symbolic data, that was advocated in the early days of connectionism (Hinton, 1986, Elman, 1990). More recently, Hinton\u2019s approach was improved and successfully demonstrated on learning several symbolic relations (Paccanaro and Hinton, 2000). The idea of using neural networks for language modeling is not new either (e.g. Miikkulainen and Dyer, 1991). In contrast, here we push this idea to alarge scale, and concentrate on learning ast tistical modelof the distribution of word sequences, rather than learning the role of words in a sentence. The approach proposed here is also related to previous proposals of character-based text compression using neural networks to predict the probability of the next character (Schmidhuber, 1996). The idea of using a neural network for language modeling has also been independently proposed by Xu and Rudnicky (2000), although experiments are with networks without hidden units and a single input word, which limit the model to essentially capturing unigram and bigram statistics.\nThe idea of discovering some similarities between words to obtain generalization from training sequences to new sequences is not new. For example, it is exploited in approaches that are based on learning a clustering of the words (Brown et al., 1992, Pereira et al., 1993, Niesler et al., 1998, Baker\nand McCallum, 1998): each word is associated deterministically or probabilistically with a discrete class, and words in the same class are similar in some respect. In the model proposed here, instead of characterizing the similarity with a discrete random or deterministic variable (which corresponds to a soft or hard partition of the set of words), we use a continuous real-vector for each word, i.e. a learned distributed feature vector, to represent similarity between words. The experimental comparisons in this paper include results obtained with class-based n-grams (Brown et al., 1992, Ney and Kneser, 1993, Niesler et al., 1998).\nThe idea of using a vector-space representation for words has been well exploited in the area of information retrieval(for example see work by Schutze, 1993), where feature vectors for words are learned on the basis of their probability of co-occurring in the same documents (Latent Semantic Indexing, see Deerwester et al., 1990). An important difference is that here we look for a representation for words that is helpful in representing compactly the probability distribution of word sequences from natural language text. Experiments suggest that learning jointly the representation (word features) and the model is very useful. We tried (unsuccessfully) using as fixed word features for each wordw the first principal components of the co-occurrence frequencies ofw with the words occurring in text around the occurrence ofw. This is similar to what has been done with documents for information retrieval with LSI. The idea of using a continuous representation for words has however been exploited successfully by Bellegarda (1997) in the context of an n-gram based statistical language model, using LSI to dynamically identify the topic of discourse.\nThe idea of a vector-space representation for symbols in the context of neural networks has also previously been framed in terms of a parameter sharing layer, (e.g. Riis and Krogh, 1996) for secondary structure prediction, and for text-to-speech mapping (Jensen and Riis, 2000)."
        },
        {
            "heading": "2. A Neural Model",
            "text": "The training set is a sequencew1 \u00b7 \u00b7 \u00b7wT of wordswt \u2208 V, where the vocabularyV is a large but finite set. The objective is to learn a good modelf (wt , \u00b7 \u00b7 \u00b7 ,wt\u2212n+1) = P\u0302(wt |wt\u221211 ), in the sense that it gives high out-of-sample likelihood. Below, we report the geometric average of 1/P\u0302(wt |wt\u221211 ), also known asperplexity, which is also the exponential of the average negative log-likelihood. The only constraint on the model is that for any choice ofwt\u221211 , \u2211 |V| i=1 f (i,wt\u22121, \u00b7 \u00b7 \u00b7 ,wt\u2212n+1) = 1, with f > 0. By the product of these conditional probabilities, one obtains a model of the joint probability of sequences of words.\nWe decompose the functionf (wt , \u00b7 \u00b7 \u00b7 ,wt\u2212n+1) = P\u0302(wt |wt\u221211 ) in two parts: 1. A mappingC from any elementi of V to a real vectorC(i) \u2208Rm. It represents thedistributed\nfeature vectorsassociated with each word in the vocabulary. In practice,C is represented by a |V|\u00d7mmatrix of free parameters.\n2. The probability function over words, expressed withC: a functiong maps an input sequence of feature vectors for words in context,(C(wt\u2212n+1), \u00b7 \u00b7 \u00b7 ,C(wt\u22121)), to a conditional probability distribution over words inV for the next wordwt . The output ofg is a vector whosei-th element estimates the probabilityP\u0302(wt = i|wt\u221211 ) as in Figure 1.\nf (i,wt\u22121, \u00b7 \u00b7 \u00b7 ,wt\u2212n+1) = g(i,C(wt\u22121), \u00b7 \u00b7 \u00b7 ,C(wt\u2212n+1)) The function f is a composition of these two mappings (C and g), with C being sharedacross all the words in the context. With each of these two parts are associated some parameters. The\nTraining is achieved by looking for\u03b8 that maximizes the training corpus penalized log-likelihood:\nL = 1 T \u2211t log f (wt ,wt\u22121, \u00b7 \u00b7 \u00b7 ,wt\u2212n+1;\u03b8)+R(\u03b8),\nwhereR(\u03b8) is a regularization term. For example, in our experiments,R i a weight decay penalty applied only to the weights of the neural network and to theC matrix, not to the biases.3\nIn the above model, the number of free parametersonly scales linearlywith V, the number of words in the vocabulary. It alsonly scales linearlywith the ordern : the scaling factor could be reduced to sub-linear if more sharing structure were introduced, e.g. using a time-delay neural network or a recurrent neural network (or a combination of both).\nIn most experiments below, the neural network has one hidden layer beyond the word features mapping, and optionally, direct connections from the word features to the output. Therefore there are really two hidden layers: the shared word features layerC, which has no non-linearity (it would not add anything useful), and the ordinary hyperbolic tangent hidden layer. More precisely, the neural network computes the following function, with asoftmaxoutput layer, which guarantees positive probabilities summing to 1:\nP\u0302(wt |wt\u22121, \u00b7 \u00b7 \u00b7wt\u2212n+1) = e ywt\n\u2211i eyi .\n3. Thebiasesare the additive parameters of the neural network, such asb andd in equation 1 below.\nThe yi are the unnormalized log-probabilities for each output wordi, computed as follows, with parametersb,W,U ,d andH:\ny = b+Wx+U tanh(d+Hx) (1)\nwhere the hyperbolic tangent tanh is applied element by element,W is optionally zero (no direct connections), andx is the word features layer activation vector, which is the concatenation of the input word features from the matrix C:\nx = (C(wt\u22121),C(wt\u22122), \u00b7 \u00b7 \u00b7 ,C(wt\u2212n+1)). Let h be the number of hidden units, andm the number of features associated with each word. When no direct connections from word features to outputs are desired, the matrixW is set to 0. The free parameters of the model are the output biasesb (with |V| elements), the hidden layer biasesd (with h elements), the hidden-to-output weightsU (a |V|\u00d7h matrix), the word features to output weights W (a |V| \u00d7 (n\u2212 1)m matrix), the hidden layer weightsH (a h\u00d7 (n\u2212 1)m matrix), and the word featuresC (a |V|\u00d7mmatrix):\n\u03b8 = (b,d,W,U,H,C).\nThe number of free parameters is|V|(1+ nm+ h) + h(1+ (n\u2212 1)m). The dominating factor is |V|(nm+ h). Note that in theory, if there is a weight decay on the weightsW andH but not onC, thenW andH could converge towards zero whileC would blow up. In practice we did not observe such behavior when training with stochastic gradient ascent.\nStochastic gradient ascent on the neural network consists in performing the following iterative update after presenting thet-th word of the training corpus:\n\u03b8\u2190 \u03b8+ \u03b5\u2202 logP\u0302(wt |wt\u22121, \u00b7 \u00b7 \u00b7wt\u2212n+1) \u2202\u03b8\nwhere\u03b5 is the \u201clearning rate\u201d. Note that a large fraction of the parameters needs not be updated or visited after each example: the word featuresC( j) of all words j that do not occur in the input window.\nMixture of models. In our experiments (see Section 4) we have found improved performance by combining the probability predictions of the neural network with those of an interpolated trigram model, either with a simple fixed weight of 0.5, a learned weight (maximum likelihood on the validation set) or a set of weights that are conditional on the frequency of the context (using the same procedure that combines trigram, bigram, and unigram in the interpolated trigram, which is a mixture)."
        },
        {
            "heading": "3. Parallel Implementation",
            "text": "Although the number of parameters scales nicely, i.e. linearly with the size of the input window and linearly with the size of the vocabulary, the amount of computation required for obtaining the output probabilities is much greater than that required from n-gram models. The main reason is that with n-gram models, obtaining a particularP(wt |wt\u22121, . . . ,wt\u2212n+1) does not require the computation of the probabilities for all the words in the vocabulary, because of the easy normalization (performed when training the model) enjoyed by the linear combinations of relative frequencies. The main computational bottleneck with the neural implementation is the computation of the activations of the output layer.\nRunning the model (both training and testing) on a parallel computer is a way to reduce computation time. We have explored parallelization on two types of platforms: shared-memory processor machines and Linux clusters with a fast network."
        },
        {
            "heading": "3.1 Data-Parallel Processing",
            "text": "In the case of a shared-memory processor, parallelization is easily achieved, thanks to the very low communication overhead between processors, through the shared memory. In that case we have chosen a data-parallel implementation in whicheach processor works on a different subset of the data. Each processor computes the gradient for its examples, and performs stochastic gradient updates on the parameters of the model, which are simply stored in a shared-memory area. Our first implementation was extremely slow and relied on synchronization commands to make sure that each processor would not write at the same time as another one in one of the above parameter subsets. Most of the cycles of each processor were spent waiting for another processor to release a lock on the write access to the parameters.\nInstead we have chosen ansynchronous implementationwhere each processor can write at any time in the shared-memory area. Sometimes, part of an update on the parameter vector by one of the processors is lost, being overwritten by the update of another processor, and this introduces a bit of noise in the parameter updates. However, this noise seems to be very small and did not apparently slow down training.\nUnfortunately, large shared-memory parallel computers are very expensive and their processor speed tends to lag behind mainstream CPUs that can be connected in clusters. We have thus been able to obtain much faster training on fast network clusters."
        },
        {
            "heading": "3.2 Parameter-Parallel Processing",
            "text": "If the parallel computer is a network of CPUs, we generally can\u2019t afford to frequently exchange all the parameters among the processors, because that represents tens of megabytes (almost 100 megabytes in the case of our largest network), which would take too much time through a local network. Instead we have chosen toparallelize across the parameters, in particular the parameters of the output units, because that is where the vast majority of the computation is taking place, in our architecture. Each CPU is responsible for the computation of the unnormalized probability for a subset of the outputs, and performs the updates for the corresponding output unit parameters (weights going into that unit). This strategy allowed us to perform apar llelized stochastic gradient ascentwith a negligible communication overhead. The CPUs essentially need to communicate two informations: (1) the normalization factor of the output softmax, and (2) the gradients on the hidden layer (denoteda below) and word feature layer (denotedx). All the CPUs duplicate the computations that precede the computation of the output units activations, i.e., the selection of word features and the computation of the hidden layer activationa, as well as the corresponding back-propagation and update steps. However, these computations are a negligible part of the total computation for our networks.\nFor example, consider the following architecture used in the experiments on the AP (Associated Press) news data: the vocabulary size is|V|= 17,964, the number of hidden units ish= 60, the order of the model isn= 6, the number of word features ism= 100. The total number of numerical operations to process a single training example is approximately|V|(1+nm+h)+h(1+nm)+nm(where the terms correspond respectively to the computations of the output units, hidden units, and word\nfeature units). In this example the fraction of the overall computation required for computing the weighted sums of the output units is therefore approximately |V|(1+(n\u22121)m+h)|V|(1+(n\u22121)m+h)+h(1+(n\u22121)m)+(n\u22121)m = 99.7%. This calculation is approximate because the actual CPU time associated with different operations differ, but it shows that it is generally advantageous to parallelize the output units computation. The fact that all CPUs will duplicate a very small fraction of the computations is not going to hurt the total computation time for the level of parallelization sought here, i.e. of a few dozen processors. If the number of hidden units was large, parallelizing their computation would also become profitable, but we did not investigate that approach in our experiments.\nThe implementation of this strategy was done on a cluster of 1.2 GHz clock-speed Athlon processors (32 x 2 CPUs) connected through a Myrinet network (a low-latency Gigabit local area network), using the MPI (Message Passing Interface) library (Dongarra et al., 1995) for the parallelization routines. The parallelization algorithm is sketched below, for a single example(wt\u2212n+1, \u00b7 \u00b7 \u00b7 ,wt), executed in parallel by CPUi in a cluster ofM processors. CPUi (i ranging from 0 toM\u2212 1) is responsible of a block of output units starting at numberstarti = i\u00d7d|V|/Me, the block being of length min(d|V |/Me, |V|\u2212starti).\nCOMPUTATION FOR PROCESSOR i, examplet\n1. FORWARD PHASE\n(a) Perform forward computation for the word features layer: x(k)\u2190C(wt\u2212k), x = (x(1),x(2), \u00b7 \u00b7 \u00b7 ,x(n\u22121))\n(b) Perform forward computation for the hidden layer: o\u2190 d+Hx a\u2190 tanh(o)\n(c) Perform forward computation for output units in thei-th block: si \u2190 0 Loop over j in the i-th block\ni. yj \u2190 bj +a.Uj ii. If (direct connections)yj \u2190 yj +x.Wj\niii. pj \u2190 eyj iv. si \u2190 si + pj\n(d) Compute and shareS= \u2211i si among the processors. This can easily be achieved with an MPI Allreduce operation, which can efficiently compute and share this sum.\n(e) Normalize the probabilities: Loop over j in the i-th block, pj \u2190 pj/S.\n(f) Update the log-likelihood. Ifwt falls in the block of CPUi > 0, then CPUi sendspwt to CPU 0. CPU 0 computesL = logpwt and keeps track of the total log-likelihood.\n2. BACKWARD/UPDATE PHASE , with learning rate\u03b5.\n(a) Perform backward gradient computation for output units in thei-t block: clear gradient vectors\u2202L\u2202a and \u2202L \u2202x .\nLoop over j in the i-th block\ni. \u2202L\u2202yj \u2190 1 j==wt \u2212 pj ii. bj \u2190 bj + \u03b5 \u2202L\u2202yj\nIf (direct connections)\u2202L\u2202x \u2190 \u2202L\u2202x + \u2202L\u2202yj Wj \u2202L \u2202a \u2190 \u2202L\u2202a + \u2202L\u2202yj Uj If (direct connections)Wj \u2190Wj + \u03b5 \u2202L\u2202yj x Uj \u2190Uj + \u03b5 \u2202L\u2202yj a\n(b) Sum and share\u2202L\u2202x and \u2202L \u2202a across processors. This can easily be achieved with an MPI\nAllreduce operation.\n(c) Back-propagate through and update hidden layer weights: Loop overk between 1 andh,\n\u2202L \u2202ok \u2190 (1\u2212a2k) \u2202L \u2202ak\n\u2202L \u2202x \u2190 \u2202L\u2202x +H \u2032 \u2202L\u2202o d\u2190 d+ \u03b5 \u2202L\u2202o H\u2190 H + \u03b5 \u2202L\u2202ox\u2032\n(d) Update word feature vectors for the input words: Loop overk between 1 andn\u22121\nC(wt\u2212k)\u2190C(wt\u2212k)+ \u03b5 \u2202L\u2202x(k) where \u2202L\u2202x(k) is thek-th block (of lengthm) of the vector \u2202L \u2202x .\nThe weight decay regularization was not shown in the above implementation but can easily be put in (by subtracting the weight decay factor times the learning rate times the value of the parameter, from each parameter, at each update). Note that parameter updates are done directly rather than through a parameter gradient vector, to increase speed, a limiting factor in computation speed being the access to memory, in our experiments.\nThere could be a numerical problem in the computation of the exponentials in the forward phase, whereby all thepj could be numerically zero, or one of them could be too large for computing the exponential (step 1(c)ii above). To avoid this problem, the usual solution is to subtract the maximum of theyj \u2019s before taking the exponentials in thesoftmax. Thus we have added an extra Allreduce operation to share among theM processors the maximum of theyj \u2019s, before computing the exponentials inpj . Let qi be the maximum of theyj \u2019s in block i. Then the overall maximum Q = maxi qi is collectively computed and shared among theM processors. The exponentials are then computed as follows:pj \u2190 eyj\u2212Q (instead of step 1(c)ii to guarantee that at least one of the pj \u2019s will be numerically non-zero, and the maximum of the exponential\u2019s argument is 1.\nBy comparing clock time of the parallel version with clock time on a single processor, we found that the communication overhead was only 1/15th of the total time (for one training epoch): thus we get an almost perfect speed-up through parallelization, using this algorithm on a fast network.\nOn clusters with a slow network, it might be possible to still obtain an efficient parallelization by performing the communications everyK examples (amini-batch) rather than for each example. This requires storingK versions of the activities and gradients of the neural network in each processor. After the forward phase on theK examples, the probability sums must be shared among the\nprocessors. Then theK backward phases are initiated, to obtain theK partial gradient vectors\u2202L\u2202a and \u2202L\u2202x . After exchanging these gradient vectors among the processors, each processor can complete the backward phase and update parameters. This method mainly saves time because of the savings in network communication latency (the amount of data transferred is the same). It may lose in convergence time ifK is too large, for the same reason that batch gradient descent is generally much slower than stochastic gradient descent (LeCun et al., 1998)."
        },
        {
            "heading": "4. Experimental Results",
            "text": "Comparative experiments were performed on the Brown corpus which is a stream of 1,181,041 words, from a large variety of English texts and books. The first 800,000 words were used for training, the following 200,000 for validation (model selection, weight decay, early stopping) and the remaining 181,041 for testing. The number of different words is 47,578 (including punctuation, distinguishing between upper and lower case, and including the syntactical marks used to separate texts and paragraphs). Rare words with frequency\u2264 3 were merged into a single symbol, reducing the vocabulary size to|V|= 16,383.\nAn experiment was also run on text from the Associated Press (AP) News from 1995 and 1996. The training set is a stream of about 14 million (13,994,528) words, the validation set is a stream of about 1 million (963,138) words, and the test set is also a stream of about 1 million (963,071) words. The original data has 148,721 different words (including punctuation), which was reduced to |V|= 17964 by keeping only the most frequent words (and keeping punctuation), mapping upper case to lower case, mapping numeric forms to special symbols, mapping rare words to a special symbol and mapping proper nouns to another special symbol.\nFor training the neural networks, the initial learning rate was set to\u03b5o = 10\u22123 (after a few trials with a tiny data set), and gradually decreased according to the following schedule:\u03b5t = \u03b5o1+rt where t represents the number of parameter updates done andr is a decrease factor that was heuristically chosen to ber = 10\u22128."
        },
        {
            "heading": "4.1 N-Gram Models",
            "text": "The first benchmark against which the neural network was compared is an interpolated or smoothed trigram model (Jelinek and Mercer, 1980). Letqt = l( f req(wt\u22121,wt\u22122)) represents the discretized frequency of occurrence of the input context(wt\u22121,wt\u22122).4 Then the conditional probability estimates have the form of a conditional mixture:\nP\u0302(wt |wt\u22121,wt\u22122) = \u03b10(qt)p0 + \u03b11(qt)p1(wt)+ \u03b12(qt)p2(wt |wt\u22121)+ \u03b13(qt)p3(wt |wt\u22121,wt\u22122)\nwith conditional weights\u03b1i(qt) \u2265 0,\u2211i \u03b1i(qt) = 1. The base predictors are the following:p0 = 1/|V |, p1(i) is a unigram (relative frequency of wordi in the training set),p2(i| j) is the bigram (relative frequency of wordi when the previous word isj), and p3(i| j,k) is the trigram (relative frequency of wordi when the previous 2 words arej and k). The motivation is that when the frequency of(wt\u22121,wt\u22122) is large, p3 is most reliable, whereas when it is lower, the lower-order statistics ofp2, p1, or evenp0 are more reliable. There is a different set of mixture weights\u03b1 for each of the discrete values ofqt (which are context frequency bins). They can be easily estimated with\n4. We usedl(x) = d\u2212 log((1+x)/T)e where f req(wt\u22121,wt\u22122) is the frequency of occurrence of the input context and T is the size of the training corpus.\nthe EM algorithm in about 5 iterations, on a set of data (the validation set) not used for estimating the unigram, bigram and trigram relative frequencies. The interpolated n-gram was used to form a mixture with the MLPs since they appear to make \u201cerrors\u201d in very different ways.\nComparisons were also made with other state-of-the-art n-gram models: back-off n-gram models with theModified Kneser-Neyalgorithm (Kneser and Ney, 1995, Chen and Goodman., 1999), as well as class-based n-gram models (Brown et al., 1992, Ney and Kneser, 1993, Niesler et al., 1998). The validation set was used to choose the order of the n-gram and the number of word classes for the class-based models. We used the implementation of these algorithms in the SRI Language Modeling toolkit, described by Stolcke (2002) and inwww.speech.sri.com/projects/srilm/ . They were used for computing the back-off models perplexities reported below, noting that we did not give a special status to end-of-sentence tokens in the accounting of the log-likelihood, just as for our neural network perplexity. All tokens (words and punctuation) were treated the same in averaging the log-likelihood (hence in obtaining the perplexity)."
        },
        {
            "heading": "4.2 Results",
            "text": "Below are measures of test set perplexity (geometric average of 1/P\u0302(wt |wt\u221211 )) for different models P\u0302. Apparent convergence of the stochastic gradient ascent procedure was obtained after around 10 to 20 epochs for the Brown corpus. On the AP News corpus we were not able to see signs of overfitting (on the validation set), possibly because we ran only 5 epochs (over 3 weeks using 40 CPUs). Early stopping on the validation set was used, but was necessary only in our Brown experiments. A weight decay penalty of 10\u22124 was used in the Brown experiments and a weight decay of 10\u22125 was used in the APNews experiments (selected by a few trials, based on validation set perplexity). Table 1 summarizes the results obtained on the Brown corpus. All the back-off models of the table are modified Kneser-Ney n-grams, which worked significantly better than standard back-off models. Whenm is specified for a back-off model in the table, a class-based n-gram is used (m is the number of word classes). Random initialization of the word features was done (similarly to initialization of neural network weights), but we suspect that better results might be obtained with a knowledge-based initialization.\nThemain result is that significantly better results can be obtained when using the neural network, in comparison with the best of the n-grams, with a test perplexity difference of about 24% on Brown and about 8% on AP News, when taking the MLP versus the n-gram that worked best on the validation set. The table also suggests that the neural network was able to take advantage of more context (on Brown, going from 2 words of context to 4 words brought improvements to the neural network, not to the n-grams). It also shows that the hidden units are useful (MLP3 vs MLP1 and MLP4 vs MLP2), and that mixing the output probabilities of the neural network with the interpolated trigram always helps to reduce perplexity. The fact that simple averaging helps suggests that the neural network and the trigram make errors (i.e. low probability given to an observed word) in different places. The results do not allow to say whether the direct connections from input to output are useful or not, but suggest that on a smaller corpus at least, better generalization can be obtained without the direct input-to-output connections, at the cost of longer training: without direct connections the network took twice as much time to converge (20 epochs instead of 10), albeit to a slightly lower perplexity. A reasonable interpretation is that direct input-to-output connections provide a bit more capacity and faster learning of the \u201clinear\u201d part of the mapping from word features to log-\nThe difference is 24% in the case of the best n-gram (a class-based model with 500 word classes).n : order of the model.c : number of word classes in class-based n-grams.h : number of hidden units.m : number of word features for MLPs, number of classes for class-based n-grams.direct: whether there are direct connections from word features to outputs. mix: whether the output probabilities of the neural network are mixed with the output of the trigram (with a weight of 0.5 on each). The last three columns give perplexity on the training, validation and test sets.\nprobabilities. On the other hand, without those connections the hidden units form a tight bottleneck which might force better generalization.\nTable 2 gives similar results on the larger corpus (AP News), albeit with a smaller difference in perplexity (8%). Only 5 epochs were performed (in approximately three weeks with 40 CPUs). The class-based model did not appear to help the n-gram models in this case, but the high-order modified Kneser-Ney back-off model gave the best results among the n-gram models."
        },
        {
            "heading": "5. Extensions and Future Work",
            "text": "In this section, we describe extensions to the model described above, and directions for future work."
        },
        {
            "heading": "5.1 An Energy Minimization Network",
            "text": "A variant of the above neural network can be interpreted as an energy minimization model following Hinton\u2019s recent work on products of experts (Hinton, 2000). In the neural network described in the previous sections the distributed word features are used only for the \u201cinput\u201d words and not for the \u201coutput\u201d word (next word). Furthermore, a very large number of parameters (the majority) are expanded in the output layer: the semantic or syntactic similarities between output words are not exploited. In the variant described here, the output word is also represented by its feature vector. The network takes in input a sub-sequence of words (mapped to their feature vectors) and outputs an energy functionE which is low when the words form a likely sub-sequence, high when it is unlikely. For example, the network outputs an \u201cenergy\u201d function\nE(wt\u2212n+1, \u00b7 \u00b7 \u00b7 ,wt) = v. tanh(d+Hx)+ n\u22121 \u2211 i=0 bwt\u2212i\nwhereb is the vector of biases (which correspond to unconditional probabilities),d is the vector of hidden units biases,v is the output weight vector, andH is the hidden layer weight matrix, and unlike in the previous model, input and output words contribute tox:\nx = (C(wt),C(wt\u22121),C(wt\u22122), \u00b7 \u00b7 \u00b7 ,C(wt\u2212n+1). The energy functionE(wt\u2212n+1, \u00b7 \u00b7 \u00b7 ,wt) can be interpreted as an unnormalized log-probability for the joint occurrence of(wt\u2212n+1, \u00b7 \u00b7 \u00b7 ,wt). To obtain a conditional probability\u0302P(wt |wt\u22121t\u2212n+1) it is enough (but costly) to normalize over the possible values ofwt , as follows:\nP\u0302(wt |wt\u22121, \u00b7 \u00b7 \u00b7 ,wt\u2212n+1) = e \u2212E(wt\u2212n+1,\u00b7\u00b7\u00b7 ,wt )\n\u2211i e\u2212E(wt\u2212n+1,\u00b7\u00b7\u00b7 ,wt\u22121,i)\nNote that the total amount of computation is comparable to the architecture presented earlier, and the number of parameters can also be matched if thev parameter is indexed by the identity of the target word (wt ). Note that onlybwt remains after the above softmax normalization (any linear function of thewt\u2212i for i > 0 is canceled by the softmax normalization). As before, the parameters of the model can be tuned by stochastic gradient ascent on logP\u0302(wt |wt\u22121, \u00b7 \u00b7 \u00b7 ,wt\u2212n+1), using similar computations.\nIn the products-of-experts framework, the hidden units can be seen as the experts: the joint probability of a sub-sequence(wt\u2212n+1, \u00b7 \u00b7 \u00b7 ,wt) is proportional to the exponential of a sum of terms associated with each hidden unitj, vj tanh(dj + Hjx). Note that because we have chosen to decompose the probability of a whole sequence in terms of conditional probabilities for each element,\nthe computation of the gradient is tractable. This is not the case for example with products-ofHMMs (Brown and Hinton, 2000), in which the product is over experts that view the whole sequence, and which can be trained with approximate gradient algorithms such as the contrastive divergence algorithm (Brown and Hinton, 2000). Note also that this architecture and the productsof-experts formulation can be seen as extensions of the very successfulMaximum Entropy models (Berger et al., 1996), but where the basis functions (or \u201cfeatures\u201d, here the hidden units activations) are learned by penalized maximum likelihood at the same time as the parameters of the features linear combination, instead of being learned in an outer loop, with greedy feature subset selection methods.\nWe have implemented and experimented with the above architecture, and have developed a speed-up technique for the neural network training, based on importance sampling and yielding a 100-fold speed-up (Bengio and Sen\u00e9cal, 2003).\nOut-of-vocabulary words. An advantage of this architecture over the previous one is that it can easily deal with out-of-vocabulary words (and even assign them a probability!). The main idea is to first guess an initial feature vector for such a word, by taking a weighted convex combination of the feature vectors of other words that could have occurred in the same context, with weights proportional to their conditional probability. Suppose that the network assigned a probabilityP\u0302( |wt\u22121t\u2212n+1) to wordsi \u2208V in contextwt\u22121t\u2212n+1, and that in this context we observe a new wordj 6\u2208V. We initialize the feature vectorC( j) for j as follows:C( j)\u2190 \u2211i\u2208V C(i)P\u0302(i|wt\u22121t\u2212n+1). We can then incorporatej in V and re-compute probabilities for this slightly larger set (which only requires a renormalization for all the words, except for wordi, which requires a pass through the neural network). This feature vectorC(i) can then be used in the input context part when we try to predict the probabilities of words that follow wordi."
        },
        {
            "heading": "5.2 Other Future Work",
            "text": "There are still many challenges ahead to follow-up on this work. In the short term, methods to speed-up training and recognition need to be designed and evaluated. In the longer term, more ways to generalize should be introduced, in addition to the two main ways exploited here. Here are some ideas that we intend to explore:\n1. Decomposing the network in sub-networks, for example using a clustering of the words. Training many smaller networks should be easier and faster.\n2. Representing the conditional probability with a tree structure where a neural network is applied at each node, and each node represents the probability of a word class given the context and the leaves represent the probability of words given the context. This type of representation has the potential to reduce computation time by a factor|V|/ log|V| (see Bengio, 2002).\n3. Propagating gradients only from a subset of the output words. It could be the words that are conditionally most likely (based on a faster model such as a trigram, see Schwenk and Gauvain, 2002, for an application of this idea), or it could be a subset of the words for which the trigram has been found to perform poorly. If the language model is coupled to a speech recognizer, then only the scores (unnormalized probabilities) of the acoustically ambiguous words need to be computed. See also Bengio and Sen\u00e9cal (2003) for a new accelerated training method using importance sampling to select the words.\n4. Introducing a-priori knowledge. Several forms of such knowledge could be introduced, such as: semantic information (e.g., from WordNet, see Fellbaum, 1998), low-level grammatical information (e.g., using parts-of-speech), and high-level grammatical information, e.g., coupling the model to a stochastic grammar, as suggested in Bengio (2002). The effect of longer term context could be captured by introducing more structure and parameter sharing in the neural network, e.g. using time-delay or recurrent neural networks. In such a multi-layered network the computation that has been performed for small groups of consecutive words does not need to be redone when the network input window is shifted. Similarly, one could use a recurrent network to capture potentially even longer term information about the subject of the text.\n5. Interpreting (and possibly using) the word feature representation learned by the neural network. A simple first step would start withm = 2 features, which can be more easily displayed. We believe that more meaningful representations will require large training corpora, especially for larger values ofm.\n6. Polysemous words are probably not well served by the model presented here, which assigns to each word a single point in a continuous semantic space. We are investigating extensions of this model in which each word is associated with multiple points in that space, each associated with the different senses of the word."
        },
        {
            "heading": "6. Conclusion",
            "text": "The experiments on two corpora, one with more than a million examples, and a larger one with above 15 million words, have shown that the proposed approach yields much better perplexity than a state-of-the-art method, the smoothed trigram, with differences between 10 and 20% in perplexity.\nWe believe that the main reason for these improvements is that the proposed approach allows to take advantage of the learned distributed representation to fight the curse of dimensionality with its own weapons: each training sentence informs the model about a combinatorial number of other sentences.\nThere is probably much more to be done to improve the model, at the level of architecture, computational efficiency, and taking advantage of prior knowledge. An important priority of future research should be to improve speed-up techniques5 as well as ways to increase capacity without increasing training time too much (to deal with corpora with hundreds of millions of words or more). A simple idea to take advantage of temporal structure and extend the size of the input window to include possibly a whole paragraph (without increasing too much the number of parameters or computation time) is to use a time-delay and possibly recurrent neural networks. Evaluations of the type of models presented here in applicative contexts would also be useful, but see work already done by Schwenk and Gauvain (2002) for improvements in speech recognition word error rate.\nMore generally, the work presented here opens the door to improvements in statistical language models brought by replacing \u201ctables of conditional probabilities\u201d by more compact and smoother representations based on distributed representations that can accommodate far more conditioning variables. Whereas much effort has been spent in statistical language models (e.g. stochastic grammars) to restrict or summarize the conditioning variables in order to avoid overfitting, the type of\n5. See work by Bengio and Sen\u00e9cal (2003) for a 100-fold speed-up technique.\nmodels described here shifts the difficulty elsewhere: many more computations are required, but computation and memory requirements scale linearly, not exponentially with the number of conditioning variables."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "The authors would like to thank L\u00e9on Bottou, Yann Le Cun and Geoffrey Hinton for useful discussions. This research was made possible by funding from the NSERC granting agency, as well as the MITACS and IRIS networks."
        }
    ],
    "title": "A Neural Probabilistic Language Model"
}