{
    "abstractText": "Automatic interpretation of the relation between the constituents of a noun compound, e.g. olive oil (source) and baby oil (purpose) is an important task for many NLP applications. Recent approaches are typically based on either noun-compound representations or paraphrases. While the former has initially shown promising results, recent work suggests that the success stems from memorizing single prototypical words for each relation. We explore a neural paraphrasing approach that demonstrates superior performance when such memorization is not possible.",
    "authors": [
        {
            "affiliations": [],
            "name": "Vered Shwartz"
        }
    ],
    "id": "SP:0f2297838a40cdc4a94935f250aa98b6afb4dd67",
    "references": [
        {
            "authors": [
                "Mart\u0131\u0301n Abadi",
                "Ashish Agarwal",
                "Paul Barham",
                "Eugene Brevdo",
                "Zhifeng Chen",
                "Craig Citro",
                "Greg S Corrado",
                "Andy Davis",
                "Jeffrey Dean",
                "Matthieu Devin"
            ],
            "title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems",
            "year": 2016
        },
        {
            "authors": [
                "Marco Baroni",
                "Roberto Zamparelli."
            ],
            "title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space",
            "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Associ-",
            "year": 2010
        },
        {
            "authors": [
                "Corina Dima"
            ],
            "title": "Proceedings of the 1st Workshop on Representation Learning for NLP, Association for Computational Linguistics, chapter On the Compositionality and Semantic Interpretation of English Noun Compounds",
            "year": 2016
        },
        {
            "authors": [
                "Corina Dima",
                "Erhard Hinrichs."
            ],
            "title": "Automatic noun compound interpretation using deep neural networks and word embeddings",
            "venue": "IWCS 2015 page 173.",
            "year": 2015
        },
        {
            "authors": [
                "Georgiana Dinu",
                "Nghia The Pham",
                "Marco Baroni."
            ],
            "title": "General estimation and evaluation of compositional distributional semantic models",
            "venue": "Proceedings of the Workshop on Continuous Vector Space Models and their",
            "year": 2013
        },
        {
            "authors": [
                "Iris Hendrickx",
                "Zornitsa Kozareva",
                "Preslav Nakov",
                "Diarmuid \u00d3 S\u00e9aghdha",
                "Stan Szpakowicz",
                "Tony Veale."
            ],
            "title": "Semeval-2013 task 4: Free paraphrases of noun compounds",
            "venue": "Second Joint Conference on Lexical and Computational Semantics (*SEM),",
            "year": 2013
        },
        {
            "authors": [
                "Sepp Hochreiter",
                "J\u00fcrgen Schmidhuber."
            ],
            "title": "Long short-term memory",
            "venue": "Neural computation 9(8):1735\u20131780.",
            "year": 1997
        },
        {
            "authors": [
                "Nam Su Kim",
                "Preslav Nakov."
            ],
            "title": "Large-scale noun compound interpretation using bootstrapping and the web as a corpus",
            "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing. Associa-",
            "year": 2011
        },
        {
            "authors": [
                "Diederik Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980 .",
            "year": 2014
        },
        {
            "authors": [
                "Omer Levy",
                "Steffen Remus",
                "Chris Biemann",
                "Ido Dagan"
            ],
            "title": "Do supervised distributional methods really learn lexical inference relations",
            "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the As-",
            "year": 2015
        },
        {
            "authors": [
                "Jeff Mitchell",
                "Mirella Lapata."
            ],
            "title": "Composition in distributional models of semantics",
            "venue": "Cognitive science 34(8):1388\u20131429.",
            "year": 2010
        },
        {
            "authors": [
                "Preslav Nakov",
                "Marti Hearst."
            ],
            "title": "Using verbs to characterize noun-noun relations",
            "venue": "International Conference on Artificial Intelligence: Methodology, Systems, and Applications. Springer, pages 233\u2013 244.",
            "year": 2006
        },
        {
            "authors": [
                "Vivi Nastase",
                "Stan Szpakowicz."
            ],
            "title": "Exploring noun-modifier semantic relations",
            "venue": "Fifth international workshop on computational semantics (IWCS-5). pages 285\u2013301.",
            "year": 2003
        },
        {
            "authors": [
                "Paul Nulty",
                "Fintan Costello."
            ],
            "title": "General and specific paraphrases of semantic relations between nouns",
            "venue": "Natural Language Engineering 19(03):357\u2013 384.",
            "year": 2013
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Richard Socher",
                "Christopher Manning."
            ],
            "title": "Glove: Global vectors for word representation",
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computa-",
            "year": 2014
        },
        {
            "authors": [
                "Siva Reddy",
                "Diana McCarthy",
                "Suresh Manandhar."
            ],
            "title": "An empirical study on compositionality in compound nouns",
            "venue": "Proceedings of 5th International Joint Conference on Natural Language Processing. Asian Federation of Natural Language",
            "year": 2011
        },
        {
            "authors": [
                "Diarmuid O S\u00e9aghdha",
                "Ann Copestake."
            ],
            "title": "Interpreting compound nouns with kernel methods",
            "venue": "Natural Language Engineering 19(3):331\u2013356.",
            "year": 2013
        },
        {
            "authors": [
                "Vered Shwartz",
                "Yoav Goldberg",
                "Ido Dagan."
            ],
            "title": "Improving hypernymy detection with an integrated path-based and distributional method",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2016
        },
        {
            "authors": [
                "Richard Socher",
                "Brody Huval",
                "D. Christopher Manning",
                "Y. Andrew Ng."
            ],
            "title": "Semantic compositionality through recursive matrix-vector spaces",
            "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and",
            "year": 2012
        },
        {
            "authors": [
                "Karen Sp\u00e4rck Jones."
            ],
            "title": "Compound noun interpretation problems",
            "venue": "Technical report, University of Cambridge, Computer Laboratory.",
            "year": 1983
        },
        {
            "authors": [
                "Nitesh Surtani",
                "Soma Paul."
            ],
            "title": "A vsm-based statistical model for the semantic relation interpretation of noun-modifier pairs",
            "venue": "RANLP. pages 636\u2013645.",
            "year": 2015
        },
        {
            "authors": [
                "Stephen Tratz."
            ],
            "title": "Semantically-enriched parsing for natural language understanding",
            "venue": "University of Southern California.",
            "year": 2011
        },
        {
            "authors": [
                "Stephen Tratz",
                "Eduard Hovy."
            ],
            "title": "A taxonomy, dataset, and classifier for automatic noun compound interpretation",
            "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computa-",
            "year": 2010
        },
        {
            "authors": [
                "Tim Van de Cruys",
                "Stergos Afantenos",
                "Philippe Muller."
            ],
            "title": "Melodi: A supervised distributional approach for free paraphrasing of noun compounds",
            "venue": "Second Joint Conference on Lexical and Computational Semantics (*SEM), Vol-",
            "year": 2013
        },
        {
            "authors": [
                "Fabio Massimo Zanzotto",
                "Ioannis Korkontzelos",
                "Francesca Fallucchi",
                "Suresh Manandhar."
            ],
            "title": "Estimating linear models for compositional distributional semantics",
            "venue": "Proceedings of the 23rd International Conference on Computational Linguistics.",
            "year": 2010
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :1\n80 3.\n08 07\n3v 1\n[ cs\n.C L\n] 2\n1 M\nar 2\n01 8\nbetween the constituents of a noun compound, e.g. olive oil (source) and baby oil (purpose) is an important task for many NLP applications. Recent approaches are typically based on either noun-compound representations or paraphrases. While the former has initially shown promising results, recent work suggests that the success stems from memorizing single prototypical words for each relation. We explore a neural paraphrasing approach that demonstrates superior performance when such memorization is not possible."
        },
        {
            "heading": "1 Introduction",
            "text": "Automatic classification of a noun-compound (NC) to the implicit semantic relation that holds between its constituent words is beneficial for applications that require text understanding. For instance, a personal assistant asked \u201cdo I have a morning meeting tomorrow?\u201d should search the calendar for meetings occurring in the morning, while for group meeting it should look for meetings with specific participants. The NC classification task is a challenging one, as the meaning of an NC is often not easily derivable from the meaning of its constituent words (Spa\u0308rck Jones, 1983).\nPrevious work on the task falls into two main approaches. The first maps NCs to paraphrases that express the relation between the constituent words (e.g. Nakov and Hearst, 2006; Nulty and Costello, 2013), such as mapping coffee cup and garbage dump to the pattern [w1] CONTAINS [w2]. The second approach computes a representation for NCs from the distributional representation of their individual constituents. While this approach\n\u2217Work done during an internship at Google.\nyielded promising results, recently, Dima (2016) showed that similar performance is achieved by representing the NC as a concatenation of its constituent embeddings, and attributed it to the lexical memorization phenomenon (Levy et al., 2015).\nIn this paper we apply lessons learned from the parallel task of semantic relation classification. We adapt HypeNET (Shwartz et al., 2016) to the NC classification task, using their path embeddings to represent paraphrases and combining with distributional information. We experiment with various evaluation settings, including settings that make lexical memorization impossible. In these settings, the integrated method performs better than the baselines. Even so, the performance is mediocre for all methods, suggesting that the task is difficult and warrants further investigation.1"
        },
        {
            "heading": "2 Background",
            "text": "Various tasks have been suggested to address noun-compound interpretation. NC paraphrasing extracts texts explicitly describing the implicit relation between the constituents, for example student protest is a protest LED BY, BE SPONSORED BY, or BE ORGANIZED BY students (e.g. Nakov and Hearst, 2006; Kim and Nakov, 2011; Hendrickx et al., 2013; Nulty and Costello, 2013). Compositionality prediction determines to what extent the meaning of the NC can be expressed in terms of the meaning of its constituents, e.g. spelling bee is non-compositional, as it is not related to bee (e.g. Reddy et al., 2011). In this paper we focus on the NC classification task, which is defined as follows: given a pre-defined set of relations, classify nc = w1w2 to the relation that holds between w1 and w2. We review the various\n1The code is available at https://github.com/tensorflow/ models/tree/master/research/lexnet nc.\nfeatures used in the literature for classification.2"
        },
        {
            "heading": "2.1 Compositional Representations",
            "text": "In this approach, classification is based on a vector representing the NC (w1 w2), which is obtained by applying a function to its constituents\u2019 distributional representations: ~vw1 , ~vw2 \u2208 R n. Various functions have been proposed in the literature.\nMitchell and Lapata (2010) proposed 3 simple combinations of ~vw1 and ~vw2 (additive, multiplicative, dilation). Others suggested to represent compositions by applying linear functions, encoded as matrices, over word vectors. Baroni and Zamparelli (2010) focused on adjective-noun compositions (AN) and represented adjectives as matrices, nouns as vectors, and ANs as their multiplication. Matrices were learned with the objective of minimizing the distance between the learned vector and the observed vector (computed from corpus occurrences) of each AN. The full-additive model (Zanzotto et al., 2010; Dinu et al., 2013) is a similar approach that works on any two-word composition, multiplying each word by a square matrix: nc = A \u00b7 ~vw1 +B \u00b7 ~vw2 .\nSocher et al. (2012) suggested a non-linear composition model. A recursive neural network operates bottom-up on the output of a constituency parser to represent variable-length phrases. Each constituent is represented by a vector that captures its meaning and a matrix that captures how it modifies the meaning of constituents that it combines with. For a binary NC, nc = g(W \u00b7 [~vw1 ;~vw2 ]), where W \u2208 R2n\u00d7n and g is a non-linear function.\nThese representations were used as features in NC classification, often achieving promising results (e.g. Van de Cruys et al., 2013; Dima and Hinrichs, 2015). However, Dima (2016) recently showed that similar performance is achieved by representing the NC as a concatenation of its constituent embeddings, and argued that it stems from memorizing prototypical words for each relation. For example, classifying any NC with the head oil to the SOURCE relation, regardless of the modifier."
        },
        {
            "heading": "2.2 Paraphrasing",
            "text": "In this approach, the paraphrases of an NC, i.e. the patterns connecting the joint occurrences of the constituents in a corpus, are treated as features. For example, both paper cup and steel knife\n2Leaving out features derived from lexical resources (e.g. Nastase and Szpakowicz, 2003; Tratz and Hovy, 2010).\nmay share the feature MADE OF. Se\u0301aghdha and Copestake (2013) leveraged this \u201crelational similarity\u201d in a kernel-based classification approach. They combined the relational information with the complementary lexical features of each constituent separately. Two NCs labeled to the same relation may consist of similar constituents (paper-steel, cup-knife) and may also appear with similar paraphrases. Combining the two information sources has shown to be beneficial, but it was also noted that the relational information suffered from data sparsity: many NCs had very few paraphrases, and paraphrase similarity was based on ngram overlap.\nRecently, Surtani and Paul (2015) suggested to represent NCs in a vector space model (VSM) using paraphrases as features. These vectors were used to classify new NCs based on the nearest neighbor in the VSM. However, the model was only tested on a small dataset and performed similarly to previous methods."
        },
        {
            "heading": "3 Model",
            "text": "We similarly investigate the use of paraphrasing for NC relation classification. To generate a signal for the joint occurrences of w1 and w2, we follow the approach used by HypeNET (Shwartz et al., 2016). For an w1w2 in the dataset, we collect all the dependency paths that connect w1 and w2 in the corpus, and learn path embeddings as detailed in Section 3.2. Section 3.1 describes the classification models with which we experimented."
        },
        {
            "heading": "3.1 Classification Models",
            "text": "Figure 1 provides an overview of the models: path-based, integrated, and integrated-NC, each which incrementally adds new features not present in the previous model. In the following sections, ~x denotes the input vector representing the NC. The network classifies NC to the highest scoring relation: r = argmaxi softmax(~o)i, where ~o is the output layer. All networks contain a single hidden layer whose dimension is |x| 2 . k is the number of relations in the dataset. See Appendix A for additional technical details.\nPath-based. Classifies the NC based only on the paths connecting the joint occurrences of w1 and w2 in the corpus, denoted P (w1, w2). We define the feature vector as the average of its path embeddings, where the path embedding ~p of a path p is\nweighted by its frequency fp,(w1,w2):\n~x = ~vP (w1,w2) =\n\u2211 p\u2208P (w1,w2)\nfp,(w1,w2) \u00b7 ~p\u2211 p\u2208P (w1,w2) fp,(w1,w2)\nIntegrated. We concatenate w1 and w2\u2019s word embeddings to the path vector, to add distributional information: x = [~vw1 , ~vw2 , ~vP (w1,w2)]. Potentially, this allows the network to utilize the contextual properties of each individual constituent, e.g. assigning high probability to SUBSTANCE-MATERIAL-INGREDIENT for edible w1s (e.g. vanilla pudding, apple cake). Integrated-NC. We add the NC\u2019s observed vector ~vnc as additional distributional input, providing the contexts in which w1w2 occur as an NC: ~vnc = [~vw1 , ~vw2 , ~vnc, ~vP (w1,w2)]. Like Dima (2016), we learn NC vectors using the GloVe algorithm (Pennington et al., 2014), by replacing each NC occurrence in the corpus with a single token.\nThis information can potentially help clustering NCs that appear in similar contexts despite having low pairwise similarity scores between their constituents. For example, gun violence and abortion rights belong to the TOPIC relation and may appear in similar news-related contexts, while (gun, abortion) and (violence, rights) are dissimilar."
        },
        {
            "heading": "3.2 Path Embeddings",
            "text": "Following HypeNET, for a path p composed of edges e1, ..., ek , we represent each edge by the concatenation of its lemma, part-of-speech tag, dependency label and direction vectors: ~ve = [~vl, ~vpos, ~vdep, ~vdir]. The edge vectors ~ve1 , ..., ~vek are encoded using an LSTM (Hochreiter and Schmidhuber, 1997), and the last output vector ~p is used as the path embedding.\nWe use the NC labels as distant supervision. While HypeNET predicts a word pair\u2019s label from the frequency-weighted average of the path vectors, we differ from it slightly and compute the label from the frequency-weighted average of the predictions obtained from each path separately:\n~o =\n\u2211 p\u2208P (w1,w2)\nfp,(w1,w2) \u00b7 softmax(~p)\u2211 p\u2208P (w1,w2) fp,(w1,w2)\nr = argmaxi ~oi\nWe conjecture that label distribution averaging allows for more efficient training of path embeddings when a single NC contains multiple paths."
        },
        {
            "heading": "4 Evaluation",
            "text": ""
        },
        {
            "heading": "4.1 Dataset",
            "text": "We follow Dima (2016) and evaluate on the Tratz (2011) dataset, with 19,158 instances and two levels of labels: fine-grained (Tratz-fine, 37 relations) and coarse-grained (Tratz-coarse, 12 relations). We report results on both versions. See Tratz (2011) for the list of relations.\nDataset Splits Dima (2016) showed that a classifier based only on vw1 and vw2 performs on par with compound representations, and that the success comes from lexical memorization (Levy et al., 2015): memorizing the majority label of single words in particular slots of the compound (e.g. TOPIC for travel guide, fishing guide, etc.). This memorization paints a skewed picture of the stateof-the-art performance on this difficult task.\nTo better test this hypothesis, we evaluate on 4 different splits of the datasets to train, test, and validation sets: (1) random, in a 75:20:5 ratio, (2)\nlexical-full, in which the train, test, and validation sets each consists of a distinct vocabulary. The split was suggested by Levy et al. (2015), and it randomly assigns words to distinct sets, such that for example, including travel guide in the train set promises that fishing guide would not be included in the test set, and the models do not benefit from memorizing that the head guide is always annotated as TOPIC. Given that the split discards many NCs, we experimented with two additional splits: (3) lexical-mod split, in which the w1 words are unique in each set, and (4) lexical-head split, in which the w2 words are unique in each set. Table 2 displays the sizes of each split."
        },
        {
            "heading": "4.2 Baselines",
            "text": "Frequency Baselines. mod freq classifies w1w2 to the most common relation in the train set for NCs with the same modifier (w1w \u2032 2), while head freq considers NCs with the same head (w\u20321w2). 5\nDistributional Baselines. Ablation of the pathbased component from our models: Dist uses only w1 and w2\u2019s word embeddings: ~x = [~vw1 , ~vw2 ], while Dist-NC includes also the NC embedding: ~x = [~vw1 , ~vw2 , ~vnc]. The network architecture is defined similarly to our models (Section 3.1).\nCompositional Baselines. We re-train Dima\u2019s (2016) models, various combinations of NC rep-\n4In practice, in lexical-full this is a random baseline, in lexical-head it is the modifier frequency baseline, and in lexical-mod it is the head frequency baseline.\n5Unseen heads/modifiers are assigned a random relation.\nresentations (Zanzotto et al., 2010; Socher et al., 2012) and single word embeddings in a fully connected network.6"
        },
        {
            "heading": "4.3 Results",
            "text": "Table 1 shows the performance of various methods on the datasets. Dima\u2019s (2016) compositional models perform best among the baselines, and on the random split, better than all the methods. On the lexical splits, however, the baselines exhibit a dramatic drop in performance, and are outperformed by our methods. The gap is larger in the lexical-full split. Finally, there is usually no gain from the added NC vector in Dist-NC and Integrated-NC."
        },
        {
            "heading": "5 Analysis",
            "text": "Path Embeddings. To focus on the changes from previous work, we analyze the performance of the path-based model on the Tratz-fine random split. This dataset contains 37 relations and the model performance varies across them. Some relations, such as MEASURE and PERSONAL TITLE yield reasonable performance (F1 score of 0.87 and 0.68). Table 3 focuses on these relations and illustrates the indicative paths that the model has learned for each relation. We compute these by performing the analysis in Shwartz et al. (2016), where each path is fed into the pathbased model, and is assigned to its best-scoring relation. For each relation, we consider paths with a score \u2265 0.8.\nOther relations achieve very low F1 scores, indicating that the model is unable to learn them at all. Interestingly, the four relations with the lowest performance in our model7 are also those\n6We only include the compositional models, and omit the \u201cbasic\u201d setting which is similar to our Dist model. For the full details of the compositional models, see Dima (2016).\n7 LEXICALIZED, TOPIC OF COGNITION&EMOTION,\nWHOLE+ATTRIBUTE&FEAT, PARTIAL ATTR TRANSFER\nwith the highest error rate in Dima (2016), very likely since they express complex relations. For example, the LEXICALIZED relation contains noncompositional NCs (soap opera) or lexical items whose meanings departed from the combination of the constituent meanings. It is expected that there are no paths that indicate lexicalization. In PARTIAL ATTRIBUTE TRANSFER (bullet train), w1 transfers an attribute to w2 (e.g. bullet transfers speed to train). These relations are not expected to be expressed in text, unless the text aims to explain them (e.g. train as fast as a bullet).\nLooking closer at the model confusions shows that it often defaulted to general relations like OBJECTIVE (recovery plan) or RELATIONAL-NOUNCOMPLEMENT (eye shape). The latter is described as \u201cindicating the complement of a relational noun (e.g., son of, price of)\u201d, and the indicative paths for this relation indeed contain many variants of \u201c[w2] of [w1]\u201d, which potentially can occur with NCs in other relations. The model also confused between relations with subtle differences, such as the different topic relations. Given that these relations were conflated to a single relation in the inter-annotator agreement computation in Tratz and Hovy (2010), we can conjecture that even humans find it difficult to distinguish between them.\nNC Embeddings. To understand why the NC embeddings did not contribute to the classification, we looked into the embeddings of the\nTratz-fine test NCs; 3091/3831 (81%) of them had embeddings. For each NC, we looked for the 10 most similar NC vectors (in terms of cosine similarity), and compared their labels. We have found that only 27.61% of the NCs were mostly similar to NCs with the same label. The problem seems to be inconsistency of annotations rather than low embeddings quality. Table 4 displays some examples of NCs from the test set, along with their most similar NC in the embeddings, where the two NCs have different labels."
        },
        {
            "heading": "6 Conclusion",
            "text": "We used an existing neural dependency path representation to represent noun-compound paraphrases, and along with distributional information applied it to the NC classification task. Following previous work, that suggested that distributional methods succeed due to lexical memorization, we show that when lexical memorization is not possible, the performance of all methods is much worse. Adding the path-based component helps mitigate this issue and increase performance."
        },
        {
            "heading": "Acknowledgments",
            "text": "We would like to thank Marius Pasca, Susanne Riehemann, Colin Evans, Octavian Ganea, and Xiang Li for the fruitful conversations, and Corina Dima for her help in running the compositional baselines."
        },
        {
            "heading": "A Technical Details",
            "text": "To extract paths, we use a concatenation of English Wikipedia and the Gigaword corpus.8 We consider sentences with up to 32 words and dependency paths with up to 8 edges, including satellites, and keep only 1,000 paths for each nouncompound. We compute the path embeddings in advance for all the paths connecting NCs in the dataset (\u00a73.2), and then treat them as fixed embeddings during classification (\u00a73.1).\nWe use TensorFlow (Abadi et al., 2016) to train the models, fixing the values of the hyperparameters after performing preliminary experiments on the validation set. We set the mini-batch size to 10, use Adam optimizer (Kingma and Ba, 2014) with the default learning rate, and apply word dropout with probability 0.1. We train up to 30 epochs with early stopping, stopping the training when the F1 score on the validation set drops 8 points below the best performing score.\nWe initialize the distributional embeddings with the 300-dimensional pre-trained GloVe embeddings (Pennington et al., 2014) and the lemma embeddings (for the path-based component) with the 50-dimensional ones. Unlike HypeNET, we do not update the embeddings during training. The lemma, POS, and direction embeddings are initialized randomly and updated during training. NC embeddings are learned using a concatenation of Wikipedia and Gigaword. Similarly to the original GloVe implementation, we only keep the most frequent 400,000 vocabulary terms, which means that roughly 20% of the noun-compounds do not have vectors and are initialized randomly in the model.\n8 https://catalog.ldc.upenn.edu/ldc2003t05"
        }
    ],
    "title": "Olive Oil is Made of Olives, Baby Oil is Made for Babies: Interpreting Noun Compounds using Paraphrases in a Neural Model",
    "year": 2018
}