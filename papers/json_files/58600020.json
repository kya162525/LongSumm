{
    "abstractText": "We propose a method to encode rotation equivariance or invariance into convolutional neural networks (CNNs). Each convolutional filter is applied with several orientations and returns a vector field that represents the magnitude and angle of the highest scoring rotation at the given spatial location. To propagate information about the main orientation of the different features to each layer in the network, we propose an enriched orientation pooling, i.e. max and argmax operators over the orientation space, allowing to keep the dimensionality of the feature maps low and to propagate only useful information. We name this approach RotEqNet. We apply RotEqNet to three datasets: first, a rotation invariant classification problem, the MNIST-rot benchmark, in which we improve over the state-of-the-art results. Then, a neuron membrane segmentation benchmark, where we show that RotEqNet can be applied successfully to obtain equivariance to rotation with a simple fully convolutional architecture. Finally, we improve significantly the state-of-the-art on the problem of estimating cars\u2019 absolute orientation in aerial images, a problem where the output is required to be covariant with respect to the object\u2019s orientation.",
    "authors": [
        {
            "affiliations": [],
            "name": "Diego Marcos"
        },
        {
            "affiliations": [],
            "name": "Michele Volpi"
        },
        {
            "affiliations": [],
            "name": "Nikos Komodakis"
        },
        {
            "affiliations": [],
            "name": "Devis Tuia"
        }
    ],
    "id": "SP:5c0c12910627bafbe46e47cbd40f9c2bdfcd5c49",
    "references": [
        {
            "authors": [
                "I. Arganda-Carreras",
                "S.C. Turaga",
                "D.R. Berger",
                "D. Cire\u015fan",
                "A. Giusti",
                "L.M. Gambardella",
                "J. Schmidhuber",
                "D. Laptev",
                "S. Dwivedi",
                "J.M. Buhmann"
            ],
            "title": "Crowdsourcing the creation of image segmentation algorithms for connectomics",
            "venue": "Frontiers in Neuroanatomy,",
            "year": 2015
        },
        {
            "authors": [
                "T. Beier",
                "B. Andres",
                "U. K\u00f6the",
                "F.A. Hamprecht"
            ],
            "title": "An efficient fusion move algorithm for the minimum cost lifted multicut problem",
            "venue": "Proceeding of the European Conf. on Computer Vision (ECCV), pages 715\u2013730. Springer,",
            "year": 2016
        },
        {
            "authors": [
                "A. Cardona",
                "S. Saalfeld",
                "S. Preibisch",
                "B. Schmid",
                "A. Cheng",
                "J. Pulokas",
                "P. Tomancak",
                "V. Hartenstein"
            ],
            "title": "An integrated micro-and macroarchitectural analysis of the drosophila brain by computer-assisted serial section electron microscopy",
            "venue": "PLoS Biol, 8(10):e1000502,",
            "year": 2010
        },
        {
            "authors": [
                "H. Chen",
                "X.J. Qi",
                "J.Z. Cheng",
                "P.A. Heng"
            ],
            "title": "Deep contextual networks for neuronal structure segmentation",
            "venue": "Proceeding of the Conf. on Artificial Intelligence (AAAI),",
            "year": 2016
        },
        {
            "authors": [
                "G. Cheng",
                "P. Zhou",
                "J. Han"
            ],
            "title": "RIFD-CNN: Rotation- Invariant and Fisher Discriminative convolutional neural networks for object detection",
            "venue": "Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 2884\u20132893,",
            "year": 2016
        },
        {
            "authors": [
                "T.S. Cohen",
                "M. Welling"
            ],
            "title": "Group equivariant convolutional networks",
            "venue": "arXiv preprint arXiv:1602.07576,",
            "year": 2016
        },
        {
            "authors": [
                "M. Drozdzal",
                "E. Vorontsov",
                "G. Chartrand",
                "S. Kadoury",
                "C. Pal"
            ],
            "title": "The importance of skip connections in biomedical image segmentation",
            "venue": "Proceeding of the Deep Learning and Data Labeling for Medical Applications Workshop (DLMIA), pages 179\u2013187. Springer,",
            "year": 2016
        },
        {
            "authors": [
                "A. Fakhry",
                "H. Peng",
                "S. Ji"
            ],
            "title": "Deep models for brain em image segmentation: novel insights and improved performance",
            "venue": "Bioinformatics, page btw165,",
            "year": 2016
        },
        {
            "authors": [
                "R. Gens",
                "P.M. Domingos"
            ],
            "title": "Deep symmetry networks",
            "venue": "Advances in neural information processing systems, pages 2537\u20132545,",
            "year": 2014
        },
        {
            "authors": [
                "J.F. Henriques",
                "A. Vedaldi"
            ],
            "title": "Warped convolutions: Efficient invariance to spatial transformations",
            "venue": "arXiv preprint arXiv:1609.04382,",
            "year": 2016
        },
        {
            "authors": [
                "G. Hu",
                "Y. Yang",
                "D. Yi",
                "J. Kittler",
                "W. Christmas",
                "S.Z. Li",
                "T. Hospedales"
            ],
            "title": "When face recognition meets with deep learning: an evaluation of convolutional neural networks for face recognition",
            "venue": "Proceedings of the IEEE Conf. on Computer Vision Workshops (CVPRW), pages 142\u2013150,",
            "year": 2015
        },
        {
            "authors": [
                "S. Ioffe",
                "C. Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "arXiv preprint arXiv:1502.03167,",
            "year": 2015
        },
        {
            "authors": [
                "M. Jaderberg",
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Spatial transformer networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2015
        },
        {
            "authors": [
                "J.J. Kivinen",
                "C.K. Williams"
            ],
            "title": "Transformation equivariant boltzmann machines",
            "venue": "Proceedings of the Intl. Conf. on Artificial Neural Networks (ICANN), pages 1\u20139. Springer,",
            "year": 2011
        },
        {
            "authors": [
                "I. Kokkinos"
            ],
            "title": "Surpassing humans in boundary detection using deep learning",
            "venue": "arXiv preprint arXiv:1511.07386,",
            "year": 2015
        },
        {
            "authors": [
                "D. Laptev",
                "N. Savinov",
                "J.M. Buhmann",
                "M. Pollefeys"
            ],
            "title": "TI-pooling: transformation-invariant pooling for feature learning in convolutional neural networks",
            "venue": "arXiv preprint arXiv:1604.06318,",
            "year": 2016
        },
        {
            "authors": [
                "H. Larochelle",
                "D. Erhan",
                "A. Courville",
                "J. Bergstra",
                "Y. Bengio"
            ],
            "title": "An empirical evaluation of deep architectures on problems with many factors of variation",
            "venue": "Proceedings of the Intl. Conf. on Machine Learning (ICML), pages 473\u2013480. ACM,",
            "year": 2007
        },
        {
            "authors": [
                "K. Lenc",
                "A. Vedaldi"
            ],
            "title": "Understanding image representations by measuring their equivariance and equivalence",
            "venue": "Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 991\u2013999,",
            "year": 2015
        },
        {
            "authors": [
                "J. Long",
                "E. Shelhamer",
                "T. Darrell"
            ],
            "title": "Fully convolutional networks for semantic segmentation",
            "venue": "Proceeding of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2015
        },
        {
            "authors": [
                "D. Marcos",
                "M. Volpi",
                "D. Tuia"
            ],
            "title": "Learning rotation invariant convolutional filters for texture classification",
            "venue": "arXiv preprint arXiv:1604.06720,",
            "year": 2016
        },
        {
            "authors": [
                "J. Ngiam",
                "Z. Chen",
                "D. Chia",
                "P.W. Koh",
                "Q.V. Le",
                "A.Y. Ng"
            ],
            "title": "Tiled convolutional neural networks",
            "venue": "Advances in Neural Information Processing Systems, pages 1279\u2013 1287,",
            "year": 2010
        },
        {
            "authors": [
                "H. Noh",
                "S. Hong",
                "B. Han"
            ],
            "title": "Learning deconvolution network for semantic segmentation",
            "venue": "Proceeding of the IEEE Conf. Computer Vision and Pattern Recognition (CVPR),",
            "year": 2015
        },
        {
            "authors": [
                "O. Ronneberger",
                "P. Fischer",
                "T. Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "Proceedings of the Intl. Conf. on Medical Image Computing and Computer-Assisted Intervention (MICCAI), pages 234\u2013241. Springer,",
            "year": 2015
        },
        {
            "authors": [
                "L. Sifre",
                "S. Mallat"
            ],
            "title": "Rotation, scaling and deformation invariant scattering for texture discrimination",
            "venue": "Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 1233\u20131240,",
            "year": 2013
        },
        {
            "authors": [
                "P.Y. Simard",
                "D. Steinkraus",
                "J.C. Platt"
            ],
            "title": "Best practices for convolutional neural networks applied to visual document analysis",
            "venue": "Proceedings of the Intl. Conf. on Document Analysis and Recognition (ICDAR), volume 3, pages 958\u2013962,",
            "year": 2003
        },
        {
            "authors": [
                "K. Sohn",
                "H. Lee"
            ],
            "title": "Learning invariant representations with local transformations",
            "venue": "arXiv preprint arXiv:1206.6418,",
            "year": 2012
        },
        {
            "authors": [
                "N. Srivastava",
                "G.E. Hinton",
                "A. Krizhevsky",
                "I. Sutskever",
                "R. Salakhutdinov"
            ],
            "title": "Dropout: a simple way to prevent neural networks from overfitting",
            "venue": "Journal of Machine Learning Research, 15(1):1929\u20131958,",
            "year": 2014
        },
        {
            "authors": [
                "A. Vedaldi",
                "K. Lenc"
            ],
            "title": "Matconvnet \u2013 convolutional neural networks for matlab",
            "venue": "Proceeding of the ACM Intl. Conf. on Multimedia,",
            "year": 2015
        },
        {
            "authors": [
                "X. Wang",
                "D.F. Fouhey",
                "A. Gupta"
            ],
            "title": "Designing deep networks for surface normal estimation",
            "venue": "Proceeding of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2015
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In many Computer Vision problems, such as overhead (aerial or satellite) or biomedical image analysis, there are no dominant up-down or left-right relationships. For example, if the task is to detect cars in aerial images, the absolute orientation of a car is not a discriminant feature. Only the relative orientation to surrounding elements such as roads and buildings can establish a prior on the car\npresence likelihood. If the absolute orientation of the image image is changed, e.g. by following a different flightpath, we would expect the car detector to score the same values over all the cars, independently from their new orientations in the rotated image. In this case, we say that the problem is rotation equivariant: rotating the input is expected to result in the same output, but rotated. On the other hand, if we were confronted to a classification setting, in which we are only interested in the presence or absence of cars in the scene, the car flag (i.e whether any car is present or not) and the corresponding score should remain constant, no matter the absolute orientation of the input scene. In this case the problem is rotation invariant. The more general case would be rotation covariance, in which the output changes as a function of the rotation of the input with some predefined behavior. Taking again the same example, a rotation covariant problem would be to retrieve the absolute orientation of cars with respect to geographical coordinates: in this case, a rotation of the image should produce a corresponding change of the angle predicted for each car.\nar X\niv :1\n61 2.\n09 34\n6v 1\n[ cs\n.C V\n] 2\n9 D\nec 2\nThroughout this article we will make use of the terms equivariance, invariance and covariance of a function f(\u00b7) with respect to a transformation g(\u00b7) in the following sense:\n- equivariance: f(g(\u00b7)) = g(f(\u00b7)), - invariance: f(g(\u00b7)) = f(\u00b7), - covariance: f(g(\u00b7)) = g\u2032(f(\u00b7)),\nwhere g\u2032(\u00b7) is another transformation that is itself function of g(\u00b7). With the above definitions, equivariance and invariance are special cases of covariance. We illustrate these properties in Fig. 1.\nIn this paper, we propose a network that naturally encodes these three properties with respect to rotations. In the following we will recall how they are achieved for translations, before discussing our own proposition."
        },
        {
            "heading": "1.1 Dealing with translations in CNNs",
            "text": "The success of Convolutional Neural Networks (CNNs) is partly due to the translation equivariant nature of the convolution operation. To understand why it is more complex to achieve natural equivariance to rotations by means of convolutions, we will briefly summarize how translation equivariance is obtained by standard CNNs.\nTranslation equivariance in CNNs: A convolution of an image x \u2208 RM\u00d7N\u00d7d with a filter w \u2208 Rm\u00d7n\u00d7d, written y = w \u2217 x, is computed by applying the same scalar product operation over all overlapping m \u00d7 n windows (unit stride) on x. If x undergoes an integer translation in the horizontal and vertical directions by (p, q) pixels, the same pixel neighborhoods in x will exist in the translated x, but again translated by (p, q) pixels. Therefore, any neighborhood-based operation such as the convolution is translation equivariant when applied to images or other data structures characterized by local neighborhoods with identical arrangement. The fact that the operation is local and produces a single scalar per neighborhood has another advantageous effect: the output can be effortlessly re-arranged in useful ways. For instance, one can set the spatial structure of the activations to match the one of the input.\nFrom equivariance to invariance or covariance: Translation equivariance or invariance are highly desirable in problems such as image classification, where a\nlocal translation does not change the object class (invariance); and in dense prediction tasks as semantic segmentation and edge extraction, where the output should be subject to the same translation as the input (equivariance). In order to obtain full invariance to small translations, standard CNN architectures apply an additional invariant, non-linear local pooling operator, such as the average or the maximum. This usually comes at the cost of some information loss, typically related to the local distribution of the input values. Such loss is usually controlled by limiting the spatial influence of the pooling operator. If some form of translation covariance is required, some of this information should reach the last layer, for instance via skip connections [22]."
        },
        {
            "heading": "1.2 Incorporating rotation equivariance in CNNs",
            "text": "If we want to account for an additional transformation (as in our case jointly achieving equivariance to 2D translation and rotation), the structure of the layer activations is no longer straightforward. One possibility would be to return a series of values corresponding to the application of rotated versions of the canonical filter w at each location in x. In this case, the activations y would be a 3D tensor where a translation in the 3rd dimension corresponds to a rotation of w. This would correspond to a rotation covariant operator that keeps all the orientation information in the image. The covariance achieved in this way could easily be transformed into equivariance or invariance by means of pooling across orientations and spatial windows in deeper layers.\nWhen processing the 2nd layer in the network, convolutional filters at different orientations must become 3D and be applied as 3D convolutions across the roto-translation space. This must be done for each orientation of the 3D filters, thus yielding a 4D tensor as output. The 3rd layer will return a 5D feature map, the 4th layer a 6D one and so on. This explosion in the dimensionality would result in an enormous increase of the memory requirements, the number of parameters and the computational complexity. Therefore, this na\u0131\u0308ve approach to rotation covariant CNNs would not be practical for CNNs with more than one convolutional layer. Note that this problem does not arise when using standard convolutions achieving translation\nequivariance only, since the output feature maps are always of the same dimensionality as the inputs, except for the dimension accounting for the number of features.\nTo overcome the exponential increase of the number of parameters, but still propagate useful information about the most important orientations, we propose to perform a single-binned max-pooling operation across the newly added orientation dimension. At each location, it takes the largest activation across orientations and the angle at which it occurred. This way, we are able to keep the 2D arrangement of the image throughout the CNN layers, while achieving inherent rotation equivariance and allowing the network to make use of the information about the orientation of feature activations in previous layers. Similar to spatial max-pooling, this propagates only information about the maximal activation, discarding all information about non-maximal activations. Since the atomic element in the hidden feature maps is no longer a scalar as in conventional CNNs but a 2D vector, each map can be treated as a vector field. We therefore name the proposed method Rotation Equivariant Vector Field\nNetworks (RotEqNet)."
        },
        {
            "heading": "2 Related work",
            "text": "Two families of approaches explicitly account for rotation invariance or equivariance: 1) those that transform the inputs (image or feature maps) and 2) those that rotate the filters. The proposed RotEqNet belongs to the latter.\n1) Rotating the inputs: Jaderberg et al. [13] propose the Spatial Transformer layer, which learns how to crop and transform a region of the image (or a feature map) that is passed to the next layer. This operator tends to find a relevant region for the task at hand and then transforms it to some canonical form, improving the learning process by reducing pose variations in subsequent layers. Laptev et al. [16] use several rotated versions of the image as inputs to the same CNN and perform a pooling operation across the different feature vectors at the first fully connected layer. Such scheme allows another subsequent fully connected layer to choose among ro-\ntated inputs to compute the classification. In the work by Cheng et al. [5], several rotated versions of an image are used by the CNN in single minibatch. Their representations after the first fully connected layer are then encouraged to be similar, forcing the CNN to learn rotation invariance. Henriques et al. [10] warp the images such that the translation equivariance inherent to convolutions is transformed into rotation and scale equivariance.\nOn the one hand, these methods have the advantage of using conventional CNN implementations, since they transform only the input images or the feature maps. On the other hand, their main disadvantage is that they can only consider global transformations of the input image. If this is well suited for tasks such as image classification, it limits their applicability to other problems (e.g. semantic segmentation), where relative orientations of some objects with respect to others can help identifying them. It is worth mentioning that this family of approaches also includes standard data augmentation strategies consisting in applying random rotations and flips to training samples [25]: given enough training samples and model capacity, a CNN might learn that different orientations should score the same by simply learning equivalent filters at different orientations [18].\n2) Rotating the filters: Gens and Domingos [9] tackle the problem of the exploding dimensionality (discussed in Sect. 1.2) by applying learnable kernel-based pooling operations and sampling the symmetry space at each layer. This way, they avoid applying the filters exhaustively across the (high dimensional) feature maps by selectively sampling few rotations. By doing so, only the least important information is lost from layer to layer. Cohen et al. [6] use a smaller symmetry group, composed of a flipping and four 90o rotations, and perform pooling within the group (what they call coset pooling). They apply coset pooling only in the deeper layers of the network, since they found that pooling in the early layers discards important information and harms the classification performance. Instead of defining explicitly a symmetry group, Ngiam et al. [21] pool across several untied filters, thus letting the network learn what kind of invariance is useful. Sifre et al. [24] use wavelets that are separable in the rototranslational space instead of learned filters, allowing for more efficient computation. Another approach for avoiding the dimensionality explosion in the space of rotations\nis to limit the network to be shallow: Sohn et al. [26] and Kivinen et al. [14] propose such a scheme with unsupervised Restricted Boltzmann Machines (RBM), while Marcos et al. [20] implement it with supervised CNNs consisting of a single convolutional layer.\nThese works find a compromise between the computational resources required and the amount of orientation information kept throughout the layers, by either keeping the model shallow or accounting for a very small amount of orientations. We propose to avoid such compromise by applying a pooling over a large number of orientations and passing forward both the maximum magnitude and the orientation at which it occurred. This modification allows to build deeper rotation equivariant architectures, where the deeper layers become aware of the dominant orientations in the previous ones with minimal memory requirements, since we avoid the above mentioned explosion of the dimensionality of the feature maps and filters."
        },
        {
            "heading": "3 Rotation equivariant vector field networks",
            "text": "In this work we focus on achieving rotation equivariance by performing convolutions with several rotated instances of the same filter (see Fig. 2), which we call the canonical filter. The canonical filter w is rotated at R different orientations, evenly spaced in a given interval of angles. In the experiments of Sect. 4 we deal with problems requiring either full invariance, equivariance or covariance, so we use the interval \u03b1 = [0o, 360o]. However, this interval could be tightened if an adequate range of tilts is known beforehand.\nThe output of filter w at a specific image location will consist of the magnitude of the maximal activation across the orientations, plus the corresponding angle. If we convert this polar representation into Cartesian coordinates, each filter w produces a vector field feature map z \u2208 RH\u00d7W\u00d72, where the output of each location consists of two values [u, v] \u2208 R2 representing the maximal activation in both magnitude and direction. Since the feature maps have become vector fields, from this moment on the filters applied to them must also be vector fields, as seen in Fig. 3.\nThe advantage of representing z \u2208 RH\u00d7W\u00d72 in Carte-\nsian coordinates is that the horizontal and vertical components [u, v] are orthogonal, and thus a convolution of two vector fields can be computed using standard convolutions separately in each component:\n(z \u2217w) = (zu \u2217wu) + (zv \u2217wv), (1)\nwhere subscripts u and v denote horizontal and vertical components."
        },
        {
            "heading": "3.1 RotEqNet building blocks",
            "text": "RotEqNet requires different CNN building blocks to be able to handle vectors fields as inputs or outputs (Fig. 2 and Fig. 3). In the following, we present our reformulation of traditional CNN blocks to account for both vector field activations and filters. The implementation is based on the MatConvNet [28] toolbox."
        },
        {
            "heading": "3.1.1 Rotating convolution and its transpose",
            "text": "Rotating convolution (RotConv): Given an input image withm/2 zero-padding x \u2208 RH+m/2\u00d7W+m/2\u00d7d, we apply the filter w \u2208 Rm\u00d7m\u00d7d at R different orientations, corresponding to the rotation angles:\n\u03b1r = 360\nR r \u2200r = 1, 2 . . . R. (2)\nEach of these rotated versions of the filter (Fig. 2b and Fig. 3b) is computed by resampling w with bicubic interpolation after rotation of \u03b1r degrees around the filter center:\nwr = rotate(w, \u03b1r). (3)\nNote that interpolation when resampling the filters or the image is always required unless only rotations of multiples of 90o are considered. In practice, this means that the rotation equivariance will only be approximate (a fact that we will use to our advantage in the experiments, see Sect. 4) .\nThe position [i\u2032, j\u2032] after rotation of a specific filter weight, originally located at [i, j] in the canonical form, is\n[i\u2032, j\u2032] = [i, j] [ cos(\u03b1r) sin(\u03b1r) \u2212 sin(\u03b1r) cos(\u03b1r) ] . (4)\nCoordinates are relative to the center of the filter. Since the rotation can force weights near the corners of the filter to be relocated outside of its spatial support, only the\nweights within a circle of diameter m pixels are used to compute the convolutions. The output tensor y \u2208 RH\u00d7W\u00d7R (Fig. 2c and Fig. 3c) consists ofR feature maps computed as\ny(r) = (x \u2217wr) \u2200r = 1, 2 . . . R, (5)\nwhere (\u2217) is a standard convolution operator such that (x \u2217w)[i, j] = \u2211 m \u2211 n x[i\u2212m, j \u2212 n]w[m,n], (6)\nwhere [m,n] is the neighborhood considered by the filter. The tensor y encodes the roto-translation output space such that rotation in the input corresponds to a translation across the feature maps. Only the canonical, non rotated, version of w is actually stored in the model. During backpropagation, gradients corresponding to each rotated filter, \u2207wr, are aligned back to the canonical form and added together:\n\u2207w = \u2211 r rotate(\u2207wr,\u2212\u03b1r). (7)\nNote that this block can be applied on conventional color images and feature maps (Fig. 2a) or on vector field feature maps (Fig. 2b) by applying it independently in each component and summing the resulting 3D tensors (see Eq. (1)).\nRotating convolution transpose (RotConv>): When tackling problems such as semantic segmentation, boundary detection or normal estimation, it has been shown that dense prediction strategies returning outputs of the same size as the inputs offer the best results [15, 19, 22, 29]. The convolution transpose operator, the adjoint of the convolution operator, can be used to learn interpolation filters to upsample activations [19, 22].\nAs with the convolution, the rotating convolution transpose can also be implemented using a standard convolution transpose module:\nx = \u2211 r (y(r) \u2217wr)>. (8)\nwhere (\u2217)> is the convolution transpose operator. Note that the output can also be a vector field, just by defining w as a vector field filter."
        },
        {
            "heading": "3.1.2 Orientation pooling and its un-pooling",
            "text": "Orientation pooling (OP): Given the 3D tensor y, the role of the orientation pooling is to convert it to a 2D vector field z \u2208 RH\u00d7W\u00d72 (Fig. 2d and Fig. 3d). It avoids the exploding dimensionality problem, while keeping information about the maximally activating orientation of w. First, we extract a 2D map of the largest activations \u03c1 \u2208 RH\u00d7W and their corresponding orientations \u03b8 \u2208 RH\u00d7W . Specifically, for activations located at [i, j]:\n\u03c1[i, j] = max r y[i, j, r], (9) \u03b8[i, j] = 360\nR arg maxr y[i, j, r]. (10)\nThis can be treated as a polar representation of a 2D vector field as long as \u03c1[i, j] \u2265 0 \u2200i, j, condition that is met when using any function on y that returns non-negative values prior to the OP. We employ the common Rectified Linear Unit (ReLu) operation, defined as ReLu(x) = max(x, 0), to \u03c1, as it provides non-saturating, sparse nonlinear activations offering stable training. This representation can then be transformed into Cartesian coordinates as:\nu = ReLu(\u03c1) cos(\u03b8) (11) v = ReLu(\u03c1) sin(\u03b8) (12)\nwith u,v \u2208 RH\u00d7W . The 2D vector field z is then built as:\nz = [ 1 0 ] u + [ 0 1 ] v (13)\nOrientation un-pooling: The adjoint of the orientation pooling is the orientation un-pooling. It takes a vector field z and outputs a stack of scalar maps y, where each individual map y(r) is such that:\ny(r) = \u03c1 \u00b7 [ \u03b1r \u2212 360\n2R < \u03b8 \u2265 \u03b1r +\n360\n2R\n] , r \u2208 1 . . . R.\n(14) The resulting y will contain mostly zeros and its values will be such that the result of applying an orientation pooling operator to y will result in z. The orientation un-pooling block is required for the backpropagation of gradients through an OP block. It can also be used for rotation equivariant up-sampling after a RotConv> layer, to\nconvert vector field z into roto-translation y feature maps. An alternative up-sampling is to use a RotConv operator directly on z."
        },
        {
            "heading": "3.1.3 Spatial pooling (SP) for vector fields",
            "text": "The max-pooling operator is commonly used in CNNs for obtaining some invariance to small deformations and reducing the size of the feature maps. This is done by taking the input feature map x \u2208 RM\u00d7N\u00d7d and downsampling it to xp \u2208 R M p \u00d7 N p \u00d7d. This operation is performed by taking the maximum value contained in each one of the C non-overlapping p \u00d7 p regions of x, indexed by c. It is computed as xp[c] = maxi\u2208c x[i], which can be generalized as:\nyp[c] = y[j], where j = arg maxi\u2208c y[i]. (15)\nThis allows us to define vector field max-pooling as:\nzp[c] = z[j], where j = arg maxi\u2208c \u03c1[i], (16)\nwhere \u03c1 is a standard scalar map containing the magnitudes of the vectors in z."
        },
        {
            "heading": "3.1.4 Batch normalization (BN) for vector fields",
            "text": "BN [12] normalizes each feature map across a minibatch to zero mean and unit standard deviation. It has been shown to improve the convergence of commonly employed stochastic gradient descent training algorithms.\nIn our case, the feature maps are not scalar fields, but vector fields representing the magnitude of an activation and the orientation of the filter generating it. In this case, BN should limit its effect to normalizing the magnitudes of the vectors to unit standard deviation. It would not make sense to normalize the angles, since their values are already bounded and changing the distribution would alter important information about relative and global orientations. Given a vector field feature map z and its map of magnitudes \u03c1, we compute batch normalization as:\nz\u0302 = z\u221a\nvar(\u03c1) . (17)"
        },
        {
            "heading": "3.1.5 Dropout for vector fields",
            "text": "Another commonly used layer in CNNs is Dropout [27]. Dropout makes use of the ideas behind ensemble learning (that an ensemble of independently trained learners tends to produce a better average response than any learner by itself) by randomly switching off some of the neurons at each training iteration. This reduces the risk of overfitting by preventing groups of neurons to co-adapt too tightly. Applying Dropout to vector fields only requires to ensure that both components of the filter are being switched off or on simultaneously. We apply Dropout on both fully connected and convolutional layers, the latter by switching on or off a filter for all locations in each of the samples."
        },
        {
            "heading": "3.2 The Rot-O-Block",
            "text": "It is not always desirable to use the maximum activation angle of a filter. For instance, when the filter shows an approximate radial symmetry, this information would be misleading, since all directions are equivalent, but OP chooses only one. The consequence is that two image locations that are almost identical can produce orthogonal activations. Thus, we propose to use a learning block, that allows each filter to return a learned combination of the vector field and the magnitude field outputs. We name it Rot-O-block.\nThe structure of this block is shown in Fig. 4. It takes as input a scalar tensor yin \u2208 RM\u00d7N\u00d7R\u00b7K , where R is the number of orientations used by the previous RotConv (RC in Fig. 4) layer and K is the number of learnable filters. An orientation pooling operation (OP) is applied on yin: this returns simultaneously the corresponding vector field tensor z \u2208 RM\u00d7N\u00d7K\u00d72 and its magnitude map \u03c1 \u2208 RM\u00d7N\u00d7K . Then, we separate the flows and process them separately by applying a RotConv on each activation. Spatial pooling (SP) and dropout can be optionally applied separately on the independent activations. The outputs are finally concatenated to produce the output yout."
        },
        {
            "heading": "4 Experiments",
            "text": "We explore the performance of the proposed method on datasets where the orientation of the patterns of interest\nis arbitrary. This is very often the case in biomedical imaging and remote sensing, since the orientation of the camera is usually not correlated to the patterns of interest. We will apply RotEqNet to problems from these two fields, as well to a well-known computer vision benchmark dataset, MNIST-rot. Each one of these case studies will allow us to analyze the performances of RotEqNet in problems needing respectively invariance, equivariance and covariance to rotations. Based on a sensitivity analysis, all the results are reported using a number of orientations R = 17 for the RotConv and RotConv> layers."
        },
        {
            "heading": "4.1 Invariance: MNIST-rot",
            "text": "MNIST-rot [17] is a variant of the original MNIST digit recognition dataset with an additional random rotation between 0\u25e6 and 360\u25e6 applied to each 28 \u00d7 28 image. The training set is also considerably smaller than the standard MNIST, with 12k samples, from which 10k are used for training and 2k for validation and model selection. The test set consists of 50k samples. Since we aim at predicting the same label independently from the rotation that has been applied, the MNIST-rot problem requires rotation invariance.\nModel: We test three CNN models with the same architecture, shown in Table 1: 1) a cascade of three RotO-Blocks, 2) a network with three blocks using only the vector field path of the Rot-O-Block and 3) a network using only the scalar field path. The models are trained for 150 epochs, starting with a large learning rate, weight decay and dropout rate of 0.01, 0.05 and 0.4, respectively. All the three hyper-parameters were gradually reduced to reach 0.0001, 0.005 and 0.0. The hyper-parameters were kept the same across all layers and all networks. We used batch normalization before every convolutional layer ex-\ncept the first and last ones.\nTest time data augmentation: We also perform data augmentation at test time, a technique often used with approximately invariant or equivariant CNNs [8, 11]. In particular, we show to the network several rotated versions of the same image using a fixed set of angles between 0o and 90o. Even if doing rotation-based data augmentation at test time might seem counter-intuitive in a rotation invariant model, the different rotations coupled to resampling of the images and the filters (cf. Sect. 3.1.1) will produce slightly different activations. After this, we average the scores obtained by all views of the same image to compute the final prediction.\nResults: The results obtained on the test set of the MNIST-rot dataset are reported in Table 2. The proposed RotEqNet, with the Rot-O-Block architecture, allows to match the results obtained by state-of-the-art TIpooling [16] and even improve over them when using testtime data augmentation.\nThe latter further improves the accuracy, by allowing the network to score different resamplings for each test image. This is very beneficial, in particular since input images are relatively small and consequently interpolation plays an even more critical role."
        },
        {
            "heading": "4.2 Equivariance: ISBI 2012 Challenge",
            "text": "This dataset [1] considers the problem of segmenting neuronal structures in electron microscope (EM) stacks [3]. In this semantic segmentation problem we need to locate precisely the neuron membrane boundaries. A rotation in the inputs should lead to the same rotation in the output, making the ISBI 2012 problem a good candidate to study rotation equivariance.\nThe data consist of two EM stacks of drosophila neurons, each with 30 images of size 512 \u00d7 512 pixels (Fig. 5a). One stack is used for training and the other for testing. The ground truth for the training stack consists of binary images of the same resolution as the data (Fig. 5b). The ground truth for the test stack is kept private and the results are to be submitted to an evaluation server 1.\nModel: We transform the original binary membrane vs. non-membrane segmentation problem into a three class segmentation problem: 1) non-membrane, 2) central membrane pixels and 3) membrane border pixels. Pixels within the membrane in the original label image but not belonging to either 2) or 3) are consider to belong to no class (Fig. 5c). This way we can assign a higher penalization to the non-membrane pixels right next to the membrane that those in the middle of the cells. The central membrane scores are used as the final prediction.\nSince we are dealing with a dense prediction problem in which spatial autorcorrelation at different resolution levels has to be learned, we apply an encoder-decoder CNN\n1http://brainiac2.mit.edu/isbi challenge/\ninvolving either two or three stages in the encoder and decoder blocks. Table 3 shows the architecture for the two-stage CNN. The output from the two-stage and threestage RotEqNets are averaged together to obtain the final result.\nResults: A detailed explanation on the evaluation metrics used in the challenge can be found in the ISBI 2012\nchallenge website 1 and in [1]. The winners of the challenge were Chen et al. [4], although Beier et al. [2] had the highest scores at the time of writing. These two works focus on the use of a post-processing pipeline. Our rotation equivariant prediction provides results comparable to the state-of-the-art using raw CNN probabilities [7, 8, 23] with a linear and relatively simple architecture (see Table 4)."
        },
        {
            "heading": "4.3 Covariance: car orientation estimation",
            "text": "In the last experiment, we test our method in a rotation covariant setting involving the estimation of car orientations from above-head imagery. We use the dataset provided by the authors of [10], which is based on images issued from Google Maps. The dataset is composed by 15 tiles, where cars\u2019 bounding boxes and car orientations have been manually annotated. We implement our approach in a way similar to [10]. We crop a 48\u00d748 square patch around every car, based on the centerpoint of the provided bounding box. We then use these crops for both training and testing of the model. As in [10], we use the cars in the first 10 images (388 cars) to train the network and those in the last 5 images (193 cars) to test.\nModel: The function we want to learn is covariant with respect to rotations because a rotation by \u2206\u03b1o in the input image should result in a numerical increase by \u2206\u03b1o of the angle predicted. In particular, we try to predict the sine and cosine of \u03b1o, because they are continuous with respect to \u2206\u03b1o. The network\u2019s architecture is illustrated in Table 5. For the output we use a tanh non-linearity, followed by a layer normalizing the output vector to unitnorm.Since the first fully connected layer is not rotation equivariant nor invariant, it is important to train the network with data augmentation based on rotations and flippings. This ensures that there will be no preferred orientations inherited from a biased training set. The weight decay and dropout rate were fixed to 0.1 and 0.3 respectively and the learning rate was gradually decreased from 10\u22123 to 10\u22125. All the filters were initialized from a normal distribution with zero mean and 10\u22123 standard deviation.\nResults: We report in Table 6 the average output of the last 30 training epochs out of a total of 330 for each\nmodel. This allows to overcome the lack of a validation set to select the best model. We compare three similar architectures with different degrees of orientation information passed to the last layer: 1) The two RotConv layers return a vector field, 2) the two RotConv layers return scalar fields and 3) only the second RotConv returns a vector field.\nThe use of vector outputs of RotEqNet improves substantially the results, outperforming both state of the art\non the dataset [10] and a version of RotEqNet using only scalar outputs at each layers. We also observe that alternating a first RotConv layer returning a scalar output and a second returning a vector output improves performances, probably due to the fact that at the original resolution of the images the orientation vector field is noisy, while it becomes more informative after the first spatial pooling. In Fig. 6 we show the error distribution in the test set for the hybrid model. Note how most samples, 83%, are predicted with less than 15o of error, while most of the contribution to the total error comes from the 7% of samples with error higher than 150o, in which the front of the car has been mistaken with the rear.\nSensitivity to R: In order to understand the sensitivity of RotEqNet to the number of angles R, we trained the hybrid model using R = 21 and tested it for different values of R (see Table 7). We observed relatively small changes in the test error. Since R = 17 gave the best results, we chose it for all the experiments."
        },
        {
            "heading": "5 Conclusion",
            "text": "We have presented a new way of hard-coding rotation equivariance in CNNs by applying each filter at different orientations and extracting a vector field feature map that encodes the maximum activation magnitude and angle.\nExperiments on classification, segmentation and orientation estimation problems show the suitability of this approach for solving problems that are inherently rotation equivariant, invariant or covariant."
        }
    ],
    "title": "Rotation equivariant vector field networks",
    "year": 2022
}