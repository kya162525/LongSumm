{
    "abstractText": "This paper addresses a common methodological flaw in the comparison of variable selection methods. A practical approach to guide the search or the selection process is to compute cross-validation performance estimates of the different variable subsets. Used with computationally intensive search algorithms, these estimates may overfit and yield biased predictions. Therefore, they cannot be used reliably to compare two selection methods, as is shown by the empirical results of this paper. Instead, like in other instances of the model selection problem, independent test sets should be used for determining the final performance. The claims made in the literature about the superiority of more exhaustive search algorithms over simpler ones are also revisited, and some of them infirmed.",
    "authors": [
        {
            "affiliations": [],
            "name": "Juha Reunanen"
        }
    ],
    "id": "SP:0170602900d114d723555ec2c02c8f82a0e1ba9e",
    "references": [
        {
            "authors": [
                "H. John",
                "Ron Kohavi",
                "Karl Pfleger"
            ],
            "title": "Irrelevant features and the subset selection problem",
            "venue": "In Proc. of the 11th Int. Conf. on Machine Learning",
            "year": 1997
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "In a typical prediction task, there may be a large number of candidate variables available that could be used for building an automatic predictor. If this number of candidate variables is denoted with n, the variable selection problem is often defined as selecting thed < n variables that allow the construction of the best predictor. There can be many reasons for selecting only a subset of the variables:\n1. It is cheaper to measure onlyd variables.\n2. Prediction accuracy might be improved through exclusion of irrelevant variables.\n3. The predictor to be built is usually simpler and potentially faster when less input variables are used.\n4. Knowing which variables are relevant can give insight into the nature of the prediction problem at hand.\nThe number of subsets to be considered grows exponentially with the number of candidate variables,n. This means that even with a moderaten, not all of these subsets can be evaluated. Hence, many heuristic algorithms have been proposed for determining the order in which the subset space should be traversed (Marill and Green, 1963, Whitney, 1971, Stearns, 1976, Kittler, 1978, Siedlecki and Sklansky, 1988, Pudil et al., 1994, Somol et al., 1999, Somol and Pudil, 2000). When a new heuristic algorithm is devised, it is usually compared to at least some of the existing ones in\nc\u00a92003 Juha Reunanen.\norder to show why it is needed. The goal of this study is to show that the conclusions can be quite different depending on how this comparison is done."
        },
        {
            "heading": "2. Evaluation of Subsets",
            "text": "Before running a search algorithm, one has to define what to search for. In variable selection, the aim is usually to find small variable subsets that enable the construction of accurate predictors. Consequently, the accuracies of the predictors to be built need to be estimated in order to know whether a good subset has been found. Additionally, because no exhaustive search through the subset space can usually be done, the accuracy estimate is also used to guide the heuristic search to the more beneficial parts of the space.\nIn the experiments of this paper, the accuracy is estimated using the so-calledwrapperapproach (John et al., 1994), where the particular predictor architecture that one ultimately intends to use is utilized already in the variable selection phase. Usually one builds predictors using subsets of the data samples available for the training, and tests them with the rest of the samples. A common choice iscross-validation(CV), where the data samples are randomly divided into a number of folds. Then, the samples belonging to one fold are used as a test set, whereas those belonging to other folds are used as a training set while building the predictor. This is repeated for all the folds one at a time, and when the errors in predicting the test set are counted up, a reasonable estimate for prediction accuracy is obtained. The special case where the number of folds is set equal to the number of samples available is often calledave-one-out cross-validation(LOOCV).\nAn alternative to using a wrapper would be thefilter approach, where the evaluation measure is computed more directly from the data, without using the ultimate predictor architecture at all. Some wrapper and filter strategies have been compared for example by Kohavi and John (1997).\nThek nearest neighbors (kNN) prediction rule (see e.g., Schalkoff, 1992) is used in the experiments of this paper. In order to use that rule to predict the class of a new sample, one first finds the k training samples that are closest to the new sample in the variable space. The new sample is then given the class label suggested by the majority of thesek training samples."
        },
        {
            "heading": "3. Search Algorithms",
            "text": "Many heuristic algorithms have been proposed for the variable selection problem. However, as this article is not a comparison but rather an attempt to highlight the problems in making comparisons, no complete list of different algorithms is given. Instead, only two algorithms are described. More of them are listed for example by Kudo and Sklansky (2000)."
        },
        {
            "heading": "3.1 Sequential Forward Selection",
            "text": "Sequential forward selection (SFS) was first used for variable selection by Whitney (1971). SFS starts the search with an empty variable subset. During one step, all the variables that have not yet been selected are considered for selection, and their impacts on the evaluation score are recorded. In the end of the step, the variable whose inclusion resulted in the best score is included in the set. Then, a new step is started, and the remaining variables are considered. This is repeated until a prespecified number of variables has been included. For comparison purposes, the search is usually repeated until all the variables are included.\nSFS has thenestingproperty, which is often seen as a drawback in variable and feature selection literature. This means that once a variable is included, it cannot be excluded later, even if it might be possible to increase the evaluation score by doing so."
        },
        {
            "heading": "3.2 Sequential Forward Floating Selection",
            "text": "The sequential forward floating selection (SFFS) algorithm (Pudil et al., 1994) has in many comparisons (Pudil et al., 1994, Jain and Zongker, 1997, Kudo and Sklansky, 2000) proven to be superior to the SFS algorithm. It is more complex and the search takes more time, but in return for this, it seems that one can obtain better variable subsets.\nThe central idea employed in SFFS for fighting the problem of nesting is that after the inclusion of one variable, the algorithm starts a backtracking phase where variables are excluded. This exclusion is carried on for as long as better variable subsets of the corresponding sizes are found. When no better subset is found, the algorithm goes back to the first step and includes the best currently excluded variable, which is again followed by the backtracking phase.\nThe original SFFS algorithm had a minor flaw (Somol et al., 1999): when first excluding a number of variables while backtracking and then including other variables, it may be that one ends up with a variable subset that is worse than the one that was found before backtracking took place. This can easily be remedied by doing some bookkeeping: one just checks whether this is the case and if so, jumps back to the better subset of the same size that was found earlier. The corrected version is used in the experiments of this paper."
        },
        {
            "heading": "4. Experiments",
            "text": "The experiments reported here compare one of the simplest and least intensive search algorithms, SFS, to the more complex SFFS. This is done using thekNN rule withk= 1 together with a LOOCV wrapper approach.\nContrary to some articles making comparisons between different search algorithms (such as Kudo and Sklansky, 2000), the benefits of the variable subsets obtained are validated with a test set that is not shown to the search algorithm during the search. This is the only way to check whether the more complex search algorithm has really found better subsets, or whether it has just overfitted to the discrepancy between the evaluation measure and the ground truth, which is here measured using the test set.\nTraining and test sets are obtained from the original set by dividing it randomly so that the proportions of the different classes are preserved. During the search, the training set is further divided in order to be able to estimate prediction accuracy. After the search, predictors built using the training set and its corresponding variable subsets are used to predict the class labels of the samples in the test set to see whether overfitting has taken place."
        },
        {
            "heading": "4.1 Datasets",
            "text": "The following datasets publicly available at the UCI Machine Learning Repository1 are used. They are summarized in Table 1.\n1. http://www.ics.uci.edu/\u223cmlearn/MLRepository.html\nsonar This dataset can be found in theundocumented/connectionist-bench directory of the repository. The task is to discriminate between sonar signals bounced off metal cylinders and roughly cylindrical rocks. This dataset is chosen because it is quite easy to compare results obtained with it to the rather explicit results of Kudo and Sklansky (2000).\nionosphere This classic dataset is related to a problem of determining whether a signal received by a radar is \u201cgood\u201d, which means that the signal contains potentially useful information about the ionosphere. This is not the case when the signal transmitted by the radar passes straight through the ionosphere.\nwaveform This is an artificial dataset generated using a program whose source code is available in the repository. Roughly half of the attributes are known to be noise with respect to the class labels of the samples.\ndermatology The dermatology dataset is about determining the type of an eryhemato-squamous disease, which is a real problem in dermatology. One variable describing a patient\u2019s age is removed from the set because it has some missing values.\nspambaseIn this problem domain the task is to determine whether a particular e-mail message is an advertisement that the receiver would never want to read. However, the dataset is personalized for a particular user whose name and address are indications of non-spam, so the results are too positive for a general-purpose spam filter.\nspectf This dataset is related to diagnosis based on cardiac SPECT images. Each of the patients is classified into two categories, normal and abnormal.\nmushroom Here, the problem is one of classifying mushrooms to those that are edible and to those that are poisonous. Out of the 21 categorical variables with no missing values, 112 binary features are obtained using 1-of-N coding. This means that if a categorical variable hasN possible values,N corresponding binary features are used. Thekt of these is assigned the value 1 when the original variable is equal to thekth of the possible values, and 0 otherwise."
        },
        {
            "heading": "4.2 Results",
            "text": "The sonar dataset was experimented with by Kudo and Sklansky (2000), whose results show that SFFS is able to find subsets superior to those found with SFS with almost all subset sizes. However, they used the whole dataset for selecting the variable subsets and also for evaluating their performances. In the following, it is shown that the interpretation and the conclusions turn out to be quite different when independent test data is used.\nIn Figure 1, results similar to those by Kudo and Sklansky are depicted using the solid and the dashed curves. The results are not exactly the same as theirs, because only half of the whole set is used here. Still, the superiority of SFFS seems as clear as in their results. Now predictors can be built using the training set and the best variable subsets of each size that were found. Based on these LOOCV curves of the figure, one would suspect that a predictor built using a subset found with SFFS must perform better than one due to SFS. However, the results of classifying the test set are shown with the dotted and dash-dotted lines, and it is evident that with respect to the test set, SFFS is far from outperforming SFS.\nWhy this happens can easily be understood by looking at Figure 2. There, each variable subset of size ten evaluated with both algorithms is plotted according to its LOOCV-estimated prediction accuracy and the accuracy obtained with the test data. The figure reveals that there is not much correlation between these values. It is true that SFFS has found many subsets whose LOOCV score is higher than the best one found by SFS. Still, as far as the prediction accuracy for the test set is concerned, these subsets do not perform any better. Also finding and evaluating a subset which has high accuracy for the test set is of not much use unless the LOOCV score for the very same subset is high as well: this is because otherwise no algorithm chooses such a set, no matter how useful it might be in reality and with respect to the test set.\nFigure 3 shows that SFFS does indeed evaluate many more subsets than SFS. Because the execution time of each algorithm is proportional to the number of evaluations done by it, it has been shown that SFFS takes a lot more time to run, but in return for this, does not necessarily yield subsets that would be any better.\nIn fact, it turns out that based on the LOOCV curves of Figure 1, SFFS has attained at least as high a score as SFS in all the 60 cases (different variable subset sizes), and the performance is actually higher in 50 of these cases. Conversely, the other two curves show that with respect to previously unseen test data, the subsets found with SFFS are better in only 18 cases, and at least as good in 28 cases, which means that those found with SFS are actually better in 32 cases. Moreover, the mean difference in the LOOCV-estimated classification rates is 3.56 percentage points in favor of SFFS, whereas the difference in the actual test set classification rates is 0.44 percentage points on average \u2014 in favor of SFS!\nMore results like these are given in Table 2 for the different datasets described in Section 4.1. Similar runs as those depicted in Figure 1 are repeated ten times with different random divisions\nconstitute the test set. Note that this is not related to cross-validation, but to the selection of the \u201cindependent\u201d held out test data: the parameterf is used to make sure that the training sets do not get prohibitively large in those cases where the dataset has lots of samples. The distribution of the samples among thec classes in the original set are shown in the column s, and the number of training samples used (roughly the total in columns divided by the value in columnf ) is given in the last column, which is denoted bym.\ninto training and test sets in order to remove the possibility for an unfortunate division. The random allocations are independent, but an outer layer of cross-validation could just as well be used. There is also one row where the results are not like the others, namely that for the mushroom dataset, which will be discussed later.\nIn Table 2 one can see that based on the LOOCV estimates obtained during training, the subsets found by SFFS seem to be at least as good as those found by SFS in 96\u2013100% of different subset sizes. However, when the results for the test sets are examined, it turns out that (except for the mushroom dataset) SFFS is better in less than half the cases, and at least as good in slightly more than half. Furthermore, the average increase in correct classification rate due to using SFFS instead of SFS as estimated by LOOCV during the search is always positive, and it would seem that on the average, an increase of one or two percentage points can be be obtained. Unfortunately, there is no such increase when the classification performances are evaluated using a test set not seen during the subset selection process: in all the cases, once again excepting the mushroom set, the average increase \u2014 or decrease \u2014 seems to be statistically rather insignificant.\nOn the other hand, the results for the mushroom dataset in the tables and in Figure 4 are remarkably different. This is clearly a case where SFFS does give results superior to those of SFS: almost perfect accuracy is reached with half the number of variables. This shows that the capability of SFFS to avoid the nesting effect can sometimes be very useful. However, whether this is the case here cannot be determined based only on the training set, but test data not seen during the search is needed.\nIn Table 2, the results are summarized by averaging over all the possible subset sizes. However, in practice one is usually more interested in some specific region where the accuracy is high. Table 2 could be reproduced for any such region \u2014 an example would be the subset sizes that are, according to the estimated accuracies, among the ten best. Another example could be the sets which have more\nthan ten variables included and also more than ten variables excluded. Because there is no room here for the results for all such regions, it suffices to say that with these datasets they were, at least for most such regions, essentially similar to those given in the tables.\nTo make it possible to examine the effect of choosing a particular accuracy estimation method such as LOOCV and a particular predictor architecture such as the 1NN rule, results for 5-fold CV with the 1NN and the C4.5 (Quinlan, 1993) predictor algorithms are provided in the appendix of this paper."
        },
        {
            "heading": "5. Related Work",
            "text": "The fact presented once again in this paper that independent test data should be used for estimating the actual performance of the selection methods is obviously not a new one from a theory point of view (Kohavi and Sommerfield, 1995, Kohavi and John, 1997, Scheffer and Herbrich, 1997). However, it is still too often forgotten as far as applications are concerned, at least in the field of variable selection. This holds even to the extent that it is actually difficult to find comparisons where it is explicitly stated that the methodology used is proper, i.e., that the test data used to obtain the final results to be compared is not shown to the search algorithm when the variable subsets are determined.\nthe third column shows the proportion of cases where the test set was better classified using variable subsets found by SFFS. The difference between the values in the second column and the respective values in the third column can be thought of as the amount of overfitting due to an intensive search. The fourth and the fifth column are similar, but here equivalent scores are included in the proportions, so the numbers are always greater than those in the second and the third column. The sixth column shows on average how many percentage points the classification results seem to be boosted due to using SFFS instead of SFS, and the seventh column shows how much they are actually boosted. In all the columns, the parenthesized value is the standard deviation based on the ten random divisions into training and test sets.\nThe experimental results of this paper might thus put some doubt on the rather widely-accepted belief that floating search methods, SFFS and its backward counterpart SBFS (Pudil et al., 1994), are superior to the simple sequential ones, SFS (Whitney, 1971) and SBS (Marill and Green, 1963). For example, in their review article, Jain et al. (2000) state that \u201cin almost any large feature selection problem, these methods [SFFS and SBFS] perform better than the straightforward sequential searches, SFS and SBS.\u201d Claims like this (see also Pudil et al., 1994, Jain and Zongker, 1997, Kudo and Sklansky, 2000) make many users of variable selection techniques (see e.g., Vyzas and Picard, 1999, Healey and Picard, 2000, Wiltschi et al., 2000) believe that they should not use the simple algorithms, although according to the results shown in this paper they may give equally good results in significantly less time."
        },
        {
            "heading": "6. Conclusions",
            "text": "This paper stresses that variable selection is nothing but a particular form of model selection \u2014 the good practice of dividing the available data into separate training and test sets should not be forgotten. In light of the experimental evidence, ignorance of such established methodology yields wrong conclusions. In particular, the paper demonstrates that intensive search techniques like SFFS do not necessarily outperform a simpler and faster method like SFS, provided that the comparison is done properly.\nOne should not confuse the use of cross-validation to guide the search and a possible use of cross-validation to compare final predictor performance. Indeed, given enough computer resources, the split between training data and test data can be repeated several times and performances on various test sets thus obtained averaged. In that respect, LOOCV can also be used in an \u201couter\nloop\u201d to compare variable selection methods, by running the variable selection algorithms on as many training sets as there are examples, each time testing on the held out example. LOOCV can also be used in the \u201cinner loop\u201d to guide the search of the variable selection process on the training data. This paper only warns about the use of the inner loop LOOCV to compare methods.\nFinally, when faced with the choice of a search strategy, the quantity of data available is in practice likely to influence the methodology, either for computational or statistical reasons: If a lot of data is available, extensive search strategies may be computationally unrealistic. If very little data is available, reserving a sufficiently large test set to decide statistically which search strategy is best may be impossible. In such a case, one might recommend to use a bias in favor of the simplest search strategies that are less prone to overfitting. Between these two extreme cases, it may be worth trying alternative methods, using a comparison methodology similar to the one outlined in this paper.\nAppendix\nThe results presented in Section 4.2 are computed with the 1NN prediction rule using LOOCV evaluation. In order to examine the effect of choosing these particular methods, more results are shown in Tables 3 and 4. In Table 3, results are shown for the 1NN rule when the accuracy is estimated with 5-fold cross-validation. On the other hand, Table 4 presents the figures computed still with 5-fold CV but this time for the C4.5 algorithm with pruning enabled (Quinlan, 1993).\nThe main difference between the results for 1NN and LOOCV (Table 2) as compared to those for 1NN and 5-fold CV (Table 3) appears to be highlighted by the fourth column, where the values are close to 100% in Table 2, but remarkably smaller in Table 3. It seems that this difference is caused by the random folding in 5-fold CV, which makes it possible for SFS to sometimes get better evaluation scores than SFFS. Another thing is that equal evaluation scores are not so likely anymore, which causes the differences between the values in the second column as compared to those in the fourth one to be in general much smaller in Table 3 than in Table 2.\nWhen 5-fold CV is used, the results between the 1NN rule (Table 3) and the C4.5 algorithm (Table 4) do not seem to differ remarkably."
        }
    ],
    "title": "Overfitting in Making Comparisons Between Variable Selection Methods"
}