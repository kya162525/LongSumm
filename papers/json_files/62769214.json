{
    "abstractText": "Predicting attributes from face images in the wild is a challenging computer vision problem. To automatically describe face attributes from face containing images, traditionally one needs to cascade three technical blocks \u2014 face localization, facial descriptor construction, and attribute classification \u2014 in a pipeline. As a typical classification problem, face attribute prediction has been addressed using deep learning. Current state-of-the-art performance was achieved by using two cascaded Convolutional Neural Networks (CNNs), which were specifically trained to learn face localization and attribute description. In this paper, we experiment with an alternative way of employing the power of deep representations from CNNs. Combining with conventional face localization techniques, we use off-the-shelf architectures trained for face recognition to build facial descriptors. Recognizing that the describable face attributes are diverse, our face descriptors are constructed from different levels of the CNNs for different attributes to best facilitate face attribute prediction. Experiments on two large datasets, LFWA and CelebA, show that our approach is entirely comparable to the state-of-the-art. Our findings not only demonstrate an efficient face attribute prediction approach, but also raise an important question: how to leverage the power of off-the-shelf CNN representations for novel tasks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yang Zhong"
        },
        {
            "affiliations": [],
            "name": "Josephine Sullivan"
        },
        {
            "affiliations": [],
            "name": "Haibo Li"
        }
    ],
    "id": "SP:6bb95a0f3668cd36407c85899b71c9fe44bf9573",
    "references": [
        {
            "authors": [
                "Timo Ahonen",
                "Abdenour Hadid",
                "Matti Pietikainen"
            ],
            "title": "Face description with local binary patterns: Application to face recognition",
            "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
            "year": 2006
        },
        {
            "authors": [
                "Hossein Azizpour",
                "Ali Sharif Razavian",
                "Josephine Sullivan",
                "Atsuto Maki",
                "Stefan Carlsson"
            ],
            "title": "From generic to specific deep representations for visual recognition",
            "venue": "arXiv preprint arXiv:1406.5774,",
            "year": 2014
        },
        {
            "authors": [
                "Rong-En Fan",
                "Kai-Wei Chang",
                "Cho-Jui Hsieh",
                "Xiang-Rui Wang",
                "Chih-Jen Lin"
            ],
            "title": "Liblinear: A library for large linear classification",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2008
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
            "venue": "arXiv preprint arXiv:1502.01852,",
            "year": 2015
        },
        {
            "authors": [
                "Gary B. Huang",
                "Marwan Mattar",
                "Tamara Berg",
                "Erik Learned-miller. E"
            ],
            "title": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments",
            "venue": "Technical report,",
            "year": 2007
        },
        {
            "authors": [
                "Andrej Karpathy",
                "Li Fei-Fei"
            ],
            "title": "Deep visual-semantic alignments for generating image descriptions",
            "venue": "arXiv preprint arXiv:1412.2306,",
            "year": 2014
        },
        {
            "authors": [
                "Vahdat Kazemi",
                "Josephine Sullivan"
            ],
            "title": "One millisecond face alignment with an ensemble of regression trees",
            "venue": "In Computer Vision and Pattern Recognition (CVPR),",
            "year": 2014
        },
        {
            "authors": [
                "I. Kemelmacher-Shlizerman",
                "S. Seitz",
                "D. Miller",
                "E. Brossard"
            ],
            "title": "The megaface benchmark: 1 million faces for recognition at scale",
            "venue": "ArXiv e-prints,",
            "year": 2015
        },
        {
            "authors": [
                "Neeraj Kumar",
                "Alexander C Berg",
                "Peter N Belhumeur",
                "Shree K Nayar"
            ],
            "title": "Attribute and simile classifiers for face verification",
            "venue": "In Computer Vision,",
            "year": 2009
        },
        {
            "authors": [
                "Neeraj Kumar",
                "Alexander C Berg",
                "Peter N Belhumeur",
                "Shree K Nayar"
            ],
            "title": "Describable visual attributes for face verification and image search",
            "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
            "year": 1962
        },
        {
            "authors": [
                "David G Lowe"
            ],
            "title": "Distinctive image features from scale-invariant keypoints",
            "venue": "International journal of computer vision,",
            "year": 2004
        },
        {
            "authors": [
                "Omkar M Parkhi",
                "Andrea Vedaldi",
                "Andrew Zisserman"
            ],
            "title": "Deep face recognition",
            "venue": "Proceedings of the British Machine Vision,",
            "year": 2015
        },
        {
            "authors": [
                "Ali S Razavian",
                "Hossein Azizpour",
                "Josephine Sullivan",
                "Stefan Carlsson"
            ],
            "title": "Cnn features offthe-shelf: an astounding baseline for recognition",
            "venue": "In Computer Vision and Pattern Recognition Workshops (CVPRW),",
            "year": 2014
        },
        {
            "authors": [
                "Florian Schroff",
                "Dmitry Kalenichenko",
                "James Philbin"
            ],
            "title": "Facenet: A unified embedding for face recognition and clustering",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv preprint arXiv:1409.1556,",
            "year": 2014
        },
        {
            "authors": [
                "Yi Sun",
                "Yuheng Chen",
                "Xiaogang Wang",
                "Xiaoou Tang"
            ],
            "title": "Deep learning face representation by joint identification-verification",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2014
        },
        {
            "authors": [
                "Yi Sun",
                "Ding Liang",
                "Xiaogang Wang",
                "Xiaoou Tang"
            ],
            "title": "Deepid3: Face recognition with very deep neural networks",
            "venue": "arXiv preprint arXiv:1502.00873,",
            "year": 2015
        },
        {
            "authors": [
                "Yi Sun",
                "Xiaogang Wang",
                "Xiaoou Tang"
            ],
            "title": "Deep learning face representation from predicting 10,000 classes",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2013
        },
        {
            "authors": [
                "Christian Szegedy",
                "Wei Liu",
                "Yangqing Jia",
                "Pierre Sermanet",
                "Scott Reed",
                "Dragomir Anguelov",
                "Dumitru Erhan",
                "Vincent Vanhoucke",
                "Andrew Rabinovich"
            ],
            "title": "Going deeper with convolutions",
            "venue": "arXiv preprint arXiv:1409.4842,",
            "year": 2014
        },
        {
            "authors": [
                "Yaniv Taigman",
                "Ming Yang",
                "Marc\u2019Aurelio Ranzato",
                "Lior Wolf"
            ],
            "title": "Deepface: Closing the gap to human-level performance in face verification",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2013
        },
        {
            "authors": [
                "Oriol Vinyals",
                "Alexander Toshev",
                "Samy Bengio",
                "Dumitru Erhan"
            ],
            "title": "Show and tell: A neural image caption generator",
            "venue": "arXiv preprint arXiv:1411.4555,",
            "year": 2014
        },
        {
            "authors": [
                "Xinchen Yan",
                "Jimei Yang",
                "Kihyuk Sohn",
                "Honglak Lee"
            ],
            "title": "Attribute2image: Conditional image generation from visual attributes",
            "venue": "arXiv preprint arXiv:1512.00570,",
            "year": 2015
        },
        {
            "authors": [
                "Zhicheng Yan",
                "Hao Zhang",
                "Robinson Piramuthu",
                "Vignesh Jagadeesh",
                "Dennis DeCoste",
                "Wei Di",
                "Yizhou Yu"
            ],
            "title": "Hd-cnn: Hierarchical deep convolutional neural network for large scale visual recognition",
            "venue": "In ICCV\u201915: Proc. IEEE 15th International Conf. on Computer Vision,",
            "year": 2015
        },
        {
            "authors": [
                "Dong Yi",
                "Zhen Lei",
                "Shengcai Liao",
                "Stan Z Li"
            ],
            "title": "Learning face representation from scratch",
            "venue": "arXiv preprint arXiv:1411.7923,",
            "year": 2014
        },
        {
            "authors": [
                "Xiaogang Wang Ziwei Liu",
                "Ping Luo",
                "Xiaoou Tang"
            ],
            "title": "Deep learning face attributes in the wild",
            "venue": "In Proceedings of International Conference on Computer Vision (ICCV),",
            "year": 2015
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The recent success achieved by the Convolutional Neural Networks (CNNs) has vastly driven the advances in many aspects of computer vision, such as image classification and object detection, and pushed the boundaries of understanding image content through computer vision. In face recognition, we have witnessed great improvements brought by CNNs in solving the challenging large-scale face verification and recognition tasks [5, 8].\nLike recognizing identities, describing attributes from face images in the wild has been an active research topic for years. Being able to automatically describe face attributes from face images in the wild is very challenging but can be very helpful. For instance, one can not only build identifiers directly based on attributes [10], but also efficiently construct highly flexible large-scale hierarchical datasets, which can further benefit image classification and attribute-to-image generation [23, 22].\nThe general process of predicting face attributes is to construct face representations and train domain classifiers for prediction. As summarized in Figure 1, traditional approaches (Pipeline 1) construct low-level descriptors, such as SIFT [11] and LBP [1], through landmark detection. These descriptors are then utilized for building attribute classifiers. Similarly, by using CNNs one can also employ massive sentence and image training instances to construct end-to-end deep architectures (Pipeline 2) for learning semantic-visual correspondences as in [6, 21]. However, such approaches are rather resource demanding.\nar X\niv :1\n60 2.\n03 93\n5v 2\n[ cs\n.C V\n] 2\n1 Ju\nAn intuitive alternative way (Pipeline 3) is to decompose the end-to-end network (by functionality) into a localization network, a feature construction network, and attribute classifiers, and build them individually as in [25]. By cascading the trained components, such a pipeline can achieve stateof-the-art performance. However, the requirements of this approach on data and training efforts seem enormous. In addition, it appears that to reach the best performance, high-level features must be used in the concatenated deep networks; fine-tuning on the pre-trained off-the-shelf high-level abstraction yields significantly better performance.\nHowever, given that many face attributes are locally orientated and different layers of CNN features encode different levels of visual information, we believe face attributes would not be best represented by merely high-level features from deep neural networks. Thus, in this paper, we alternatively tackle the face attribute prediction problem using a pipeline composed of a conventional localization component, an off-the-shelf CNN, and attribute classifiers (Pipeline 4 in Figure 1). Our focus is finding proper feature representations from pre-trained CNNs to boost attribute perdition. We use off-the-shelf architectures and a publicly accessible model intended for face recognition to do feature construction, and investigate what types of feature representations from the network can efficiently improve face attribute prediction.\nOur investigations show that intermediate representations from pre-trained CNNs have distinct advantages over high-level features for the target face attribute prediction problem. By simply utilizing these features, we achieved very promising results on a par with the state-of-the-art, produced by the intensively trained two-stage CNN, on two recently released face attribute prediction datasets CelebA and LFWA [25]. Our findings also suggest that off-the-shelf intermediate CNN representations could be easily utilized when transferring from the source problem to novel detection and classification tasks."
        },
        {
            "heading": "2 Related Work",
            "text": "Traditionally, face descriptors were built from hand-crafted features. These features were constructed either from the whole face area, or extracted from detected local components and concatenated into a train of descriptors [9]. Classifiers were trained based on these features to recognize the presence and quantitative degree of the domain attributes. Recently, Liu. et al. [25] proposed a cascaded learning framework to perform attribute prediction in the wild. By pre-training and finetuning on large object dataset and face datasets, it efficiently localizes faces and produces semantic attributes for arbitrary face sizes without alignment.\nAs a strong feature learner, CNN has been successfully applied in face recognition, especially for solving the challenging face recognition in the wild problem [5]. Besides the DeepID series approaches [18, 16, 17], related efforts have also been made to pose correction [20], architectures design [19, 14] and data collection skills [12]. With recently launched hardware platforms and the publicly accessible large-scale dataset [24], developing deep learning based face recognition approaches becomes feasible with less resources."
        },
        {
            "heading": "3 Attribute Prediction using CNNs Off-the-shelf",
            "text": ""
        },
        {
            "heading": "3.1 Overview",
            "text": "To describe face appearances using CNN features, it is critical to first consider a proper face representation from the deep neural network. One natural way is to represent faces using the discriminatively learned features, from the high-level hidden layers, mostly used for representing identities in face recognition tasks, as in [18]. In this case, appearance attributes are embedded in the activation of neurons in the discriminative feature.\nHowever, to describe the appearance using deep representations from CNNs, it is easy to expect that the selected representation should preserve the variability to describe the appearance variations regarding facial physical characteristics, such as \u201cbig (eyes)\u201d and \u201copen (mouth)\u201d. While on the contrary, when attributes are identity correlated (e.g. gender and ethnicity), such representation should be robust with respect to non-identity related interference. Thus, the representation that most suitable to describe a certain attribute highly depends on the property (e.g. if subject to identity) of the attribute itself. Given that a CNN enables its intermediate representations to maintain both discriminality and rich spatial information [2, 13], it is therefore tenable to employ flexible selections of feature presentations for predicting face attributes."
        },
        {
            "heading": "3.2 Experiments",
            "text": ""
        },
        {
            "heading": "3.2.1 Procedures",
            "text": "To identify the most effective deep representations, our method explores the attribute prediction power of intermediate representations versus the final representation 1 from CNNs trained for face recognition. Therefore, we first trained a face classification CNN (or use a publicly available model), then we evaluated the prediction performance of the representations extracted from different levels of the CNN. The training of CNNs and the evaluation of prediction power were conducted separately on two independent datasets: the WebFace [24] was used for CNN training, and the CelebA and LFWA for evaluation. We used two well known off-the-shelf architectures (configurations of filter stacks) in our experiments to benefit from the latest development in CNN architecture design.\nNetwork architecture: The networks used in our experiments shared the same format: they were composed of off-the-shelf filter stacks followed by two Fully Connected (denoted by FC1 and FC2) layers. Considering the ease of training and efficient inference during test phase, we selected Google\u2019s FaceNet NN.1 [14] (shortened as \u201cFaceNet\u201d in the following) and VGG\u2019s \u201cvery deep\u201d model [15] as the structure of convolutional (conv.) layers. The CNNs were trained in the most fundamental flat classification manner with a Softmax layer attached to the last FC layer during training. We used dropout regularization between FCs to prevent overfitting and the dropout rate was set to 0.5 for all FC layers in our networks. PReLU[4] rectification was attached to each convolution and FC layer.\nTraining: Around 10, 000 identities with 350, 000 image instances of the WebFace dataset were used. Random mirroring, slight rotation and jittering were utilized as data augmentation. The learning rate was initially set to 0.015, and then decreased by a factor of 10 when the validation set accuracy stopped increasing. The networks were trained by 3 decreasing learning rates. Faces were segmented and normalized to a size of 120\u00d7 120 and randomly cropped patches of 112\u00d7 112 were fed into the network.\nFeature Extraction: To extract face descriptors from CNNs, only the center patch (112\u00d7 112) and its mirrored version of aligned face images were fed into the CNNs unless otherwise stated. We aligned faces using feature points detected by random forests [7]. We took the averaged representations of the two fed-in patches at different levels of the network, i.e. \u201cSpat.1 \u00d7 1\u201d, \u201cSpat.3 \u00d7 3\u201d, \u201cFC1\u201d, and \u201cFC2\u201d , as shown in Figure 2, and evaluated their attribute estimation performance to identify the most effective representation corresponding to each attribute.\nThe output of the last conv. filter stack was selected as the representative of the intermediate representations since it was shown to have the most discriminality and spatial information for recognition\n1The high level abstraction used for representing identity, which is often extracted from the last FC layer.\nand image retrieval [2]. Extra max. pooling steps were applied to reduce the dimension of intermediate spatial representations. Then Spat.3\u00d7 3 and Spat.1\u00d7 1 were of 3\u00d7 3\u00d7K and 1\u00d7 1\u00d7K in our experiments regardless of the network, where K represents the channel depth of the employed network.\nAttribute prediction: The face attribute prediction performance was evaluated on the released version of CelebA and LFWA datasets2. The CelebA contains approximately 200, 000 images of 10, 000 identities and LFWA has 13, 233 images of 5, 749 identities. Each image in CelebA and LFWA is annotated with 40 binary attribute tags. We used the same procedure to build our attribute classifier as in [25]: binary linear SVM [3] classifiers were trained directly for all levels of representations (i.e. FCs and Spat.\u2032s) to classify face attributes. On the CelebA, the training set for each attribute classifier had 20, 000 image instances (where available). Since this dataset and the data for training our CNN are independent (the learning targets are also different), we tested the attribute prediction accuracy of our classifiers across the whole dataset through random selection of training and testing face instances. On the LFWA, we took the training instances defined by the dataset. We report the prediction accuracy as the mean of True Acceptance Rate and True Rejection Rate for each attribute on both CelebA and LFWA datasets.\nEvaluations and Comparison: The same evaluation protocol as in [25] was used in our experiments. Since the features in our experiment was extracted from aligned face images and the alignment process was independent of the network, we selected the corresponding approach (\u201c[17]+ ANet\u201d in [25]) as the baseline method. The current state of the art in [25] is denoted by \u201cTwo-stage CNN\u201d and \u201cLNet+ANet\u201d in this paper.\nThe above mentioned procedures were used in the following experiments. We first employed our FaceNet to thoroughly study the discrepancy between different representation types for face attribute prediction. The identified best performing off-the-shelf features were utilized to challenge on the CelebA and the LFWA to compare with [25]. We then extended our experiments by further investigating different configurations of the FaceNet, the VGG\u2019s \u201cvery deep\u201d architecture, and the publicly available VGG-Face model3 to ensure the discrepancy in attribute prediction power among the deep representations."
        },
        {
            "heading": "3.2.2 Performance Discrepancy between Deep Representations",
            "text": "Our intuition as stated above was that the intermediate face representations would be more suitable for describing diverse types of attributes regarding their physical characteristics and image conditions. To validate this, we trained a face recognition CNN with a structure of FaceNet. The length of both FC layers was set to 512 to reduce the risk of overfitting. The recognition rate of the trained FaceNet on the validation set was less than 98% and the face verification performance on the LFW [5] was 97.5%. We then extracted the four types of face representations, Spat.1b1, Spat.3b3, FC1, and FC2, from our trained model and linear classifiers were constructed and evaluated respectively\n2http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html. Released in Oct. 2015. 3http://www.robots.ox.ac.uk/\u02dcvgg/software/vgg_face/, accessed in Nov. 2015.\non the training set. The prediction performance for each representation type on all attributes is shown in Figure 3.\nWhile it is intuitive that FC2, the identity discriminative feature, is unlikely to be the best choice for describing facial attributes all the time, it is still astonishing that FC2 was significantly outperformed (> 5% in prediction accuracy) by others on 13 attributes. Similar disadvantages can also be observed on the LFWA dataset. It is easy to find that:\n1. Representations at different levels of the network feature quite diverse performance in attribute description.\n2. Intermediate representations, especially Spat.3\u00d7 3, are likely more effective in telling the weak identity-related attributes describing expressions and image conditions, which counts more on spatial information.\nFor instance, for attributes related to mouth and eyes which can produce dynamic facial expressions, such performance gaps are significant. For \u201cSpat.3\u00d73\u201d which better preserved spatial information, it is more effective in specifying shape and motion of facial components (e.g. Attribute 20 and 34 on CelebA in Figure 3). This is natural since intermediate representations contain mid-level features composed by low-level ones, thus they are more suitable to describe local facial attributes.\nOur investigations show that the best performing representations achieved attribute prediction accuracy of 86.6% on CelebA and 84.7% on LFWA, which is on a par with state-of-the-art \u201cTwo-stage CNN\u201d approach which was trained with massive image classification and face data. The comparative results are listed in Table 1 and shown in Figure 4. One can see that:\n1. By leveraging the intermediate deep representations from various levels of CNNs, the equivalent baseline approach is outperformed with a big margin.\n2. Even without fine-tuning the pre-trained CNN, our average prediction performance is still comparable to the state-of-the-art on both datasets.\nHere we noticed that the intermediate representations dominated the best representations of the attributes. This indicates that spatial information, i.e. location and magnitude of activation in conv. filter responses, is significant for describing attributes; if one wants to utilize high-level features, which implicitly embeds spatial information, fine-tuning must be conducted on the high-level abstractions to enhance such useful spatial information."
        },
        {
            "heading": "3.2.3 Further Validations",
            "text": "To further verify the potential utility of intermediate spatial representations for face attribute prediction, we also evaluated various network architectures trained by different configurations, which for each model are listed in Table 2. CelebA was selected as the evaluation dataset due to its larger scale.\nSpecifically, we first evaluated two different networks of the FaceNet architecture. Model 1 was trained by the first 8, 000 identities that has the most images on the WebFace dataset (i.e. taking away the long-tail data). Since the length of the representing features plays a vital role in face representation [12], we then evaluated the influences of varying FC layer lengths in face attribute prediction with Model 2 by increasing the length of FC layers to 1024. The receptive field was kept the same (112\u00d7 112). We also cross-validated the utility of the deep representations with VGG type architectures in Model 3 and 4. Model 3 had filter stack as VGG fitler-Config.C [15], but with a duplicated conv. and pooling section appended to the fifth pooling so that it was even deeper. (Thus, for this configuration, the filter stack directly gave output with size of 3\u00d7 3. It was then max. pooled to get 1\u00d7 1 output.) To bring more divergence, we decreased the input size to 96 \u00d7 96 (still cropped from 120 \u00d7 120) and set FCs to 4096. Model 4 was the off-the-shelf VGG-Face network. The receptive area for Model 4 was 224\u00d7224. The corresponding results in terms of the averaged prediction accuracy are provided in Table 3.\nWe observed that on average the spatial representations excelled on more than 75% of the 40 attributes. The spatial representation from the off-the-shelf VGG-Face model even dominated the best representations. We attribute it to the dramatic increase of the receptive area. The intermediate representations embedded more detailed spatial information also further boosted the performance of FC2, which was as effective as the Spat.3 \u00d7 3. The slightly worse performance of the features from Model 3 can be attributed to the lower receptive field and the 6th extra pooling, which caused transfer of prediction power from Spat.3\u00d7 3 to Spat.1\u00d7 1. Through further analysis of the results, we found that the intermediate spatial representations predicted 5 attributes (\u201cBags Under Eyes\u201d , \u201cBlurry\u201d, \u201cMouth S. Open\u201d, \u201cPale Skin\u201d and \u201cNarrow Eyes\u201d) much better than the last FC representations.\nWe believe the reason intermediate spatial representations outperformed on so many attributes is that these human describable attributes are more likely to be identified from the spatial information captured by human brains. Considering these attributes are semantic concepts relating to specific domains and these domains by themselves alone can hardly be used to pin-point a specific identity\nfrom a crowd of people, the utility of the intermediate features, which are less discriminating than the high level abstraction, from CNNs makes sense."
        },
        {
            "heading": "4 Conclusions",
            "text": "In this paper, we address the problem of predicting face attributes using CNNs trained for face recognition. We employ CNNs with off-the-shelf architectures and publicly available models \u2014 Google\u2019s FaceNet and VGG\u2019s \u201cvery deep\u201d model \u2014 with the conventional pipeline to study the prediction power of different representations from the trained CNNs. Our investigations present the correspondence diversity between the best performing representations and the human describable attributes. They also reveal that the intermediate representations from CNNs are very effective in predicting facial attributes in general. Although previous works have shown that fine-tuning the pretrained networks brought significant improvements when transferring to novel tasks, we empirically demonstrate that intermediate deep features from pre-trained networks can also form a promising alternative. By properly leveraging these off-the-shelf CNN representations, we achieved accurate attribute prediction on a par with current state-of-the-art performance."
        },
        {
            "heading": "Acknowledgments",
            "text": "We gratefully acknowledge the support from NVIDIA Corporation for GPU donations. We have enjoyed discussions with Ali Sharif Razavian and Atsuto Maki."
        }
    ],
    "title": "Face Attribute Prediction Using Off-the-Shelf CNN Features",
    "year": 2016
}