{
    "abstractText": "We present the 1.0 release of our paraphrase database, PPDB. Its English portion, PPDB:Eng, contains over 220 million paraphrase pairs, consisting of 73 million phrasal and 8 million lexical paraphrases, as well as 140 million paraphrase patterns, which capture many meaning-preserving syntactic transformations. The paraphrases are extracted from bilingual parallel corpora totaling over 100 million sentence pairs and over 2 billion English words. We also release PPDB:Spa, a collection of 196 million Spanish paraphrases. Each paraphrase pair in PPDB contains a set of associated scores, including paraphrase probabilities derived from the bitext data and a variety of monolingual distributional similarity scores computed from the Google n-grams and the Annotated Gigaword corpus. Our release includes pruning tools that allow users to determine their own precision/recall tradeoff.",
    "authors": [
        {
            "affiliations": [],
            "name": "Juri Ganitkevitch"
        },
        {
            "affiliations": [],
            "name": "Benjamin Van Durme"
        },
        {
            "affiliations": [],
            "name": "Chris Callison-Burch"
        }
    ],
    "id": "SP:a94636f4676a6d4ff6df0a7f5fb7b89fecdefb9f",
    "references": [
        {
            "authors": [
                "Alfred V. Aho",
                "Jeffrey D. Ullman."
            ],
            "title": "The Theory of Parsing, Translation, and Compiling",
            "venue": "Prentice Hall.",
            "year": 1972
        },
        {
            "authors": [
                "Colin Bannard",
                "Chris Callison-Burch."
            ],
            "title": "Paraphrasing with bilingual parallel corpora",
            "venue": "Proceedings of ACL.",
            "year": 2005
        },
        {
            "authors": [
                "Jonathan Berant",
                "Jacob Goldberger",
                "Ido Dagan."
            ],
            "title": "Global learning of typed entailment rules",
            "venue": "Proceedings of ACL.",
            "year": 2011
        },
        {
            "authors": [
                "Rahul Bhagat",
                "Deepak Ravichandran."
            ],
            "title": "Large scale acquisition of paraphrases for learning surface patterns",
            "venue": "Proceedings of ACL/HLT.",
            "year": 2008
        },
        {
            "authors": [
                "Chris Callison-Burch",
                "Philipp Koehn",
                "Christof Monz",
                "Josh Schroeder."
            ],
            "title": "Findings of the 2009 Workshop on Statistical Machine Translation",
            "venue": "Proceedings of WMT, pages 1\u201328, Athens, Greece, March.",
            "year": 2009
        },
        {
            "authors": [
                "Tsz Ping Chan",
                "Chris Callison-Burch",
                "Benjamin Van Durme."
            ],
            "title": "Reranking bilingually extracted paraphrases using monolingual distributional similarity",
            "venue": "EMNLP Workshop on GEMS.",
            "year": 2011
        },
        {
            "authors": [
                "David Chiang."
            ],
            "title": "A hierarchical phrase-based model for statistical machine translation",
            "venue": "Proceedings of ACL.",
            "year": 2005
        },
        {
            "authors": [
                "Kenneth Church",
                "Patrick Hanks."
            ],
            "title": "Word association norms, mutual information and lexicography",
            "venue": "Computational Linguistics, 6(1):22\u201329.",
            "year": 1991
        },
        {
            "authors": [
                "Trevor Cohn",
                "Mirella Lapata."
            ],
            "title": "Sentence compression beyond word deletion",
            "venue": "Proceedings of the COLING.",
            "year": 2008
        },
        {
            "authors": [
                "Bill Dolan",
                "Chris Quirk",
                "Chris Brockett"
            ],
            "title": "Unsupervised construction of large paraphrase corpora",
            "year": 2004
        },
        {
            "authors": [
                "Andreas Eisele",
                "Yu Chen."
            ],
            "title": "MultiUN: A multilingual corpus from united nation documents",
            "venue": "Proceedings of LREC, Valletta, Malta.",
            "year": 2010
        },
        {
            "authors": [
                "Juri Ganitkevitch",
                "Chris Callison-Burch",
                "Courtney Napoles",
                "Benjamin Van Durme."
            ],
            "title": "Learning sentential paraphrases from bilingual parallel corpora for text-to-text generation",
            "venue": "Proceedings of EMNLP.",
            "year": 2011
        },
        {
            "authors": [
                "Juri Ganitkevitch",
                "Benjamin Van Durme",
                "Chris Callison-Burch."
            ],
            "title": "Monolingual distributional similarity for text-to-text generation",
            "venue": "Proceedings of *SEM. Association for Computational Linguistics.",
            "year": 2012
        },
        {
            "authors": [
                "Paul Kingsbury",
                "Martha Palmer."
            ],
            "title": "From treebank to propbank",
            "venue": "Proceedings of LREC.",
            "year": 2002
        },
        {
            "authors": [
                "Philipp Koehn",
                "Josh Schroeder."
            ],
            "title": "Experiments in domain adaptation for statistical machine translation",
            "venue": "Proceedings of WMT, Prague, Czech Republic, June. Association for Computational Linguistics.",
            "year": 2007
        },
        {
            "authors": [
                "Philipp Koehn."
            ],
            "title": "Europarl: A parallel corpus for statistical machine translation",
            "venue": "MT summit, volume 5.",
            "year": 2005
        },
        {
            "authors": [
                "Philipp Koehn."
            ],
            "title": "Statistical Machine Translation",
            "venue": "Cambridge University Press.",
            "year": 2010
        },
        {
            "authors": [
                "Mirella Lapata",
                "Frank Keller."
            ],
            "title": "Web-based models for natural language processing",
            "venue": "ACM Transactions on Speech and Language Processing, 2(1).",
            "year": 2005
        },
        {
            "authors": [
                "Dekang Lin",
                "Patrick Pantel."
            ],
            "title": "Discovery of inference rules from text",
            "venue": "Natural Language Engineering.",
            "year": 2001
        },
        {
            "authors": [
                "Dekang Lin",
                "Kenneth Church",
                "Heng Ji",
                "Satoshi Sekine",
                "David Yarowsky",
                "Shane Bergsma",
                "Kailash Patil",
                "Emily Pitler",
                "Rachel Lathbury",
                "Vikram Rao",
                "Kapil Dalwani",
                "Sushant Narsale."
            ],
            "title": "New tools for web-scale n-grams",
            "venue": "Proceedings of LREC.",
            "year": 2010
        },
        {
            "authors": [
                "Mitchell P. Marcus",
                "Mary Ann Marcinkiewicz",
                "Beatrice Santorini."
            ],
            "title": "Building a large annotated corpus of english: the Penn Treebank",
            "venue": "Computational Linguistics, 19(2).",
            "year": 1993
        },
        {
            "authors": [
                "Angelo Mendonca",
                "David Andrew Graff",
                "Denise DiPersio."
            ],
            "title": "Spanish Gigaword Second Edition",
            "venue": "Linguistic Data Consortium.",
            "year": 2009
        },
        {
            "authors": [
                "Ndapandula Nakashole",
                "Gerhard Weikum",
                "Fabian Suchanek."
            ],
            "title": "PATTY: a taxonomy of relational patterns with semantic types",
            "venue": "Proceedings of EMNLP.",
            "year": 2012
        },
        {
            "authors": [
                "Courtney Napoles",
                "Matt Gormley",
                "Benjamin Van Durme."
            ],
            "title": "Annotated gigaword",
            "venue": "Proceedings of AKBC-WEKEX 2012.",
            "year": 2012
        },
        {
            "authors": [
                "Roberto Navigli",
                "Simone Paolo Ponzetto."
            ],
            "title": "BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network",
            "venue": "Artificial Intelligence, 193.",
            "year": 2012
        },
        {
            "authors": [
                "Chris Quirk",
                "Chris Brockett",
                "William Dolan."
            ],
            "title": "Monolingual machine translation for paraphrase generation",
            "venue": "Proceedings of EMNLP.",
            "year": 2004
        },
        {
            "authors": [
                "Stefan Riezler",
                "Alexander Vasserman",
                "Ioannis Tsochantaridis",
                "Vibhu Mittal",
                "Yi Liu."
            ],
            "title": "Statistical machine translation for query expansion in answer retrieval",
            "venue": "Proceedings of the 45th Annual Meeting of the ACL.",
            "year": 2007
        },
        {
            "authors": [
                "Matthew Snover",
                "Nitin Madnani",
                "Bonnie Dorr",
                "Richard Schwartz."
            ],
            "title": "Ter-plus: paraphrase, semantic, and alignment enhancements to translation edit rate",
            "venue": "Machine Translation, 23(2-3):117\u2013127.",
            "year": 2010
        },
        {
            "authors": [
                "Rion Snow",
                "Daniel Jurafsky",
                "Andrew Y. Ng."
            ],
            "title": "Semantic taxonomy induction from heterogenous evidence",
            "venue": "Proceedings of the ACL/Coling.",
            "year": 2006
        },
        {
            "authors": [
                "Ralf Steinberger",
                "Bruno Pouliquen",
                "Anna Widiger",
                "Camelia Ignat",
                "Tomaz Erjavec",
                "Dan Tufis",
                "D\u00e1niel Varga."
            ],
            "title": "The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages",
            "venue": "Proceedings of LREC, Genoa, Italy.",
            "year": 2006
        },
        {
            "authors": [
                "J\u00f6rg Tiedemann."
            ],
            "title": "News from OPUS: A collection of multilingual parallel corpora with tools and interfaces",
            "venue": "Recent Advances in Natural Language Processing, volume 5.",
            "year": 2009
        },
        {
            "authors": [
                "Benjamin Van Durme",
                "Ashwin Lall."
            ],
            "title": "Online generation of locality sensitive hash signatures",
            "venue": "Proceedings of ACL, Short Papers.",
            "year": 2010
        },
        {
            "authors": [
                "Shiqi Zhao",
                "Cheng Niu",
                "Ming Zhou",
                "Ting Liu",
                "Sheng Li."
            ],
            "title": "Combining multiple resources to improve SMT-based paraphrasing model",
            "venue": "Proceedings of ACL/HLT.",
            "year": 2008
        },
        {
            "authors": [
                "Liang Zhou",
                "Chin-Yew Lin",
                "Dragos Stefan Munteanu",
                "Eduard Hovy."
            ],
            "title": "Paraeval: Using paraphrases to evaluate summaries automatically",
            "venue": "Proceedings of HLT/NAACL.",
            "year": 2006
        }
    ],
    "sections": [
        {
            "text": "We present the 1.0 release of our paraphrase database, PPDB. Its English portion, PPDB:Eng, contains over 220 million paraphrase pairs, consisting of 73 million phrasal and 8 million lexical paraphrases, as well as 140 million paraphrase patterns, which capture many meaning-preserving syntactic transformations. The paraphrases are extracted from bilingual parallel corpora totaling over 100 million sentence pairs and over 2 billion English words. We also release PPDB:Spa, a collection of 196 million Spanish paraphrases. Each paraphrase pair in PPDB contains a set of associated scores, including paraphrase probabilities derived from the bitext data and a variety of monolingual distributional similarity scores computed from the Google n-grams and the Annotated Gigaword corpus. Our release includes pruning tools that allow users to determine their own precision/recall tradeoff."
        },
        {
            "heading": "1 Introduction",
            "text": "Paraphrases, i.e. differing textual realizations of the same meaning, have proven useful for a wide variety of natural language processing applications. Past paraphrase collections include automatically derived resources like DIRT (Lin and Pantel, 2001), the MSR paraphrase corpus and phrase table (Dolan et al., 2004; Quirk et al., 2004), among others. Although several groups have independently extracted paraphrases using Bannard and CallisonBurch (2005)\u2019s bilingual pivoting technique (see Zhou et al. (2006), Riezler et al. (2007), Snover et al. (2010), among others), there has never been an official release of this resource.\nIn this work, we release version 1.0 of the ParaPhrase DataBase PPDB,1 a collection of ranked English and Spanish paraphrases derived by:\n\u2022 Extracting lexical, phrasal, and syntactic paraphrases from large bilingual parallel corpora (with associated paraphrase probabilities).\n\u2022 Computing distributional similarity scores for each of the paraphrases using the Google ngrams and the Annotated Gigaword corpus.\nIn addition to the paraphrase collection itself, we provide tools to filter PPDB to only retain high precision paraphrases, scripts to limit the collection to phrasal or lexical paraphrases (synonyms), and software that enables users to extract paraphrases for languages other than English."
        },
        {
            "heading": "2 Extracting Paraphrases from Bitexts",
            "text": "To extract paraphrases we follow Bannard and Callison-Burch (2005)\u2019s bilingual pivoting method. The intuition is that two English strings e1 and e2 that translate to the same foreign string f can be assumed to have the same meaning. We can thus pivot over f and extract he1, e2i as a pair of paraphrases, as illustrated in Figure 1. The method extracts a diverse set of paraphrases. For thrown into jail, it extracts arrested, detained, imprisoned, incarcerated, jailed, locked up, taken into custody, and thrown into prison, along with a set of incorrect/noisy paraphrases that have different syntactic types or that are due to misalignments.\nFor PPDB, we formulate our paraphrase collection as a weighted synchronous context-free grammar (SCFG) (Aho and Ullman, 1972; Chiang, 2005)\n1Freely available at http://paraphrase.org.\nwith syntactic nonterminal labels, similar to Cohn and Lapata (2008) and Ganitkevitch et al. (2011). An SCFG rule has the form:\nr def = C ! hf, e,\u21e0, ~'i,\nwhere the left-hand side of the rule, C, is a nonterminal and the right-hand sides f and e are strings of terminal and nonterminal symbols. There is a one-toone correspondence, \u21e0, between the nonterminals in f and e: each nonterminal symbol in f has to also appear in e. Following Zhao et al. (2008), each rule r is annotated with a vector of feature functions ~' = {'1...'N} which are combined in a log-linear model (with weights ~ ) to compute the cost of applying r:\ncost(r) = NX\ni=1\ni log 'i. (1)\nTo create a syntactic paraphrase grammar we first extract a foreign-to-English translation grammar from a bilingual parallel corpus, using techniques from syntactic machine translation (Koehn, 2010). Then, for each pair of translation rules where the left-hand side C and foreign string f match:\nr1 def = C ! hf, e1,\u21e01, ~'1i\nr2 def = C ! hf, e2,\u21e02, ~'2i,\nwe pivot over f to create a paraphrase rule rp:\nrp def = C ! he1, e2,\u21e0p, ~'pi,\nwith a combined nonterminal correspondency function \u21e0p. Note that the common source side f implies that e1 and e2 share the same set of nonterminal symbols.\nThe paraphrase rules obtained using this method are capable of making well-formed generalizations of meaning-preserving rewrites in English. For instance, we extract the following example paraphrase, capturing the English possessive rule:\nNP ! the NP1 of NNS 2 | the NNS2 \u2019s NP1.\nThe paraphrase feature vector ~'p is computed from the translation feature vectors ~'1 and ~'2 by following the pivoting idea. For instance, we estimate the conditional paraphrase probability p(e2|e1) by marginalizing over all shared foreign-language translations f :\np(e2|e1) \u21e1 X\nf\np(e2|f)p(f |e1). (2)"
        },
        {
            "heading": "3 Scoring Paraphrases Using Monolingual",
            "text": "Distributional Similarity\nThe bilingual pivoting approach anchors paraphrases that share an interpretation because of a shared foreign phrase. Paraphrasing methods based on monolingual text corpora, like DIRT (Lin and Pantel, 2001), measure the similarity of phrases based on distributional similarity. This results in a range of different types of phrases, including paraphrases, inference rules and antonyms. For instance, for thrown into prison DIRT extracts good paraphrases like arrested, detained, and jailed. However, it also extracts phrases that are temporarily or causally related like began the trial of, cracked down on, interrogated, prosecuted and ordered the execution of, because they have similar distributional properties. Since bilingual pivoting rarely extracts these non-paraphrases, we can use monolingual distributional similarity to re-rank paraphrases extracted from bitexts (following Chan et al. (2011)) or incorporate a set of distributional similarity scores as features in our log-linear model.\nEach similarity score relies on precomputed distributional signatures that describe the contexts that a phrase occurs in. To describe a phrase e, we gather counts for a set of contextual features for each occurrence of e in a corpus. Writing the context vector for the i-th occurrence of e as ~se,i, we can aggregate over all occurrences of e, resulting in a distributional signature for e, ~se = P i ~se,i. Following the intuition that phrases with similar meanings occur in\nthe long-term\nachieve25\ngoals 23\nplans 97\ninvestment 10\nconfirmed64\nrevise43 the long-term\nthe long-term the long-term\nthe long-term the long-term .. ..\nL-achieve = 25\nL-confirmed = 64\nL-revise = 43\nR-goals = 23\nR-plans = 97\nR-investment = 10\nthe long-term = ~ sig\n(a) The n-gram corpus records the long-term as preceded by revise (43 times), and followed by plans (97 times). We add corresponding features to the phrase\u2019s distributional signature retaining the counts of the original n-grams.\nlong-term investment holding on to\ndet amod\nthe\nJJ NN VBG IN TO DT\nNP PP\nVP\nthe long-term = ~ sig\ndep-det-R-investment\npos-L-TO\npos-R-NN\nlex-R-investment\nlex-L-to\ndep-amod-R-investment\nsyn-gov-NP syn-miss-L-NN\nlex-L-on-to\npos-L-IN-TO\ndep-det-R-NN dep-amod-R-NN\n(b) Here, position-aware lexical and part-of-speech ngram features, labeled dependency links , and features reflecting the phrase\u2019s CCG-style label NP/NN are included in the context vector.\nFigure 2: Features extracted for the phrase the long term from the n-gram corpus (2a) and Annotated Gigaword (2b).\nsimilar contexts, we can then quantify the goodness of e0 as a paraphrase of e by computing the cosine similarity between their distributional signatures:\nsim(e, e0) = ~se \u00b7 ~se0 |~se||~se0 | .\nA wide variety of features have been used to describe the distributional context of a phrase. Rich, linguistically informed feature-sets that rely on dependency and constituency parses, part-of-speech tags, or lemmatization have been proposed in work such as by Church and Hanks (1991) and Lin and Pantel (2001). For instance, a phrase is described by the various syntactic relations such as: \u201cwhat verbs have this phrase as the subject?\u201d, or \u201cwhat adjectives modify this phrase?\u201d. Other work has used simpler n-gram features, e.g. \u201cwhat words or bigrams have we seen to the left of this phrase?\u201d. A substantial body of work has focussed on using this type of feature-set for a variety of purposes in NLP (Lapata and Keller, 2005; Bhagat and Ravichandran, 2008; Lin et al., 2010; Van Durme and Lall, 2010).\nFor PPDB, we compute n-gram-based context signatures for the 200 million most frequent phrases in the Google n-gram corpus (Brants and Franz, 2006; Lin et al., 2010), and richer linguistic signatures for 175 million phrases in the Annotated Gigaword corpus (Napoles et al., 2012). Our features extend beyond those previously used in the work by Ganitkevitch et al. (2012). They are:\n\u2022 n-gram based features for words seen to the left and right of a phrase.\n\u2022 Position-aware lexical, lemma-based, part-ofspeech, and named entity class unigram and bigram features, drawn from a three-word window to the right and left of the phrase.\n\u2022 Incoming and outgoing (wrt. the phrase) dependency link features, labeled with the corresponding lexical item, lemmata and POS.\n\u2022 Syntactic features for any constituents governing the phrase, as well as for CCG-style slashed constituent labels for the phrase.\nFigure 2 illustrates the feature extraction for an example phrase."
        },
        {
            "heading": "4 English Paraphrases \u2013 PPDB:Eng",
            "text": "We combine several English-to-foreign bitext corpora to extract PPDB:Eng: Europarl v7 (Koehn, 2005), consisting of bitexts for the 19 European languages, the 109 French-English corpus (CallisonBurch et al., 2009), the Czech, German, Spanish and French portions of the News Commentary data (Koehn and Schroeder, 2007), the United Nations French- and Spanish-English parallel corpora (Eisele and Chen, 2010), the JRC Acquis corpus (Steinberger et al., 2006), Chinese and Arabic\nnewswire corpora used for the GALE machine translation campaign,2 parallel Urdu-English data from the NIST translation task,3 the French portion of the OpenSubtitles corpus (Tiedemann, 2009), and a collection of Spanish-English translation memories provided by TAUS.4\nThe resulting composite parallel corpus has more than 106 million sentence pairs, over 2 billion English words, and spans 22 pivot languages. To apply the pivoting technique to this multilingual data, we treat the various pivot languages as a joint NonEnglish language. This simplifying assumption allows us to share statistics across the different languages and apply Equation 2 unaltered.\nTable 1 presents a breakdown of PPDB:Eng by paraphrase type. We distinguish lexical (a single word), phrasal (a continuous string of words), and syntactic paraphrases (expressions that may contain both words and nonterminals), and separate out identity paraphrases. While we list lexical and phrasal paraphrases separately, it is possible that a single word paraphrases as a multi-word phrase and vice versa \u2013 so long they share the same syntactic label."
        },
        {
            "heading": "5 Spanish Paraphrases \u2013 PPDB:Spa",
            "text": "We also release a collection of Spanish paraphrases: PPDB:Spa is extracted analogously to its English counterpart and leverages the Spanish portions of the bitext data available to us, totaling almost 355 million Spanish words, in nearly 15 million sentence pairs. The paraphrase pairs in PPDB:Spa are anno-\n2 http://projects.ldc.upenn.edu/gale/\ndata/Catalog.html\n3LDC Catalog No. LDC2010T23 4 http://www.translationautomation.com/\ntated with distributional similarity scores based on lexical features collected from the Spanish portion of the multilingual release of the Google n-gram corpus (Brants and Franz, 2009), and the Spanish Gigaword corpus (Mendonca et al., 2009). Table 2 gives a breakdown of PPDB:Spa."
        },
        {
            "heading": "6 Analysis",
            "text": "To estimate the usefulness of PPDB as a resource for tasks like semantic role labeling or parsing, we analyze its coverage of Propbank predicates and predicate-argument tuples (Kingsbury and Palmer, 2002). We use the Penn Treebank (Marcus et al., 1993) to map Propbank annotations to patterns which allow us to search PPDB:Eng for paraphrases that match the annotated predicate. Figure 3 illus-\ntrates this mapping. In order to quantify PPDB\u2019s precision-recall tradeoff in this context, we perform a sweep over our collection, beginning with the full set of paraphrase pairs and incrementally discarding the lowest-scoring ones. We choose a simple estimate for each paraphrase pair\u2019s score by uniformly combining its paraphrase probability features in Eq. 1.\nThe top graph in Figure 4a shows PPDB\u2019s coverage of predicates (e.g. VBP ! expect) at the type level (i.e. counting distinct predicates), as well as the token level (i.e. counting predicate occurrences in the corpus). We also keep track of average number of paraphrases per covered predicate type for varying pruning levels. We find that PPDB has a predicate type recall of up to 52% (accounting for 97.5% of tokens). Extending the experiment to full predicate-argument relations with up to two arguments (e.g. S ! NNS expect S), we obtain a 27% type coverage rate that accounts for 40% of tokens (Figure 4b). Both rates hold even as we prune the database down to only contain high precision paraphrases. Our pruning method here is based on a simple uniform combination of paraphrase probabilities and similarity scores.\nTo gauge the quality of our paraphrases, the authors judged 1900 randomly sampled predicate paraphrases on a scale of 1 to 5, 5 being the best. The bottom graph in Figure 4a plots the resulting human score average against the sweep used in the cover-\nage experiment. It is clear that even with a simple weighing approach, the PPDB scores show a clear correlation with human judgements. Therefore they can be used to bias the collection towards greater recall or higher precision."
        },
        {
            "heading": "7 Conclusion and Future Work",
            "text": "We present the 1.0 release of PPDB:Eng and PPDB:Spa, two large-scale collections of paraphrases in English and Spanish. We illustrate the resource\u2019s utility with an analysis of its coverage of Propbank predicates. Our results suggest that PPDB will be useful in a variety of NLP applications.\nFuture releases of PPDB will focus on expanding the paraphrase collection\u2019s coverage with regard to both data size and languages supported. Furthermore, we intend to improve paraphrase scoring by incorporating additional sources of information, as well as by better utilizing information present in the data, like domain or topic. We will also address points of refinement such as handling of phrase ambiguity, and effects specific to individual pivot languages. Our aim is for PPDB to be a continuously updated and improving resource.\nFinally, we will explore extensions to PPDB to include aspects of related large-scale resources such as lexical-semantic hierarchies (Snow et al., 2006), textual inference rules (Berant et al., 2011), relational patterns (Nakashole et al., 2012), and (lexical) conceptual networks (Navigli and Ponzetto, 2012)."
        },
        {
            "heading": "Acknowledgements",
            "text": "We would like to thank Frank Ferraro for his Propbank processing tools. This material is based on research sponsored by the NSF under grant IIS-1249516 and DARPA under agreement number FA8750-13-2-0017 (the DEFT program). The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes. The views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or endorsements of DARPA or the U.S. Government."
        }
    ],
    "title": "PPDB: The Paraphrase Database",
    "year": 2013
}