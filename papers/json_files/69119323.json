{
    "abstractText": "We introduce a loss for metric learning, which is inspired by the Lowe\u2019s matching criterion for SIFT. We show that the proposed loss, that maximizes the distance between the closest positive and closest negative example in the batch, is better than complex regularization methods; it works well for both shallow and deep convolution network architectures. Applying the novel loss to the L2Net CNN architecture results in a compact descriptor named HardNet. It has the same dimensionality as SIFT (128) and shows state-of-art performance in wide baseline stereo, patch verification and instance retrieval benchmarks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Anastasiya Mishchuk"
        },
        {
            "affiliations": [],
            "name": "Dmytro Mishkin"
        },
        {
            "affiliations": [],
            "name": "Filip Radenovi\u0107"
        },
        {
            "affiliations": [],
            "name": "Ji\u0159i Matas"
        }
    ],
    "id": "SP:00d2e3b8d882aa0735aaa914fb6c4c7bddf88043",
    "references": [
        {
            "authors": [
                "Josef Sivic",
                "Andrew Zisserman"
            ],
            "title": "Video google: A text retrieval approach to object matching in videos",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2003
        },
        {
            "authors": [
                "Filip Radenovic",
                "Giorgos Tolias",
                "Ondrej Chum"
            ],
            "title": "CNN image retrieval learns from BoW: Unsupervised fine-tuning with hard examples",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2016
        },
        {
            "authors": [
                "Matthew Brown",
                "David G. Lowe"
            ],
            "title": "Automatic panoramic image stitching using invariant features",
            "venue": "International Journal of Computer Vision (IJCV),",
            "year": 2007
        },
        {
            "authors": [
                "Dmytro Mishkin",
                "Jiri Matas",
                "Michal Perdoch",
                "Karel Lenc"
            ],
            "title": "Wxbs: Wide baseline stereo generalizations",
            "venue": "Arxiv 1504.06603,",
            "year": 2015
        },
        {
            "authors": [
                "Johannes L. Schonberger",
                "Filip Radenovic",
                "Ondrej Chum",
                "Jan-Michael Frahm"
            ],
            "title": "From single image query to detailed 3D reconstruction",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2015
        },
        {
            "authors": [
                "Johannes L. Schonberger",
                "Jan-Michael Frahm"
            ],
            "title": "Structure-from-motion revisited",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "Christopher B. Choy",
                "JunYoung Gwak",
                "Silvio Savarese",
                "Manmohan Chandraker"
            ],
            "title": "Universal correspondence network",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Alex Kendall",
                "Matthew Grimes",
                "Roberto Cipolla"
            ],
            "title": "Posenet: A convolutional network for real-time 6-DOF camera relocalization",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2015
        },
        {
            "authors": [
                "David G. Lowe"
            ],
            "title": "Distinctive image features from scale-invariant keypoints",
            "venue": "International Journal of Computer Vision (IJCV),",
            "year": 2004
        },
        {
            "authors": [
                "Relja Arandjelovic",
                "Andrew Zisserman"
            ],
            "title": "Three things everyone should know to improve object retrieval",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2012
        },
        {
            "authors": [
                "Krystian Mikolajczyk",
                "Cordelia Schmid"
            ],
            "title": "Scale & affine invariant interest point detectors",
            "venue": "International Journal of Computer Vision (IJCV),",
            "year": 2004
        },
        {
            "authors": [
                "Ethan Rublee",
                "Vincent Rabaud",
                "Kurt Konolige",
                "Gary Bradski"
            ],
            "title": "ORB: An efficient alternative to SIFT or SURF",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2011
        },
        {
            "authors": [
                "Kwang Moo Yi",
                "Eduard Trulls",
                "Vincent Lepetit",
                "Pascal Fua"
            ],
            "title": "LIFT: Learned invariant feature transform",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2016
        },
        {
            "authors": [
                "Xufeng Han",
                "T. Leung",
                "Y. Jia",
                "R. Sukthankar",
                "A.C. Berg"
            ],
            "title": "Matchnet: Unifying feature and metric learning for patch-based matching",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2015
        },
        {
            "authors": [
                "Sergey Zagoruyko",
                "Nikos Komodakis"
            ],
            "title": "Learning to compare image patches via convolutional neural networks",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2015
        },
        {
            "authors": [
                "Andrei Bursuc",
                "Giorgos Tolias",
                "Herve Jegou"
            ],
            "title": "Kernel local descriptors with implicit rotation matching",
            "venue": "In ACM International Conference on Multimedia Retrieval,",
            "year": 2015
        },
        {
            "authors": [
                "Jingming Dong",
                "Stefano Soatto"
            ],
            "title": "Domain-size pooling in local descriptors: DSP-SIFT",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2015
        },
        {
            "authors": [
                "Vassileios Balntas",
                "Karel Lenc",
                "Andrea Vedaldi",
                "Krystian Mikolajczyk"
            ],
            "title": "HPatches: A benchmark and evaluation of handcrafted and learned local descriptors",
            "venue": "In Conference on Computer Vision and Pattern Recognition",
            "year": 2017
        },
        {
            "authors": [
                "Johannes L. Schonberger",
                "Hans Hardmeier",
                "Torsten Sattler",
                "Marc Pollefeys"
            ],
            "title": "Comparative evaluation of hand-crafted and learned local features",
            "venue": "In Conference on Computer Vision and Pattern Recognition",
            "year": 2017
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrea Vedaldi",
                "Andrew Zisserman"
            ],
            "title": "Descriptor learning using convex optimisation",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2012
        },
        {
            "authors": [
                "Marius Muja",
                "David G. Lowe"
            ],
            "title": "Fast approximate nearest neighbors with automatic algorithm configuration",
            "venue": "In International Conference on Computer Vision Theory and Application (VISSAPP),",
            "year": 2009
        },
        {
            "authors": [
                "Edgar Simo-Serra",
                "Eduard Trulls",
                "Luis Ferraz",
                "Iasonas Kokkinos",
                "Pascal Fua",
                "Francesc Moreno-Noguer"
            ],
            "title": "Discriminative learning of deep convolutional feature point descriptors",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2015
        },
        {
            "authors": [
                "Vassileios Balntas",
                "Edgar Riba",
                "Daniel Ponsa",
                "Krystian Mikolajczyk"
            ],
            "title": "Learning local feature descriptors with triplets and shallow convolutional neural networks",
            "venue": "In British Machine Vision Conference (BMVC),",
            "year": 2016
        },
        {
            "authors": [
                "Bin Fan Yurun Tian",
                "Fuchao Wu"
            ],
            "title": "L2-Net: Deep learning of discriminative patch descriptor in euclidean space",
            "venue": "In Conference on Computer Vision and Pattern Recognition",
            "year": 2017
        },
        {
            "authors": [
                "Sergey Ioffe",
                "Christian Szegedy"
            ],
            "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
            "venue": "ArXiv 1502.03167,",
            "year": 2015
        },
        {
            "authors": [
                "Vinod Nair",
                "Geoffrey E. Hinton"
            ],
            "title": "Rectified linear units improve restricted boltzmann machines",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2010
        },
        {
            "authors": [
                "Nitish Srivastava",
                "Geoffrey E Hinton",
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Ruslan Salakhutdinov"
            ],
            "title": "Dropout: a simple way to prevent neural networks from overfitting",
            "venue": "Journal of Machine Learning Research (JMLR),",
            "year": 1929
        },
        {
            "authors": [
                "Andrew M. Saxe",
                "James L. McClelland",
                "Surya Ganguli"
            ],
            "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
            "venue": "In Proceedings of ICLR,",
            "year": 2014
        },
        {
            "authors": [
                "Vijay Kumar B. G",
                "Gustavo Carneiro",
                "Ian Reid"
            ],
            "title": "Learning local image descriptors with deep siamese and triplet convolutional networks by minimising global loss functions",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "Krystian Mikolajczyk",
                "Tinne Tuytelaars",
                "Cordelia Schmid",
                "Andrew Zisserman",
                "Jiri Matas",
                "Frederik Schaffalitzky",
                "Timor Kadir",
                "Luc Van Gool"
            ],
            "title": "A comparison of affine region detectors",
            "venue": "International Journal of Computer Vision (IJCV),",
            "year": 2005
        },
        {
            "authors": [
                "Arun Mukundan",
                "Giorgos Tolias",
                "Ondrej Chum"
            ],
            "title": "Multiple-kernel local-patch descriptor",
            "venue": "In British Machine Vision Conference,",
            "year": 2017
        },
        {
            "authors": [
                "D. Randall Wilson",
                "Tony R. Martinez"
            ],
            "title": "The general inefficiency of batch training for gradient descent learning",
            "venue": "Neural Networks,",
            "year": 2003
        },
        {
            "authors": [
                "James Philbin",
                "Ondrej Chum",
                "Michael Isard",
                "Josef Sivic",
                "Andrew Zisserman"
            ],
            "title": "Object retrieval with large vocabularies and fast spatial matching",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2007
        },
        {
            "authors": [
                "James Philbin",
                "Ondrej Chum",
                "Michael Isard",
                "Josef Sivic",
                "Andrew Zisserman"
            ],
            "title": "Lost in quantization: Improving particular object retrieval in large scale image databases",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2008
        },
        {
            "authors": [
                "R. Mitra",
                "N. Doiphode",
                "U. Gautam",
                "S. Narayan",
                "S. Ahmed",
                "S. Chandran",
                "A. Jain"
            ],
            "title": "A Large Dataset for Improving Patch Matching",
            "year": 2018
        },
        {
            "authors": [
                "Jiri Matas",
                "Ondrej Chum",
                "Martin Urban",
                "Tomas Pajdla"
            ],
            "title": "Robust wide baseline stereo from maximally stable extrema regions",
            "venue": "In British Machine Vision Conference (BMVC),",
            "year": 2002
        },
        {
            "authors": [
                "Michal Perdoch",
                "Ondrej Chum",
                "Jiri Matas"
            ],
            "title": "Efficient representation of local geometry for large scale object retrieval",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2009
        },
        {
            "authors": [
                "C. Lawrence Zitnick",
                "Krishnan Ramnath"
            ],
            "title": "Edge foci interest points",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2011
        },
        {
            "authors": [
                "Dmytro Mishkin",
                "Jiri Matas",
                "Michal Perdoch"
            ],
            "title": "Mods: Fast and robust method for twoview matching",
            "venue": "Computer Vision and Image Understanding,",
            "year": 2015
        },
        {
            "authors": [
                "Daniel C. Hauagge",
                "Noah Snavely"
            ],
            "title": "Image matching using local symmetry features",
            "venue": "In Computer Vision and Pattern Recognition (CVPR),",
            "year": 2012
        },
        {
            "authors": [
                "Gehua Yang",
                "Charles V Stewart",
                "Michal Sofka",
                "Chia-Ling Tsai"
            ],
            "title": "Registration of challenging image pairs: Initialization, estimation, and decision",
            "venue": "Pattern Analysis and Machine Intelligence (PAMI),",
            "year": 1973
        },
        {
            "authors": [
                "Basura Fernando",
                "Tatiana Tommasi",
                "Tinne Tuytelaars"
            ],
            "title": "Location recognition over large time lags",
            "venue": "Computer Vision and Image Understanding,",
            "year": 2015
        },
        {
            "authors": [
                "Herve Jegou",
                "Matthijs Douze",
                "Cordelia Schmid"
            ],
            "title": "Improving bag-of-features for large scale image search",
            "venue": "International Journal of Computer Vision (IJCV),",
            "year": 2010
        },
        {
            "authors": [
                "Giorgos Tolias",
                "Herve Jegou"
            ],
            "title": "Visual query expansion with or without geometry: refining local descriptors by feature aggregation",
            "venue": "Pattern Recognition,",
            "year": 2014
        },
        {
            "authors": [
                "Herve Jegou",
                "Matthijs Douze",
                "Cordelia Schmid"
            ],
            "title": "On the burstiness of visual elements",
            "venue": "In Computer Vision and Pattern Recognition (CVPR),",
            "year": 2009
        },
        {
            "authors": [
                "Herve Jegou",
                "Cordelia Schmid",
                "Hedi Harzallah",
                "Jakob Verbeek"
            ],
            "title": "Accurate image search using the contextual dissimilarity measure",
            "venue": "Pattern Analysis and Machine Intelligence (PAMI),",
            "year": 2010
        },
        {
            "authors": [
                "Andrej Mikulik",
                "Michal Perdoch",
                "Ond\u0159ej Chum",
                "Ji\u0159\u00ed Matas"
            ],
            "title": "Learning vocabularies over a fine quantization",
            "venue": "International Journal of Computer Vision (IJCV),",
            "year": 2013
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Many computer vision tasks rely on finding local correspondences, e.g. image retrieval [1, 2], panorama stitching [3], wide baseline stereo [4], 3D-reconstruction [5, 6]. Despite the growing number of attempts to replace complex classical pipelines with end-to-end learned models, e.g., for image matching [7], camera localization [8], the classical detectors and descriptors of local patches are still in use, due to their robustness, efficiency and their tight integration. Moreover, reformulating the task, which is solved by the complex pipeline as a differentiable end-to-end process is highly challenging.\nAs a first step towards end-to-end learning, hand-crafted descriptors like SIFT [9, 10] or detectors [9, 11, 12] have been replace with learned ones, e.g., LIFT [13], MatchNet [14] and DeepCompare [15]. However, these descriptors have not gained popularity in practical applications despite good performance in the patch verification task. Recent studies have confirmed that SIFT and its variants (RootSIFT-PCA [16], DSP-SIFT [17]) significantly outperform learned descriptors in image matching and small-scale retrieval [18], as well as in 3D-reconstruction [19]. One of the conclusions made in [19] is that current local patches datasets are not large and diverse enough to allow the learning of a high-quality widely-applicable descriptor.\nIn this paper, we focus on descriptor learning and, using a novel method, train a convolutional neural network (CNN), called HardNet. We additionally show that our learned descriptor significantly outperforms both hand-crafted and learned descriptors in real-world tasks like image retrieval and two view matching under extreme conditions. For the training, we use the standard patch correspondence data thus showing that the available datasets are sufficient for going beyond the state of the art."
        },
        {
            "heading": "2 Related work",
            "text": "Classical SIFT local feature matching consists of two parts: finding nearest neighbors and comparing the first to second nearest neighbor distance ratio threshold for filtering false positive matches. To\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nar X\niv :1\n70 5.\n10 87\n2v 4\n[ cs\n.C V\n] 1\n2 Ja\nbest of our knowledge, no work in local descriptor learning fully mimics such strategy as the learning objective.\nSimonyan and Zisserman [20] proposed a simple filter plus pooling scheme learned with convex optimization to replace the hand-crafted filters and poolings in SIFT. Han et al. [14] proposed a twostage siamese architecture \u2013 for embedding and for two-patch similarity. The latter network improved matching performance, but prevented the use of fast approximate nearest neighbor algorithms like kd-tree [21]. Zagoruyko and Komodakis [15] have independently presented similar siamese-based method which explored different convolutional architectures. Simo-Serra et al [22] harnessed hardnegative mining with a relative shallow architecture that exploited pair-based similarity.\nThe three following papers have most closedly followed the classical SIFT matching scheme. Balntas et al [23] used a triplet margin loss and a triplet distance loss, with random sampling of the patch triplets. They show the superiority of the triplet-based architecture over a pair based. Although, unlike SIFT matching or our work, they sampled negatives randomly. Choy et al [7] calculate the distance matrix for mining positive as well as negative examples, followed by pairwise contrastive loss.\nTian et al [24] use n matching pairs in batch for generating n2 \u2212 n negative samples and require that the distance to the ground truth matchings is minimum in each row and column. No other constraint on the distance or distance ratio is enforced. Instead, they propose a penalty for the correlation of the descriptor dimensions and adopt deep supervision [25] by using intermediate feature maps for matching. Given the state-of-art performance, we have adopted the L2Net [24] architecture as base for our descriptor. We show that it is possible to learn even more powerful descriptor with significantly simpler learning objective without need of the two auxiliary loss terms."
        },
        {
            "heading": "3 The proposed descriptor",
            "text": ""
        },
        {
            "heading": "3.1 Sampling and loss",
            "text": "Our learning objective mimics SIFT matching criterion. The process is shown in Figure 1. First, a batch X = (Ai, Pi)i=1..n of matching local patches is generated, where A stands for the anchor and P for the positive. The patches Ai and Pi correspond to the same point on 3D surface. We make sure that in batch X , there is exactly one pair originating from a given 3D point. Second, the 2n patches in X are passed through the network shown in Figure 2.\nL2 pairwise distance matrix D = cdist(a, p), where, d(ai, pj) = \u221a\n2\u2212 2aipj , i = 1..n, j = 1..n of size n\u00d7 n is calculated, where ai and pj denote the descriptors of patches Ai and Pj respectively.\nNext, for each matching pair ai and pi the closest non-matching descriptors i.e. the 2nd nearest neighbor, are found respectively:\nai \u2013 anchor descriptor, pi \u2013 positive descriptor,\npjmin \u2013 closest non-matching descriptor to ai, where jmin = argminj=1..n,j 6=i d(ai, pj),\nakmin \u2013 closest non-matching descriptor to pi where kmin = argmink=1..n,k 6=i d(ak, pi).\nThen from each quadruplet of descriptors (ai, pi, pjmin , akmin), a triplet is formed: (ai, pi, pjmin), if d(ai, pjmin) < d(akmin , pi) and (pi, ai, akmin) otherwise.\nOur goal is to minimize the distance between the matching descriptor and closest non-matching descriptor. These n triplet distances are fed into the triplet margin loss:\nL = 1\nn \u2211 i=1,n max (0, 1 + d(ai, pi)\u2212min (d(ai, pjmin), d(akmin , pi))) (1)\nwhere min (d(ai, pjmin), d(akmin , pi) is pre-computed during the triplet construction.\nThe distance matrix calculation is done on GPU and the only overhead compared to the random triplet sampling is the distance matrix calculation and calculating the minimum over rows and columns. Moreover, compared to usual learning with triplets, our scheme needs only two-stream CNN, not three, which results in 30% less memory consumption and computations.\nUnlike in [24], neither deep supervision for intermediate layers is used, nor a constraint on the correlation of descriptor dimensions. We experienced no significant over-fitting."
        },
        {
            "heading": "3.2 Model architecture",
            "text": "The HardNet architecture, Figure 2, is identical to L2Net [24]. Padding with zeros is applied to all convolutional layers, to preserve the spatial size, except to the final one. There are no pooling layers, since we found that they decrease performance of the descriptor. That is why the spatial size is reduced by strided convolutions. Batch normalization [26] layer followed by ReLU [27] non-linearity is added after each layer, except the last one. Dropout [28] regularization with 0.1 dropout rate is applied before the last convolution layer. The output of the network is L2 normalized to produce 128-D descriptor with unit-length. Grayscale input patches with size 32\u00d7 32 pixels are normalized by subtracting the per-patch mean and dividing by the per-patch standard deviation.\nOptimization is done by stochastic gradient descent with learning rate of 0.1, momentum of 0.9 and weight decay of 0.0001. Learning rate was linearly decayed to zero within 10 epochs for the most of the experiments in this paper. Weights were initialized to orthogonally [29] with gain equal to 0.6, biases set to 0.01 Training is done with PyTorch library [30].\nPost-NIPS update. After two major updates of PyTorch library, we were not able to reproduce the results. Therefore, we did a hyperparameter search and we found that increasing the learning rate to 10 and dropout rate to 0.3 leads to better results \u2013 see Figure 5 and Table 1. We have obtained the same results on different machines."
        },
        {
            "heading": "3.3 Model training",
            "text": "UBC Phototour [3], also known as Brown dataset. It consists of three subsets: Liberty, Notre Dame and Yosemite with about 400k normalized 64x64 patches in each. Keypoints were detected by DoG detector and verified by 3D model.\nTest set consists of 100k matching and non-matching pairs for each sequence. Common setup is to train descriptor on one subset and test on two others. Metric is the false positive rate (FPR) at point of 0.95 true positive recall. It was found out by Michel Keller that [14] and [23] evaluation procedure reports FDR (false discovery rate) instead of FPR (false positive rate). To avoid the incomprehension of results we\u2019ve decided to provide both FPR and FDR rates and re-estimated the scores for straight comparison. Results are shown in Table 1. Proposed descriptor outperforms competitors, with training augmentation, or without it. We haven\u2018t included results on multiscale patch sampling or so called \u201ccenter-surrounding\u201d architecture for two reasons. First, architectural choices are beyond the scope of current paper. Second, it was already shown in [24, 31] that \u201ccenter-surrounding\u201d consistently improves results on Brown dataset for different descriptors, while hurts matching performance on other, more realistic setups, e.g., on Oxford-Affine [32] dataset.\nIn the rest of paper we use descriptor trained on Liberty sequence, which is a common practice, to allow a fair comparison. TFeat [23] and L2Net [24] use the same dataset for training."
        },
        {
            "heading": "3.4 Exploring the batch size influence",
            "text": "We study the influence of mini-batch size on the final descriptor performance. It is known that small mini-batches are beneficial to faster convergence and better generalization [34], while large batches allow better GPU utilization. Our loss function design should benefit from seeing more hard negative patches to learn to distinguish them from true positive patches. We report the results for batch sizes 16, 64, 128, 512, 1024, 2048. We trained the model described in Section 3.2 using Liberty sequence of Brown dataset. Results are shown in Figure 3. As expected, model performance improves with increasing the mini-batch size, as more examples are seen to get harder negatives. Although, increasing batch size to more than 512 does not bring significant benefit."
        },
        {
            "heading": "4 Empirical evaluation",
            "text": "Recently, Balntas et al. [23] showed that good performance on patch verification task on Brown dataset does not always mean good performance in the nearest neighbor setup and vice versa. Therefore, we have extensively evaluated learned descriptors on real-world tasks like two view matching and image retrieval.\nWe have selected RootSIFT [10], PCW [33], TFeat-M* [23], and L2Net [24] for direct comparison with our descriptor, as they show the best results on a variety of datasets."
        },
        {
            "heading": "4.1 Patch descriptor evaluation",
            "text": "HPatches [18] is a recent dataset for local patch descriptor evaluation. It consists of 116 sequences of 6 images. The dataset is split into two parts: viewpoint \u2013 59 sequences with significant viewpoint change and illumination \u2013 57 sequences with significant illumination change, both natural and artificial. Keypoints are detected by DoG, Hessian and Harris detectors in the reference image and reprojected to the rest of the images in each sequence with 3 levels of geometric noise: Easy, Hard, and Tough variants. The HPatches benchmark defines three tasks: patch correspondence verification, image matching and small-scale patch retrieval. We refer the reader to the HPatches paper [18] for a detailed protocol for each task.\nResults are shown in Figure 5. L2Net and HardNet have shown similar performance on the patch verification task with a small advantage of HardNet. On the matching task, even the non-augmented version of HardNet outperforms the augmented version of L2Net+ by a noticeable margin. The difference is larger in the TOUGH and HARD setups. Illumination sequences are more challenging than the geometric ones, for all the descriptors. We have trained network with TFeat architecture, but with proposed loss function \u2013 it is denoted as HardTFeat. It outperforms original version in matching and retrieval, while being on par with it on patch verification task.\nIn patch retrieval, relative performance of the descriptors is similar to the matching problem: HardNet beats L2Net+. Both descriptors significantly outperform the previous state-of-the-art, showing the superiority of the selected deep CNN architecture over the shallow TFeat model.\nWe also ran another patch retrieval experiment, varying the number of distractors (non-matching patches) in the retrieval dataset. The results are shown in Figure 4. TFeat descriptor performance, which is comparable to L2Net in the presence of low number distractors, degrades quickly as the size of the the database grows. At about 10,000 its performance drops below SIFT. This experiment explains why TFeat performs relatively poorly on the Oxford5k [35] and Paris6k [36] benchmarks, which contain around 12M and 15M distractors, respectively, see Section 4.4 for more details. Performance of the HardNet decreases slightly for both augmented and plain version and the difference in mAP to other descriptors grows with the increasing complexity of the task.\nPost-NIPS update. We have studied impact of training dataset on descriptor performance. In particular, we compared commonly used Liberty subset with patches extracted by DoG detector, full Brown dataset \u2013 subsets Liberty, Notredame and Yosemite with patches, extracted by DoG and Harris detectors. We also included results by Mitra et al., who trained HardNet on new large scale PhotoSync dataset. Results are shown in Figure 6."
        },
        {
            "heading": "4.2 Ablation study",
            "text": "For better understanding of the significance of the sampling strategy and the loss function, we conduct experiments summarized in Table 2. We train our HardNet model (architecture is exactly the same as L2Net model), change one parameter at a time and evaluate its impact.\nThe following sampling strategies are compared: random, the proposed \u201chardest-in-batch\u201d, and the \u201cclassical\u201d hard negative mining, i.e. selecting in each epoch the closest negatives from the full training set. The following loss functions are tested: softmin on distances, triplet margin with margin m = 1, contrastive with margins m = 1,m = 2. The last is the maximum possible distance for unit-normed descriptors. Mean mAP on HPatches Matching task is shown in Table 2.\nThe proposed \u201chardest-in-batch\u201d clearly outperforms all the other sampling strategies for all loss functions and it is the main reason for HardNet good performance. The random sampling and \u201cclassical\u201d hard negative mining led to huge overfit, when training loss was high, but test performance was low and varied several times from run to run. This behavior was observed with all loss function.\nEASY HARD TOUGH\nDIFFSEQ SAMESEQ VIEWPT ILLUM\n91.81%\n89.29%\n88.43%\n87.12%\n0 50 100\nPatch Verification mAP [%]\nLibertyNIPS\nLiberty\nFullBrown6\nPhotoSync 58.01%\n54.64%\n52.76%\n50.38%\n0 20 40 60 80 100\nImage Matching mAP [%]\nLibertyNIPS\nLiberty\nFullBrown6\nPhotoSync 75.16%\n72.66%\n69.66%\n66.82%\n0 50 100\nPatch Retrieval mAP [%]\nLibertyNIPS\nLiberty\nFullBrown6\nPhotoSync\nFigure 6: Comparison of HardNet versions, trained on different datasets. LibertyNIPS, Liberty \u2013 Liberty[3], FullBrown6 \u2013 all six subsets of Brown dataset [3], PhotoSync \u2013 new dataset from Mitra et al. [37]. Left to right: Verification, matching and retrieval results on HPatches dataset. Marker color indicates the level of geometrical noise in: EASY, HARD and TOUGH. Marker type indicates the experimental setup. DIFFSEQ and SAMESEQ shows the source of negative examples for the verification task. VIEWPT and ILLUM indicate the type of sequences for matching. None of the descriptors above was trained on HPatches. HardNet++ results are not presented since it was trained on HPatches.\nSimilar results for random sampling were reported in [24]. The poor results of hard negative mining (\u201chardest-in-the-training-set\u201d) are surprising. We guess that this is due to dataset label noise, the mined \u201chard negatives\u201d are actually positives. Visual inspection confirms this. We were able to get reasonable results with random and hard negative mining sampling only with additional correlation penalty on descriptor channels (CPR), as proposed in [24].\nRegarding the loss functions, softmin gave the most stable results across all sampling strategies, but it is marginally outperformed by contrastive and triplet margin loss for our strategy. One possible explanation is that the triplet margin loss and contrastive loss with a large margin have constant non-zero derivative w.r.t to both positive and negative samples, see Figure 7. In the case of contrastive loss with a small margin, many negative examples are not used in the optimization (zero derivatives), while the softmin derivatives become small, once the distance to the positive example is smaller than to the negative one."
        },
        {
            "heading": "4.3 Wide baseline stereo",
            "text": "To validate descriptor generalization and their ability to operate in extreme conditions, we tested them on the W1BS dataset [4]. It consists of 40 image pairs with one particular extreme change between the images:\nAppearance (A): difference in appearance due to seasonal or weather change, occlusions, etc; Geometry (G): difference in scale, camera and object position; Illumination (L): significant difference in intensity, wavelength of light source; Sensor (S): difference in sensor data (IR, MRI).\nMoreover, local features in W1BS dataset are detected with MSER [38], Hessian-Affine [11] (in implementation from [39]) and FOCI [40] detectors. They fire on different local structures than DoG. Note that DoG patches were used for the training of the descriptors. Another significant difference to\nthe HPatches setup is the absence of the geometrical noise: all patches are perfectly reprojected to the target image in pair. The testing protocol is the same as for the HPatches matching task.\nResults are shown in Figure 8. HardNet and L2Net perform comparably, former is performing better on images with geometrical and appearance changes, while latter works a bit better in map2photo and visible-vs-infrared pairs. Both outperform SIFT, but only by a small margin. However, considering the significant amount of the domain shift, descriptors perform very well, while TFeat loses badly to SIFT. HardTFeat significantly outperforms the original TFeat descriptor on the W1BS dataset, showing the superiority of the proposed loss.\nGood performance on patch matching and verification task does not automatically lead to the better performance in practice, e.g. to more images registered. Therefore we also compared descriptor on wide baseline stereo setup with two metric: number of successfully matched image pairs and average number of inliers per matched pair, following the matcher comparison protocol from [4]. The only change to the original protocol is that first fast matching steps with ORB detector and descriptor were removed, as we are comparing \u201cSIFT-replacement\u201d descriptors.\nThe results are shown in Table 3. Results on Edge Foci (EF) [40], Extreme view [41] and Oxford Affine [11] datasets are saturated and all the descriptors are good enough for matching all image pairs. HardNet has an a slight advantage in a number of inliers per image. The rest of datasets: SymB [42], GDB [43], WxBS [4] and LTLL [44] have one thing in common: image pairs are or from different domain than photo (e.g. drawing to drawing) or cross-domain (e.g., drawing to photo). Here HardNet outperforms learned descriptors and is on-par with hand-crafted RootSIFT. We would like to note that HardNet was not learned to match in different domain, nor cross-domain scenario, therefore such results show the generalization ability."
        },
        {
            "heading": "4.4 Image retrieval",
            "text": "We evaluate our method, and compare against the related ones, on the practical application of image retrieval with local features. Standard image retrieval datasets are used for the evaluation, i.e., Oxford5k [35] and Paris6k [36] datasets. Both datasets contain a set of images (5062 for Oxford5k and 6300 for Paris6k) depicting 11 different landmarks together with distractors. For each of the\n11 landmarks there are 5 different query regions defined by a bounding box, constituting 55 query regions per dataset. The performance is reported as mean average precision (mAP) [35].\nIn the first experiment, for each image in the dataset, multi-scale Hessian-affine features [32] are extracted. Exactly the same features are described by ours and all related methods, each of them producing a 128-D descriptor per feature. Then, k-means with approximate nearest neighbor [21] is used to learn a 1 million visual vocabulary on an independent dataset, that is, when evaluating on Oxford5k, the vocabulary is learned with descriptors of Paris6k and vice versa. All descriptors of testing dataset are assigned to the corresponding vocabulary, so finally, an image is represented by the histogram of visual word occurrences, i.e., the bag-of-words (BoW) [1] representation, and an inverted file is used for an efficient search. Additionally, spatial verification (SV) [35], and standard query expansion (QE) [36] are used to re-rank and refine the search results. Comparison with the related work on patch description is presented in Table 4. HardNet+ and L2Net+ perform comparably across both datasets and all settings, with slightly better performance of HardNet+ on average across all results (average mAP 69.5 vs. 69.1). RootSIFT, which was the best performing descriptor in image retrieval for a long time, falls behind with average mAP 66.0 across all results.\nWe also trained HardNet++ version \u2013 with all available training data at the moment: union of Brown and HPatches datasets, instead of just Liberty sequence from Brown for the HardNet+. It shows the benefits of having more training data and is performing best for all setups.\nFinally, we compare our descriptor with the state-of-the-art image retrieval approaches that use local features. For fairness, all methods presented in Table 5 use the same local feature detector as described before, learn the vocabulary on an independent dataset, and use spatial verification (SV) and query expansion (QE). In our case (HardNet++\u2013HQE), a visual vocabulary of 65k visual words is learned, with additional Hamming embedding (HE) [45] technique that further refines descriptor assignments with a 128 bits binary signature. We follow the same procedure as RootSIFT\u2013HQE [46] method, by replacing RootSIFT with our learned HardNet++ descriptor. Specifically, we use: (i) weighting of the votes as a decreasing function of the Hamming distance [47]; (ii) burstiness suppression [47]; (iii) multiple assignments of features to visual words [36, 48]; and (iv) QE with feature aggregation [46]. All parameters are set as in [46]. The performance of our method is the best reported on both Oxford5k and Paris6k when learning the vocabulary on an independent dataset (mAP 89.1 was reported [10] on Oxford5k by learning it on the same dataset comprising the relevant images), and using the same amount of features (mAP 89.4 was reported [46] on Oxford5k when using twice as many local features, i.e., 22M compared to 12.5M used here)."
        },
        {
            "heading": "5 Conclusions",
            "text": "We proposed a novel loss function for learning a local image descriptor that relies on the hard negative mining within a mini-batch and the maximization of the distance between the closest positive and\nclosest negative patches. The proposed sampling strategy outperforms classical hard-negative mining and random sampling for softmin, triplet margin and contrastive losses.\nThe resulting descriptor is compact \u2013 it has the same dimensionality as SIFT (128), it shows stateof-art performance on standard matching, patch verification and retrieval benchmarks and it is fast to compute on a GPU. The training source code and the trained convnets are available at https://github.com/DagnyT/hardnet."
        },
        {
            "heading": "Acknowledgements",
            "text": "The authors were supported by the Czech Science Foundation Project GACR P103/12/G084, the Austrian Ministry for Transport, Innovation and Technology, the Federal Ministry of Science, Research and Economy, and the Province of Upper Austria in the frame of the COMET center, the CTU student grant SGS17/185/OHK3/3T/13, and the MSMT LL1303 ERC-CZ grant. Anastasiya Mishchuk was supported by the Szkocka Research Group Grant."
        }
    ],
    "title": "Working hard to know your neighbor\u2019s margins: Local descriptor learning loss",
    "year": 2018
}