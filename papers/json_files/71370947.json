{
    "abstractText": "Convolutional networks for image classification progressively reduce resolution until the image is represented by tiny feature maps in which the spatial structure of the scene is no longer discernible. Such loss of spatial acuity can limit image classification accuracy and complicate the transfer of the model to downstream applications that require detailed scene understanding. These problems can be alleviated by dilation, which increases the resolution of output feature maps without reducing the receptive field of individual neurons. We show that dilated residual networks (DRNs) outperform their non-dilated counterparts in image classification without increasing the model\u2019s depth or complexity. We then study gridding artifacts introduced by dilation, develop an approach to removing these artifacts (\u2018degridding\u2019), and show that this further increases the performance of DRNs. In addition, we show that the accuracy advantage of DRNs is further magnified in downstream applications such as object localization and semantic segmentation.",
    "authors": [
        {
            "affiliations": [],
            "name": "Fisher Yu"
        },
        {
            "affiliations": [],
            "name": "Vladlen Koltun"
        },
        {
            "affiliations": [],
            "name": "Thomas Funkhouser"
        }
    ],
    "id": "SP:ecce1bb683c23fb66acab52fa09f666ada5bb3cd",
    "references": [
        {
            "authors": [
                "L.-C. Chen",
                "G. Papandreou",
                "I. Kokkinos",
                "K. Murphy",
                "A.L. Yuille"
            ],
            "title": "DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs",
            "year": 2016
        },
        {
            "authors": [
                "M. Cordts",
                "M. Omran",
                "S. Ramos",
                "T. Rehfeld",
                "M. Enzweiler",
                "R. Benenson",
                "U. Franke",
                "S. Roth",
                "B. Schiele"
            ],
            "title": "The Cityscapes dataset for semantic urban scene understanding",
            "venue": "In CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "C. Galleguillos",
                "S.J. Belongie"
            ],
            "title": "Context based object categorization: A critical survey",
            "venue": "Computer Vision and Image Understanding,",
            "year": 2010
        },
        {
            "authors": [
                "R.B. Girshick",
                "J. Donahue",
                "T. Darrell",
                "J. Malik"
            ],
            "title": "Regionbased convolutional networks for accurate object detection and segmentation",
            "year": 2016
        },
        {
            "authors": [
                "B. Hariharan",
                "P.A. Arbel\u00e1ez",
                "R.B. Girshick",
                "J. Malik"
            ],
            "title": "Hypercolumns for object segmentation and fine-grained localization",
            "venue": "In CVPR,",
            "year": 2015
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In CVPR, 2016",
            "year": 2016
        },
        {
            "authors": [
                "A.G. Howard"
            ],
            "title": "Some improvements on deep convolutional neural network based image classification",
            "year": 2013
        },
        {
            "authors": [
                "A. Krizhevsky",
                "I. Sutskever",
                "G.E. Hinton"
            ],
            "title": "ImageNet classification with deep convolutional neural networks",
            "venue": "In NIPS,",
            "year": 2012
        },
        {
            "authors": [
                "Y. LeCun",
                "B. Boser",
                "J.S. Denker",
                "D. Henderson",
                "R.E. Howard",
                "W. Hubbard",
                "L.D. Jackel"
            ],
            "title": "Backpropagation applied to handwritten zip code recognition",
            "venue": "Neural Computation,",
            "year": 1989
        },
        {
            "authors": [
                "J. Long",
                "E. Shelhamer",
                "T. Darrell"
            ],
            "title": "Fully convolutional networks for semantic segmentation",
            "venue": "In CVPR,",
            "year": 2015
        },
        {
            "authors": [
                "H. Noh",
                "S. Hong",
                "B. Han"
            ],
            "title": "Learning deconvolution network for semantic segmentation",
            "venue": "In ICCV,",
            "year": 2015
        },
        {
            "authors": [
                "O. Russakovsky",
                "J. Deng",
                "H. Su",
                "J. Krause",
                "S. Satheesh",
                "S. Ma",
                "Z. Huang",
                "A. Karpathy",
                "A. Khosla",
                "M.S. Bernstein",
                "A.C. Berg",
                "F. Li"
            ],
            "title": "ImageNet large scale visual recognition challenge",
            "year": 2015
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "In ICLR,",
            "year": 2015
        },
        {
            "authors": [
                "C. Szegedy",
                "W. Liu",
                "Y. Jia",
                "P. Sermanet",
                "S.E. Reed",
                "D. Anguelov",
                "D. Erhan",
                "V. Vanhoucke",
                "A. Rabinovich"
            ],
            "title": "Going deeper with convolutions",
            "venue": "In CVPR,",
            "year": 2015
        },
        {
            "authors": [
                "A. Torralba",
                "R. Fergus",
                "W.T. Freeman"
            ],
            "title": "80 million tiny images: A large data set for nonparametric object and scene recognition",
            "venue": "PAMI, 30(11),",
            "year": 2008
        },
        {
            "authors": [
                "B. Triggs"
            ],
            "title": "Empirical filter estimation for subpixel interpolation and matching",
            "venue": "In ICCV,",
            "year": 2001
        },
        {
            "authors": [
                "P. Wang",
                "P. Chen",
                "Y. Yuan",
                "D. Liu",
                "Z. Huang",
                "X. Hou",
                "G. Cottrell"
            ],
            "title": "Understanding convolution for semantic segmentation",
            "year": 2017
        },
        {
            "authors": [
                "F. Yu",
                "V. Koltun"
            ],
            "title": "Multi-scale context aggregation by dilated convolutions",
            "venue": "In ICLR,",
            "year": 2016
        },
        {
            "authors": [
                "B. Zhou",
                "A. Khosla",
                "\u00c0. Lapedriza",
                "A. Oliva",
                "A. Torralba"
            ],
            "title": "Learning deep features for discriminative localization",
            "venue": "In CVPR,",
            "year": 2016
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Convolutional networks were originally developed for classifying hand-written digits [9]. More recently, convolutional network architectures have evolved to classify much more complex images [8, 13, 14, 6]. Yet a central aspect of network architecture has remained largely in place. Convolutional networks for image classification progressively reduce resolution until the image is represented by tiny feature maps that retain little spatial information (7\u00d77 is typical).\nWhile convolutional networks have done well, the almost complete elimination of spatial acuity may be preventing these models from achieving even higher accuracy, for example by preserving the contribution of small and thin objects that may be important for correctly understanding the image. Such preservation may not have been important in the context of hand-written digit classification, in which a single object dominated the image, but may help in the analysis of complex natural scenes where multiple objects and their relative configurations must be taken into account.\nFurthermore, image classification is rarely a convolutional network\u2019s raison d\u2019e\u0302tre. Image classification is most often a proxy task that is used to pretrain a model before it is transferred to other applications that involve more detailed scene understanding [4, 10]. In such tasks, severe loss of spatial acuity is a significant handicap. Existing techniques compensate for the lost resolution by introducing up-convolutions [10, 11], skip connections [5], and other post-hoc measures.\nMust convolutional networks crush the image in order to classify it? In this paper, we show that this is not necessary, or even desirable. Starting with the residual network architecture, the current state of the art for image classification [6], we increase the resolution of the network\u2019s output by replacing a subset of interior subsampling layers by dilation [18]. We show that dilated residual networks (DRNs) yield improved image classification performance. Specifically, DRNs yield higher accuracy in ImageNet classification than their non-dilated counterparts, with no increase in depth or model complexity.\nThe output resolution of a DRN on typical ImageNet input is 28\u00d728, comparable to small thumbnails that convey the structure of the image when examined by a human [15]. While it may not be clear a priori that average pooling can properly handle such high-resolution output, we show that it can, yielding a notable accuracy gain. We then study gridding artifacts introduced by dilation, propose a scheme for removing these artifacts, and show that such \u2018degridding\u2019 further improves the accuracy of DRNs.\nWe also show that DRNs yield improved accuracy on downstream applications such as weakly-supervised object localization and semantic segmentation. With a remarkably simple approach, involving no fine-tuning at all, we obtain state-of-the-art top-1 accuracy in weakly-supervised localization on ImageNet. We also study the performance of DRNs on semantic segmentation and show, for example, that a 42-layer DRN outperforms a ResNet-101 baseline on the Cityscapes dataset by more than 4 percentage points, despite lower depth by a factor of 2.4.\n1\nar X\niv :1\n70 5.\n09 91\n4v 1\n[ cs\n.C V\n] 2\n8 M\nay 2\n01 7"
        },
        {
            "heading": "2. Dilated Residual Networks",
            "text": "Our key idea is to preserve spatial resolution in convolutional networks for image classification. Although progressive downsampling has been very successful in classifying digits or iconic views of objects, the loss of spatial information may be harmful for classifying natural images and can significantly hamper transfer to other tasks that involve spatially detailed image understanding. Natural images often feature many objects whose identities and relative configurations are important for understanding the scene. The classification task becomes difficult when a key object is not spatially dominant \u2013 for example, when the labeled object is thin (e.g., a tripod) or when there is a big background object such as a mountain. In these cases, the background response may suppress the signal from the object of interest. What\u2019s worse, if the object\u2019s signal is lost due to downsampling, there is little hope to recover it during training. However, if we retain high spatial resolution throughout the model and provide output signals that densely cover the input field, backpropagation can learn to preserve important information about smaller and less salient objects.\nThe starting point of our construction is the set of network architectures presented by He et al. [6]. Each of these architectures consists of five groups of convolutional layers. The first layer in each group performs downsampling by striding: that is, the convolutional filter is only evaluated at even rows and columns. Let each group of layers be denoted by G`, for ` = 1, . . . , 5. Denote the ith layer in group ` by G`i . For simplicity of exposition, consider an idealized model in which each layer consists of a single feature map: the extension to multiple feature maps is straightforward. Let f `i be the filter associated with layer G`i . In the original model, the output of G`i is\n(G`i \u2217 f `i )(p) = \u2211\na+b=p\nG`i (a) f `i (b), (1)\nwhere the domain of p is the feature map in G`i . This is followed by a nonlinearity, which does not affect the presented construction.\nA naive approach to increasing resolution in higher layers of the network would be to simply remove subsampling (striding) from some of the interior layers. This does increase downstream resolution, but has a detrimental side effect that negates the benefits: removing subsampling correspondingly reduces the receptive field in subsequent layers. Thus removing striding such that the resolution of the output layer is increased by a factor of 4 also reduces the receptive field of each output unit by a factor of 4. This severely reduces the amount of context that can inform the prediction produced by each unit. Since contextual information is important in disambiguating local cues [3], such reduction in receptive field is an unacceptable price to pay for higher resolution. For this reason, we use dilated convolutions [18] to\nincrease the receptive field of the higher layers, compensating for the reduction in receptive field induced by removing subsampling. The effect is that units in the dilated layers have the same receptive field as corresponding units in the original model.\nWe focus on the two final groups of convolutional layers: G4 and G5. In the original ResNet, the first layer in each group (G41 and G51 ) is strided: the convolution is evaluated at even rows and columns, which reduces the output resolution of these layers by a factor of 2 in each dimension. The first step in the conversion to DRN is to remove the striding in both G41 and G51 . Note that the receptive field of each unit in G41 remains unaffected: we just doubled the output resolution of G41 without affecting the receptive field of its units. However, subsequent layers are all affected: their receptive fields have been reduced by a factor of 2 in each dimension. We therefore replace the convolution operators in those layers by 2-dilated convolutions [18]:\n(G4i \u22172 f4i )(p) = \u2211\na+2b=p\nG4i (a) f4i (b) (2)\nfor all i \u2265 2. The same transformation is applied to G51 : (G51 \u22172 f51 )(p) = \u2211\na+2b=p\nG51(a) f51 (b). (3)\nSubsequent layers in G5 follow two striding layers that have been eliminated. The elimination of striding has reduced their receptive fields by a factor of 4 in each dimension. Their convolutions need to be dilated by a factor of 4 to compensate for the loss:\n(G5i \u22174 f5i )(p) = \u2211\na+4b=p\nG5i (a) f5i (b) (4)\nfor all i \u2265 2. Finally, as in the original architecture, G5 is followed by global average pooling, which reduces the output feature maps to a vector, and a 1\u00d71 convolution that maps this vector to a vector that comprises the prediction scores for all classes. The transformation of a ResNet into a DRN is illustrated in Figure 1.\nThe converted DRN has the same number of layers and parameters as the original ResNet. The key difference is that the original ResNet downsamples the input image by a factor of 32 in each dimension (a thousand-fold reduction in area), while the DRN downsamples the input by a factor of 8. For example, when the input resolution is 224\u00d7224, the output resolution of G5 in the original ResNet is 7\u00d77, which is not sufficient for the spatial structure of the input to be discernable. The output of G5 in a DRN is 28\u00d728. Global average pooling therefore takes in 24 times more values, which can help the classifier recognize objects that cover a smaller number of pixels in the input image and take such objects into account in its prediction.\nThe presented construction could also be applied to earlier groups of layers (G1, G2, or G3), in the limit retaining the full resolution of the input. We chose not to do this because a downsampling factor of 8 is known to preserve most of the information necessary to correctly parse the original image at pixel level [10]. Furthermore, a 28\u00d728 thumbnail, while small, is sufficiently resolved for humans to discern the structure of the scene [15]. Additional increase in resolution has costs and should not be pursued without commensurate gains: when feature map resolution is increased by a factor of 2 in each dimension, the memory consumption of that feature map increases by a factor of 4. Operating at full resolution throughout, with no downsampling at all, is beyond the capabilities of current hardware."
        },
        {
            "heading": "3. Localization",
            "text": "Given a DRN trained for image classification, we can directly produce dense pixel-level class activation maps without any additional training or parameter tuning. This allows a DRN trained for image classification to be immediately used for object localization and segmentation.\nTo obtain high-resolution class activation maps, we remove the global average pooling operator. We then connect the final 1\u00d71 convolution directly to G5. A softmax is applied to each column in the resulting volume to convert the\npixelwise prediction scores to proper probability distributions. This procedure is illustrated in Figure 2. The output of the resulting network is a set of activation maps that have the same spatial resolution as G5 (28\u00d728). Each classification category y has a corresponding activation map. For each pixel in this map, the map contains the probability that the object observed at this pixel is of category y.\nThe activation maps produced by our construction serve the same purpose as the results of the procedure of Zhou et al. [19]. However, the procedures are fundamentally different. Zhou et al. worked with convolutional networks that produce drastically downsampled output that is not sufficiently resolved for object localization. For this reason, Zhou et al. had to remove layers from the classification network, introduce parameters that compensate for the ablated layers, and then fine-tune the modified models to train the new parameters. Even then, the output resolution obtained by Zhou et al. was quite small (14\u00d714) and the classification performance of the modified networks was impaired.\nIn contrast, the DRN was designed to produce highresolution output maps and is trained in this configuration from the start. Thus the model trained for image classification already produces high-resolution activation maps. As our experiments will show, DRNs are more accurate than the original ResNets in image classification. Since DRNs produce high-resolution output maps from the start, there is no need to remove layers, add parameters, and retrain the model for localization. The original accurate classification model can be used for localization directly."
        },
        {
            "heading": "4. Degridding",
            "text": "The use of dilated convolutions can cause gridding artifacts. Such artifacts are shown in Figure 3(c) and have also been observed in concurrent work on semantic segmentation [17]. Gridding artifacts occur when a feature map has higher-frequency content than the sampling rate of the dilated convolution. Figure 4 shows a didactic example. In Figure 4(a), the input feature map has a single active pixel. A 2-dilated convolution (Figure 4(b)) induces a corresponding grid pattern in the output (Figure 4(c)).\nIn this section, we develop a scheme for removing gridding artifacts from output activation maps produced by DRNs. The scheme is illustrated in Figure 5. A DRN con-\nstructed as described in Section 2 is referred to as DRN-A and is illustrated in Figure 5(a). An intermediate stage of the construction described in the present section is referred\n3 x 3 64 3 x 3 64\n3 x 3 64 3 x 3 64\n3 x 3 128 3 x 3 128\n3 x 3 128 3 x 3 128\n3 x 3 256 3 x 3 256\n3 x 3 256 3 x 3 256\n3 x 3 512 3 x 3 512\n3 x 3 512 3 x 3 512 Pooling 7 x 7 64\n7 x 7 16\n3 x 3 512 3 x 3 512 3 x 3 32 3 x 3 32 3 x 3 16 3 x 3 16 3 x 3 64 3 x 3 64 3 x 3 64 3 x 3 64 3 x 3 128 3 x 3 128 3 x 3 128 3 x 3 128 3 x 3 256 3 x 3 256 3 x 3 256 3 x 3 256 3 x 3 512 3 x 3 512 3 x 3 512 3 x 3 512\n3 x 3 512 3 x 3 512\nDilation 1 1 1 1 2 4 2 1\nLevel 1 2 3 4 5 6 7 8\n7 x 7 16\n3 x 3 512 3 x 3 512 3 x 3 32 3 x 3 32 3 x 3 16 3 x 3 16 3 x 3 64 3 x 3 64 3 x 3 64 3 x 3 64 3 x 3 128 3 x 3 128 3 x 3 128 3 x 3 128 3 x 3 256 3 x 3 256 3 x 3 256 3 x 3 256 3 x 3 512 3 x 3 512 3 x 3 512 3 x 3 512\n3 x 3 512 3 x 3 512 (a) DRN-A-18\n(c) DRN-C-26\n(b) DRN-B-26\nFigure 5: Changing the DRN architecture to remove gridding artifacts from the output activation maps. Each rectangle is a Conv-BN-ReLU group and the numbers specify the filter size and the number of channels in that layer. The bold green lines represent downsampling by stride 2. The networks are divided into levels, such that all layers within a given level have the same dilation and spatial resolution. (a) DRN-A dilates the ResNet model directly, as described in Section 2. (b) DRN-B replaces an early max pooling layer by residual blocks and adds residual blocks at the end of the network. (c) DRN-C removes residual connections from some of the added blocks. The rationale for each step is described in the text.\nto as DRN-B and is illustrated in Figure 5(b). The final construction is referred to as DRN-C, illustrated in Figure 5(c). Removing max pooling. As shown in Figure 5(a), DRN-A inherits from the ResNet architecture a max pooling operation after the initial 7\u00d77 convolution. We found that this max pooling operation leads to high-amplitude high-frequency activations, as shown in Figure 6(b). Such high-frequency activations can be propagated to later layers and ultimately exacerbate gridding artifacts. We thus replace max pooling by convolutional filters, as shown in Figure 5(b). The effect of this transformation is shown in Figure 6(c).\nAdding layers. To remove gridding artifacts, we add convolutional layers at the end of the network, with progressively lower dilation. Specifically, after the last 4-dilated layer in DRN-A (Figure 5(a)), we add a 2-dilated residual block followed by a 1-dilated block. These become levels 7 and 8 in DRN-B, shown in Figure 5(b). This is akin\nto removing aliasing artifacts using filters with appropriate frequency [16]. Removing residual connections. Adding layers with decreasing dilation, as described in the preceding paragraph, does not remove gridding artifacts entirely because of residual connections. The residual connections in levels 7 and 8 of DRN-B can propagate gridding artifacts from level 6. To remove gridding artifacts more effectively, we remove the residual connections in levels 7 and 8. This yields the DRN-C, our proposed construction, illustrated in Figure 5(c). Note that the DRN-C has higher depth and capacity than the corresponding DRN-A or the ResNet that had been used as the starting point. However, we will show that the presented degridding scheme has a dramatic effect on accuracy, such that the accuracy gain compensates for the added depth and capacity. For example, experiments will demonstrate that DRN-C-26 has similar image classification accuracy to DRN-A-34 and higher object localization and semantic segmentation accuracy than DRN-A-50.\nThe activations inside a DRN-C are illustrated in Figure 7. This figure shows a feature map from the output of each level in the network. The feature map with the largest average activation magnitude is shown."
        },
        {
            "heading": "5. Experiments",
            "text": ""
        },
        {
            "heading": "5.1. Image Classification",
            "text": "Training is performed on the ImageNet 2012 training set [12]. The training procedure is similar to He et al. [6]. We use scale and aspect ratio augmentation as in Szegedy et al. [14] and color perturbation as in Krizhevsky et al. [8] and Howard [7]. Training is performed by SGD with mo-\nmentum 0.9 and weight decay 10\u22124. The learning rate is initially set to 10\u22121 and is reduced by a factor of 10 every 30 epochs. Training proceeds for 120 epochs total.\nThe performance of trained models is evaluated on the ImageNet 2012 validation set. The images are resized so that the shorter side has 256 pixels. We use two evaluation protocols: 1-crop and 10-crop. In the 1-crop protocol, prediction accuracy is measured on the central 224\u00d7224 crop. In the 10-crop protocol, prediction accuracy is measured on 10 crops from each image. Specifically, for each image we take the center crop, four corner crops, and flipped versions of these crops. The reported 10-crop accuracy is averaged over these 10 crops.\nResNet vs. DRN-A. Table 1 reports the accuracy of different models according to both evaluation protocols. Each DRN-A outperforms the corresponding ResNet model, despite having the same depth and capacity. For example, DRN-A-18 and DRN-A-34 outperform ResNet-18 and ResNet-34 in 1-crop top-1 accuracy by 2.43 and 2.92 percentage points, respectively. (A 10.5% error reduction in the case of ResNet-34\u2192 DRN-A-34.)\nDRN-A-50 outperforms ResNet-50 in 1-crop top-1 accuracy by more than a percentage point. For comparison, the corresponding error reduction achieved by ResNet-152 over ResNet-101 is 0.3 percentage points. (From 22.44 to 22.16 on the center crop.) These results indicate that even the direct transformation of a ResNet into a DRN-A, which does not change the depth or capacity of the model at all, significantly improves classification accuracy.\nDRN-A vs. DRN-C. Table 1 also shows that the degridding construction described in Section 4 is beneficial. Specif-\nically, each DRN-C significantly outperforms the corresponding DRN-A. Although the degridding procedure increases depth and capacity, the resultant increase in accuracy is so substantial that the transformed DRN matches the accuracy of deeper models. Specifically, DRN-C-26, which is derived from DRN-A-18, matches the accuracy of the deeper DRN-A-34. In turn, DRN-C-42, which is derived from DRN-A-34, matches the accuracy of the deeper DRN-A-50. Comparing the degridded DRN to the original ResNet models, we see that DRN-C-42 approaches the accuracy of ResNet-101, although the latter is deeper by a factor of 2.4."
        },
        {
            "heading": "5.2. Object Localization",
            "text": "We now evaluate the use of DRNs for weakly-supervised object localization, as described in Section 3. As shown in Figure 3, class activation maps provided by DRNs are much better spatially resolved than activation maps extracted from the corresponding ResNet.\nWe evaluate the utility of the high-resolution activation maps provided by DRNs for weakly-supervised object localization using the ImageNet 2012 validation set. We first predict the image categories based on 10-crop testing. Since the ground truth is in the form of bounding boxes, we need to fit bounding boxes to the activation maps. We predict the object bounding boxes by analyzing the class responses on all the response maps. The general idea is to find tight bounding boxes that cover pixels for which the dominant response indicates the correct object class. Specifically, given C response maps of resolution W\u00d7H, let f(c, w, h) be the response at location (w, h) on the cth response map. In the ImageNet dataset, C is 1000. We identify the dominant class at each location:\ng(w, h) = { c | \u22001 \u2264 c\u2032 \u2264 C. f(c, w, h) \u2265 f(c\u2032, w, h) } .\nFor each class ci, define the set of valid bounding boxes as\nBi = { ((w1, h1), (w2, h2))| \u2200g(w, h) = ci and f(w, h, ci) > t.\nw1 \u2264 w \u2264 w2 and h1 \u2264 h \u2264 h2 } ,\nwhere t is an activation threshold. The minimal bounding box for class ci is defined as\nbi = argmin ((w1,h1),(w2,h2))\u2208Bi\n(w2 \u2212 w1)(h2 \u2212 h1).\nTo evaluate the accuracy of DRNs on weakly-supervised object localization, we simply compute the minimal bounding box bi for the predicted class i on each image. In the localization challenge, a predicted bounding box is considered accurate when its IoU with the ground-truth box is greater than 0.5. Table 2 reports the results. Note that the classification networks are used for localization directly, with no fine-tuning.\nAs shown in Table 2, DRNs outperform the corresponding ResNet models. (Compare ResNet-18 to DRN-A-18, ResNet-34 to DRN-A-34, and ResNet-50 to DRN-A-50.) This again illustrates the benefits of the basic DRN construction presented in Section 2. Furthermore, DRN-C-26 significantly outperforms DRN-A-50, despite having much lower depth. This indicates that that the degridding scheme described in Section 4 has particularly significant benefits for applications that require more detailed spatial image analysis. DRN-C-26 also outperforms ResNet-101."
        },
        {
            "heading": "5.3. Semantic Segmentation",
            "text": "We now transfer DRNs to semantic segmentation. Highresolution internal representations are known to be important for this task [10, 18, 2]. Due to the severe downsampling in prior image classification architectures, their transfer to semantic segmentation necessitated post-hoc adaptations such as up-convolutions, skip connections, and posthoc dilation [10, 1, 11, 18]. In contrast, the high resolution of the output layer in a DRN means that we can transfer a classification-trained DRN to semantic segmentation by simply removing the global pooling layer and operating the network fully-convolutionally [10], without any additional structural changes. The predictions synthesized by the output layer are upsampled to full resolution using bilinear interpolation, which does not involve any parameters.\nWe evaluate this capability using the Cityscapes dataset [2]. We use the standard Cityscapes training and validation sets. To understand the properties of the models themselves, we only use image cropping and mirroring for training. We do not use any other data augmentation and do not append additional modules to the network. The results are reported in Table 3.\nAll presented models outperform a comparable baseline setup of ResNet-101, which was reported to achieve a mean IoU of 66.6 [1]. For example, DRN-C-26 outperforms the ResNet-101 baseline by more than a percentage point, despite having 4 times lower depth. The DRN-C-42 model outperforms the ResNet-101 baseline by more than 4 percentage points, despite 2.4 times lower depth.\nComparing different DRN models, we see that both DRN-C-26 and DRN-C-42 outperform DRN-A-50, sug-\ngesting that the degridding construction presented in Section 4 is particularly beneficial for dense prediction tasks. A qualitative comparison between DRN-A-50 and DRN-C-26 is shown in Figure 8. As the images show, the predictions of DRN-A-50 are marred by gridding artifacts even though the model was trained with dense pixel-level supervision. In contrast, the predictions of DRN-C-26 are not only more accurate, but also visibly cleaner."
        },
        {
            "heading": "6. Conclusion",
            "text": "We have presented an approach to designing convolutional networks for image analysis. Rather than progressively reducing the resolution of internal representations until the spatial structure of the scene is no longer discernible, we keep high spatial resolution all the way through the final output layers. We have shown that this simple transformation improves image classification accuracy, outperforming state-of-the-art models. We have then shown that accuracy can be increased further by modifying the construction to\nalleviate gridding artifacts introduced by dilation. The presented image classification networks produce informative output activations, which can be used directly for weakly-supervised object localization, without any finetuning. The presented models can also be used for dense prediction tasks such as semantic segmentation, where they outperform deeper and higher-capacity baselines.\nThe results indicate that dilated residual networks can be used as a starting point for image analysis tasks that involve complex natural images, particularly when detailed understanding of the scene is important. We will release code and pretrained models to support future research and applications."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work was supported by Intel and the National Science Foundation (IIS-1251217 and VEC 1539014/1539099)."
        }
    ],
    "title": "Dilated Residual Networks",
    "year": 2017
}