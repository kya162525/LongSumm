{
    "abstractText": "In past and recent years, the issues related to managing technical debt received significant attention by researchers from both industry and academia. There are several factors that contribute to technical debt. One of these is represented by code bad smells, i.e., symptoms of poor design and implementation choices. While the repercussions of smells on code quality have been empirically assessed, there is still only anecdotal evidence on when and why bad smells are introduced. To fill this gap, we conducted a large empirical study over the change history of 200 open source projects from different software ecosystems and investigated when bad smells are introduced by developers, and the circumstances and reasons behind their introduction. Our study required the development of a strategy to identify smellintroducing commits, the mining of over 0.5M commits, and the manual analysis of 9,164 of them (i.e., those identified as smellintroducing). Our findings mostly contradict common wisdom stating that smells are being introduced during evolutionary tasks. In the light of our results, we also call for the need to develop a new generation of recommendation systems aimed at properly planning smell refactoring activities.",
    "authors": [
        {
            "affiliations": [],
            "name": "Michele Tufano"
        },
        {
            "affiliations": [],
            "name": "Fabio Palomba"
        },
        {
            "affiliations": [],
            "name": "Gabriele Bavota"
        },
        {
            "affiliations": [],
            "name": "Rocco Oliveto"
        },
        {
            "affiliations": [],
            "name": "Massimiliano Di Penta"
        },
        {
            "affiliations": [],
            "name": "Andrea De Lucia"
        },
        {
            "affiliations": [],
            "name": "Denys Poshyvanyk"
        }
    ],
    "id": "SP:b7f62dd7d0feb4c35199ecf58c15ada5abc6675c",
    "references": [
        {
            "authors": [
                "M. Abbes",
                "F. Khomh",
                "Y.-G. Gu\u00e9h\u00e9neuc",
                "G. Antoniol"
            ],
            "title": "An empirical study of the impact of two antipatterns, Blob and Spaghetti Code, on program comprehension",
            "venue": "15th European Conference on Software Maintenance and Reengineering, CSMR 2011, 1-4 March 2011, Oldenburg, Germany. IEEE Computer Society, 2011, pp. 181\u2013190.",
            "year": 2011
        },
        {
            "authors": [
                "G. Antoniol",
                "K. Ayari",
                "M. Di Penta",
                "F. Khomh",
                "Y.-G. Gu\u00e9h\u00e9neuc"
            ],
            "title": "Is it a bug or an enhancement?: a text-based approach to classify change requests",
            "venue": "Proceedings of the 2008 conference of the Centre for Advanced Studies on Collaborative Research, October 27-30, 2008, Richmond Hill, Ontario, Canada. IBM, 2008, p. 23.",
            "year": 2008
        },
        {
            "authors": [
                "R. Arcoverde",
                "A. Garcia",
                "E. Figueiredo"
            ],
            "title": "Understanding the longevity of code smells: preliminary results of an explanatory survey",
            "venue": "Proceedings of the International Workshop on Refactoring Tools. ACM, 2011, pp. 33\u201336.",
            "year": 2011
        },
        {
            "authors": [
                "V. Arnaoudova",
                "M. Di Penta",
                "G. Antoniol",
                "Y.-G. Gu\u00e9h\u00e9neuc"
            ],
            "title": "A new family of software anti-patterns: Linguistic anti-patterns",
            "venue": "17th European Conference on Software Maintenance and Reengineering, CSMR 2013, Genova, Italy, March 5-8, 2013. IEEE Computer Society, 2013, pp. 187\u2013196.",
            "year": 2013
        },
        {
            "authors": [
                "A. Bachmann",
                "C. Bird",
                "F. Rahman",
                "P.T. Devanbu",
                "A. Bernstein"
            ],
            "title": "The missing links: bugs and bug-fix commits.",
            "venue": "Proceedings of the 18th ACM SIGSOFT International Symposium on Foundations of Software Engineering,",
            "year": 2010
        },
        {
            "authors": [
                "G. Bavota",
                "G. Canfora",
                "M. Di Penta",
                "R. Oliveto",
                "S. Panichella"
            ],
            "title": "The evolution of project inter-dependencies in a software ecosystem: The case of apache",
            "venue": "2013 IEEE International Conference on Software Maintenance, Eindhoven, The Netherlands, September 22-28, 2013, 2013, pp. 280\u2013289.",
            "year": 2013
        },
        {
            "authors": [
                "G. Bavota",
                "B.D. Carluccio",
                "A. De Lucia",
                "M. Di Penta",
                "R. Oliveto",
                "O. Strollo"
            ],
            "title": "When does a refactoring induce bugs? an empirical study",
            "venue": "Proceedings of the IEEE International Working Conference on Source Code Analysis and Manipulation. Riva del Garda, Italy: IEEE Computer Society, 2012, pp. 104\u2013113.",
            "year": 2012
        },
        {
            "authors": [
                "G. Bavota",
                "A. Qusef",
                "R. Oliveto",
                "A. De Lucia",
                "D. Binkley"
            ],
            "title": "An empirical analysis of the distribution of unit test smells and their impact on software maintenance",
            "venue": "28th IEEE International Conference on Software Maintenance, ICSM 2012, Trento, Italy, September 23-28, 2012. IEEE Computer Society, 2012, pp. 56\u201365. 11",
            "year": 2012
        },
        {
            "authors": [
                "C. Bird",
                "N. Nagappan",
                "B. Murphy",
                "H. Gall",
                "P.T. Devanbu"
            ],
            "title": "Don\u2019t touch my code!: examining the effects of ownership on software quality",
            "venue": "SIGSOFT/FSE\u201911 19th ACM SIGSOFT Symposium on the Foundations of Software Engineering and 13rd European Software Engineering Conference, Szeged, Hungary, September 5-9, 2011. ACM, 2011, pp. 4\u201314.",
            "year": 2011
        },
        {
            "authors": [
                "M. Boussaa",
                "W. Kessentini",
                "M. Kessentini",
                "S. Bechikh",
                "S.B. Chikha"
            ],
            "title": "Competitive coevolutionary code-smells detection",
            "venue": "Search Based Software Engineering - 5th International Symposium, SSBSE 2013, St. Petersburg, Russia, August 24-26, 2013. Proceedings, ser. Lecture Notes in Computer Science. Springer, 2013, pp. 50\u201365.",
            "year": 2013
        },
        {
            "authors": [
                "N. Brown",
                "Y. Cai",
                "Y. Guo",
                "R. Kazman",
                "M. Kim",
                "P. Kruchten",
                "E. Lim",
                "A. MacCormack",
                "R.L. Nord",
                "I. Ozkaya",
                "R.S. Sangwan",
                "C.B. Seaman",
                "K.J. Sullivan",
                "N. Zazworka"
            ],
            "title": "Managing technical debt in softwarereliant systems",
            "venue": "Proceedings of the Workshop on Future of Software Engineering Research, at the 18th ACM SIGSOFT International Symposium on Foundations of Software Engineering. Santa Fe, NM, USA: ACM, 2010, pp. 47\u201352.",
            "year": 2010
        },
        {
            "authors": [
                "O. Chaparro",
                "G. Bavota",
                "A. Marcus",
                "M. Di Penta"
            ],
            "title": "On the impact of refactoring operations on code quality metrics",
            "venue": "Proceedings of the 30th International Conference on Software Maintenance and Evolution (ICSME 2014), 2014, p. To appear.",
            "year": 2014
        },
        {
            "authors": [
                "A. Chatzigeorgiou",
                "A. Manakos"
            ],
            "title": "Investigating the evolution of bad smells in object-oriented code",
            "venue": "International Conference on the Quality of Information and Communications Technology (QUATIC). IEEE, 2010, pp. 106\u2013115.",
            "year": 2010
        },
        {
            "authors": [
                "S.R. Chidamber",
                "C.F. Kemerer"
            ],
            "title": "A metrics suite for object oriented design",
            "venue": "IEEE Transactions on Software Engineering (TSE), vol. 20, no. 6, pp. 476\u2013493, June 1994.",
            "year": 1994
        },
        {
            "authors": [
                "W. Cunningham"
            ],
            "title": "The WyCash portfolio management system",
            "venue": "OOPS Messenger, vol. 4, no. 2, pp. 29\u201330, 1993.",
            "year": 1993
        },
        {
            "authors": [
                "M. Fischer",
                "M. Pinzger",
                "H. Gall"
            ],
            "title": "Populating a release history database from version control and bug tracking systems",
            "venue": "19th International Conference on Software Maintenance (ICSM 2003), 22- 26 September 2003, Amsterdam, The Netherlands, 2003, pp. 23\u2013.",
            "year": 2003
        },
        {
            "authors": [
                "R.J. Grissom",
                "J.J. Kim"
            ],
            "title": "Effect sizes for research: A broad practical approach, 2nd ed",
            "venue": "Lawrence Earlbaum Associates,",
            "year": 2005
        },
        {
            "authors": [
                "F. Hermans",
                "M. Pinzger",
                "A. van Deursen"
            ],
            "title": "Detecting and visualizing inter-worksheet smells in spreadsheets",
            "venue": "34th International Conference on Software Engineering, ICSE 2012, June 2-9, 2012, Zurich, Switzerland. IEEE, 2012, pp. 441\u2013451.",
            "year": 2012
        },
        {
            "authors": [
                "\u2014\u2014"
            ],
            "title": "Detecting code smells in spreadsheet formulas",
            "venue": "28th IEEE International Conference on Software Maintenance, ICSM 2012, Trento, Italy, September 23-28, 2012. IEEE Computer Society, 2012, pp. 409\u2013 418.",
            "year": 2012
        },
        {
            "authors": [
                "K. Herzig",
                "S. Just",
                "A. Zeller"
            ],
            "title": "It\u2019s not a bug, it\u2019s a feature: how misclassification impacts bug prediction",
            "venue": "35th International Conference on Software Engineering, ICSE \u201913, San Francisco, CA, USA, May 18-26, 2013. IEEE / ACM, 2013, pp. 392\u2013401.",
            "year": 2013
        },
        {
            "authors": [
                "F. Khomh",
                "M. Di Penta",
                "Y.-G. Gueheneuc"
            ],
            "title": "An exploratory study of the impact of code smells on software change-proneness",
            "venue": "Proceedings of the 16th Working Conference on Reverse Engineering. Lille, France: IEEE CS Press, 2009, pp. 75\u201384.",
            "year": 2009
        },
        {
            "authors": [
                "F. Khomh",
                "M. Di Penta",
                "Y.-G. Gu\u00e9h\u00e9neuc",
                "G. Antoniol"
            ],
            "title": "An exploratory study of the impact of antipatterns on class change- and fault-proneness",
            "venue": "Empirical Software Engineering, vol. 17, no. 3, pp. 243\u2013275, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "P. Kruchten",
                "R.L. Nord",
                "I. Ozkaya"
            ],
            "title": "Technical debt: From metaphor to theory and practice",
            "venue": "IEEE Software, vol. 29, no. 6, pp. 18\u201321, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "M.M. Lehman",
                "L.A. Belady"
            ],
            "title": "Software Evolution - Processes of Software Change",
            "year": 1985
        },
        {
            "authors": [
                "S. Lemma Abebe",
                "S. Haiduc",
                "P. Tonella",
                "A. Marcus"
            ],
            "title": "The effect of lexicon bad smells on concept location in source code",
            "venue": "11th IEEE Working Conference on Source Code Analysis and Manipulation, SCAM 2011, Williamsburg, VA, USA, September 25-26, 2011. IEEE, 2011, pp. 125\u2013134.",
            "year": 2011
        },
        {
            "authors": [
                "W. Li",
                "R. Shatnawi"
            ],
            "title": "An empirical study of the bad smells and class error probability in the post-release object-oriented system evolution",
            "venue": "Journal of Systems and Software, pp. 1120\u20131128, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "E. Lim",
                "N. Taksande",
                "C.B. Seaman"
            ],
            "title": "A balancing act: What software practitioners have to say about technical debt",
            "venue": "IEEE Software, vol. 29, no. 6, pp. 22\u201327, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "A. Lozano",
                "M. Wermelinger",
                "B. Nuseibeh"
            ],
            "title": "Assessing the impact of bad smells using historical information",
            "venue": "Ninth international workshop on Principles of software evolution: in conjunction with the 6th ESEC/FSE joint meeting, ser. IWPSE \u201907. New York, NY, USA: ACM, 2007, pp. 31\u201334.",
            "year": 2007
        },
        {
            "authors": [
                "R. Marinescu"
            ],
            "title": "Detection strategies: Metrics-based rules for detecting design flaws",
            "venue": "20th International Conference on Software Maintenance (ICSM 2004), 11-17 September 2004, Chicago, IL, USA. IEEE Computer Society, 2004, pp. 350\u2013359.",
            "year": 2004
        },
        {
            "authors": [
                "N. Moha",
                "Y.-G. Gu\u00e9h\u00e9neuc",
                "L. Duchien",
                "A.-F.L. Meur"
            ],
            "title": "Decor: A method for the specification and detection of code and design smells",
            "venue": "IEEE Transactions on Software Engineering, vol. 36, pp. 20\u201336, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "I. Neamtiu",
                "G. Xie",
                "J. Chen"
            ],
            "title": "Towards a better understanding of software evolution: an empirical study on open-source software",
            "venue": "Journal of Software: Evolution and Process, vol. 25, no. 3, pp. 193\u2013218, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "F. Palomba",
                "G. Bavota",
                "M. Di Penta",
                "R. Oliveto",
                "A. De Lucia"
            ],
            "title": "Do they really smell bad? a study on developers\u2019 perception of bad code smells",
            "venue": "In Proceedings of the 30th IEEE International Conference on Software Maintenance and Evolution (ICSME\u201914), Victoria, Canada, 2014, to appear.",
            "year": 2014
        },
        {
            "authors": [
                "F. Palomba",
                "G. Bavota",
                "M. Di Penta",
                "R. Oliveto",
                "A. De Lucia",
                "D. Poshyvanyk"
            ],
            "title": "Detecting bad smells in source code using change history information",
            "venue": "Automated Software Engineering (ASE), 2013 IEEE/ACM 28th International Conference on, Nov 2013, pp. 268\u2013278.",
            "year": 2013
        },
        {
            "authors": [
                "D.L. Parnas"
            ],
            "title": "Software aging",
            "venue": "Proceedings of the 16th International Conference on Software Engineering, Sorrento, Italy, May 16-21, 1994. IEEE Computer Society / ACM Press, 1994, pp. 279\u2013287.",
            "year": 1994
        },
        {
            "authors": [
                "R. Peters",
                "A. Zaidman"
            ],
            "title": "Evaluating the lifespan of code smells using software repository mining",
            "venue": "European Conference on Software Maintenance and ReEngineering. IEEE, 2012, pp. 411\u2013416.",
            "year": 2012
        },
        {
            "authors": [
                "D. Ratiu",
                "S. Ducasse",
                "T. G\u0131\u0302rba",
                "R. Marinescu"
            ],
            "title": "Using history information to improve design flaws detection",
            "venue": "8th European Conference on Software Maintenance and Reengineering (CSMR 2004), 24-26 March 2004, Tampere, Finland, Proceeding. IEEE Computer Society, 2004, pp. 223\u2013232.",
            "year": 2004
        },
        {
            "authors": [
                "D. Sheskin"
            ],
            "title": "Handbook of Parametric and Nonparametric Statistical Procedures., second edition ed",
            "year": 2000
        },
        {
            "authors": [
                "D.I.K. Sj\u00f8berg",
                "A.F. Yamashita",
                "B.C.D. Anda",
                "A. Mockus",
                "T. Dyb\u00e5"
            ],
            "title": "Quantifying the effect of code smells on maintenance effort",
            "venue": "IEEE Trans. Software Eng., vol. 39, no. 8, pp. 1144\u20131156, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "N. Tsantalis",
                "A. Chatzigeorgiou"
            ],
            "title": "Identification of move method refactoring opportunities",
            "venue": "IEEE Transactions on Software Engineering, vol. 35, no. 3, pp. 347\u2013367, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "R. Wu",
                "H. Zhang",
                "S. Kim",
                "S.-C. Cheung"
            ],
            "title": "ReLink: recovering links between bugs and changes",
            "venue": "SIGSOFT/FSE\u201911 19th ACM SIGSOFT Symposium on the Foundations of Software Engineering (FSE-19) and ESEC\u201911: 13rd European Software Engineering Conference (ESEC-13), Szeged, Hungary, September 5-9, 2011. ACM, 2011, pp. 15\u201325.",
            "year": 2011
        },
        {
            "authors": [
                "G. Xie",
                "J. Chen",
                "I. Neamtiu"
            ],
            "title": "Towards a better understanding of software evolution: An empirical study on open source software",
            "venue": "2013 IEEE International Conference on Software Maintenance, vol. 0, pp. 51\u201360, 2009.",
            "year": 2013
        },
        {
            "authors": [
                "A. Yamashita",
                "L. Moonen"
            ],
            "title": "Exploring the impact of inter-smell relations on software maintainability: An empirical study",
            "venue": "International Conference on Software Engineering (ICSE). IEEE, 2013, pp. 682\u2013691.",
            "year": 2013
        },
        {
            "authors": [
                "A.F. Yamashita",
                "L. Moonen"
            ],
            "title": "Do code smells reflect important maintainability aspects?",
            "venue": "IEEE International Conference on Software Maintenance,",
            "year": 2012
        },
        {
            "authors": [
                "\u2014\u2014"
            ],
            "title": "Do developers care about code smells? an exploratory survey",
            "venue": "20th Working Conference on Reverse Engineering, WCRE 2013, Koblenz, Germany, October 14-17, 2013. IEEE, 2013, pp. 242\u2013251. 12",
            "year": 2013
        },
        {
            "authors": [
                "A. Zeller"
            ],
            "title": "Why Programs Fail: A Guide to Systematic Debugging",
            "year": 2009
        }
    ],
    "sections": [
        {
            "text": "I. INTRODUCTION\nTechnical debt is a metaphor introduced by Cunningham to indicate \u201cnot quite right code which we postpone making it right\u201d [18]. The metaphor explains well the trade-offs between delivering the most appropriate but still immature product, in the shortest time possible [12], [18], [27], [31], [42]. While the repercussions of \u201ctechnical debt\u201d on software quality have been empirically proven, there is still a lack of empirical evidence related to how, when, and why various forms of technical debt occur in software projects [12]. This represents an obstacle for an effective and efficient management of technical debt.\nBad code smells (shortly \u201ccode smells\u201d or \u201csmells\u201d), i.e., symptoms of poor design and implementation choices [20], represent one important factor contributing to technical debt, and possibly affecting the maintainability of a software system [27]. In the past and, most notably, in recent years, several studies investigated the relevance that code smells have for developers [36], [50], the extent to which code smells tend to remain in a software system for long periods of time [3], [15], [32], [40], as well as the side effects of code smells, such as increase in change- and fault-proneness [25], [26] or decrease of software understandability [1] and maintainability [43], [49], [48].\nThe research community has been also actively developing approaches and tools for detecting smells [11], [34], [37], [44], [33], and, whenever possible, triggering refactoring operations. Such tools rely on different types of analysis techniques, such as constraint-based reasoning over metric values [33], [34],\nstatic code analysis [44], or analysis of software changes [37]. While these tools provide relatively accurate and complete identification of a wide variety of smells, most of them work by \u201ctaking a snapshot\u201d of the system or by looking at recent changes, hence providing a snapshot-based recommendation to the developer. Hence, they do not consider the circumstances that could have caused the smell introduction. In order to better support developers in planning actions to improve design and source code quality, it is imperative to have a contextualized understanding of the circumstances under which particular smells occur.\nHowever, to the best of our knowledge, there is no comprehensive empirical investigation into when and why code smells are introduced in software projects. Common wisdom suggests that urgent maintenance activities and pressure to deliver features while prioritizing time-to-market over code quality are often the causes of such smells. Generally speaking, software evolution has always been considered as one of the reasons behind \u201csoftware aging\u201d [38] or \u201cincreasing complexity\u201d [28][35][47]. Broadly speaking, smells can also manifest themselves not only in the source code but also in software lexicons [29], [4], and can even affect other types of artifacts, such as spreadsheets [22], [23] or test cases [9].\nIn this paper we fill the void in terms of our understanding of code smells, reporting the results of a large-scale empirical study conducted on the evolution history of 200 open source projects belonging to three software ecosystems, namely Android, Apache and Eclipse. The study aimed at investigating (i) when smells are introduced in software projects, and (ii) why they are introduced, i.e., under what circumstances smell introductions occur and who are the developers responsible for introducing smells. To address these research questions, we developed a metric-based methodology for analyzing the evolution of code entities in change histories of software projects to determine when code smells start manifesting themselves and whether this happens suddenly (i.e., because of a pressure to quickly introduce a change), or gradually (i.e., because of medium-to-long range design decisions). We mined over 0.5M commits and we manually analyzed 9,164 of them classified as smell-introducing. We are unaware of any published technical debt, in general, and code smell study, in particular, of comparable size. The results achieved allowed us to report quantitative and qualitative evidence on when and why smells are introduced in software projects as well as implications of these results, often contradicting common wisdom."
        },
        {
            "heading": "II. STUDY DESIGN",
            "text": "The goal of the study is to analyze change history of software projects, with the purpose of investigating when code smells are introduced by developers, and the circumstances and reasons behind smell appearances. More specifically, the study aims at addressing the following two research questions:\n\u2022 RQ1: When are code smells introduced? This research question aims at investigating to what extent the common wisdom suggesting that \u201ccode smells are introduced as a consequence of continuous maintenance and evolution activities\u201d [20] applies. Specifically, we study \u201cwhen\u201d code smells are introduced in software systems, to understand whether smells are introduced as soon as a code entity is created, whether smells are suddenly introduced in the context of specific maintenance activities, or whether, instead, smells appear \u201cgradually\u201d during software evolution. To this aim, we investigated the presence of possible trends in the history of code artifacts that characterize the introduction of specific types of smells. \u2022 RQ2: Why are code smells introduced? The second research question aims at empirically investigating under which circumstances developers are more prone to introducing code smells. We focus on factors that are indicated as possible causes for code smell introduction in the existing literature [20]: the commit goal (e.g., is the developer implementing a new feature or fixing a bug?), the project status (e.g., is the change performed in proximity to a major release deadline?), and the developer status (e.g., is the developer a newcomer or a senior project member?)."
        },
        {
            "heading": "A. Context Selection",
            "text": "The context of the study consists of the change history of 200 projects belonging to three software ecosystems, namely Android, Apache, and Eclipse. Table I reports for each of them (i) the number of projects analyzed, (ii) size ranges in terms of the number of classes and KLOC, (iii) the overall number of commits and issues analyzed, and (iv) the average, minimum, and maximum length of the projects\u2019 story (in years) analyzed in each ecosystem. All the analyzed projects are hosted in Git repositories and have associated issue trackers. The Android ecosystem contains a random selection of 70 open source apps mined from the f-droid1 forge. The Apache ecosystem consists of 100 Java projects randomly selected among those available2. Finally, the Eclipse ecosystem consists of 30 projects randomly mined from the list of GitHub repositories managed by the Eclipse Foundation3. The choice of the ecosystems to analyze\n1https://f-droid.org/ 2https://projects.apache.org/indexes/quick.html 3https://github.com/eclipse\nis not random, but rather driven by the motivation to consider projects having (i) different sizes, e.g., Android apps are by their nature smaller than projects in Apache\u2019s and Eclipse\u2019s ecosystems, (ii) different architectures, e.g., we have Android mobile apps, Apache libraries, and plug-in based architectures in Eclipse projects, and (iii) different development bases, e.g., Android apps are often developed by small teams whereas several Apache projects are carried out by dozens of developers [7]. Also, we limited our study to 200 projects since, as it will be shown later, the analysis we carried out is not only computationally expensive, but also requires manual analysis of thousands of data points. To sum up, we mined 579,671 commits and 4,803 issues.\nWe focus our study on the following types of smells: 1) Blob Class: a large class with different responsibilities\nthat monopolizes most of the system\u2019s processing [13]. 2) Class Data Should be Private: a class exposing its\nattributes, violating the information hiding principle [20]. 3) Complex Class: a class having a high cyclomatic com-\nplexity [13]. 4) Functional Decomposition: a class where inheritance and\npolymorphism are poorly used, declaring many private fields and implementing few methods [13].\n5) Spaghetti Code: a class without structure that declares long methods without parameters [13].\nWhile several other smells exist in literature [13], [20], we needed to limit our analysis to a subset due to computational constraints. However, we carefully kept a mix of smells related to complex/large code components (e.g., Blob Class, Complex Class) as well as smells related to the lack of adoption of good Object-Oriented coding practices (e.g., Class Data Should be Private, Functional Decomposition). Thus, the smells considered are representative of categories of smells investigated in previous studies (see Section V)."
        },
        {
            "heading": "B. Data Extraction and Analysis",
            "text": "This subsection describes the data extraction and analysis process that we followed to answer our research questions.\n1) When are code smells introduced?: To answer RQ1 we firstly cloned the 200 Git repositories. Then, we analyzed each repository ri using a tool that we developed (named as HistoryMiner), with the purpose of identifying smellintroducing commits. Our tool mines the entire change history of ri, checks out each commit in chronological order, and runs an implementation of the DECOR smell detector based on the original rules defined by Moha et al. [34]. DECOR identifies smells using detection rules based on the values of internal quality metrics4. The choice of using DECOR was driven by the fact that (i) it is a state-of-the-art smell detector having a high accuracy in detecting smells [34]; and (ii) it applies simple detection rules that allow it to be very efficient. Note that we ran DECOR on all source code files contained in ri only for the first commit of ri. In the subsequent commits\n4An example of detection rule exploited to identify Blob classes can be found at http://tinyurl.com/paf9gp6.\nDECOR has been executed only on code files added and modified in each specific commit to save computational time. As an output, our tool produces, for each source code file fj \u2208 ri the list of commits in which fj has been involved, specifying if fj has been added, deleted, or modified and if fj was affected, in that specific commit, by one of the five considered smells.\nStarting from the data generated by the HistoryMiner we compute, for each type of smell (smellk) and for each source code file (fj), the number of commits performed on fj since the first commit involving fj and adding the file to the repository, up to the commit in which DECOR detects that fj as affected by smellk. Clearly, such numbers are only computed for files identified as affected by the specific smellk.\nWhen analyzing the number of commits needed for a smell to affect a code component, we can fall into two possible scenarios. In the first scenario (the least expected according to the \u201csoftware aging\u201d theory [38]) smell instances are introduced during the creation of source code artifacts, i.e., in the first commit involving a source code file. In the second scenario, smell instances are introduced after several commits and, thus as result of multiple maintenance activities. For the latter scenario, besides running the DECOR smell detector for the project snapshot related to each commit, the HistoryMiner also computes, for each snapshot and for each source code artifact, a set of quality metrics (see Table II). As done for DECOR, quality metrics are computed for all code artifacts only during the first commit, and updated at each subsequent commit for added and modified files. The purpose of this analysis is to understand whether the trend followed by such metrics differ between files affected by a specific type of smell and files not affected by such a smell. For example, we expect that classes becoming Blobs will exhibit a higher growth rate than classes that are not going to become Blobs.\nIn order to analyze the evolution of the quality metrics, we needed to identify the function that best approximates the data distribution, i.e., the values of the considered metrics computed in a sequence of commits. We found that the best model is the linear function (more details are available in our technical report [45]). Having identified the model to be used, we computed, for each file fj \u2208 ri, the regression line of its quality metric values. If file fj is affected by a specific smellk, we compute the regression line considering the quality metric values computed for each commit involving fj from the first commit (i.e., where the file was added to the versioning system) to the commit where the instance of smellk was detected in fj . Instead, if fj is not affected by\nany smell, we considered only the first nth commits involving the file fj , where n is the average number of commits required by smellk to affect code instances. Then, for each metric reported in Table II, we compared the distributions of regression line slopes for cleanly and smelly files. The comparison is performed using a two-tailed Mann-Whitney U test [17]. The results are intended as statistically significant at \u03b1 = 0.05. We also estimated the magnitude of the observed differences using the Cliff\u2019s Delta (or d), a non-parametric effect size measure [21] for ordinal data. We followed the guidelines in [21] to interpret the effect size values: small for d < 0.33 (positive as well as negative values), medium for 0.33 \u2264 d < 0.474 and large for d \u2265 0.474.\nOverall, the data extraction for RQ1 (i.e., the smells detection and metric computation at each commit for the 200 systems) took eight weeks on a Linux server having 7 quadcore 2.67 GHz CPU (28 cores) and 24 Gb of RAM.\n2) Why are code smells introduced?: One challenge arising when answering RQ2 is represented by the identification of the specific commit (or also possibly a set of commits) where the smell has been introduced (from now on referred to as a smellintroducing commit). Such information is crucial to explain under which circumstances these commits were performed. A trivial solution would have been to use the results of our RQ1 and consider the commit cs in which DECOR detects for the first time a smell instance smellk in a source code file fj as a commit-introducing smell in fj . However, while this solution would work for smell instances that are introduced in the first commit involving fj (there is no doubt on the commit that introduced the smell), it would not work for smell instances that are the consequence of several changes, performed in n different commits involving fj . In such a circumstance, on one hand, we cannot simply assume that the first commit in which DECOR identifies the smell is the one introducing that smell, because the smell appearance might be the result of several small changes performed across the n commits. On the other hand, we cannot assume that all n commits performed on fj are those (gradually) introducing the smell, since just some of them might have pushed fj toward a smelly direction. Thus, to identify the smell-introducing commits for a file fj affected by an instance of a specific smell (smellk), we designed the following heuristic:\n\u2022 if smellk has been introduced in the commit c1 where fj has been added to the repository, then c1 is the smellintroducing commit; \u2022 else given C = {c1, c2, . . . , cn} the set of commits involving fj and leading to the detection of smellk in\ncn we use the results of RQ1 to select the set of quality metrics M allowing to discriminate between the groups of files that are affected and not affected in their history by smellk. These metrics are those for which we found statistically significant difference between the slope of the regression lines for the two groups of files accompanied by at least a medium effect size. Let s be the slope of the regression line for the metric m \u2208 M built when considering all commits leading fj to become affected by a smell and si the slope of the regression line for the metric m built when considering just two subsequent commits, i.e., ci\u22121 and ci for each i \u2208 [2, ..., n]. A commit ci \u2208 C is considered as a smell-introducing commit if |si| > |s|, i.e., the commit ci significantly contributes to the increment (or decrement) of the metric m.\nFig. 1 reports an example aimed at illustrating the smellintroducing commit identification for a file fj . Suppose that fj has been involved in eight commits (from c1 to c8), and that in c8 a Blob instance has been identified by DECOR in fj . Also, suppose that the results of our RQ1 showed that the LOC metric is the only one \u201ccharacterizing\u201d the Blob introduction, i.e., the slope of the LOC regression line for Blobs is significantly different than the one of the regression line built for classes which are not affected by the Blob smell. The black line in Fig. 1 represents the LOC regression line computed among all the involved commits, having a slope of 1.3. The gray lines represent the regression lines between pairs of commits (ci\u22121, ci) where ci is not classified as a smellintroducing commit (their slope is lower than 1.3). Finally, the red-dashed lines represent the regression lines between pairs of commits (ci\u22121, ci) where ci is classified as a smell-introducing commit (their slope is higher than 1.3). Thus, the smellintroducing commits in the example depicted in Fig. 1 are:\nc3, c5, and c7. Overall, we obtained 9,164 smell-introducing commits in the 200 systems, that we used to answer RQ2.\nAfter having identified smell-introducing commits, with the purpose of understanding why a smell was introduced in a project, we classified them by assigning to each commit one or more tags among those reported in Table III. The first set of tags (i.e., commit goal tags) aims at explaining what the developer was doing when introducing the smell. To assign such tags we firstly downloaded the issues for all 200 projects from their JIRA or BUGZILLA issue trackers. Then, we checked whether any of the 9,164 smell-introducing commits were related to any of the collected issues. To link issues to commits we used (and complemented) two existing approaches. The first one is the regular expression-based approach by Fischer et al. [19] matching the issue ID in the commit note. The second one is a re-implementation of the ReLink approach proposed by Wu et al. [46], which considers the following constraints: (i) matching the committer/authors with issue tracking contributor name/email; (ii) the time interval between the commit and the last comment posted by the same author/contributor on the issue tracker must be less than seven days; and (iii) Vector Space Model (VSM) [6] cosine similarity between the commit note and the last comment referred above greater than 0.7. RELINK has been shown to accurately link issues and commits (89% for precision and 78% for recall) [46]. When it was possible to identify a link between one of the smell-introducing commits and an issue, and the issue type was one of the goal-tags in our design (i.e., bug, enhancement, or new feature), such tag was automatically assigned to the commit and its correctness was double checked by one of the authors, which verified the correctness of the issue category (e.g., that an issue classified as bug actually was a bug). This happens in 471 cases, i.e., for a small percentage (5%) of the commits, which is not surprising and in agreement with previous findings [5]. In the remaining 8,693 cases, two of the authors manually analyzed the commits, assigning one or more of the goal-tags by relying on the analysis of the commit message and of the unix diff between the commit under analysis and its predecessor.\nConcerning the project-status tags (see Table III), the Working on release tag can assume as possible values one day, one week, one month, or more than one month before issuing of a major release. The aim of such a tag is to indicate whether, when introducing the smell, the developer was close\nto a project\u2019s deadline. We just consider major releases since those are the ones generally representing a real deadline for developers, while minor releases are sometimes issued just due to a single bug fix. To assign such tags, one of the authors identified the dates in which the major releases were issued by exploiting the GIT tags (often used to tag releases), and the commit messages left by developers. Concerning the Project startup tag, it can assume as values one week, one month, one year, or more than one year after the project\u2019s start date. This tag can be easily assigned by comparing the commit date with the date in which the project started (i.e., the date of the first commit). This tag can be useful to verify whether during the project\u2019s startup, when the project design might not be fully clear, developers are more prone to introducing smells. Note that the Project startup tag can be affected by the presence of projects migrated to git and with a partially available history. For this reason we checked whether the first release tagged in the versioning system were either 0.1 or 1.0 (note that this might be an approximation since projects starting from 1.0 could have a previous 0.x history). As a result, we excluded from the Project startup analysis 31 projects, for a total of 552 smell-introducing commits.\nFinally, we assigned developer-status tags to smellintroducing commits. The Workload tag measures how busy a developer was when introducing the bad smell. In particular, we measured the Workload of each developer involved in a project using time windows of one month, starting from the date in which the developer joined the project (i.e., performed the first commit). The Workload of a developer during one month is measured in terms of the number of commits she performed in that month. We are aware that such a measure (i) is approximated because different commits can require different amount of work; and (ii) a developer could also work on other projects. When analyzing a smell-introducing commit performed by a developer d during a month m, we computed the workload distribution for all developers of the project at m. Then, given Q1 and Q3, the first and the third quartile of such distribution, respectively, we assigned: low as Workload tag if the developer performing the commit had a workload less than Q1, medium if Q1 \u2264 workload < Q3, high if the workload was higher than Q3.\nThe Ownership tag is assigned if the developer performing the smell-introducing commit is the owner of the file on which the smell has been detected. As defined by Bird et al. [10], a file owner is a developer responsible for more than 75% of the commits performed on the file. Lastly, the Newcomer tag is assigned if the smell-introducing commit falls among the first 3 commits in the project for the developer performing it.\nAfter assigning all the described tags to each of the 9,164 smell-introducing commits, we analyzed the results by reporting descriptive statistics of the number of commits to which each tag type had been assigned. Also, we discuss several qualitative examples helping to explain our findings.\n\u25cf"
        },
        {
            "heading": "III. ANALYSIS OF THE RESULTS",
            "text": "This section reports the analysis of the results achieved aiming at answering our two research questions."
        },
        {
            "heading": "A. When are code smells introduced?",
            "text": "Fig. 2 shows the distribution of the number of commits required by each type of smell to manifest itself. The results are grouped by ecosystems; also, we report the Overall results (all ecosystems together). As we can observe in Fig. 2, in almost all cases the median number of commits needed by a smell to affect code components is zero, except for Blob on Android (median=3) and Complex Class on Eclipse (median=1). In other words, most of the smell instances (at least half of them) are introduced when a code entity is added to the versioning system. This is quite a surprising finding, considering the common wisdom that smells are generally the result of continuous maintenance activities [20].\nHowever, the analysis of the box plots also reveals (i) the presence of several outliers; and that (ii) for some smells, in particular Blob and Complex Class, the distribution is quite skewed. This means that besides smell instances introduced in the first commit, there are also several smell instances that are introduced as a result of several changes performed on the file during its evolution. In order to better understand such phenomenon, we analyzed how the values of some quality metrics change during the evolution of such files.\nTable V presents the descriptive statistics (mean and median) of the slope of the regression line computed, for each metric, for both smelly and clean files. Also, Table V reports the results of the Mann-Whitney test and Cliff\u2019s d effect size (Large, Medium, or Small) obtained when analyzing the difference between the slope of regression lines for clean and smelly files. Column cmp of Table V shows a \u2191 (\u2193) if for the metric m there is a statistically significant difference in the m\u2019s slope between the two groups of files, with the smelly ones exhibiting a higher (lower) slope; a \u201d\u2212\u201d is shown when the difference is not statistically significant.\nThe analysis of the results reveals that for all the smells, but Functional Decomposition, the files affected by smells show a higher slope than clean files. This suggests that the files that will be affected by a smell exhibit a steeper growth in terms\nof metric values than files that are not becoming smelly. In other words, when a smell is going to appear, its symptoms (metric value increases) occur very fast, and not gradually. For example, considering the Apache ecosystem, we can see a clear difference between the growth of LOC in Blob and clean classes. Indeed, this latter have a mean growth in terms of LOC characterized by a slope of 0.40, while the slope for Blobs is, on average, 91.82. To make clear the interpretation of such data, let us suppose we plot both regression lines on the Cartesian plane. The regression line for Blobs will have an inclination of 89.38\u25e6, indicating an abrupt growth of LOC, while the inclination of the regression line for clean classes will be 21.8\u25e6, indicating less steep increase of LOC. The same happens when considering the LCOM cohesion metric (the higher the LCOM, the lower the class cohesion). For the overall dataset, the slope for classes that will become Blobs is 849.90 as compared to the 0.25 of clean classes. Thus, while the cohesion of classes generally decreases over time, classes destined to become Blobs exhibit cohesion metric loss orders of magnitude faster than clean classes. In general, the results in Table V show strong differences in the metrics\u2019 slope between clean and smelly files, indicating that it could be possible to create recommenders warning developers when the changes performed on a specific code component show a dangerous trend that could lead to the introduction of a bad smell.\nThe Functional Decomposition (FD) smell deserves a separate discussion. As we can see in Table V, the slope of the regression line for files affected by such a smell is negative.\nThis means that during the evolution of files affected by Functional Decomposition we can observe a decrement (rather than an increment) of the metric values. The rationale behind such a result is intrinsic in the definition of this smell. Specifically, one of the symptoms of such a smell is represented by a class with a single action, such as a function. Thus, the changes that could introduce a Functional Decomposition might be the removal of responsibilities (i.e., methods). This clearly results in the decrease of some metrics, such as NOM, LOC and WMC. As an example, let us consider the class DisplayKMeans of Apache Mahout. The class implemented the K-means clustering algorithm in its original form. However, after three commits the only operation performed by the class was the visualization of the clusters. Indeed, developers moved the actual implementation of the clustering algorithm in the class Job of the package kmeans, introducing a Functional Decomposition in DisplayKMeans.\nOverall, from the analysis of Table V we can conclude that (i) LOC characterizes the introduction of all the smells; (ii) LCOM, WMC, RFC and NOM characterize all the smells but Class Data Should be Private; (iii) CBO does not characterize the introduction of any smell; and (iv) the only metrics characterizing the introduction of Class Data Should be Private are LOC and NOA. Summary for RQ1. Most of the smell instances are introduced when files are created. However, there are also cases, especially for Blob and Complex Class, where the smells manifest themselves after several changes performed on the file. In these cases, files that will become smelly exhibit specific trends for some quality metric values that are significantly different than those of clean files."
        },
        {
            "heading": "B. Why are code smells introduced?",
            "text": "To answer RQ2, we analyze the percentage of smellintroducing commits classified according to the category of tags, i.e.,commit goal, project status, and developer status. Commit-Goal: Table VII reports the percentage of smellintroducing commits assigned to each tag of the category commit-goal. Among the three different ecosystems analyzed, results show that smell instances are mainly introduced when developers perform enhancement operations on the system. When considering the three ecosystems altogether, for all the considered types of smells the percentage of smell-introducing commits tagged as enhancement ranges between 60% and 66%. Note that by enhancement we mean changes applied by developers on existing features aimed at improving them. For example, a Functional Decomposition was introduced in the class CreateProjectFromArchetypeMojo of Apache Maven when the developer performed the \u201cfirst pass at implementing the feature of being able to specify additional goals that can be run after the creation of a project from an archetype\u201d (as reported in the commit log).\nNote that when considering both enhancement or new feature the percentage of smell-introducing commits exceeds, on average, 80%. This indicates, as expected, that the most smellprone activities are performed by developers when adding\nnew features or improving existing features. However, there is also a non-negligible number of smell-introducing commits tagged as bug fixing (between 6% and 16%). This means that also during corrective maintenance developers might introduce a smell, especially when the bug fixing is complex and requires changes to several code entities. For example, the class SecuredModel of Apache Jena builds the security model when a semantic Web operation is requested by the user. In order to fix a bug that did not allow the user to perform a safe authentication, the developer had to update the model, implementing more security controls. This required changing several methods present in the class (10 out of 34). Such changes increase the whole complexity of the class (the WMC metric increased from 29 to 73) making SecuredModel a Complex Class.\nAnother interesting observation from the results reported in Table VII is related to the number of smell-introducing commits tagged as refactoring (between 4% and 11%). While refactoring is the principal treatment to remove smells, we found 394 cases in which developers introduced new smells when performing refactoring operations. For example, the class EC2ImageExtension of Apache jClouds implements the ImageExtension interface, which provides the methods for creating an image. During the evolution, developers added methods for building a new image template as well as a method for managing image layout options (e.g., its alignment) in the EC2ImageExtension class. Subsequently, a developer performed an Extract Class refactoring operation aimed at reorganizing the responsibility of the class. Indeed, the developer split the original class into two new classes, i.e., ImageTemplateImpl and CreateImageOptions. However, the developer also introduced a Functional Decomposition in the class CreateImageOptions since such a class, after the refactoring, contains just one method, i.e., the one in charge of managing the image options. This result sheds light on the dark side of refactoring; besides the risk of introducing faults [8], when performing refactoring operations, there is also the risk of introducing design problems.\nLooking into the ecosystems, the general trend discussed so far holds for Apache and Eclipse. Regarding Android, we notice something different for Complex Class and Spaghetti Code smells. In these cases, the smell-introducing commits are mainly due to the introduction of new features. Such a difference could be due to the particular development model used for Android apps. Specifically, we manually analyzed the instances of smells identified in the 70 Android apps, and we observed that in the majority of cases classes affected by a smell are those extending the Android Activity class, i.e., a class extended by developers to provide features to the app\u2019s users.\nSpecifically, we observed that quite often developers introduce a Complex Class or a Spaghetti Code smell when adding a new feature to their apps by extending the Activity class. For example, the class ArticleViewActivity of the Aard5 app became a Complex Class after the addition of several new features (spread across 50 commits after its creation), such as the management of page buttons and online visualization of the article. All these changes contributed to increase the slope of the regression line for the RFC metric of a factor of 3.91 and for WMC of a factor of 2.78. Project status: Table VIII reports the percentage of smellintroducing commits assigned to each tag of the category project-status. As expected, most of the smells are introduced the last month before issuing a release. Indeed, the percentage of smells introduced more than one month prior to issuing\n5Aard is an offline Wikipedia reader.\na release is really low (ranging between 0% and 11%). This\nconsideration holds for all the ecosystems and for all the bad smells analyzed, thus confirming the common wisdom that the deadline pressure on developers can be one of the main causes for smell introduction.\nConsidering the project startup tag, the results are quite unexpected. Indeed, a high number of smell instances are introduced few months after the project startup. This is particularly true for Blob, Class Data Should Be Private, and Complex Class, where more than half of the instances are introduced in the first year of system\u2019s lifecycle. Instead, Functional Decomposition, and especially Spaghetti Code, seem to be the types of smells that take more time to manifest themselves with more than 75% of Spaghetti Code instances introduced after the first year. This result contradicts, at least in part, the common wisdom that smells are introduced after several continuous maintenance activities and, thus, are more pertinent to advanced phases of the development process [20], [38]. Developer status: Finally, Table IX reports the percentage of smell-introducing commits assigned to each tag of the category developer-status. From the analysis of the results it is evident that the developers\u2019 workload negatively influences the quality of the source code produced. On the overall dataset, at least in 55% of cases the developer who introduces the smell has a high workload. For example, on the InvokerMavenExecutor class in Apache Maven a developer introduced a Blob smell while adding the command line parsing to enable users alternate the settings. When performing such a change, the developer had relatively high workload while working on nine other different classes (in this case, the workload was classified as high).\nWe can also observe that generally the developers who introduce a smell are not newcomers while often they are owners of the files. At the first glance, this could look like an unexpected result. The owner of the file\u2014one of the most experienced developers of the file\u2014is the one that has the higher likelihood of introducing a smell. However, as also discussed by Zeller in his book Why programs fail, more experienced developers tend to perform more complex and critical tasks [51]. Thus, it is likely that their commits are more prone to introducing design problems.\nSummary for RQ2. Smells are generally introduced by developers when enhancing existing features or implementing new ones. As expected, smells are generally introduced in the last month before issuing a deadline, while there is a considerable number of instances introduced in the first year from the project startup. Finally, developers that introduce smells are generally the owners of the file and they are more prone to introducing smells when they have higher workloads."
        },
        {
            "heading": "IV. THREATS TO VALIDITY",
            "text": "The main threats related to the relationship between theory and observation (construct validity) are due to imprecisions/errors in the measurements we performed. Above all, we relied on DECOR rules to detect smells. Notice that our re-implementation uses the exact rules defined by Moha et al. [34], and has been already used in our previous work [37]. Nevertheless, we are aware that our results can be affected by the presence of false positives and false negatives. Moha et al. reported for DECOR a precision above 60% and a recall of 100% on Xerces 2.7.0. As for the precision, other than relying on Moha et al. assessment, we have manually validated a subset of the 4,627 detected smell instances. This manual validation has been performed by two authors independently, and cases of disagreement were discussed. In total, 1,107 smells were validated, including 241 Blob instances, 317 Class Data Should Be Private, 166 Complex Class, 65 Spaghetti Code, and 318 Functional Decomposition. Such a (stratified) sample is deemed to be statistically significant for a 95% confidence level and \u00b110% confidence interval [41]. The results of the manual validation indicated a mean precision of 73%, and specifically 79% for Blob, 62% for Class Data Should Be Private, 74% for Complex Class, 82% for Spaghetti Code, and 70% for Functional Decomposition. In addition, we replicated all the analysis performed to answer our two research questions by just considering the smell-introducing commits (2,555) involving smell instances that have been manually validated as true positives. The results achieved in this analysis (available in our replication package [45]) are perfectly inline with those obtained in our paper on the complete set of 9,164 smell-introducing commits, confirming all our findings. Finally, we are aware that our study can also suffer from presence of false negatives. However, (i) the sample of investigated smell instances is pretty large (4,627 instances), and (ii) the DECOR\u2019s claimed recall is very high.\nAs explained in Section II, the heuristic for excluding projects with incomplete history from the Project startup analysis may have failed to discard some projects. Also, we excluded the first commit from a project\u2019s history involving Java files from the analysis of smell-introducing commits, because such commits are likely to be imports from old versioning systems, and, therefore, we only focused our attention (in terms of first commit) on the addition of new files during the observed history period. Concerning the tags used to characterize smell-introducing changes, the commit classification was performed by two different authors and results were compared and discussed in cases of inconsistencies. Also, a second check\nwas performed for those commits linked to issues (only 471 out of 9,164 commits), to avoid problems due to incorrect issue classification [2], [24].\nThe analysis of developer-related tags was performed using the Git author information instead of relying on committers (not all authors have commit privileges in open source projects, hence observing committers would give an imprecise and partial view of the reality). However, there is no guarantee that the reported authorship is always accurate and complete. We are aware that the Workload tag measures the developers\u2019 activity within a single project, while in principle one could be busy on other projects or different other activities. One possibility to mitigate such a threat could have been to measure the workload of a developer within the entire ecosystem. However, in our opinion, this would have introduced some bias, i.e., assigning a high workload to developers working on several projects of the same ecosystem and a low workload to those that, while not working on other projects of the same ecosystem, could have been busy on projects outside the ecosystem. It is also important to point out that, in terms of relationship between Workload tag and smell introduction, we obtained consistent results across three ecosystems, which at least mitigates the presence of a possible threat. Also, estimating the Workload by just counting commits is an approximation. However, we do not use the commit size because there might be a small commit requiring a substantial effort as well.\nAs for the threats that could have influenced the results (internal validity), we performed the study by comparing classes affected (and not) by a specific type of smell. However, there can be also cases of classes affected by different types of smells at the same time. Our investigation revealed that such classes represent a minority (3% for Android, 5% for Apache, and 9% for Eclipse), and, therefore, the interaction between different types of smells is not particularly interesting to investigate, given also the complexity it would have added to the study design and to its presentation. Finally, while in RQ2 we studied tags related to different aspects of a software project lifetime\u2014characterizing commits, developers, and the project status itself\u2014we are aware that there could be many other factors that could have influenced the introduction of smells. In any case, it is worth noting that it is beyond the scope of this work to make any claims related to causation of the relationship between the introduction of smells and product or process factors characterizing a software project.\nThe main threats related to the relationship between the treatment and the outcome (conclusion validity) are represented by the analysis method exploited in our study. In RQ1, we used non-parametric tests (Mann-Whitney) and effect size measures (Cliff\u2019s Delta), as well as regression analysis. Results of RQ2 are, instead, reported in terms of descriptive statistics and analyzed from purely observational point of view.\nFinally, regarding the generalization of our findings (external validity) this is, to the best of our knowledge, the largest study\u2014in terms of number of projects (200)\u2014concerning the analysis of code smells and of their evolution. However, we are aware that we limited our attention to only five types of\nsmells. As explained in Section II, this choice is justified by the need for limiting the computation since we wanted to analyze a large number of projects. Also, we tried to diversify the types of smells including smells representing violations of OO principles and \u201csize-related\u201d smells. Last, but not least, we made sure to include smells\u2014such as Complex Class, Blob, and Spaghetti Code\u2014that previous studies indicated to be perceived by developers as severe problems [36]. Nevertheless, further studies aiming at replicating our work on other smells, with projects developed in other programming languages, are desirable."
        },
        {
            "heading": "V. RELATED WORK",
            "text": "This section discusses work investigating the evolution of code smells in software systems and their effect on maintenance activities and on software quality. Khomh et al. [26], [25] studied the relationship between the presence of code smells and software change- and fault-proneness. They found that classes affected by code smells tend to be significantly more change- [25] and fault- prone [26] than other classes. Empirical evidence demonstrating that some bad smells correlate with higher fault-proneness has also been reported by Li and Shatnawi [30]. Abbes et al. [1] conducted three controlled experiments with the aim of investigating the impact of Blob and Spaghetti Code smells on program comprehension. Their results indicated that single occurrence of such smells does not significantly decrease developer\u2019s performance, while the coexistence of multiple bad smell instances in the same class significantly reduces developers\u2019 performance during maintenance tasks. Similar results were obtained by Yamashita and Moonen [48] when studying the interaction of different code smells. Their results indicate that the maintenance problems are strictly related to the presence of more bad smells in the same file. They also investigated the impact of bad smells on maintainability characteristics [49]. As discussed in Section IV we do not focus on smell co-occurrences because they happen in a very small percentage (< 9%) of affected classes. The controlled experiment conducted by Sj\u00f8berg et al. [43] confirmed that smells do not always constitute a problem, and that often class size impacts maintainability more than the presence of smells. Other studies investigate the impact of smells and their perception by surveying project developers. Arcoverde et al. [3] investigated how developers react to the presence of bad smells in their code. The results of their survey indicate that code smells often remain in source code for a long time and the main reason for postponing their removal through refactoring activities is to avoid API modifications [3]. A recent paper presented an empirical study aimed at investigating the perception of 13 types of smells [36], by showing to the participants code snippets containing (or not) smells. The results of such a study show that smells related to complex/long source code are generally perceived as harmful, while the other types of smells are not perceived or perceived only if the \u201cintensity\u201d of the problem is high. Yamashita and Moonen [50] conducted a survey involving 85 professionals, concerning the perceived severity of code smells and the need\nto remove them. Their results indicated that 32% of developers do not know (or know little) about code smells, and those who are aware about the problem, pointed out that in many cases smell removal was not a priority, because of time pressure or lack of adequate tool support. In summary, although with contrasting results, the studies discussed above provide a general evidence that\u2014at least in specific circumstances\u2014 code smells have negative effects on software quality. Despite that, however, developers seem reluctant to perform activities aimed at their removal. Chatzigeorgiou and Manakos [15] analyzed this phenomena and their results indicate that in most cases, the design problems persist up to the latest examined version accumulating even more as the project matures. Very few bad smells are removed from the project, and in the vast majority of these cases this was not due to specific refactoring activities, but rather a side-effect of adaptive maintenance [15]. They also pointed out some findings consistent with our RQ1, i.e., they indicated that a conspicuous percentage of smells were introduced when the affected source code entity was added in the system[15]. However, their study does not provide quantitative data showing the magnitude of such phenomenon, as we do. It is also important to point out that we performed our analysis at commit-level (unlike to the related work that conducted those studies at release level), which allowed us to identify when bad smells appear in the source code. Finally, our results are based on 200 analyzed systems instead of two systems analyzed by the study that we mentioned earlier. Peters and Zaidman [39] analyzed the bad smells\u2019 lifespan focusing on developers\u2019 behavior in the presence of such smells, confirming that often, even if the developers are aware of the bad smells\u2019 presence, they do not perform refactoring activities."
        },
        {
            "heading": "VI. CONCLUSION AND LESSONS LEARNED",
            "text": "This paper presented a large-scale empirical study conducted over the commit history of 200 open source projects and aimed at understanding when and why bad code smells are introduced. These results provide several valuable findings for the research community: Lesson 1. Most of times code artifacts are affected by bad smells since their creation. This result contradicts the common wisdom that bad smells are generally due to a negative effect of software evolution. Also, this finding highlights that the introduction of most smells can simply be avoided by performing quality checks at commit time. In other words, instead of running smell detector time-to-time on the entire system, these tools could be used during commit activities (in particular circumstances, such as before issuing a release) to avoid or at least limit the introduction of bad code smells. Lesson 2. Code artifacts becoming smelly as consequence of maintenance and evolution activities are characterized by peculiar metrics\u2019 trends, different from those of clean artifacts. This is in agreement with previous findings on the historical evolution of code smells [32], [37], [40]. Also, such results encourage the development of recommenders able of alerting software developers when changes applied to a code artifacts\nresult in worrisome metric trends, generally characterizing artifacts that will be affected by a smell. Lesson 3. While implementing new features and enhancing existing ones are, as expected, the main activities during which developers tend to introduce smells, we found almost 400 cases in which refactoring operations introduced smells. This result is quite surprising, given that one of the goals behind refactoring is the removal of bad smells [20]. This finding highlights the need for techniques and tools aimed at assessing the impact of refactoring operations on source code before their actual application (e.g., see the recent work by Chaparro et al. [14]). Lesson 4. Newcomers are not necessary responsible for introducing bad smalls, while developers with high workloads and release pressure are more prone to introducing smell instances. This result highlights that code inspection practices should be strengthened when developers are working under these stressful conditions.\nThese lessons learned represent the main input for our future research agenda on the topic, mainly focused on designing and developing a new generation of code quality-checkers, such as those described in Lesson 2."
        }
    ],
    "title": "When and Why Your Code Starts to Smell Bad",
    "year": 2014
}