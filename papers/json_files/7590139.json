{
    "abstractText": "The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance. Introduction The many recent successes in scaling reinforcement learning (RL) to complex sequential decision-making problems were kick-started by the Deep Q-Networks algorithm (DQN; Mnih et al. 2013, 2015). Its combination of Q-learning with convolutional neural networks and experience replay enabled it to learn, from raw pixels, how to play many Atari games at human-level performance. Since then, many extensions have been proposed that enhance its speed or stability. Double DQN (DDQN; van Hasselt, Guez, and Silver 2016) addresses an overestimation bias of Q-learning (van Hasselt 2010), by decoupling selection and evaluation of the bootstrap action. Prioritized experience replay (Schaul et al. 2015) improves data efficiency, by replaying more often transitions from which there is more to learn. The dueling network architecture (Wang et al. 2016) helps to generalize across actions by separately representing state values and action advantages. Learning from multi-step bootstrap targets (Sutton 1988; Sutton and Barto 1998), as used in A3C (Mnih et al. 2016), shifts the bias-variance tradeoff and helps to propagate newly observed rewards faster to earlier visited states. Distributional Q-learning (Bellemare, Dabney, and Munos 2017) learns a categorical distribution of discounted returns, instead of estimating the mean. Noisy DQN (Fortunato et al. 2017) uses stochastic network layers for exploration. This list is, of course, far from exhaustive. Each of these algorithms enables substantial performance improvements in isolation. Since they do so by addressing Copyright c \u00a9 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. 7 44 100 200 Millions of frames 0% 100% 200% M e d ia n h u m a n -n o rm a liz e d s co re DQN DDQN Prioritized DDQN Dueling DDQN A3C Distributional DQN Noisy DQN Rainbow Figure 1: Median human-normalized performance across 57 Atari games. We compare our integrated agent (rainbowcolored) to DQN (grey) and six published baselines. Note that we match DQN\u2019s best performance after 7M frames, surpass any baseline within 44M frames, and reach substantially improved final performance. Curves are smoothed with a moving average over 5 points. radically different issues, and since they build on a shared framework, they could plausibly be combined. In some cases this has been done: Prioritized DDQN and Dueling DDQN both use double Q-learning, and Dueling DDQN was also combined with prioritized experience replay. In this paper we propose to study an agent that combines all the aforementioned ingredients. We show how these different ideas can be integrated, and that they are indeed largely complementary. In fact, their combination results in new stateof-the-art results on the benchmark suite of 57 Atari 2600 games from the Arcade Learning Environment (Bellemare et al. 2013), both in terms of data efficiency and of final performance. Finally we show results from ablation studies to help understand the contributions of the different components. ar X iv :1 71 0. 02 29 8v 1 [ cs .A I] 6 O ct 2 01 7 Background Reinforcement learning addresses the problem of an agent learning to act in an environment in order to maximize a scalar reward signal. No direct supervision is provided to the agent, for instance it is never directly told the best action. Agents and environments. At each discrete time step t = 0, 1, 2 . . ., the environment provides the agent with an observation St, the agent responds by selecting an action At, and then the environment provides the next reward Rt+1, discount \u03b3t+1, and state St+1. This interaction is formalized as a Markov Decision Process, or MDP, which is a tuple \u3008S,A, T, r, \u03b3\u3009, where S is a finite set of states, A is a finite set of actions, T (s, a, s\u2032) = P [St+1 = s\u2032 | St = s,At = a] is the (stochastic) transition function, r(s, a) = E[Rt+1 | St = s,At = a] is the reward function, and \u03b3 \u2208 [0, 1] is a discount factor. In our experiments MDPs will be episodic with a constant \u03b3t = \u03b3, except on episode termination where \u03b3t = 0, but the algorithms are expressed in the general form. On the agent side, action selection is given by a policy \u03c0 that defines a probability distribution over actions for each state. From the state St encountered at time t, we define the discounted return Gt = \u2211\u221e k=0 \u03b3 (k) t Rt+k+1 as the discounted sum of future rewards collected by the agent, where the discount for a reward k steps in the future is given by the product of discounts before that time, \u03b3 t = \u220fk i=1 \u03b3t+i. An agent aims to maximize the expected discounted return by finding a good policy. The policy may be learned directly, or it may be constructed as a function of some other learned quantities. In value-based reinforcement learning, the agent learns an estimate of the expected discounted return, or value, when following a policy \u03c0 starting from a given state, v(s) = E\u03c0[Gt|St = s], or state-action pair, q(s, a) = E\u03c0[Gt|St = s,At = a]. A common way of deriving a new policy from a state-action value function is to act -greedily with respect to the action values. This corresponds to taking the action with the highest value (the greedy action) with probability (1\u2212 ), and to otherwise act uniformly at random with probability . Policies of this kind are used to introduce a form of exploration: by randomly selecting actions that are sub-optimal according to its current estimates, the agent can discover and correct its estimates when appropriate. The main limitation is that it is difficult to discover alternative courses of action that extend far into the future; this has motivated research on more directed forms of exploration. Deep reinforcement learning and DQN. Large state and/or action spaces make it intractable to learn Q value estimates for each state and action pair independently. In deep reinforcement learning, we represent the various components of agents, such as policies \u03c0(s, a) or values q(s, a), with deep (i.e., multi-layer) neural networks. The parameters of these networks are trained by gradient descent to minimize some suitable loss function. In DQN (Mnih et al. 2015) deep networks and reinforcement learning were successfully combined by using a convolutional neural net to approximate the action values for a given state St (which is fed as input to the network in the form of a stack of raw pixel frames). At each step, based on the current state, the agent selects an action -greedily with respect to the action values, and adds a transition (St, At, Rt+1, \u03b3t+1, St+1) to a replay memory buffer (Lin 1992), that holds the last million transitions. The parameters of the neural network are optimized by using stochastic gradient descent to minimize the loss (Rt+1 + \u03b3t+1 max a\u2032 q\u03b8(St+1, a \u2032)\u2212 q\u03b8(St, At)) , (1) where t is a time step randomly picked from the replay memory. The gradient of the loss is back-propagated only into the parameters \u03b8 of the online network (which is also used to select actions); the term \u03b8 represents the parameters of a target network; a periodic copy of the online network which is not directly optimized. The optimization is performed using RMSprop (Tieleman and Hinton 2012), a variant of stochastic gradient descent, on mini-batches sampled uniformly from the experience replay. This means that in the loss above, the time index t will be a random time index from the last million transitions, rather than the current time. The use of experience replay and target networks enables relatively stable learning of Q values, and led to superhuman performance on several Atari games. Extensions to DQN DQN has been an important milestone, but several limitations of this algorithm are now known, and many extensions have been proposed. We propose a selection of six extensions that each have addressed a limitation and improved overall performance. To keep the size of the selection manageable, we picked a set of extensions that address distinct concerns (e.g., just one of the many addressing exploration). Double Q-learning. Conventional Q-learning is affected by an overestimation bias, due to the maximization step in Equation 1, and this can harm learning. Double Q-learning (van Hasselt 2010), addresses this overestimation by decoupling, in the maximization performed for the bootstrap target, the selection of the action from its evaluation. It is possible to effectively combine this with DQN (van Hasselt, Guez, and Silver 2016), using the loss (Rt+1+\u03b3t+1q\u03b8(St+1, argmax a\u2032 q\u03b8(St+1, a ))\u2212q\u03b8(St, At)). This change was shown to reduce harmful overestimations that were present for DQN, thereby improving performance. Prioritized replay. DQN samples uniformly from the replay buffer. Ideally, we want to sample more frequently those transitions from which there is much to learn. As a proxy for learning potential, prioritized experience replay (Schaul et al. 2015) samples transitions with probability pt relative to the last encountered absolute TD error: pt \u221d \u2223\u2223\u2223Rt+1 + \u03b3t+1 max a\u2032 q\u03b8(St+1, a \u2032)\u2212 q\u03b8(St, At) \u2223\u2223\u2223\u03c9 , where \u03c9 is a hyper-parameter that determines the shape of the distribution. New transitions are inserted into the replay buffer with maximum priority, providing a bias towards recent transitions. Note that stochastic transitions might also be favoured, even when there is little left to learn about them. Dueling networks. The dueling network is a neural network architecture designed for value based RL. It features two streams of computation, the value and advantage streams, sharing a convolutional encoder, and merged by a special aggregator (Wang et al. 2016). This corresponds to the following factorization of action values: q\u03b8(s, a) = v\u03b7(f\u03be(s)) + a\u03c8(f\u03be(s), a)\u2212 \u2211 a\u2032 a\u03c8(f\u03be(s), a \u2032) Nactions , where \u03be, \u03b7, and \u03c8 are, respectively, the parameters of the shared encoder f\u03be, of the value stream v\u03b7 , and of the advantage stream a\u03c8; and \u03b8 = {\u03be, \u03b7, \u03c8} is their concatenation. Multi-step learning. Q-learning accumulates a single reward and then uses the greedy action at the next step to bootstrap. Alternatively, forward-view multi-step targets can be used (Sutton 1988). We define the truncated n-step return from a given state St as",
    "authors": [
        {
            "affiliations": [],
            "name": "Matteo Hessel"
        },
        {
            "affiliations": [],
            "name": "Joseph Modayil"
        },
        {
            "affiliations": [],
            "name": "Hado van Hasselt"
        },
        {
            "affiliations": [],
            "name": "Tom Schaul"
        },
        {
            "affiliations": [],
            "name": "Georg Ostrovski"
        },
        {
            "affiliations": [],
            "name": "Will Dabney"
        },
        {
            "affiliations": [],
            "name": "Dan Horgan"
        },
        {
            "affiliations": [],
            "name": "Bilal Piot"
        },
        {
            "affiliations": [],
            "name": "Mohammad Azar"
        },
        {
            "affiliations": [],
            "name": "David Silver"
        }
    ],
    "id": "SP:0ab3f7ecbdc5a33565a234215604a6ca9d155a33",
    "references": [
        {
            "authors": [
                "M.G. Bellemare",
                "Y. Naddaf",
                "J. Veness",
                "M. Bowling"
            ],
            "title": "The arcade learning environment: An evaluation platform for general agents",
            "venue": "J. Artif. Intell. Res. (JAIR) 47:253\u2013 279.",
            "year": 2013
        },
        {
            "authors": [
                "M.G. Bellemare",
                "S. Srinivasan",
                "G. Ostrovski",
                "T. Schaul",
                "D. Saxton",
                "R. Munos"
            ],
            "title": "Unifying count-based exploration and intrinsic motivation",
            "venue": "NIPS.",
            "year": 2016
        },
        {
            "authors": [
                "M.G. Bellemare",
                "W. Dabney",
                "R. Munos"
            ],
            "title": "A distributional perspective on reinforcement learning",
            "venue": "ICML.",
            "year": 2017
        },
        {
            "authors": [
                "C. Blundell",
                "B. Uria",
                "A. Pritzel",
                "Y. Li",
                "A. Ruderman",
                "J.Z. Leibo",
                "J. Rae",
                "D. Wierstra",
                "D. Hassabis"
            ],
            "title": "Model-Free Episodic Control",
            "venue": "ArXiv e-prints.",
            "year": 2016
        },
        {
            "authors": [
                "A. Dosovitskiy",
                "V. Koltun"
            ],
            "title": "Learning to act by predicting the future",
            "venue": "CoRR abs/1611.01779.",
            "year": 2016
        },
        {
            "authors": [
                "M. Fortunato",
                "M.G. Azar",
                "B. Piot",
                "J. Menick",
                "I. Osband",
                "A. Graves",
                "V. Mnih",
                "R. Munos",
                "D. Hassabis",
                "O. Pietquin",
                "C. Blundell",
                "S. Legg"
            ],
            "title": "Noisy networks for exploration",
            "venue": "CoRR abs/1706.10295.",
            "year": 2017
        },
        {
            "authors": [
                "M. Hausknecht",
                "P. Stone"
            ],
            "title": "Deep recurrent Qlearning for partially observable MDPs",
            "venue": "arXiv preprint arXiv:1507.06527.",
            "year": 2015
        },
        {
            "authors": [
                "F.S. He",
                "Y. Liu",
                "A.G. Schwing",
                "J. Peng"
            ],
            "title": "Learning to play in a day: Faster deep reinforcement learning by optimality tightening",
            "venue": "CoRR abs/1611.01606.",
            "year": 2016
        },
        {
            "authors": [
                "M. Jaderberg",
                "V. Mnih",
                "W.M. Czarnecki",
                "T. Schaul",
                "J.Z. Leibo",
                "D. Silver",
                "K. Kavukcuoglu"
            ],
            "title": "Reinforcement learning with unsupervised auxiliary tasks",
            "venue": "CoRR abs/1611.05397.",
            "year": 2016
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "Proceedings of the 3rd International Conference on Learning Representations (ICLR).",
            "year": 2014
        },
        {
            "authors": [
                "T.D. Kulkarni",
                "K. Narasimhan",
                "A. Saeedi",
                "J.B. Tenenbaum"
            ],
            "title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation",
            "venue": "CoRR abs/1604.06057.",
            "year": 2016
        },
        {
            "authors": [
                "T.D. Kulkarni",
                "A. Saeedi",
                "S. Gautam",
                "S.J. Gershman"
            ],
            "title": "Deep successor reinforcement learning",
            "venue": "arXiv preprint arXiv:1606.02396.",
            "year": 2016
        },
        {
            "authors": [
                "Lin",
                "L.-J."
            ],
            "title": "Self-improving reactive agents based on reinforcement learning, planning and teaching",
            "venue": "Machine Learning 8(3):293\u2013321.",
            "year": 1992
        },
        {
            "authors": [
                "V. Mnih",
                "K. Kavukcuoglu",
                "D. Silver",
                "A. Graves",
                "I. Antonoglou",
                "D. Wierstra",
                "M.A. Riedmiller"
            ],
            "title": "Playing atari with deep reinforcement learning",
            "venue": "CoRR abs/1312.5602.",
            "year": 2013
        },
        {
            "authors": [
                "S.",
                "Hassabis, D."
            ],
            "title": "Human-level control through deep reinforcement learning",
            "venue": "Nature 518(7540):529\u2013533.",
            "year": 2015
        },
        {
            "authors": [
                "V. Mnih",
                "A.P. Badia",
                "M. Mirza",
                "A. Graves",
                "T. Lillicrap",
                "T. Harley",
                "D. Silver",
                "K. Kavukcuoglu"
            ],
            "title": "Asynchronous methods for deep reinforcement learning",
            "venue": "International Conference on Machine Learning.",
            "year": 2016
        },
        {
            "authors": [
                "A. Nair",
                "P. Srinivasan",
                "S. Blackwell",
                "C. Alcicek",
                "R. Fearon",
                "A. De Maria",
                "V. Panneershelvam",
                "M. Suleyman",
                "C. Beattie",
                "S. Petersen",
                "S. Legg",
                "V. Mnih",
                "K. Kavukcuoglu",
                "D. Silver"
            ],
            "title": "Massively parallel methods for deep reinforcement learning",
            "venue": "arXiv preprint arXiv:1507.04296.",
            "year": 2015
        },
        {
            "authors": [
                "B. O\u2019Donoghue",
                "R. Munos",
                "K. Kavukcuoglu",
                "V. Mnih"
            ],
            "title": "Pgq: Combining policy gradient and q-learning",
            "venue": "CoRR abs/1611.01626",
            "year": 2016
        },
        {
            "authors": [
                "I. Osband",
                "C. Blundell",
                "A. Pritzel",
                "B.V. Roy"
            ],
            "title": "Deep exploration via bootstrapped dqn",
            "venue": "NIPS.",
            "year": 2016
        },
        {
            "authors": [
                "T. Salimans",
                "J. Ho",
                "X. Chen",
                "I. Sutskever"
            ],
            "title": "Evolution strategies as a scalable alternative to reinforcement learning",
            "venue": "CoRR abs/1703.03864.",
            "year": 2017
        },
        {
            "authors": [
                "T. Schaul",
                "J. Quan",
                "I. Antonoglou",
                "D. Silver"
            ],
            "title": "Prioritized experience replay",
            "venue": "Proc. of ICLR.",
            "year": 2015
        },
        {
            "authors": [
                "J. Schulman",
                "S. Levine",
                "P. Moritz",
                "M. Jordan",
                "P. Abbeel"
            ],
            "title": "Trust region policy optimization",
            "venue": "Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37, ICML\u201915, 1889\u2013 1897. JMLR.org.",
            "year": 2015
        },
        {
            "authors": [
                "S. Sharma",
                "A.S. Lakshminarayanan",
                "B. Ravindran"
            ],
            "title": "Learning to repeat: Fine grained action repetition for deep reinforcement learning",
            "venue": "arXiv preprint arXiv:1702.06054.",
            "year": 2017
        },
        {
            "authors": [
                "B.C. Stadie",
                "S. Levine",
                "P. Abbeel"
            ],
            "title": "Incentivizing exploration in reinforcement learning with deep predictive models",
            "venue": "CoRR abs/1507.00814.",
            "year": 2015
        },
        {
            "authors": [
                "R.S. Sutton",
                "A.G. Barto"
            ],
            "title": "Reinforcement Learning: An Introduction",
            "venue": "The MIT press, Cambridge MA.",
            "year": 1998
        },
        {
            "authors": [
                "R.S. Sutton"
            ],
            "title": "Learning to predict by the methods of temporal differences",
            "venue": "Machine learning 3(1):9\u201344.",
            "year": 1988
        },
        {
            "authors": [
                "T. Tieleman",
                "G. Hinton"
            ],
            "title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning 4(2):26\u201331",
            "year": 2012
        },
        {
            "authors": [
                "H. van Hasselt",
                "A. Guez",
                "M. Hessel",
                "V. Mnih",
                "D. Silver"
            ],
            "title": "Learning values across many orders of magnitude",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2016
        },
        {
            "authors": [
                "H. van Hasselt",
                "A. Guez",
                "D. Silver"
            ],
            "title": "Deep reinforcement learning with double Q-learning",
            "venue": "In Proc. of AAAI,",
            "year": 2016
        },
        {
            "authors": [
                "A.S. Vezhnevets",
                "S. Osindero",
                "T. Schaul",
                "N. Heess",
                "M. Jaderberg",
                "D. Silver",
                "K. Kavukcuoglu"
            ],
            "title": "Feudal networks for hierarchical reinforcement learning",
            "venue": "CoRR abs/1703.01161.",
            "year": 2017
        },
        {
            "authors": [
                "Z. Wang",
                "T. Schaul",
                "M. Hessel",
                "H. van Hasselt",
                "M. Lanctot",
                "N. de Freitas"
            ],
            "title": "Dueling network architectures for deep reinforcement learning",
            "venue": "In Proceedings of The 33rd International Conference on Machine Learning,",
            "year": 2016
        }
    ],
    "sections": [
        {
            "heading": "Introduction",
            "text": "The many recent successes in scaling reinforcement learning (RL) to complex sequential decision-making problems were kick-started by the Deep Q-Networks algorithm (DQN; Mnih et al. 2013, 2015). Its combination of Q-learning with convolutional neural networks and experience replay enabled it to learn, from raw pixels, how to play many Atari games at human-level performance. Since then, many extensions have been proposed that enhance its speed or stability.\nDouble DQN (DDQN; van Hasselt, Guez, and Silver 2016) addresses an overestimation bias of Q-learning (van Hasselt 2010), by decoupling selection and evaluation of the bootstrap action. Prioritized experience replay (Schaul et al. 2015) improves data efficiency, by replaying more often transitions from which there is more to learn. The dueling network architecture (Wang et al. 2016) helps to generalize across actions by separately representing state values and action advantages. Learning from multi-step bootstrap targets (Sutton 1988; Sutton and Barto 1998), as used in A3C (Mnih et al. 2016), shifts the bias-variance tradeoff and helps to propagate newly observed rewards faster to earlier visited states. Distributional Q-learning (Bellemare, Dabney, and Munos 2017) learns a categorical distribution of discounted returns, instead of estimating the mean. Noisy DQN (Fortunato et al. 2017) uses stochastic network layers for exploration. This list is, of course, far from exhaustive.\nEach of these algorithms enables substantial performance improvements in isolation. Since they do so by addressing\nCopyright c\u00a9 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nradically different issues, and since they build on a shared framework, they could plausibly be combined. In some cases this has been done: Prioritized DDQN and Dueling DDQN both use double Q-learning, and Dueling DDQN was also combined with prioritized experience replay. In this paper we propose to study an agent that combines all the aforementioned ingredients. We show how these different ideas can be integrated, and that they are indeed largely complementary. In fact, their combination results in new stateof-the-art results on the benchmark suite of 57 Atari 2600 games from the Arcade Learning Environment (Bellemare et al. 2013), both in terms of data efficiency and of final performance. Finally we show results from ablation studies to help understand the contributions of the different components.\nar X\niv :1\n71 0.\n02 29\n8v 1\n[ cs\n.A I]\n6 O\nct 2\n01 7"
        },
        {
            "heading": "Background",
            "text": "Reinforcement learning addresses the problem of an agent learning to act in an environment in order to maximize a scalar reward signal. No direct supervision is provided to the agent, for instance it is never directly told the best action.\nAgents and environments. At each discrete time step t = 0, 1, 2 . . ., the environment provides the agent with an observation St, the agent responds by selecting an action At, and then the environment provides the next reward Rt+1, discount \u03b3t+1, and state St+1. This interaction is formalized as a Markov Decision Process, or MDP, which is a tuple \u3008S,A, T, r, \u03b3\u3009, where S is a finite set of states, A is a finite set of actions, T (s, a, s\u2032) = P [St+1 = s\u2032 | St = s,At = a] is the (stochastic) transition function, r(s, a) = E[Rt+1 | St = s,At = a] is the reward function, and \u03b3 \u2208 [0, 1] is a discount factor. In our experiments MDPs will be episodic with a constant \u03b3t = \u03b3, except on episode termination where \u03b3t = 0, but the algorithms are expressed in the general form.\nOn the agent side, action selection is given by a policy \u03c0 that defines a probability distribution over actions for each state. From the state St encountered at time t, we define the discounted return Gt = \u2211\u221e k=0 \u03b3 (k) t Rt+k+1 as the discounted sum of future rewards collected by the agent, where the discount for a reward k steps in the future is given by the product of discounts before that time, \u03b3(k)t = \u220fk i=1 \u03b3t+i. An agent aims to maximize the expected discounted return by finding a good policy.\nThe policy may be learned directly, or it may be constructed as a function of some other learned quantities. In value-based reinforcement learning, the agent learns an estimate of the expected discounted return, or value, when following a policy \u03c0 starting from a given state, v\u03c0(s) = E\u03c0[Gt|St = s], or state-action pair, q\u03c0(s, a) = E\u03c0[Gt|St = s,At = a]. A common way of deriving a new policy from a state-action value function is to act -greedily with respect to the action values. This corresponds to taking the action with the highest value (the greedy action) with probability (1\u2212 ), and to otherwise act uniformly at random with probability . Policies of this kind are used to introduce a form of exploration: by randomly selecting actions that are sub-optimal according to its current estimates, the agent can discover and correct its estimates when appropriate. The main limitation is that it is difficult to discover alternative courses of action that extend far into the future; this has motivated research on more directed forms of exploration.\nDeep reinforcement learning and DQN. Large state and/or action spaces make it intractable to learn Q value estimates for each state and action pair independently. In deep reinforcement learning, we represent the various components of agents, such as policies \u03c0(s, a) or values q(s, a), with deep (i.e., multi-layer) neural networks. The parameters of these networks are trained by gradient descent to minimize some suitable loss function.\nIn DQN (Mnih et al. 2015) deep networks and reinforcement learning were successfully combined by using a convolutional neural net to approximate the action values for a\ngiven state St (which is fed as input to the network in the form of a stack of raw pixel frames). At each step, based on the current state, the agent selects an action -greedily with respect to the action values, and adds a transition (St, At, Rt+1, \u03b3t+1, St+1) to a replay memory buffer (Lin 1992), that holds the last million transitions. The parameters of the neural network are optimized by using stochastic gradient descent to minimize the loss\n(Rt+1 + \u03b3t+1 max a\u2032\nq\u03b8(St+1, a \u2032)\u2212 q\u03b8(St, At))2 , (1)\nwhere t is a time step randomly picked from the replay memory. The gradient of the loss is back-propagated only into the parameters \u03b8 of the online network (which is also used to select actions); the term \u03b8 represents the parameters of a target network; a periodic copy of the online network which is not directly optimized. The optimization is performed using RMSprop (Tieleman and Hinton 2012), a variant of stochastic gradient descent, on mini-batches sampled uniformly from the experience replay. This means that in the loss above, the time index t will be a random time index from the last million transitions, rather than the current time. The use of experience replay and target networks enables relatively stable learning of Q values, and led to superhuman performance on several Atari games."
        },
        {
            "heading": "Extensions to DQN",
            "text": "DQN has been an important milestone, but several limitations of this algorithm are now known, and many extensions have been proposed. We propose a selection of six extensions that each have addressed a limitation and improved overall performance. To keep the size of the selection manageable, we picked a set of extensions that address distinct concerns (e.g., just one of the many addressing exploration).\nDouble Q-learning. Conventional Q-learning is affected by an overestimation bias, due to the maximization step in Equation 1, and this can harm learning. Double Q-learning (van Hasselt 2010), addresses this overestimation by decoupling, in the maximization performed for the bootstrap target, the selection of the action from its evaluation. It is possible to effectively combine this with DQN (van Hasselt, Guez, and Silver 2016), using the loss\n(Rt+1+\u03b3t+1q\u03b8(St+1, argmax a\u2032\nq\u03b8(St+1, a \u2032))\u2212q\u03b8(St, At))2.\nThis change was shown to reduce harmful overestimations that were present for DQN, thereby improving performance.\nPrioritized replay. DQN samples uniformly from the replay buffer. Ideally, we want to sample more frequently those transitions from which there is much to learn. As a proxy for learning potential, prioritized experience replay (Schaul et al. 2015) samples transitions with probability pt relative to the last encountered absolute TD error:\npt \u221d \u2223\u2223\u2223Rt+1 + \u03b3t+1 max\na\u2032 q\u03b8(St+1, a\n\u2032)\u2212 q\u03b8(St, At) \u2223\u2223\u2223\u03c9 ,\nwhere \u03c9 is a hyper-parameter that determines the shape of the distribution. New transitions are inserted into the replay\nbuffer with maximum priority, providing a bias towards recent transitions. Note that stochastic transitions might also be favoured, even when there is little left to learn about them.\nDueling networks. The dueling network is a neural network architecture designed for value based RL. It features two streams of computation, the value and advantage streams, sharing a convolutional encoder, and merged by a special aggregator (Wang et al. 2016). This corresponds to the following factorization of action values:\nq\u03b8(s, a) = v\u03b7(f\u03be(s)) + a\u03c8(f\u03be(s), a)\u2212 \u2211 a\u2032 a\u03c8(f\u03be(s), a \u2032)\nNactions ,\nwhere \u03be, \u03b7, and \u03c8 are, respectively, the parameters of the shared encoder f\u03be, of the value stream v\u03b7 , and of the advantage stream a\u03c8; and \u03b8 = {\u03be, \u03b7, \u03c8} is their concatenation.\nMulti-step learning. Q-learning accumulates a single reward and then uses the greedy action at the next step to bootstrap. Alternatively, forward-view multi-step targets can be used (Sutton 1988). We define the truncated n-step return from a given state St as\nR (n) t \u2261 n\u22121\u2211 k=0 \u03b3 (k) t Rt+k+1 . (2)\nA multi-step variant of DQN is then defined by minimizing the alternative loss,\n(R (n) t + \u03b3 (n) t max\na\u2032 q\u03b8(St+n, a\n\u2032)\u2212 q\u03b8(St, At))2.\nMulti-step targets with suitably tuned n often lead to faster learning (Sutton and Barto 1998).\nDistributional RL. We can learn to approximate the distribution of returns instead of the expected return. Recently Bellemare, Dabney, and Munos (2017) proposed to model such distributions with probability masses placed on a discrete support z, where z is a vector with Natoms \u2208 N+ atoms, defined by zi = vmin + (i \u2212 1)vmax\u2212vminNatoms\u22121 for i \u2208 {1, . . . , Natoms}. The approximating distribution dt at time t is defined on this support, with the probability mass pi\u03b8(St, At) on each atom i, such that dt = (z,p\u03b8(St, At)). The goal is to update \u03b8 such that this distribution closely matches the actual distribution of returns.\nTo learn the probability masses, the key insight is that return distributions satisfy a variant of Bellman\u2019s equation. For a given state St and action At, the distribution of the returns under the optimal policy \u03c0\u2217 should match a target distribution defined by taking the distribution for the next state St+1 and action a\u2217t+1 = \u03c0\n\u2217(St+1), contracting it towards zero according to the discount, and shifting it by the reward (or distribution of rewards, in the stochastic case). A distributional variant of Q-learning is then derived by first constructing a new support for the target distribution, and then minimizing the Kullbeck-Leibler divergence between the distribution dt and the target distribution d\u2032t \u2261 (Rt+1 + \u03b3t+1z, p\u03b8(St+1, a \u2217 t+1)),\nDKL(\u03a6zd \u2032 t||dt) . (3)\nHere \u03a6z is a L2-projection of the target distribution onto the fixed support z, and a\u2217t+1 = argmaxa q\u03b8(St+1, a) is the greedy action with respect to the mean action values q\u03b8(St+1, a) = z\n>p\u03b8(St+1, a) in state St+1. As in the non-distributional case, we can use a frozen copy of the parameters \u03b8 to construct the target distribution. The parametrized distribution can be represented by a neural network, as in DQN, but withNatoms\u00d7Nactions outputs. A softmax is applied independently for each action dimension of the output to ensure that the distribution for each action is appropriately normalized.\nNoisy Nets. The limitations of exploring using -greedy policies are clear in games such as Montezuma\u2019s Revenge, where many actions must be executed to collect the first reward. Noisy Nets (Fortunato et al. 2017) propose a noisy linear layer that combines a deterministic and noisy stream,\ny = (b + Wx) + (bnoisy b + (Wnoisy w)x), (4)\nwhere b and w are random variables, and denotes the element-wise product. This transformation can then be used in place of the standard linear y = b + Wx. Over time, the network can learn to ignore the noisy stream, but will do so at different rates in different parts of the state space, allowing state-conditional exploration with a form of self-annealing.\nThe Integrated Agent In this paper we integrate all the aforementioned components into a single integrated agent, which we call Rainbow.\nFirst, we replace the 1-step distributional loss (3) with a multi-step variant. We construct the target distribution by contracting the value distribution in St+n according to the cumulative discount, and shifting it by the truncated n-step discounted return. This corresponds to defining the target distribution as d(n)t = (R (n) t + \u03b3 (n) t z, p\u03b8(St+n, a \u2217 t+n)). The resulting loss is\nDKL(\u03a6zd (n) t ||dt) ,\nwhere, again, \u03a6z is the projection onto z. We combine the multi-step distributional loss with double Q-learning by using the greedy action in St+n selected according to the online network as the bootstrap action a\u2217t+n, and evaluating such action using the target network.\nIn standard proportional prioritized replay (Schaul et al. 2015) the absolute TD error is used to prioritize the transitions. This can be computed in the distributional setting, using the mean action values. However, in our experiments all distributional Rainbow variants prioritize transitions by the KL loss, since this is what the algorithm is minimizing:\npt \u221d ( DKL(\u03a6zd (n) t ||dt) )\u03c9 .\nThe KL loss as priority might be more robust to noisy stochastic environments because the loss can continue to decrease even when the returns are not deterministic.\nThe network architecture is a dueling network architecture adapted for use with return distributions. The network\nhas a shared representation f\u03be(s), which is then fed into a value stream v\u03b7 with Natoms outputs, and into an advantage stream a\u03be with Natoms \u00d7Nactions outputs, where ai\u03be(f\u03be(s), a) will denote the output corresponding to atom i and action a. For each atom zi, the value and advantage streams are aggregated, as in dueling DQN, and then passed through a softmax layer to obtain the normalised parametric distributions used to estimate the returns\u2019 distributions:\npi\u03b8(s, a) = exp(vi\u03b7(\u03c6) + a i \u03c8(\u03c6, a)\u2212 ai\u03c8(s))\u2211\nj exp(v j \u03b7(\u03c6) + a j \u03c8(\u03c6, a)\u2212 a j \u03c8(s))\n,\nwhere \u03c6 = f\u03be(s) and ai\u03c8(s) = 1\nNactions\n\u2211 a\u2032 a i \u03c8(\u03c6, a\n\u2032). We then replace all linear layers with their noisy equivalent described in Equation (4). Within these noisy linear layers we use factorised Gaussian noise (Fortunato et al. 2017) to reduce the number of independent noise variables."
        },
        {
            "heading": "Experimental Methods",
            "text": "We now describe the methods and setup used for configuring and evaluating the learning agents.\nEvaluation Methodology. We evaluated all agents on 57 Atari 2600 games from the arcade learning environment (Bellemare et al. 2013). We follow the training and evaluation procedures of Mnih et al. (2015) and van Hasselt et al. (2016). The average scores of the agent are evaluated during training, every 1M steps in the environment, by suspending learning and evaluating the latest agent for 500K frames. Episodes are truncated at 108K frames (or 30 minutes of simulated play), as in van Hasselt et al. (2016).\nAgents\u2019 scores are normalized, per game, so that 0% corresponds to a random agent and 100% to the average score of a human expert. Normalized scores can be aggregated across all Atari levels to compare the performance of different agents. It is common to track the median human normalized performance across all games. We also consider the number of games where the agent\u2019s performance is above some fraction of human performance, to disentangle where improvements in the median come from. The mean human normalized performance is potentially less informative, as it is dominated by a few games (e.g., Atlantis) where agents achieve scores orders of magnitude higher than humans do.\nBesides tracking the median performance as a function of environment steps, at the end of training we re-evaluate the best agent snapshot using two different testing regimes. In the no-ops starts regime, we insert a random number (up to 30) of no-op actions at the beginning of each episode (as we do also in training). In the human starts regime, episodes are initialized with points randomly sampled from the initial portion of human expert trajectories (Nair et al. 2015); the difference between the two regimes indicates the extent to which the agent has over-fit to its own trajectories.\nDue to space constraints, we focus on aggregate results across games. However, in the appendix we provide full learning curves for all games and all agents, as well as detailed comparison tables of raw and normalized scores, in both the no-op and human starts testing regimes.\nHyper-parameter tuning. All Rainbow\u2019s components have a number of hyper-parameters. The combinatorial space of hyper-parameters is too large for an exhaustive search, therefore we have performed limited tuning. For each component, we started with the values used in the paper that introduced this component, and tuned the most sensitive among hyper-parameters by manual coordinate descent.\nDQN and its variants do not perform learning updates during the first 200K frames, to ensure sufficiently uncorrelated updates. We have found that, with prioritized replay, it is possible to start learning sooner, after only 80K frames.\nDQN starts with an exploration of 1, corresponding to acting uniformly at random; it anneals the amount of exploration over the first 4M frames, to a final value of 0.1 (lowered to 0.01 in later variants). Whenever using Noisy Nets, we acted fully greedily ( = 0), with a value of 0.5 for the \u03c30 hyper-parameter used to initialize the weights in the noisy stream1. For agents without Noisy Nets, we used -greedy but decreased the exploration rate faster than was previously used, annealing to 0.01 in the first 250K frames.\nWe used the Adam optimizer (Kingma and Ba 2014), which we found less sensitive to the choice of the learning rate than RMSProp. DQN uses a learning rate of \u03b1 = 0.00025 In all Rainbow\u2019s variants we used a learning rate of \u03b1/4, selected among {\u03b1/2, \u03b1/4, \u03b1/6}, and a value of 1.5\u00d7 10\u22124 for Adam\u2019s hyper-parameter.\nFor replay prioritization we used the recommended proportional variant, with priority exponent \u03c9 of 0.5, and linearly increased the importance sampling exponent \u03b2 from 0.4 to 1 over the course of training. The priority exponent \u03c9 was tuned comparing values of {0.4, 0.5, 0.7}. Using the KL loss of distributional DQN as priority, we have observed that performance is very robust to the choice of \u03c9.\nThe value of n in multi-step learning is a sensitive hyper-parameter of Rainbow. We compared values of n = 1, 3, and 5. We observed that both n = 3 and 5 did well initially, but overall n = 3 performed the best by the end.\nThe hyper-parameters (see Table 1) are identical across all 57 games, i.e., the Rainbow agent really is a single agent setup that performs well across all the games.\n1The noise was generated on the GPU. Tensorflow noise generation can be unreliable on GPU. If generating the noise on the CPU, lowering \u03c30 to 0.1 may be helpful."
        },
        {
            "heading": "Analysis",
            "text": "In this section we analyse the main experimental results. First, we show that Rainbow compares favorably to several published agents. Then we perform ablation studies, comparing several variants of the agent, each corresponding to removing a single component from Rainbow.\nComparison to published baselines. In Figure 1 we compare the Rainbow\u2019s performance (measured in terms of the median human normalized score across games) to the corresponding curves for A3C, DQN, DDQN, Prioritized DDQN, Dueling DDQN, Distributional DQN, and Noisy DQN. We thank the authors of the Dueling and Prioritized agents for providing the learning curves of these, and report our own re-runs for DQN, A3C, DDQN, Distributional DQN and Noisy DQN. The performance of Rainbow is significantly better than any of the baselines, both in data efficiency, as well as in final performance. Note that we match final performance of DQN after 7M frames, surpass the best final performance of these baselines in 44M frames, and reach substantially improved final performance.\nIn the final evaluations of the agent, after the end of training, Rainbow achieves a median score of 223% in the no-ops regime; in the human starts regime we measured a median score of 153%. In Table 2 we compare these scores to the published median scores of the individual baselines.\nIn Figure 2 (top row) we plot the number of games where an agent has reached some specified level of human normalized performance. From left to right, the subplots show on how many games the different agents have achieved 20%, 50%, 100%, 200% and 500% human normalized perfor-\nmance. This allows us to identify where the overall improvements in performance come from. Note that the gap in performance between Rainbow and other agents is apparent at all levels of performance: the Rainbow agent is improving scores on games where the baseline agents were already good, as well as improving in games where baseline agents are still far from human performance.\nLearning speed. As in the original DQN setup, we ran each agent on a single GPU. The 7M frames required to match DQN\u2019s final performance correspond to less than 10 hours of wall-clock time. A full run of 200M frames corresponds to approximately 10 days, and this varies by less than 20% between all of the discussed variants. The litera-\nture contains many alternative training setups that improve performance as a function of wall-clock time by exploiting parallelism, e.g., Nair et al. (2015), Salimans et al. (2017), and Mnih et al. (2016). Properly relating the performance across such very different hardware/compute resources is non-trivial, so we focused exclusively on algorithmic variations, allowing apples-to-apples comparisons. While we consider them to be important and complementary, we leave questions of scalability and parallelism to future work.\nAblation studies. Since Rainbow integrates several different ideas into a single agent, we conducted additional experiments to understand the contribution of the various components, in the context of this specific combination.\nTo gain a better understanding of the contribution of each component to the Rainbow agent, we performed ablation studies. In each ablation, we removed one component from the full Rainbow combination. Figure 3 shows a comparison for median normalized score of the full Rainbow to six ablated variants. Figure 2 (bottom row) shows a more detailed breakdown of how these ablations perform relative to different thresholds of human normalized performance, and Figure 4 shows the gain or loss from each ablation for every game, averaged over the full learning run.\nPrioritized replay and multi-step learning were the two most crucial components of Rainbow, in that removing either component caused a large drop in median performance. Unsurprisingly, the removal of either of these hurt early performance. Perhaps more surprisingly, the removal of multistep learning also hurt final performance. Zooming in on individual games (Figure 4), we see both components helped\nalmost uniformly across games (the full Rainbow performed better than either ablation in 53 games out of 57).\nDistributional Q-learning ranked immediately below the previous techniques for relevance to the agent\u2019s performance. Notably, in early learning no difference is apparent, as shown in Figure 3, where for the first 40 million frames the distributional-ablation performed as well as the full agent. However, without distributions, the performance of the agent then started lagging behind. When the results are separated relatively to human performance in Figure 2, we see that the distributional-ablation primarily seems to lags on games that are above human level or near it.\nIn terms of median performance, the agent performed better when Noisy Nets were included; when these are removed and exploration is delegated to the traditional - greedy mechanism, performance was worse in aggregate (red line in Figure 3). While the removal of Noisy Nets produced a large drop in performance for several games, it also provided small increases in other games (Figure 4).\nIn aggregate, we did not observe a significant difference when removing the dueling network from the full Rainbow. The median score, however, hides the fact that the impact of Dueling differed between games, as shown by Figure 4. Figure 2 shows that Dueling perhaps provided some improvement on games with above-human performance levels (# games > 200%), and some degradation on games with sub-human performance (# games > 20%).\nAlso in the case of double Q-learning, the observed difference in median performance (Figure 3) is limited, with the component sometimes harming or helping depending on the game (Figure 4). To further investigate the role of double Qlearning, we compared the predictions of our trained agents to the actual discounted returns computed from clipped rewards. Comparing Rainbow to the agent where double Qlearning was ablated, we observed that the actual returns are often higher than 10 and therefore fall outside the support of the distribution, spanning from\u221210 to +10. This leads to underestimated returns, rather than overestimations. We hypothesize that clipping the values to this constrained range counteracts the overestimation bias of Q-learning. Note, however, that the importance of double Q-learning may increase if the support of the distributions is expanded.\nIn the appendix, for each game we show final performance and learning curves for Rainbow, its ablations, and baselines."
        },
        {
            "heading": "Discussion",
            "text": "We have demonstrated that several improvements to DQN can be successfully integrated into a single learning algorithm that achieves state-of-the-art performance. Moreover, we have shown that within the integrated algorithm, all but one of the components provided clear performance benefits. There are many more algorithmic components that we were not able to include, which would be promising candidates for further experiments on integrated agents. Among the many possible candidates, we discuss several below.\nWe have focused here on value-based methods in the Q-learning family. We have not considered purely policybased RL algorithms such as trust-region policy optimisa-\ntion (Schulman et al. 2015), nor actor-critic methods (Mnih et al. 2016; O\u2019Donoghue et al. 2016).\nA number of algorithms exploit a sequence of data to achieve improved learning efficiency. Optimality tightening (He et al. 2016) uses multi-step returns to construct additional inequality bounds, instead of using them to replace the 1-step targets used in Q-learning. Eligibility traces allow a soft combination over n-step returns (Sutton 1988). However, sequential methods all leverage more computation per gradient than the multi-step targets used in Rainbow. Furthermore, introducing prioritized sequence replay raises questions of how to store, replay and prioritise sequences.\nEpisodic control (Blundell et al. 2016) also focuses on data efficiency, and was shown to be very effective in some domains. It improves early learning by using episodic memory as a complementary learning system, capable of immediately re-enacting successful action sequences.\nBesides Noisy Nets, numerous other exploration methods could also be useful algorithmic ingredients: among these Bootstrapped DQN (Osband et al. 2016), intrinsic motivation (Stadie, Levine, and Abbeel 2015) and count-based exploration (Bellemare et al. 2016). Integration of these alternative components is fruitful subject for further research.\nIn this paper we have focused on the core learning up-\ndates, without exploring alternative computational architectures. Asynchronous learning from parallel copies of the environment, as in A3C (Mnih et al. 2016), Gorila (Nair et al. 2015), or Evolution Strategies (Salimans et al. 2017), can be effective in speeding up learning, at least in terms of wallclock time. Note, however, they can be less data efficient.\nHierarchical RL has also been applied with success to several complex Atari games. Among successful applications of HRL we highlight h-DQN (Kulkarni et al. 2016a) and Feudal Networks (Vezhnevets et al. 2017).\nThe state representation could also be made more efficient by exploiting auxiliary tasks such as pixel control or feature control (Jaderberg et al. 2016), supervised predictions (Dosovitskiy and Koltun 2016) or successor features (Kulkarni et al. 2016b).\nTo evaluate Rainbow fairly against the baselines, we have followed the common domain modifications of clipping rewards, fixed action-repetition, and frame-stacking, but these might be removed by other learning algorithm improvements. Pop-Art normalization (van Hasselt et al. 2016) allows reward clipping to be removed, while preserving a similar level of performance. Fine-grained action repetition (Sharma, Lakshminarayanan, and Ravindran 2017) enabled to learn how to repeat actions. A recurrent state network\n(Hausknecht and Stone 2015) can learn a temporal state representation, replacing the fixed stack of observation frames. In general, we believe that exposing the real game to the agent is a promising direction for future research."
        }
    ],
    "title": "Rainbow: Combining Improvements in Deep Reinforcement Learning",
    "year": 2017
}