{
    "abstractText": "Networks are a fundamental tool for understanding and modeling complex systems in physics, biology, neuroscience, engineering, and social science. Many networks are known to exhibit rich, lower-order connectivity patterns that can be captured at the level of individual nodes and edges. However, higher-order organization of complex networks\u2014at the level of small network subgraphs\u2014 remains largely unknown. Here we develop a generalized framework for clustering networks based on higher-order connectivity patterns. This framework provides mathematical guarantees on the optimality of obtained clusters and scales to networks with billions of edges. The framework reveals higher-order organization in a number of networks including information propagation units in neuronal networks and hub structure in transportation networks. Results show that networks exhibit rich higher-order organizational structures that are exposed by clustering based on higher-order connectivity patterns.",
    "authors": [
        {
            "affiliations": [],
            "name": "Austin R. Benson"
        },
        {
            "affiliations": [],
            "name": "David F. Gleich"
        },
        {
            "affiliations": [],
            "name": "Jure Leskovec"
        }
    ],
    "id": "SP:e25948ed2372ea83f18bc494d2fca7369bb102a5",
    "references": [
        {
            "authors": [
                "S. Mangan"
            ],
            "title": "U",
            "venue": "Alon, Proceedings of the National Academy of Sciences 100,",
            "year": 1198
        },
        {
            "authors": [
                "C.J. Honey",
                "R. K\u00f6tter",
                "M. Breakspear"
            ],
            "title": "O",
            "venue": "Sporns, Proceedings of the National Academy of Sciences 104,",
            "year": 1024
        },
        {
            "authors": [
                "C. Seshadhri",
                "A. Pinar",
                "T.G. Kolda"
            ],
            "title": "Statistical Analysis and Data Mining: The ASA Data",
            "venue": "Science Journal 7,",
            "year": 2014
        },
        {
            "authors": [
                "L. Trevisan"
            ],
            "title": "Lecture notes on expansion, sparsest cut, and spectral graph theory, http:// www.eecs.berkeley.edu/ \u0303luca/books/expanders.pdf",
            "venue": "Accessed June",
            "year": 2015
        },
        {
            "authors": [
                "A.J. Leskovec"
            ],
            "title": "Krevl, SNAP Datasets: Stanford large network dataset collection, http: //snap.stanford.edu/data",
            "year": 2014
        },
        {
            "authors": [
                "V.D. Blondel",
                "J.-L. Guillaume",
                "R. Lambiotte"
            ],
            "title": "E",
            "venue": "Lefebvre, Journal of statistical mechanics: theory and experiment",
            "year": 2008
        },
        {
            "authors": [
                "C.D. Manning",
                "P. Raghavan",
                "H. Sch\u00fctze"
            ],
            "title": "Introduction to Information Retrieval, vol. 1 (Cambridge university",
            "year": 2008
        },
        {
            "authors": [
                "T. Chakraborty",
                "N. Ganguly",
                "A. Mukherjee"
            ],
            "title": "Advances in Social Networks Analysis and Mining (ASONAM), 2014",
            "venue": "IEEE/ACM International Conference on (IEEE,",
            "year": 2014
        },
        {
            "authors": [
                "J. Yang",
                "J. Leskovec"
            ],
            "title": "Authors would like to thank Rok Sosi\u010d for insightful comments. ARB acknowledges the support of a Stanford Graduate Fellowship. DFG acknowledges the support of NSF CCF1149756 and IIS-1422918 and DARPA SIMPLEX. JL acknowledges the support of NSF IIS1149837 and CNS-1010921, NIH BD2K, DARPA XDATA and SIMPLEX, Boeing, Lightspeed, and Volkswagen",
            "venue": "IEEE 12th International Conference on Data Mining (IEEE,",
            "year": 2012
        }
    ],
    "sections": [
        {
            "text": "Higher-order organization of complex networks\nAustin R. Benson,1 David F. Gleich,2 Jure Leskovec3\u2217\n1Institute for Computational and Mathematical Engineering, Stanford University 2Department of Computer Science, Purdue University 3Computer Science Department, Stanford University\n\u2217To whom correspondence should be addressed; E-mail: jure@cs.stanford.edu\nNetworks are a fundamental tool for understanding and modeling complex systems in physics, biology, neuroscience, engineering, and social science. Many networks are known to exhibit rich, lower-order connectivity patterns that can be captured at the level of individual nodes and edges. However, higher-order organization of complex networks\u2014at the level of small network subgraphs\u2014 remains largely unknown. Here we develop a generalized framework for clustering networks based on higher-order connectivity patterns. This framework provides mathematical guarantees on the optimality of obtained clusters and scales to networks with billions of edges. The framework reveals higher-order organization in a number of networks including information propagation units in neuronal networks and hub structure in transportation networks. Results show that networks exhibit rich higher-order organizational structures that are exposed by clustering based on higher-order connectivity patterns.\n1\nar X\niv :1\n61 2.\n08 44\n7v 1\n[ cs\n.S I]\n2 6\nD ec\n2 01\n6\nNetworks are a standard representation of data throughout the sciences, and higher-order connectivity patterns are essential to understanding the fundamental structures that control and mediate the behavior of many complex systems (1\u20137). The most common higher-order structures are small network subgraphs, which we refer to as network motifs (Figure 1A). Network motifs are considered building blocks for complex networks (1, 8). For example, feedforward loops (Figure 1A M5) have proven fundamental to understanding transcriptional regulation networks (9), triangular motifs (Figure 1A M1\u2013M7) are crucial for social networks (4), open bidirectional wedges (Figure 1A M13) are key to structural hubs in the brain (10), and two-hop paths (Figure 1A M8\u2013M13) are essential to understanding air traffic patterns (5). While network motifs have been recognized as fundamental units of networks, the higher-order organization of networks at the level of network motifs largely remains an open question.\nHere we use higher-order network structures to gain new insights into the organization of complex systems. We develop a framework that identifies clusters of network motifs. For each network motif (Figure 1A), a different higher-order clustering may be revealed (Figure 1B), which means that different organizational patterns are exposed depending on the chosen motif.\nConceptually, given a network motif M , our framework searches for a cluster of nodes S with two goals. First, the nodes in S should participate in many instances of M . Second, the set S should avoid cutting instances of M , which occurs when only a subset of the nodes from a motif are in the set S (Figure 1B). More precisely, given a motif M , the higher-order clustering framework aims to find a cluster (defined by a set of nodes S) that minimizes the following ratio:\n\u03c6M(S) = cutM(S, S\u0304)/min(volM(S), volM(S\u0304)), (1)\nwhere S\u0304 denotes the remainder of the nodes (the complement of S), cutM(S, S\u0304) is the number of instances of motif M with at least one node in S and one in S\u0304, and volM(S) is the number of nodes in instances of M that reside in S. Equation 1 is a generalization of the conductance\n2\nmetric in spectral graph theory, one of the most useful graph partitioning scores (11). We refer to \u03c6M(S) as the motif conductance of S with respect to M .\nFinding the exact set of nodes S that minimizes the motif conductance is computationally infeasible (12). To approximately minimize Equation 1 and hence identify higher-order clusters, we develop an optimization framework that provably finds near-optimal clusters (Supplementary Materials (13)). We extend the spectral graph clustering methodology, which is based on the eigenvalues and eigenvectors of matrices associated with the graph (11), to account for higher-order structures in networks. The resulting method maintains the properties of traditional spectral graph clustering: computational efficiency, ease of implementation, and mathematical guarantees on the near-optimality of obtained clusters. Specifically, the clusters identified by our higher-order clustering framework satisfy the motif Cheeger inequality (14), which means that our optimization framework finds clusters that are at most a quadratic factor away from optimal.\nThe algorithm (illustrated in Figure 1C) efficiently identifies a cluster of nodes S as follows:\n\u2022 Step 1: Given a network and a motif M of interest, form the motif adjacency matrix WM\nwhose entries (i, j) are the co-occurrence counts of nodes i and j in the motif M :\n(WM)ij = number of instances of M that contain nodes i and j. (2)\n\u2022 Step 2: Compute the spectral ordering \u03c3 of the nodes from the normalized motif Laplacian\nmatrix constructed via WM (15).\n\u2022 Step 3: Find the prefix set of \u03c3 with the smallest motif conductance, formally: S :=\narg minr \u03c6M(Sr), where Sr = {\u03c31, . . . , \u03c3r}.\nFor triangular motifs, the algorithm scales to networks with billions of edges and typically only takes several hours to process graphs of such size. On smaller networks with hundreds\n3\nof thousands of edges, the algorithm can process motifs up to size 9 (13). While the worstcase computational complexity of the algorithm for triangular motifs is \u0398(m1.5) , where m is the number of edges in the network, in practice the algorithm is much faster. By analyzing 16 real-world networks where the number of edges m ranges from 159,000 to 2 billion we found the computational complexity to scale as \u0398(m1.2). Moreover, the algorithm can easily be parallelized and sampling techniques can be used to further improve performance (16).\nThe framework can be applied to directed, undirected, and weighted networks as well as motifs (13). Moreover, it can also be applied to networks with positive and negative signs on the edges, which are common in social networks (friend vs. foe or trust vs. distrust edges) and metabolic networks (edges signifying activation vs. inhibition) (13). The framework can be used to identify higher-order structure in networks where domain knowledge suggests the motif of interest. In the Supplementary Material (13) we also show that when domain-specific higherorder pattern is not known in advance, the framework can also serve to identify which motifs are important for the modular organization of a given network (13). Such a general framework allows for a study of complex higher-order organizational structures in a number of different networks using individual motifs and sets of motifs. The framework and mathematical theory immediately extend to other spectral methods such as localized algorithms that find clusters around a seed node (17) and algorithms for finding overlapping clusters (18). To find several clusters, one can use embeddings from multiple eigenvectors and k-means clustering (13,19) or apply recursive bi-partitioning (13, 20).\nThe framework can serve to identify higher-order modular organization of networks. We apply the higher-order clustering framework to the C. elegans neuronal network, where the fournode \u201cbi-fan\u201d motif (Figure 2A) is over-expressed (1). The higher-order clustering framework then reveals the organization of the motif within the C. elegans neuronal network. We find a cluster of 20 neurons in the frontal section with low bi-fan motif conductance (Figure 2B).\n4\nThe cluster shows a way that nictation is controlled. Within the cluster, ring motor neurons (RMEL/V/R), proposed pioneers of the nerve ring (21), propagate information to IL2 neurons, regulators of nictation (22), through the neuron RIH and several inner labial sensory neurons (Figure 2C). Our framework contextualizes the sifnifance of the bi-fan motif in this control mechanism.\nThe framework also provides new insights into network organization beyond the clustering of nodes based only on edges. Results on a transportation reachability network (23) demonstrate how it finds the essential hub interconnection airports (Figure 3). These appear as extrema on the primary spectral direction (Figure 3C) when two-hop motifs (Figure 3A) are used to capture highly connected nodes and non-hubs. (The first spectral coordinate of the normalized motif Laplacian embedding was positively correlated with the airport city\u2019s metropolitan population with Pearson correlation 99% confidence interval [0.33, 0.53]). The secondary spectral direction identified the West-East geography in the North American flight network (it was negatively correlated with the airport city\u2019s longitude with Pearson correlation 99% confidence interval [-0.66, -0.50]). On the other hand, edge-based methods conflate geography and hub structure. For example, Atlanta, a large hub, is embedded next to Salina, a non-hub, with an edge-based method (Figure 3D).\nOur higher-order network clustering framework unifies motif analysis and network partitioning\u2014\ntwo fundamental tools in network science\u2014and reveals new organizational patterns and modules in complex systems. Prior efforts along these lines do not provide worst-case performance guarantees on the obtained clustering (24), do not reveal which motifs organize the network (25), or rely on expanding the size of the network (26, 27). Theoretical results in the Supplementary Material (13) also explain why classes of hypergraph partitioning methods are more general than previously assumed and how motif-based clustering provides a rigorous framework for the special case of partitioning directed graphs. Finally, the higher-order net-\n5\nwork clustering framework is generally applicable to a wide range of networks types, including directed, undirected, weighted, and signed networks.\n6\n7\n8\n9"
        },
        {
            "heading": "S1 Derivation and analysis of the motif-based spectral clus-",
            "text": "tering method\nWe now cover the background and theory for deriving and understanding the method presented in the main text. We will start by reviewing the graph Laplacian and cut and volume measures for sets of vertices in a graph. We then define network motifs in Section S1.2 and generalizes the notions of cut and volume to motifs. Our new theory is presented in Section S1.6 and then we summarize some extensions of the method. Finally, we relate our method to existing methods for directed graph clustering and hypergraph partitioning.\nS1.1 Review of the graph Laplacian for weighted, undirected graphs\nConsider a weighted, undirected graphG = (V,E), with |V | = n. Further assume thatG has no isolated nodes. LetW encode the weights, of the graph, i.e., Wij = Wji = weight of edge (i, j).\nThe diagonal degree matrixD is defined asDii = \u2211n j=1Wij , and the graph Laplacian is defined as L = D \u2212W . We now relate these matrices to the conductance of a set S, \u03c6(G)(S):\n\u03c6(G)(S) = cut(G)(S, S\u0304)/min(vol(G)(S), vol(G)(S\u0304)), (S3)\ncut(G)(S, S\u0304) = \u2211\ni\u2208S, j\u2208S\u0304\nWij, (S4)\nvol(G)(S) = \u2211 i\u2208S Dii (S5)\nHere, S\u0304 = V \\S. (Note that conductance is a symmetric measure in S and S\u0304, i.e., \u03c6(G)(S) = \u03c6(G)(S\u0304).) Conceptually, the cut and volume measures are defined as follows:\ncut(G)(S, S\u0304) = weighted sum of weights of edges that are cut (S6)\nvol(G)(S) = weighted number of edge end points in S (S7)\nSince we have assumed G has no isolated nodes, vol(G)(S) > 0. If G is disconnected, then for any connected component C, \u03c6(G)(C) = 0. Thus, we usually consider breaking G into\nS10\nconnected components as a pre-processing step for algorithms that try to find low-conductance sets.\nWe now relate the cut metric to a quadratic form on L. Later, we will derive a similar form\nfor a motif cut measure. Note that for any vector y \u2208 Rn,\nyTLy = \u2211\n(i,j)\u2208E\nwij(yi \u2212 yj)2. (S8)\nNow, define x to be an indicator vector for a set of nodes S i.e., xi = 1 if node i is in S and xi = 0 if node i is in S\u0304. Note that if an edge (i, j) is cut, then xi and xj take different values and (xi \u2212 xj)2 = 1; otherwise, (xi \u2212 xj)2 = 0. Thus,\nxTLx = cut(G)(S, S\u0304). (S9)\nS1.2 Definition of network motifs\nWe now define network motifs as used in our work. We note that there are alternative definitions in the literature (1). We consider motifs to be a pattern of edges on a small number of nodes (see Figure S4). Formally, we define a motif on k nodes by a tuple (B,A), whereB is a k\u00d7k binary matrix and A \u2282 {1, 2, . . . , k} is a set of anchor nodes. The matrix B encodes the edge pattern between the k nodes, and A labels a relevant subset of nodes for defining motif conductance. In many cases, A is the entire set of nodes. Let \u03c7A be a selection function that takes the subset of a k-tuple indexed by A, and let set(\u00b7) be the operator that takes an (ordered) tuple to an (unordered) set. Specifically,\nset((v1, v2, . . . , vk)) = {v1, v2, . . . , vk}.\nThe set of motifs in an unweighted (possibly directed) graph with adjacency matrix A, denoted M(B,A), is defined by\nM(B,A) = {(set(v), set(\u03c7A(v))) | v \u2208 V k, v1, . . . , vk distinct, Av = B}, (S10)\nS11\nM3 M5M4M1 M2\nM8M7M6 M9 M10\nM11 M12 M13 Mbifan Medge\n+\ne\nd\nc\nb\na\nM2\nA\nB\nFigure S4: A: Illustration of network motifs used throughout the main text and supplementary material. The motif Medge is used to represent equivalence to undirected the graph. B: Diagram of motif definitions. The motif is defined by a binary matrix B and an anchor set of nodes. The figure shows an anchored version of motif M2 with anchors on the nodes that form the bi-directional edge. There are two instances of the motif in the graph on the right. Note that ({a, b, d}, {a, b}) is not included in the set of motif instances because the induced subgraph on the nodes a, b, and d is not isomorphic to the graph defined by B.\nwhere Av is the k \u00d7 k adjacency matrix on the subgraph induced by the k nodes of the ordered vector v. Figure S4 illustrates these definitions. The set operator is a convenient way to avoid duplicates when defining M(B,A) for motifs exhibiting symmetries. Henceforth, we will just use (v, \u03c7A(v)) to denote (set(v), set(\u03c7A(v))) when discussing elements of M(B,A). Furthermore, we call any (v, \u03c7A(v)) \u2208 M(B,A) a motif instance. When B and A are arbitrary or clear from context, we will simply denote the motif set by M .\nWe call motifs where \u03c7A(v) = v simple motifs and motifs where \u03c7A(v) 6= v anchored motifs. Motif analysis in the literature has mostly analyzed simple motifs (29). However, the anchored motif provides us with a more general framework, and we use an anchored motif for\nS12\nthe analysis of the transportation reachability network.\nOften, a distinction is made between a functional and a structural motif (30) (or a subgraph and an induced subgraph (31)) to distinguish whether a motif specifies simply the existence of a set of edges (functional motif or subgraph) or the existence and non-existence of edges (structural motif or induced subgraph). By the definition in Equation S10, we refer to structural motifs in this work. Note that functional motifs consist of a set of structural motifs. Our clustering framework allows for the simultaneous consideration of several motifs (see Section S1.9), so we have not lost any generality in our definitions.\nS1.3 Definition of motif conductance\nRecall that the key definitions for defining conductance are the notions of cut and volume. For an unweighted graph, these are\n\u03c6(G)(S, S\u0304) = cut(G)(S, S\u0304)/min(vol(G)(S), vol(G)(S\u0304)), (S11)\ncut(G)(S, S\u0304) = number of edges cut, (S12)\nvol(G)(S) = number of edge end points in S. (S13)\nOur conceptual definition of motif conductance simply replaces an edge with a motif instance of type M :\n\u03c6 (G) M (S) = cut (G) M (S, S\u0304)/min(vol (G) M (S), vol (G) M (S\u0304)), (S14)\ncut(G)M (S, S\u0304) = number of motif instances cut, (S15)\nvol(G)M (S) = number of motif instance end points in S. (S16)\nWe say that a motif instance is cut if there is at least one anchor node in S and at least one\nS13\nanchor node in S\u0304. We can formalize this when given a motif set M as in Equation S10:\ncut(G)M (S, S\u0304) = \u2211\n(v,\u03c7A(v))\u2208M\n1(\u2203 i, j \u2208 \u03c7A(v) | i \u2208 S, j \u2208 S\u0304), (S17)\nvol(G)M (S) = \u2211\n(v,\u03c7A(v))\u2208M \u2211 i\u2208\u03c7A(v) 1(i \u2208 S), (S18)\nwhere 1(s) is the truth-value indicator function on s, i.e., 1(s) takes the value 1 if the statement s is true and 0 otherwise. Note that Equation S17 makes explicit use of the anchor set A. The motif cut measure only counts an instance of a motif as cut if the anchor nodes are separated, and the motif volume counts the number of anchored nodes in the set. However, two nodes in an achor set may a part of several motif instances. Specifically, following the definition in Equation S10, there may be many different v with the same \u03c7A(v), and the nodes in \u03c7A(v) still get counted proportional to the number of motif instances.\nS1.4 Definition of the motif adjacency matrix and motif Laplacian\nGiven an unweighted, directed graph and a motif set M , we conceptually define the motif adjacency matrix by\n(WM)ij = number of motif instances in M where i and j participate in the motif. (S19)\nOr, formally,\n(WM)ij = \u2211\n(v,\u03c7A(v))\u2208M\n1({i, j} \u2282 \u03c7A(v)), (S20)\nfor i 6= j. Note that weight is added to (WM)ij only if i and j appear in the anchor set. This is important for the transportation reachability network analyzed in the main text and in Section S6, where weight is added between cities i and j based on the number of intermediary cities that can be traversed between them.\nNext, we define the motif diagonal degree matrix by (DM)ii = \u2211n j=1(WM)ij and the motif Laplacian as LM = DM \u2212 WM . Finally, the normalized motif Laplacian is LM =\nS14\nD \u22121/2 M LMD \u22121/2 M = I \u2212D \u22121/2 M WMD \u22121/2 M . The theory in the next section will examine quadratic forms LM and derive the main clustering method that uses an eigenvector of LM .\nS1.5 Algorithm for finding a single cluster\nWe are now ready to describe the algorithm for finding a single cluster in a graph. The algorithm finds a partition of the nodes into S and S\u0304. The motif conductance is symmetric in the sense that \u03c6(G)M (S) = \u03c6 (G) M (S\u0304), so either set of nodes (S or S\u0304) could be interpreted as a cluster. However, in practice, it is common that one set is substantially smaller than the other. We consider this smaller set to represent a module in the network. The algorithm is based on the Fiedler partition (32) of the motif weighted adjacency matrix and is presented below in Algorithm 1.1\nAlgorithm 1: Motif-based clustering algorithm for finding a single cluster. Input: Directed, unweighted graph G and motif M Output: Motif-based cluster (subset of nodes in G) (WM)ij \u2190 number of instances of M that contain nodes i and j. GM \u2190 weighted graph induced WM DM \u2190 diagonal matrix with (DM)ii = \u2211 j(WM)ij\nz \u2190 eigenvector of second smallest eigenvalue for LM = I \u2212D\u22121/2M WMD \u22121/2 M \u03c3i \u2190 to be index of D\u22121/2M z with ith smallest value /* Sweep over all prefixes of \u03c3 */ S \u2190 arg minl \u03c6(GM )(Sl), where Sl = {\u03c31, . . . , \u03c3l} if |S| < |S\u0304| then\nreturn S else\nreturn S\u0304\nIt is often informative to look at all conductance values found from the sweep procedure. We refer to a plot of \u03c6(GM )(Sl) versus l as a sweep profile plot. In the following subection, we show that when the motif has three nodes, \u03c6(GM )(Sl) = \u03c6 (G) M (Sl). In this case, the sweep profile shows how motif conductance varies with the size of the sets in Algorithm 1.\nIn the following subsection, we show that when the motif M has three nodes, the cluster 1An implementation of Algorithm 1 is available in SNAP. See http://snap.stanford.edu/ higher-order/.\nS15\nsatisfies \u03c6(G)M (S) \u2264 4 \u221a \u03c6\u2217, where \u03c6\u2217 is the smallest motif conductance over all sets of nodes. In other words, the cluster is nearly optimal. Later, we extend this algorithm to allow for signed, colored, and weighted motifs and to simultaneously finding multiple clusters.\nS1.6 Motif Cheeger inequality for network motifs with three nodes\nWe now derive the motif Cheeger inequality for simple three-node motifs, or, in general, motifs with three anchor nodes. The crux of this result is deriving a relationship between the motif conductance function and the weighted motif adjacency matrix, from which the Cheeger inequality is essentially a corollary. For the rest of this section, we will use the following notation. Given an unweighted, directed G and a motif M , the corresponding weighted graph defined by Equation S20 is denoted by GM .\nThe following Lemma relates the motif volume to the volume in the weighted graph. This lemma applies to any anchor setA consisting of at least two nodes. For our main result, we will apply the lemma assuming |A| = 3. However, we will apply the lemma more generally when discussing four node motifs in Section S1.7.\nLemma 1. Let G = (V,E) be a directed, unweighted graph and let GM be the weighted graph for a motif on k nodes and |A| \u2265 2 anchor nodes. Then for any S \u2282 V ,\nvol(G)M (S) = 1\n|A| \u2212 1 vol(GM )(S)\nProof. Consider an instance (v, \u03c7A(v)) of a motif. Let (u1, . . . , u|A|) = \u03c7A(v). By Equation S20, (WM)u1,j is incremented by one for j = u2, . . . , u|A|. Since (DM)u1,u1 = \u2211 j (WM)u1,j , the motif end point u1 is counted |A| \u2212 1 times.\nS16\nxi 2 + xj 2 + xk 2 - xixj - xjxk -xkxi =\nxi = 1\nxj = 1 xk = 1\ni\nj k\nxi = -1\nxj = -1 xk = -1\ni\nj k\nxi = 1\nxj = 1 xk = -1\ni\nj k\nxi = 1\nxj = -1 xk = -1\ni\nj k\n0\n0\n4\n4\nii\nkj\nl\nxj = 1 xk = 1\nxi = 1 xl = 1\nii\nkj\nl\nxj = -1 xk = -1\nxi = -1 xl = -1\nii\nkj\nl\nxj = 1 xk = -1\nxi = 1 xl = 1\nii\nkj\nl\nxj = -1 xk = -1\nxi = 1 xl = -1\nii\nkj\nl\nxj = 1 xk = -1\nxi = 1 xl = -1\n0\n0\n6\n6\n8\n6 - xixj - xixk - xixl - xjxk - xjxl - xkxl =BA\nFigure S5: Illustrations of the quadratic forms on indicator functions for set assignment. Here, the blue nodes have assignment to set S and the green nodes have assignment to set S\u0304. The quadratic function gives the penalty for cutting that motif. A: Illustration of Equation S21. The quadratic form is proportional to the indicator on whether or not the motif is cut. B: Illustration of Equation S22. The quadratic form is equal to zero when all nodes are in the same set. However, the form penalizes 2/2 splits more than 3/1 splits.\nThe following lemma states that the truth value for determining whether three binary variables in {\u22121, 1} are not all equal is a quadratic function of the variables (see Figure S5). Because this function is quadratic, we will be able to relate motif cuts on three nodes to a quadratic form on the motif Laplacian.\nS17\nLemma 2. Let xi, xj, xk \u2208 {\u22121, 1}. Then\n4 \u00b7 1(xi, xj, xk not all the same) = x2i + x2j + x2k \u2212 xixj \u2212 xjxk \u2212 xkxi.\nIt will be easier to derive our results with binary indicator variables taking values in {\u22121, 1}. However, in terms of the quadratic form on the Laplacian, we have already seen how indicator vectors taking values in {0, 1} relate to the cut value (Equation S9). The following lemma shows that the {0, 1} and {\u22121, 1} indicator vectors are equivalent, up to a constant, for defining the cut measure in terms of the Laplacian.\nLemma 3. Let z \u2208 {0, 1}n and define x by xi = 1 if zi = 1 and xi = \u22121 if zi = 0. Then for any graph Laplacian L = D \u2212W , 4zTLz = xTLx.\nProof.\nxTLx = \u2211\n(i,j)\u2208E\nWij(xi \u2212 xj)2 = \u2211\n(i,j)\u2208E\nWij4(zi \u2212 zj)2 = 4zTLz.\nThe next lemma contains the essential result that relates motif cuts in the original graph G to weighted edge cuts in GM . In particular, the lemma shows that the motif cut measure is proportional to the cut on the weighted graph defined in Equation S19 when there are three anchor nodes.\nLemma 4. Let G = (V,E) be a directed, unweighted graph and let GM be the weighted graph for a motif with |A| = 3. Then for any S \u2282 V ,\ncut(G)M (S, S\u0304) = 1\n2 cut(GM )(S, S\u0304)\nS18\nProof. Let x \u2208 {\u22121, 1}n be an indicator vector of the node set S.\n4 \u00b7 cut(G)M (S, S\u0304) = \u2211\n(v,{i,j,k})\u2208M\n4 \u00b7 1(xi, xj, xk not all the same)\n= \u2211\n(v,{i,j,k})\u2208M\n( x2i + x 2 j + x 2 k ) \u2212 (xixj + xjxk + xkxi)\n= 1\n2 xTDMx\u2212\n1 2 xTWMx\n= 1\n2 xTLMx\n= 2 \u00b7 cut(GM )(S, S\u0304).\nThe first equality follows from the definition of cut motifs (Equation S17). The second equality follows from Lemma 2. The third equality follows from Lemma 1 and Equation S20. The fourth equality follows from the definition of LM . The fifth equality follows from Lemma 3.\nWe are now ready to prove our main result, namely that motif conductance on the original graph G is equivalent to conductance on the weighted graph GM when there are three anchor nodes. The result is a consequence of the volume and cut relationships provided by Lemmas 1 and 4.\nTheorem 5. Let G = (V,E) be a directed, unweighted graph and let WM be the weighted adjacency matrix for any motif with |A| = 3. Then for any S \u2282 V ,\n\u03c6 (G) M (S) = \u03c6 (GM )(S)"
        },
        {
            "heading": "In other words, when the number of anchor nodes is 3, the motif conductance is equal to the",
            "text": "conductance on the weighted graph defined by Equation S19.\nProof. When |A| = 3, the motif cut and motif volume are both equal to half the motif cut and motif volume measures by Lemmas 1 and 4.\nS19\nFor any motif with three anchor nodes, conductance on the weighted graph is equal to the motif conductance. Because of this, we can use results from spectral graph theory for weighted graphs (32) and re-interpret the results in terms of motif conductance. In particular, we get the following \u201cmotif Cheeger inequality\u201d.\nTheorem 6. Motif Cheeger Inequality. Suppose we use Algorithm 1 to find a low-motif conductance set S. Let \u03c6\u2217 = minS\u2032 \u03c6 (G) M (S \u2032) be the optimal motif conductance over any set of nodes"
        },
        {
            "heading": "S \u2032. Then",
            "text": "1. \u03c6(G)M (S) \u2264 4 \u221a \u03c6\u2217 and\n2. \u03c6\u2217 \u2265 \u03bb2/2\nProof. The result follows from Theorem 5 and the standard Cheeger ineqaulity (32).\nThe first part of the result says that the set of nodes S is within a quadratic factor of optimal. This provides the mathematical guarantees that our procedure finds a good cluster in a graph, if one exists. The second result provides a lower bound on the optimal motif conductance in terms of the eigenvalue. We use this bound in our analysis of a food web (see Section S7.1) to show that certain motifs do not provide good clusters, regardless of the procedure to select S.\nS1.7 Discussion of motif Cheeger inequality for network motifs with four or more nodes\nAnalogs of the indicator function in Lemma 2 for four or more variables are not quadratic. Subsequently, for motifs with |A| > 3, we no longer get the motif Cheeger inequalities guaranteed by Theorem 6. That being said, solutions found by motif-based partitioning approximate a related value of conductance. We now provide the details.\nWe begin with a lemma that shows a functional form for four binary variables taking values\nin {\u22121, 1} to not all be equal. We see that it is quartic, not quadratic.\nS20\nLemma 7. Let xi, xj, xk, xl \u2208 {\u22121, 1}. Then the indicator function on all four elements not being equal is\n8 \u00b7 1(xi, xj, xk, xl not all the same) (S21)\n= (7\u2212 xixj \u2212 xixk \u2212 xixl \u2212 xjxk \u2212 xjxl \u2212 xkxl \u2212 xixjxkxl) .\nWe almost have a quadratic form, if not for the quartic term xixjxkxl. However, we could\nuse the following related quadratic form:\n6\u2212 xixj \u2212 xixk \u2212 xixl \u2212 xjxk \u2212 xjxl \u2212 xkxl\n=  0 xi, xj, xk, xl are all the same 6 exactly three of xi, xj, xk, xl are the same 8 exactly two of xi, xj, xk, xl are \u22121.\n(S22)\nThe quadratic still takes value 0 if all four entries are the same, and takes a non-zero value otherwise. However, the quadratic takes a larger value if exactly two of the entries are \u22121. Figure S5 illustrates this idea. From this, we can provide an analogous statement to Lemma 4 for motifs with |A| = 4.\nLemma 8. Let G = (V,E) be a directed, unweighted graph and let GM be the weighted graph for a motif with |A| = 4. Then for any S \u2282 V ,\ncut(G)M (S, S\u0304) = 1\n3 cut(GM )(S, S\u0304)\u2212 \u2211 (v,{i,j,k,l})\u2208M 1 3 \u00b7 1(exactly two of i, j, k, l in S)\nS21\nProof. Let x \u2208 {\u22121, 1}n be an indicator vector of the node set S.\n6 \u00b7 cut(G)M (S, S\u0304) + \u2211\n(v,{i,j,k,l})\u2208M\n2 \u00b7 1(exactly two of i, j, k, l in S)\n= \u2211\n(v,{i,j,k,l})\u2208M\n6\u2212 xixj \u2212 xixk \u2212 xixl \u2212 xjxk \u2212 xjxl \u2212 xkxl\n= \u2211\n(v,{i,j,k,l})\u2208M\n3 2\n( x2i + x 2 j + x 2 k + x 2 l ) \u2212 (xixj + xixk + xixl + xjxk + xjxl + xkxl)\n= 1\n2 xTDMx\u2212\n1 2 xTWMx\n= 1\n2 xTLMx\n= 2 \u00b7 cut(GM )(S, S\u0304).\nThe first equality follows from Equations S17 and S22. The third equality follows from Lemma 1. The fourth equality follows from the definition ofLM . The fifth equality follows from Lemma 3.\nWith four anchor nodes, the motif cut in G is slightly different than the weighted cut in the weighted graph GM . However, Lemma 1 says that the motif volume in G is still the same as the weighted volume in GM . We use this to derive the following result.\nTheorem 9. Let G = (V,E) be a directed, unweighted graph and let WM be the weighted adjacency matrix for any motif with |A| = 4. Then for any S \u2282 V ,\n\u03c6 (G) M (S) = \u03c6\n(GM )(S)\u2212 \u2211 (v,{i,j,k,l})\u2208M 1(exactly two of i, j, k, l in S)\nvol(GM )(S)"
        },
        {
            "heading": "In other words, when there are four anchor nodes, the weighting scheme in Equation S19 models",
            "text": "the exact conductance with an additional penalty for splitting the four anchor nodes into two groups of two.\nProof. This follows from Lemmas 1 and 8.\nS22\nTo summarize, we still get a Cheeger inequality from the weighted graph, but it is in terms of a penalized version of the motif conductance \u03c6(G)M (S). However, the penalty makes sense\u2014if the group of four nodes is \u201cmore split\u201d (2 and 2 as opposed to 3 and 1), the penalty is larger. When |A| > 4, we can derive similar penalized approximations to \u03c6(G)M (S).\nS1.8 Methods for simultaneously finding multiple clusters\nFor clustering a network into k > 2 clusters based on motifs, we could recursively cut the graph using the sweep procedure with some stopping criterion (20). For example, we could continue to cut the largest remaining cluster until the graph is partitioned into some pre-specified number of clusters. We refer to this method as recursive bi-partitioning.\nIn addition, we can use the following method of Ng et al. (19). Algorithm 2: Motif-based clustering algorithm for finding several clusters.\nInput: Directed, unweighted graph G, motif M , number of clusters k Output: k disjoint motif-based clusters (WM)ij \u2190 number of instances of M that contain nodes i and j. DM \u2190 diagonal matrix with (DM)ii = \u2211 j(WM)ij z1, . . . , zk \u2190 eigenvectors of k smallest eigenvalues for LM = I \u2212D\u22121/2M WMD \u22121/2 M\nYij \u2190 zij/ \u221a\u2211k j=1 z 2 ij Embed node i into Rk by taking the ith row of the matrix Y Run k-means clustering on the embedded nodes This method does not have the same Cheeger-like guarantee on quality. However, recent theory shows that by replacing k-means with a different clustering algorithm, there is a performance guarantee (33). While this provides motivation, we use k-means for its simplicity and empirical success.\nS1.9 Extensions of the method for simultaneously analyzing several network motifs\nAll of our results carry through when considering several motifs simultaneously. In particular, suppose we are interested in clustering based on motif sets M1, . . . ,Mq for q different motifs.\nS23\nFurther suppose that we want to weight the impact of some motifs more than other motifs. Let WMj be the weighted adjacency matrix for motifMj , j = 1, . . . , q, and let \u03b1j \u2265 0 be the weight of motif Mj , then we can form the weighted adjacency matrix\nWM = q\u2211 j=1 \u03b1jWMj . (S23)\nNow, the cut and volume measures are simply weighted sums by linearity. Suppose that the Mj all have three anchor nodes and let GM be the weighted graph corresponding to WM . Then\ncut(GM )(S, S\u0304) = q\u2211 j=1 \u03b1jcut (G) Mj (S, S\u0304), vol(GM )(S) = q\u2211 j=1 \u03b1jvol (G) Mj (S),\nand Theorem 6 applies to a weighted motif conductance equal to\u2211q j=1 \u03b1jcut (G) Mj (S, S\u0304)\nmin (\u2211q\nj=1 \u03b1jvol (G) Mj\n(S), \u2211q\nj=1 \u03b1jvol (G) Mj\n(S\u0304) ) .\nS1.10 Extensions of the method to signed, colored, and weighted motifs\nOur results easily generalize for signed networks. We only have to generalize Equation S10 by allowing the adjacency matrix B to be signed. Extending the method for motifs where the edges or nodes are \u201ccolored\u201d or \u201clabeled\u201d is similar. If the edges are colored, then we again just allow the adjacency matrix B to capture this information. If the nodes in the motif are colored, we only count motif instances with the specified pattern.\nWe can also generalize the notions of motif cut and motif volume for \u201cweighted motifs\u201d, i.e., each motif has an associated nonnegative weight. Let \u03c9(v,\u03c7A(v)) be the weight of a motif instance. Our cut and volume metrics are then\ncut(G)M (S, S\u0304) = \u2211\n(v,\u03c7A(v))\u2208M\n\u03c9(v,\u03c7A(v)) 1(\u2203 i, j \u2208 \u03c7A(v) | i \u2208 S, j \u2208 S\u0304),\nvol(G)M (S) = \u2211\n(v,\u03c7A(v))\u2208M\n\u03c9(v,\u03c7A(v)) \u2211\ni\u2208\u03c7A(v)\n1(i \u2208 S).\nS24\nSubsequently, we adjust the motif adjacency matrix as follows:\n(WM)ij = \u2211\n(v,\u03c7A(v))\u2208M\n\u03c9(v,\u03c7A(v)) 1({i, j} \u2282 \u03c7A(v)) (S24)\nS1.11 Connections to directed graph partitioning\nOur framework also provides a way to analyze methods for clustering directed graphs. Existing principled generalizations of undirected graph partitioning to directed graph partitioning proceed from graph circulations (34) or random walks (35) and are difficult to interpret. Our motif-based clustering framework provides a simple, rigorous framework for directed graph partitioning. For example, consider the common heuristic of clustering the symmetrized graph W = A + AT , where A is the (directed) adjacency matrix (36). Following Theorem 5, conductance-minimizing methods for partitioningW are actually trying to minimize a weighted sum of motif-based conductances for the directed edge motif and the bi-directional edge motif:\nB1 = [ 0 1 0 0 ] , B2 = [ 0 1 1 0 ] ,\nwhere both motifs are simple (A = {1, 2}). If W1 and W2 are the motif adjacency matrices for B1 and B2, then A + AT = W = W1 + 2W2. This weighting scheme gives a weight of two to bi-directional edges in the original graph and a weight of one to uni-directional edges.\nAn alternative strategy for clustering a directed graph is to simply remove the direction on all edges, treating bi-directional and uni-directional edges the same. The resulting adjacency matrix is equivalent to the motif adjacency matrix for the bi-directional and uni-directional edges (without any relative weighting). Formally, W = W1 + W2. We refer to this \u201cmotif\u201d as Medge (Figure S4), which will later provide a convenient notation when discussing both motifbased clustering and edge-based clustering.\nS25\nS1.12 Connections to hypergraph partitioning\nFinally, we contextualize our method in the context of existing literature on hypergraph partitioning. The problem of partitioning a graph based on relationships between more than two nodes has been studied in hypergraph partitioning (37), and we can interpret motifs as hyperedges in a graph. In contrast to existing hypergraph partitioning problems, we induce the hyperedges from motifs rather than take the hyperedges as given a priori. The goal with our analysis of the Florida Bay food web, for example, was to find which hyperedge sets (induced by a motif) provide a good clustering of the network (see Section S7.1).\nIn general, our motif-based spectral clustering methodology falls into the area of encoding a hypergraph partitioning problem by a graph partitioning problem (38, 39). With simple motifs on k nodes, the motif Laplacian LM formed from WM (Equation S20) is a special case of the Rodr\u0131\u0301guez Laplacian (38, 40) for k-regular hypergraphs. The motif Cheeger inequality we proved (Theorem 6) explains why this Laplacian is appropriate for 3-regular hypergraphs. Specifically, it respects the standard cut and volume metrics for graph partitioning."
        },
        {
            "heading": "S2 Computational complexity and scalability of the method",
            "text": "We now analyze the computation of the higher-order clustering method. We first provide a theoretical analysis of the computational complexity, which depends on motif. After, we empirically analyze the time to find clusters for triangular motifs on a variety of real-world networks, ranging in size from a few hundred thousand edges to nearly two billion edges. Finally, we show that we can practically compute the motif adjacency matrix for motifs up to size 9 on a number of real-world networks.\nS26\nS2.1 Analysis of computational complexity\nWe now analyze the computational complexity of the algorithm presented in Theorem 6. Overall, the complexity of the algorithm is governed by the computations of the motif adjacency matrix WM , an eigenvector, and the sweep cut procedure. For simplicity, we assume that we can access edges in a graph in O(1) time and access and modify matrix entries in O(1) time. Let m and n denote the number of edges in the graph. Theoretically, the eigenvector can be computed in O((m + n)(log n)O(1)) time using fast Laplacian solvers (41). For the sweep cut, it takes O(n log n) to sort the indices given the eigenvector using a standard sorting algorithm such is merge sort. Computing motif conductance for each set Sr in the sweep also takes linear term. In pratice, the sweep cut step takes a small fraction of the total running time of the algorithm. For the remainder of the analysis, we consider the more nuanced issue of the time to compute WM .\nThe computational time to formWM is bounded by the time to find all instances of the motif in the graph. Naively, for a motif on k nodes, we can compute WM in \u0398(nk) time by checking each k-tuple of nodes. Furthermore, there are cases where there are \u0398(nk) motif instances in the graph, e.g., there are \u0398(n3) triangles in a complete graph. However, since most real-world networks are sparse, we instead focus on the complexity of algorithms in terms of the number of edges and the maximum degree in the graph. For this case, there are several efficient practical algorithms for real networks with available software (42\u201346).\nTheoretically, motif counting is efficient. Here we consider four classes of motifs: (1) triangles, (2) wedges (connected, non-triangle three-node motifs), (3) four-node motifs, and (4) k-cliques. Let m be the number of edges in a graph. Latapy analyzed a number of algorithms for listing all triangles in an undirected network, including an algorithm that has computational complexity \u0398(m1.5) (47). For a directed graph G, we can use the following algorithm: (1) form a new graph Gundir by removing the direction from all edges in G (2) find all triangles in Gundir,\nS27\n(3) for every triangle in Gundir, check which directed triangle motif it is in G. Since step 1 is linear and we can perform the check in step 3 in O(1) time, the same \u0398(m1.5) complexity holds for directed networks. This analysis holds regardless of the structure of the networks. However, additional properties of the network can lead to improved algorithms. For example, in networks with a power law degree sequence with exponent greater than 7/2, Berry et al. provide a randomized algorithm with expected running time \u0398(m) (48). In the case of a bounded degree graph, enumerating over all nodes and checking all pairs of neighbors takes time \u0398(nd2max), where dmax is the maximum degree in the graph. We note that with triangular motifs, the number of non-zeros in WM is less than the number of non-zeros in the original adjacency matrix. Thus, we do not have to worry about additional storage requirements.\nNext, we consider wedges (open triangles). We can list all wedges by looking at every pair of neighbors of every node. This algorithm has \u0398(nd2max) computational complexity, where n is the number of nodes and dmax is again the maximum degree in the graph (a more precise bound\nis \u0398( \u2211\nj d 2 j), where dj is the degree of node j.) If the graph is sparse, the motif adjacency\nmatrix will have more non-zeros than the original adjacency matrix, so additional storage is required. Specifically, there is fill-in for all two-hop neighbors, so the motif adjacency matrix\nhas O( \u2211\nj d 2 j) non-zeros. This is impractical for large real-world networks but manageable for\nmodestly sized networks.\nMarcus and Shavitt present an algorithm for listing all four-node motifs in an undirected graph in O(m2) time (49). We can employ the same edge direction check as for triangles to extend this result to directed graphs. Chiba and Nishizeki develop an algorithm for finding a representation of all quadrangles (motif on four nodes that contains a four-node cycle as a subgraph) in O(am) time and O(m) space, where a is the arboricity of the graph (50). The arboricity of any connected graph is bounded byO(m1/2), so this algorithm runs in timeO(m3/2).\nChiba and Nishizeki present an algorithms for k-clique enumeration that also depends on\nS28\nthe arboricity of the graph. Specifically, they provide an algorithm for enumerating all k-cliques inO(kak\u22122m) time, where a is the arboricity of the graph. This algorithm achieves the \u0398(m3/2) bound for arbitrary graphs. (We note that the triangle listing sub-case is similar in spirit to the algorithm proposed by Schank and Wagner (51)). For four-node cliques, the algorithm runs in time O(m2) time, which matches the complexity of Marcus and Shavitt (49).\nWe note that we could also employ approximation algorithms to estimate the weights in the motif adjacency matrix (52). Such methods balance computation time and accuracy. Finally, we note that the computation of WM and the computation of the eigenvector are suitable for parallel computation. There are already distributed algorithms for triangle enumeration (53), and the (parallel) eigenvector computation of a sparse matrix is a classical problem in scientific computing (54, 55).\nS2.2 Experimental results on triangular motifs\nIn this section, we demonstrate that our method scales to real-world networks with billions of edges. We tested the scalability of our method on 16 large directed graphs from a variety of real-world applications. These networks range from a couple hundred thousand to two billion edges and from 10 thousand to over 50 million nodes. Table S1 lists short descriptions of these networks. The wiki-RfA, email-EuAll, cit-HepPh, web-NotreDame, amazon0601, wiki-Talk, ego-Gplus, soc-Pokec, and soc-LiveJournal1 networks were downloaded from the SNAP collection at http://snap.stanford.edu/data/ (56). The uk-2014tpd, uk-2014-host, enwiki-2013, uk-2002, arabic-2005, twitter-2010, and sk-2005 networks were downloaded from the Laboratory for Web Algorithmics collection at http://law.di. unimi.it/datasets.php (57\u201360). Links to all datasets are available on our project website: http://snap.stanford.edu/higher-order/.\nRecall that Algorithm 1 consists of two major computational components:\nS29\n1. Form the weighted graph WM .\n2. Compute the eigenvector z of second smallest eigenvalue of the matrix LM .\nAfter computing the eigenvector, we sort the vertices and loop over prefix sets to find the lowest motif conductance set. We consider these final steps as part of the eigenvector computation for our performance experiments.\nFor each network in Table S1, we ran the method for all directed triangular motifs (M1\u2013 M7). To compute WM , we used a standard algorithm that meets the O(m3/2) bound (47, 51) with some additional pre-processing based on the motif. Specifically, the algorithm is:\n1. Take motif type M and graph G as input.\n2. (Pre-processing.) If M is M1 or M5, remove all bi-directional edges in G since these\nmotifs only contain uni-directional edges. If M is M4, remove all uni-directional edges in G as this motif only contains bi-directional edges.\n3. Form the undirected graph Gundir by removing the direction of all edges in G.\n4. Let du be the degree of node u in Gundir. Order the nodes in Gundir by increasing degree,\nbreaking ties arbitrarily. Denote this ordering by \u03c8.\n5. For every edge undirected edge {u, v} in Gundir, if \u03c8u < \u03c8v, add directed edge (u, v) to\nGdir; otherwise, add directed edge (v, u) to Gdir.\n6. For every node in u in Gdir and every pair of directed edges (u, v) and (u,w), check to\nsee if u, v, and w form motif M in G. If they do, check if the triangle forms motif M in G and update WM accordingly.\nThe algorithm runs in time \u0398(m3/2) time in the worst case, and is also known as an effective heuristic for real-world networks (48). After, we find the largest connected component of the\nS30\ngraph corresponding to the motif adjacency matrix WM , form the motif normalized Laplacian LM of the largest component, and compute the eigenvector of second smallest eigenvalue of LM . To compute the eigenvector, we use MATLAB\u2019s eigs routine with tolerance 1e-4 and the \u201csmallest algebraic\u201d option for the eigenvalue type.\nTable S2 lists the time to compute WM and the time to compute the eigenvector for each network. We omitted the time to read the graph from disk because this time strongly depends on how the graph is compressed. All experiments ran on a 40-core server with four 2.4 GHz Intel Xeon E7-4870 processors. All computations of WM were in serial and the computations of the eigenvectors were in parallel.\nOver all networks and all motifs, the longest computation of WM (including pre-processing time) was for M2 on the sk-2005 network and took roughly 52.8 hours. The longest eigenvector computation was for M6 on the sk-2005 network, and took about 1.62 hours. We note that WM only needs to be computed once per network, regardless of the eventual number of clusters that are extracted. Also, the computation ofWM can easily be accelerated by parallel computing (the enumeration of motifs can be done in parallel over nodes, for example) or by more sophisticated algorithms (48). In this work, we perform the computation of WM in serial in order to better understand the scalability.\nIn theory, the triangle enumeration time is O(m1.5). We fit a linear regression of the log of the computation time of the last step of the enumeration algorithm to the regressor log(m) and a constant term:\nlog(time) \u223c a log(m) + b (S25)\nIf the computations truly took cm1.5 for some constant c, then the regression coefficient for log(m) would be 1.5. Because of the pre-processing of the algorithm, the number of edges m depends on the motif. For example, with motifs M1 and M5, we only count the number of uni-directional edges. The pre-processing time, which is linear in the total number of edges,\nS31\nis not included in the time. The regression coefficient for log(m) (a in Equation S25) was found to be smaller 1.5 for each motif (Table S3). The largest regression coefficient was 1.31 for M3 (with 95% confidence interval 1.31 \u00b1 0.19). We also performed a regression over the aggregate times of the motifs, and the regression coefficient was 1.17 (with 95% confidence interval 1.17 \u00b1 0.09). We conclude that on real-world datasets, the algorithm for computing WM performs much better than the worst-case guarantees.\nTable S1: Summary of networks used in scalability experiments with triangular motifs. The total number of edges is the sum of the number of unidirectional edges and twice the number of bidirectional edges.\nName description # nodes # edges total unidir. bidir. wiki-RfA Adminship voting on Wikipedia 10.8K 189K 175K 7.00K email-EuAll Emails in a research institution 265K 419K 310K 54.5K cit-HepPh Citations for papers on arXiv HEP-PH 34.5K 422K 420K 657 web-NotreDame Hyperlinks on nd.edu domain 326K 1.47M 711K 380K amazon0601 Product co-purchasing on Amazon 403K 3.39M 1.50M 944K wiki-Talk Wikipedia users interactions 2.39M 5.02M 4.30M 362K ego-Gplus Circles on Google+ 108K 13.7M 10.8M 1.44M uk-2014-tpd top private domain links on .uk web 1.77M 16.9M 13.7M 1.58M soc-Pokec Pokec friendships 1.63M 30.6M 14.0M 8.32M uk-2014-host Host links on .uk web 4.77M 46.8M 33.7M 6.55M soc-LiveJournal1 LiveJournal friendships 4.85M 68.5M 17.2M 25.6M enwiki-2013 Hyperlinks on English Wikipedia 4.21M 101M 82.6M 9.37M uk-2002 Hyperlinks on .uk web 18.5M 292M 231M 30.5M arabic-2005 Hyperlinks on arabic-language web pages 22.7M 631M 477M 77.3M twitter-2010 Twitter followers 41.7M 1.47B 937M 266M sk-2005 Hyperlinks on .sk web 50.6M 1.93B 1.69B 120M\nS32\nTable S2: Time to compute the motif adjacency matrix WM and the second eigenvector of the motif normalized Laplacian LM in seconds for each directed triangular motif.\nMotif adjacency matrix WM Second eigenvector of LM Network M1 M2 M3 M4 M5 M6 M7 M1 M2 M3 M4 M5 M6 M7 wiki-RfA 1.19e+00 2.67e+00 1.71e+00 2.06e-02 1.79e+00 2.42e+00 2.35e+00 1.14e-01 2.12e-01 1.22e-01 2.12e-01 2.12e-01 2.94e-01 2.93e-01 email-EuAll 4.74e-01 8.29e-01 6.26e-01 2.46e-01 5.02e-01 5.40e-01 5.41e-01 2.29e-01 1.62e-01 2.43e-01 1.62e-01 1.62e-01 2.35e-01 1.92e-01 cit-HepPh 7.65e+00 3.36e+00 2.73e+00 6.22e+00 8.20e+00 3.29e+00 3.35e+00 2.11e+00 2.10e+00 2.11e+00 2.10e+00 2.10e+00 2.24e+00 2.30e+00 web-NotreDame 9.42e-01 2.39e+01 2.33e+01 2.30e+00 1.17e+00 8.29e+00 8.40e+00 1.86e-01 3.62e-01 5.97e-01 3.62e-01 3.62e-01 9.61e-01 2.06e+00 amazon0601 2.35e+00 8.66e+00 6.91e+00 1.82e+00 2.94e+00 5.47e+00 5.73e+00 1.23e-01 6.96e-01 4.62e+00 6.96e-01 6.96e-01 4.97e+00 4.53e+00 wiki-Talk 1.07e+01 3.00e+01 2.20e+01 3.11e+00 1.35e+01 2.09e+01 2.10e+01 1.28e+00 2.40e+00 2.51e+00 2.40e+00 2.40e+00 2.54e+00 4.52e+00 ego-Gplus 8.55e+02 2.42e+03 1.73e+03 2.08e+01 1.63e+03 2.07e+03 2.17e+03 4.42e+00 1.68e+01 2.11e+01 1.68e+01 1.68e+01 2.57e+01 4.42e+01 uk-2014-tpd 8.10e+01 5.31e+02 4.07e+02 2.56e+01 1.15e+02 3.04e+02 2.85e+02 3.59e+00 9.66e+00 9.92e+00 4.35e+00 9.66e+00 2.10e+01 2.16e+01 soc-Pokec 4.17e+01 1.34e+02 1.21e+02 3.04e+01 4.88e+01 1.00e+02 1.04e+02 1.96e+00 1.75e+01 3.91e+01 1.75e+01 1.75e+01 2.39e+01 2.45e+01 uk-2014-host 9.98e+02 4.68e+03 2.76e+03 8.90e+01 1.32e+03 2.89e+03 2.99e+03 1.81e+01 4.38e+01 6.80e+01 2.04e+01 4.38e+01 8.28e+01 8.73e+01 soc-LiveJournal1 9.08e+01 7.66e+02 6.24e+02 1.24e+02 1.24e+02 4.41e+02 4.49e+02 2.32e+00 2.20e+01 1.06e+02 2.20e+01 2.20e+01 4.49e+01 6.13e+01 enwiki-2013 8.36e+02 9.62e+02 7.09e+02 3.13e+01 9.77e+02 8.19e+02 8.38e+02 2.18e+01 7.58e+01 8.45e+01 7.58e+01 7.58e+01 2.14e+02 1.48e+02 uk-2002 1.47e+03 8.59e+03 5.17e+03 2.45e+02 1.73e+03 4.53e+03 5.29e+03 1.66e+01 8.65e+01 2.52e+02 8.65e+01 8.65e+01 7.87e+02 5.32e+02 arabic-2005 6.51e+03 7.64e+04 6.05e+04 6.08e+03 8.39e+03 3.59e+04 3.69e+04 1.98e+01 1.64e+02 4.80e+02 3.26e+02 1.64e+02 1.95e+03 1.40e+03 twitter-2010 1.21e+04 1.38e+05 1.31e+05 3.33e+04 1.99e+04 8.03e+04 7.65e+04 2.23e+02 1.23e+03 1.95e+03 1.23e+03 1.23e+03 2.22e+03 2.18e+03 sk-2005 5.52e+04 1.63e+05 1.29e+05 1.55e+04 5.23e+04 9.64e+04 8.42e+04 5.73e+01 2.94e+02 7.98e+02 2.94e+02 2.94e+02 5.83e+03 3.81e+03\nTable S3: The 95% confidence interval (CI) for the regression coefficient of the regressor log(m) in a linear model for predicting the time to compute WM , based on the computational results for the networks in Table S1. The algorithm runs is guranteed to run in time O(m3/2). \u201cCombined\u201d refers to the regression coefficient when considering all of the times.\nMotif M1 M2 M3 M4 M5 M6 M7 Combined\n95% CI 1.20\u00b1 0.19 1.30\u00b1 0.20 1.31\u00b1 0.19 0.90\u00b1 0.31 1.20\u00b1 0.20 1.27\u00b1 1.21 1.27\u00b1 0.21 1.17\u00b1 0.09\nS33\nS2.3 Experimental results on k-cliques\nOn smaller graphs, we can compute larger motifs. To illustrate the computation time, we formed the motif adjacency matrix W based on the k-cliques motif for k = 4, . . . , 9. We implemented the k-clique enumeration algorithm by Chiba and Nishizeki with the additional pre-processing of computing the (k \u2212 1)-core of the graph. (This pre-processing improves the running time in practice but does not affect the asymptotic complexity.) The motif adjacency matrices for k-cliques are sparser than the adjacency matrix of the original graph. Thus, we do not worry about spatial complexity for these motifs.\nWe ran the algorithm on nine real-world networks, ranging from roughly four thousande nodes and 88 thousand edges to over two million nodes and around five million edges (see Table S4.) Each network contained at least one 9-clique and hence at least one k-clique for k < 9. All networks were downloaded from the SNAP collection at http://snap.stanford. edu/data/ (56). All computations ran on the same server as for the triangular motifs and again there was no parallelism. We terminated computations after two hours. For five of the nine networks, the time to compute WM for the k-clique motif was under two hours for k = 4, . . . , 9 (Table S5). And for each network, the computation finished within two hours for k = 4, 5, 6. The smallest network (in terms of number of nodes and number of edges) was the Facebook ego network, where it took just under two hours to comptue WM for the 6-clique motif and over two hours for the 7-clique motif. This network has around 80,000 edges. On the other hand, for the YouTube network, which contains nearly 3 million edges, we could compute WM for the 9-clique motif in under a minute.\nWe conclude that it is possible to use our frameworks with motifs much larger than the three-node motifs on which we performed many of our experiments. However, the number of edges is not that correlated with the running time to compute WM . This makes sense becuse the Chiba and Nishizeki algorithm complexity is O(ak\u22122m), where a is the arboricity of the graph.\nS34\nHence, the dependence on the number of edges is always linear.\nTable S4: Summary of networks used in scalability experiments with k-clique motifs. For each graph, we consider all edges as undirected.\nNetwork description # nodes # edges ego-Facebook Facebook friendships 4.04K 88.2K wiki-RfA Adminship voting on Wikipedia 10.8K 182K ca-AstroPh author co-authorship 18.8K 198K email-EuAll Emails in a research institution 265K 364K cit-HepPh paper citations 34.5K 421K soc-Slashdot0811 Slashdot user interactions 77.4K 469K com-DBLP author co-authorship 317K 1.05M com-Youtube User friendships 1.13M 2.99M wiki-Talk Wikipedia users interactions 2.39M 4.66M\nTable S5: Time to compute WM for k-clique motifs (seconds). Only computations that finished within two hours are listed.\nNumber of nodes in clique (k) Network 4 5 6 7 8 9 ego-Facebook 14 317 6816 \u2013 \u2013 \u2013 wiki-RfA 6 22 63 134 218 286 ca-AstroPh 5 35 285 2164 \u2013 \u2013 email-EuAll 1 2 4 5 6 6 cit-HepPh 3 6 11 18 30 36 soc-Slashdot0811 3 12 55 282 1018 2836 com-DBLP 9 129 3234 \u2013 \u2013 \u2013 com-Youtube 12 17 25 33 35 33 wiki-Talk 64 466 2898 \u2013 \u2013 \u2013\nS35"
        },
        {
            "heading": "S3 Matrix-based interpretation of the motif-weighted adja-",
            "text": "cency matrix\nFor several motifs, the motif adjacency matrix WM (Equation S19) has a simple formula in terms of the adjacency matrix of the original, directed, unweighted graph, G. Let A be the adjacency matrix for G and let U and B be the adjacency matrix of the unidirectional and bidirectional links ofG. Formally, B = A\u25e6AT and U = A\u2212B, where \u25e6 denotes the Hadamard (entry-wise) product. Table S6 lists the formula of WM for motifs M1, M2, M3, M4, M5, M6, and M7 (see Figure S4) in terms of the matrices U and B. The central computational kernel in these computations is (X \u00b7 Y ) \u25e6 Z. When X , Y , and Z are sparse, efficient parallel algorithms have been developed and analyzed (61). If the adjacency matrix is sparse, then computing WM for these motifs falls into this framework. Table S6: Matrix-based formulations of the weighted motif adjacency matrix WM (Equation S19) for all triangular three-node simple motifs. P \u25e6Q denotes the Hadamard (entry-wise) products of matrices P and Q. If A is the adjacency matrix of a directed, unweighted graph G, then B = A \u25e6 AT and U = A\u2212B. Note that in all cases, WM is symmetric.\nMotif Matrix computations WM =\nM1 C = (U \u00b7 U) \u25e6 UT C + CT M2 C = (B \u00b7 U) \u25e6 UT + (U \u00b7B) \u25e6 UT + (U \u00b7 U) \u25e6B C + CT M3 C = (B \u00b7B) \u25e6 U + (B \u00b7 U) \u25e6B + (U \u00b7B) \u25e6B C + CT M4 C = (B \u00b7B) \u25e6B C M5 C = (U \u00b7 U) \u25e6 U + (U \u00b7 UT ) \u25e6 U + (UT \u00b7 U) \u25e6 U C + CT M6 C = (U \u00b7B) \u25e6 U + (B \u00b7 UT ) \u25e6 UT + (UT \u00b7 U) \u25e6B C M7 C = (U T \u00b7B) \u25e6 UT + (B \u00b7 U) \u25e6 U + (U \u00b7 UT ) \u25e6B C\nWith these matrix formulations, implementing the motif-based spectral partitioning algorithm for modestly sized graphs is straightforward. However, these computations become slower than standard fast triangle enumeration algorithms when the networks are large and sparse. Nevertheless, the matrix formulations provide a simple and elegant computational\nS36\n1 f u n c t i o n [ S , Sbar , c o n d u c t a n c e s ] = M o t i f S p e c t r a l P a r t i t i o n M 6 (A) 2 % S p e c t r a l p a r t i t i o n i n g f o r m o t i f M 6 3 4 B = sp on es (A & A \u2019 ) ; % b i d i r e c t i o n a l l i n k s 5 U = A \u2212 B ; % u n i d i r e c t i o n a l l i n k s 6 7 % Form m o t i f a d j a c e n c y m a t r i x f o r m o t i f M 6 . 8 % For d i f f e r e n t m o t i f s , r e p l a c e t h i s l i n e wi th a n o t h e r m a t r i x f o r m u l a t i o n . 9 W = (B \u2217 U\u2019 ) .\u2217 U\u2019 + (U \u2217 B) .\u2217 U + (U\u2019 \u2217 U) . \u2217 B ;\n10 11 % Compute e i g e n v e c t o r o f m o t i f n o r m a l i z e d L a p l a c i a n 12 D s q r t = f u l l ( sum (W, 2 ) ) ; 13 D s q r t ( D s q r t \u02dc= 0 ) = 1 . / s q r t ( D s q r t ( D s q r t \u02dc= 0 ) ) ; 14 [ I , J , V] = f i n d (W) ; 15 Ln = s p a r s e ( I , J , \u2212V .\u2217 ( D s q r t ( I ) .\u2217 D s q r t ( J ) ) , s i z e (A, 1 ) , s i z e (A, 2 ) ) ; 16 [ Z , lambdas ] = e i g s ( Ln , 2 , \u2019 s a \u2019 ) ; 17 % Matlab \u2019 s e i g s i s somet imes o u t o f o r d e r 18 [ \u02dc , e i g o r d e r ] = s o r t ( d i a g ( lambdas ) ) ; 19 y = D s q r t . \u2217 Z ( : , e i g o r d e r ( end ) ) ; 20 21 % L i n e a r t ime sweep p r o c e d u r e 22 [ \u02dc , o r d e r ] = s o r t ( y ) ; 23 C = W( o r d e r , o r d e r ) ; 24 C sums = f u l l ( sum (C , 2 ) ) ; 25 volumes = cumsum ( C sums ) ; 26 v o l u m e s o t h e r = f u l l ( sum ( sum (W) ) ) \u2217 ones ( l e n g t h ( o r d e r ) , 1 ) \u2212 volumes ; 27 c o n d u c t a n c e s = cumsum ( C sums \u2212 2 \u2217 sum ( t r i l (C) , 2 ) ) . / min ( volumes , v o l u m e s o t h e r ) ; 28 [ \u02dc , s p l i t ] = min ( c o n d u c t a n c e s ) ; 29 S = o r d e r ( 1 : s p l i t ) ; 30 Sbar = o r d e r ( ( s p l i t + 1 ) : end ) ;\nFigure S6: MATLAB implementation of the motif-based spectral partitioning algorithm for motif M6. For other motifs, line 9 can be replaced with the formulations from Table S6.\nmethod for the motif adjacency matrix WM . To demonstrate, Figure S6 provides a complete MATLAB implementation of Algorithm 1 for M6 (Figure S4). The entire algorithm including comments comrpises 28 lines of code.\nAn alternative matrix formulation comes from a motif-node adjacency matrix. LetM(B,A) be a motif set and number the instances of the motif 1, . . . , |M |, so that (vi, \u03c7A(vi)) is the ith motif. Define the |M | \u00d7n motif-node adjacency matrix AM by (AM)ij = 1(j \u2208 \u03c7A(vi)). Then\nS37\n(WM)ij = (A T MAM)ij, i 6= j. (S26)\nThis provides a convenient algebraic formulation for defining the weighted motif adjacency matrix. However, in practice, we do not use this formulation for any computations."
        },
        {
            "heading": "S4 Alternative clustering algorithms for evaluation",
            "text": "For our experiments, we compare our spectral motif-based custering to the following methods:\n\u2022 Standard, edge-based spectral clustering, which is a special case of motif-based cluster-\ning. In particular, the motifs\nB1 = [ 0 1 1 0 ] , B2 = [ 0 1 0 0 ] , A = {1, 2} (S27)\ncorrespond to removing directionality from a directed graph. We refer to the union of these two motifs as Medge.\n\u2022 Infomap, which is based on the map equation (62). Software for Infomap was down-\nloaded from http://mapequation.org/code.html. We run the algorithm the algorithm for directed links when the network under consideration is directed.\n\u2022 The Louvain method (63). Software for the Louvain method was downloaded from\nhttps://perso.uclouvain.be/vincent.blondel/research/louvain. html We use the \u201coriented\u201d version of the Louvain method for directed graphs.\nInfomap and the Louvain method are purely clustering methods in the sense that they take as input the graph and produce as output a set of labels for the nodes in the graph. In contrast to the spectral methods, we do not have control over the number of clusters. Also, only the spectral methods provide embeddings of the nodes into euclidean space, which is useful for\nS38\nvisualization. Thus, for our analysis of the transportation reachiability network in Section S6, we only compare spectral methods."
        },
        {
            "heading": "S5 Details and comparison against existing methods for the",
            "text": "C. elegans network\nWe now provide more details on the cluster found for the C. elegans network of frontal neurons (28). In this network, the nodes are neurons and the edges are synapses. The network data was downloaded from http://www.biological-networks.org/pubs/ suppl/celegans131.zip.\nS5.1 Connected components of the motif adjacency matrices\nWe again first onsider the connected components of the motif adjacency matrices as a preprocessing step. For our analysis, we consider use Mbifan, M8, and Medge (Figure S4). The original network has 131 nodes and 764 edges. The largest connected component of the motif adjacency matrix for motif Mbifan contains 112 nodes. The remaining 19 nodes are isolated and correspond to the neurons AFDL, AIAR, AINR, ASGL/R, ASIL/R, ASJL/R, ASKL/R, AVL, AWAL, AWCR, RID, RMFL, SIADR, and SIBDL/R. The largest connected component of the motif adjacency matrix for motif M8 contains 127 nodes. The remaining 4 nodes are isolated and correspond to the neurons ASJL/R and SIBDL/R. The original network is weakly connected, so the motif adjacency matrix for Medge is connected.\nS5.2 Comparison of bi-fan motif cluster to clusters found by existing methods\nWe found the motif-based clusters for motifs Mbifan, M8, and Medge by running Algorithm 1 on the largest connected component of the motif adjacency matrix. Sweep profile plots (\u03c6M(S) as\nS39\n10 0\n10 1\n10 2\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nM o ti f c o n d\nu c ta\nn c e\nSets Sr\nFigure S7: Sweep profile plot (\u03c6M(S) as a function of S from the sweep in Algorithm 1) for Mbifan (green) M8 (dark blue), and Medge (light blue).\na function of S from the sweep in Algorithm 1) are shown in Figure S7 and show that the size of the Mbifan returned by Algorithm 1 cluster is smaller than the clusters for M8 and Medge. In fact, the motif-based clusters for M8 and Medge essentially bisect the graph, containing 63 of 127 and 64 of 131 nodes, respectively. Of the 63 nodes in the M8-based cluster, only 2 are in the edge-based cluster, so these partitions give roughly the same information.\nNext, we compare the clusters found by existing methods to the Mbifan-based cluster found by Algorithm 1. We will show that existing methods do not find the same group of nodes. Let Sbifan be the Mbifan-based cluster, which consists of 20 nodes. The nodes correspond to the following neurons: IL1DL/VL, IL2DL/DR/VL/VR/L/R, OLQDL/R, RIH, RIPL/R, RMEL/R/V, and URADL/DR/VL/VR. The partitions based onM8 andMedge provide two sets of nodes each. For the subsequent analysis, we consider the set with the largest number of overlapping nodes with Sbifan. Call these sets SM8 and Sedge. We also consider the cluster found by Infomap and the Louvain method with the largest overlap with Sbifan. Call these sets SI and SL.\nTo compare the most similar clusters found by other methods to Sbifan, we look at two metrics. First, how many neurons in Sbifan are in a cluster found by existing methods (in other\nS40\nwords, the overlap). A cluster consisting of all nodes in the graph would trivially have 100% overlap with Sbifan but loses all precision in the cluster identification. Thus, we also consider the sizes of the clusters. These metrics are summarized as follows:\n|Sbifan \u2229 SM8| = 20, |SM8| = 68 |Sbifan \u2229 Sedge| = 20, |Sedge| = 64\n|Sbifan \u2229 SL| = 13, |SL| = 27 |Sbifan \u2229 SI | = 19, |SI | = 114\nWe see that Sbifan is a subset of SM8 and Sedge and has substantial overlap with SI . However, Sbifan is by far the smallest of all of these sets. We conclude that existing methods do not capture the same information as motif Mbifan.\nTo further investigate the structure found by existing methods, we show the clusters Sedge and SM8 in Figure S8. From the figure, we see that spectral clustering based on edges or motif M8 simply finds a spatially coherent cluster, rather than the control structure formed by the nodes in Sbifan.\nS41\nFigure S8: Illustration of motif-based clusters with true two-dimensional spatial dimensions of the frontal neurons of C. elegans. A: The Mbifan-based cluster consists of the labeled dark blue nodes. B: Partitioning the graph based on motif M8, where the labeled dark blue nodes are the nodes on the side of the partition with largest overlap of the nodes in A. C: Partitioning the graph based on edges, where the labeled dark blue nodes are the nodes on the side of the partition with largest overlap of the nodes in A. Note that the partitions in Figures B and C capture the cluster in Figure A, but also contain many other nodes. Essentially, the partitions in B and C are just capturing spatial information.\nS42"
        },
        {
            "heading": "S6 Details and comparison against existing methods for the",
            "text": "transportation reachability network\nThe nodes in the transportation reachability network are airports in the United States and Canada. There is an edge from city i to city j if the estimated travel time from i to j is less than some threshold (23). The network is not symmetric. The network with estimated travel times was downloaded from\nhttp://www.psi.toronto.edu/affinitypropagation/TravelRouting.mat and http://www.psi.toronto.edu/affinitypropagation/TravelRoutingCityNames. txt. We collected the latitude, longitude, and metropolitan populations of the cities using WolframAlpha and Wikipedia. All of the data is available on our project web page: http: //snap.stanford.edu/higher-order/.\nS6.1 Methods for spectral embeddings\nWe compared the motif-based spectral embedding of the transportation reachability network to spectral embeddings from other connectivity matrices. For this analysis, we ignore the travel times times and only consider the topology of the network. The two-dimensional spectral embedding for a graph defined by a (weighted) adjacency matrix W \u2208 Rn\u00d7n comes from Algorithm 2:\n1. Form the normalized Laplacian L = I \u2212D\u22121/2WD\u22121/2, where D is the diagonal degree\nmatrix with Dii = \u2211 jWij .\n2. Compute the first 3 eigenvectors z1, z2, z3 of smallest eigenvalues for L (z1 has the small-\nest eigenvalue).\n3. Form the normalized matrix Y \u2208 Rn\u00d73 by Yij = zij/ \u221a\u22113 j=1 z 2 ij .\nS43\n4. Define the primary and secondary spectral coordinates of node i to be Yi2 and Yi3, respec-\ntively.\nWe consider the following three matrices W .\n1. Motif: The sum of the motif adjacency matrix (Equation S20) for three different anchored\nmotifs:\nB1 = 0 1 11 0 1 1 1 0  , B2 = 0 1 11 0 1 0 1 0  , B3 = 0 1 01 0 1 0 1 0  , A = {1, 3}. (S28) If S is the matrix of bidirectional links in the graph (Sij = 1 if and only ifAij = Aji = 1), then the motif adjacency matrix for these motifs is WM = S2. The resulting embedding is shown in Figure 4C of the main text.\n2. Undirected: The adjacency matrix is formed by ignoring edge direction. This is the\nstandard spectral embedding. The resulting embedding is shown in Figure 4D of the main text.\n3. Undirected complement: The adjacency matrix is formed by taking the complement of\nthe undirected adjacency matrix. This matrix tends to connect non-hubs to each other.\nThe networks represented by each adjacency matrices are all connected.\nS6.2 Comparison of motif-based embedding to other embeddings\nWe computed 99% confidence intervals for the Pearson correlation of the primary spectral coordinate with the metropolitan population of the city using the Pearson correlation coefficient. Table S7 lists the confidence intervals. (Since eigenvectors are only unique up to sign, the confidence intervals are symmetric about 0. We list the interval with the largest positive end point\nS44\nTable S7: Summary of Pearson correlations for spectral embeddings of the transportation reachability network. We list the 99% confidence interval for the Pearson correlation coefficient.\nPrimary spectral coordinate Secondary spectral coordinate and metropolitan population and longitude\nEmbedding 99% confidence interval 99% confidence interval Motif 0.43 \u00b1 0.09 0.59 \u00b1 0.08 Undirected 0.11 \u00b1 0.12 0.39 \u00b1 0.11 Undirected complement 0.31 \u00b1 0.11 0.10 \u00b1 0.12\nunder this permutation to be consistent across embeddings.) The motif-based primary spectral coordinate has the strongest correlation with the city populations.\nWe repeated the computations for the correlation between the secondary spectral coordinate and the longitude of the city. Again, the motif-based clustering has the strongest correlation. Furthermore, the lower end of the confidence interval for the motif-based embedding was above the higher end of the confidence interval for the other three embeddings.\nFinally, in order to visualize these relationships, we computed Loess regressions of city metropolitan population and longitude against the primary and secondary spectral coordinates for each of the embeddings (Figure S9). The sign of the eigenvector used in each regression was chosen to match correlation shown in Figures 3C and 3D in the main text (primary spectral coordinate positively correlated with population and secondary spectral coordinate negatively correlated with longitude). The Loess regressions visualize the stronger correlation of the motifbased spectral coordinates with the metropolitan popuatlion and longitude.\nWe conclude that the embedding provided by the motif adjacency matrix more strongly captures the hub nature of airports and West-East geography of the network. To gain further insight into the relationship of the primary spectral coordinate\u2019s relationship with the hub airports, we visualize the adjacency matrix in Figure S10, where the nodes are ordered by the spectral ordering. We see a clear relationship between the spectral ordering and the connectivity.\nS45\nFigure S9: Loess regressions of city metropolitan population against the primary spectral coordinate (top) and longitude against secondary spectral coordinate (bottom) for the motif (left), undirected (middle), and undirected complement (right) adjacency matrices.\nFigure S10: Visualization of transportation reachability network. Nodes are ordered by the spectral ordering provided by the motif adjacency matrix. A black dot means no edge exists in the network. For the edges in the network, lighter colors mean longer estimated travel times.\nS46"
        },
        {
            "heading": "S7 Additional case studies",
            "text": "We next use motif-based clustering to analyze several additional networks. Our main goal is to show that motif-based clusters find markedly different structures in many real-world networks compared to edge-based clusters. For the case of a transcription regulation network of yeast, we also show that motif-based clustering more accurately finds known functional modules compared to existing methods. On the English Wikipedia article network and the Twitter network, we identify motifs that find anomalous clusters. On the Stanford web graph and in collaboration networks, we use motifs that have previously been studied in the literature and see how they reveal organizational structure in the networks.\nS7.1 Motif M6 in the Florida Bay food web\nWe now apply the higher-order clustering framework on the Florida Bay ecosystem food web (64). The dataset was downloaded from http://vlado.fmf.uni-lj.si/pub/networks/ data/bio/foodweb/Florida.paj. In this network, the nodes are compartments (roughly, organisms and species) and the edges represent directed carbon exchange (in many cases, this means that species j eats species i). Motifs model energy flow patterns between several species.\nS7.1.1 Identifying higher-order modular organization\nIn this case study, we use the framework to identify higher-order modular organization of networks. We focus on three motifs: M5 corresponds to a hierarchical flow of energy where species i and j are energy sources (prey) for species k, and i is also an energy source for j; M6 models two species that prey on each other and then compete to feed on a common third species; and M8 describes a single species serving as an energy source for two non-interacting species. Motif M5 is considered a building block for food webs (65, 66), and the prevalence of motif M6 is predicted by a certain niche model (67).\nS47\nThe framework reveals that low motif conductance (high-quality) clusters only exist for motif M6 (motif conductance 0.12), whereas clusters based on motifs M5 or M8 have high motif conductance (see Figure S11). In fact, the motif Cheeger inequality (Theorem 6) guarantees that clustering based on motif M5 or M8 will always have larger motif conductance that clustering based on M6. The inequality says that the motif conductance for any cluster in a connected motif adjacency matrix is at least half of the second smallest eigenvalue of the motif-normalized Laplacian. However, finding the cluster with optimal conductance is still computationally infeasible in general (68).\nThe lower bounds using the largest connected component of the motif adjacency matrix for motifs M5, M6, and M8 were 0.2195, 0.0335, and 0.2191, and the clusters found by the Algorithm 1 had motif conductances of 0.4414, 0.1200, and 0.4145. Thus, the cluster S found by the algorithm for M6 has smaller motif M6-conductance (0.12) than any possible cluster\u2019s motif-M5 or motif-M8 conductance. To state this formally, let C be the cluster found by the algorithm for motif M6 and let HM be the largest connected component of motif adjacency matrix for motif M . Then\n\u03c6M6(HM6 , C) \u2264 min {\nmin S \u03c6M5(HM5 , S), min S \u03c6M8(HM8 , S)\n} . (S29)\nThis means that, in terms of motif conductance, any cluster based on motifs M5 or M8 is worse than the cluser found by the algorithm in Theorem 6 for motif M6. We note that the same conclusions hold for edge-based clustering. For motif Medge, the lower bound on conductance was 0.2194 and the cluster found by the algorithm had conductance 0.4083.\nS7.1.2 Analysis of higher-order modular organization\nSubsequently, we used motif M6 to cluster the food web, revealing four clusters (Figure S11). Three represent well-known aquatic layers: (i) the pelagic system; (ii) the benthic predators of eels, toadfish, and crabs; (iii) the sea-floor ecosystem of macroinvertebrates. The fourth\nS48\ncluster identifies microfauna supported by particulate organic carbon in water and free bacteria. Table S9 lists the nodes in each cluster.\nWe also measured how well the motif-based clusters correlate to known ground truth system subgroup classifications of the nodes (64). These classes are microbial, zooplankton, and sediment organism microfauna; detritus; pelagic, demersal, and benthic fishes; demseral, seagrass, and algae producers; and macroinvertebrates (Table S9).2 We also consider a set of labels which does not include the subclassification for microfauna and producers. In this case, the labels are microfauna; detritus; pelagic, demersal, and benthic fishes; producers; and macroinvertebrates.\nTo quantify how well the clusters found by motif-based clustering reflect the ground truth labels, we used several standard evaluation criteria: adjusted rand index, F1 score, normalized mutual information, and purity (69). We compared these results to the clusters of several methods using the same evaluation criteria. In total, we evaluated six methods:\n1. Motif-based clustering with the embedding + k-means algorithm (Algorithm 2) with 500\niterations of k-means.\n2. Motif-based clustering with recursive bi-partitioning (repeated application of Algorithm 1\non the largest remaining compoennt). The process continues to cut the largest cluster until there are 4 total.\n3. Edge-based clustering with the embedding + k-means algorithm, again with 500 iterations\nof k-means.\n4. Edge-based clustering with recursive bi-partitioning with the same partitioning process.\n5. The Infomap algorithm.\n2The classifications are also available on our project web page: http://snap.stanford.edu/ higher-order/.\nS49\n6. The Louvain method.\nFor the first four algorithms, we control the number of clusters, which we set to 4. For the last two algorithms, we cannot control the number of clusters. However, both methods found 4 clusters.\nTable S10 shows that the motif-based clustering by embedding + k-means had the best performance for each classification criterion on both classifications. We conclude that the organization of compartments in the Florida Bay foodweb are better described motif M6 than by edges.\nS7.1.3 Connected components of the motif adjacency matrices\nFinally, we discuss the discuss the preprocessing step of our method, where we compute computed connected components of the motif adjacency matrices. The original network has 128 nodes and 2106 edges. The largest connected component of the motif adjacency matrix for motif M5 contains 127 of the 128 nodes. The node corresponding to the compartment of \u201croots\u201d is the only node not in the largest connected component. The two largest connected components of the motif adjacency matrix for motif M6 contain 12 and 50 nodes. The remaining 66 nodes are isolated. Table S8 lists the nodes in each component. We note that the group of 12 nodes corresponds to the green cluster in Figure S11. The motif adjacency matrix for M8 is connected. The original network is weakly connected, so the motif adjacency matrix for Medge is also connected.\nS50\nFigure S11: Higher-order organization of the Florida Bay food web. A: Sweep profile plot (\u03c6(G)M (S) as a function of S from the sweep in Algorithm 1) for different motifs on the Florida Bay ecosystem food web (64). A priori it is not clear whether the network is organized based on a given motif. For example, motifs M5 (green) and M8 (blue) do not reveal any higher-order organization (motif conductance has high values). However, the downward spikes of the red curve show that M6 reveals rich higher-order modular structure (7). Ecologically, motif M6 corresponds to two species mutually feeding on each other and also preying on a common third species. B: Clustering of the food web based on motif M6. (For illustration, edges not participating in at least one instance of the motif are omitted.) The clustering reveals three known aquatic layers: pelagic fishes (yellow), benthic fishes and crabs (red), and sea-floor macroinvertebrates (blue) as well as a cluster of microfauna and detritus (green). Our framework identifies these modules with higher accuracy (61%) than existing methods (48\u201353%). C: A higher-order cluster (yellow nodes in (B)) shows how motif M6 occurs in the pelagic layer. The needlefish and other pelagic fishes eat each other while several other fishes are prey for these two species. D: Another higher-order cluster (green nodes in (B)) shows how motif M6 occurs between microorganisms. Here, several microfauna decompose into Particulate Organic Carbon in the water (water POC) but also consume water POC. Free bacteria serves as an energy source for both the microfauna and water POC. S51\nTable S8: Connected components of the Florida Bay foodweb motif adjacency matrix for motif M6. There are 50 nodes in component 1, 12 nodes in component 2, and 66 isolated nodes.\nTwo largest components Isolated nodes Compartment (node) Component index Compartment (node) Benthic Phytoplankton 1 Barracuda Thalassia 1 2 \u00b5m Spherical Phytoplankt Halodule 1 Synedococcus Syringodium 1 Oscillatoria Drift Algae 1 Small Diatoms (<20 \u00b5m) Epiphytes 1 Big Diatoms (>20 \u00b5m) Predatory Gastropods 1 Dinoflagellates Detritivorous Polychaetes 1 Other Phytoplankton Predatory Polychaetes 1 Roots Suspension Feeding Polych 1 Coral Macrobenthos 1 Epiphytic Gastropods Benthic Crustaceans 1 Thor Floridanus Detritivorous Amphipods 1 Lobster Herbivorous Amphipods 1 Stone Crab Isopods 1 Sharks Herbivorous Shrimp 1 Rays Predatory Shrimp 1 Tarpon Pink Shrimp 1 Bonefish Benthic Flagellates 1 Other Killifish Benthic Ciliates 1 Snook Meiofauna 1 Sailfin Molly Other Cnidaridae 1 Hawksbill Turtle Silverside 1 Dolphin Echinoderma 1 Other Horsefish Bivalves 1 Gulf Pipefish Detritivorous Gastropods 1 Dwarf Seahorse Detritivorous Crabs 1 Grouper Omnivorous Crabs 1 Jacks Predatory Crabs 1 Pompano Callinectes sapidus (blue crab) 1 Other Snapper Mullet 1 Gray Snapper Blennies 1 Mojarra Code Goby 1 Grunt Clown Goby 1 Porgy Flatfish 1 Pinfish Sardines 1 Scianids Anchovy 1 Spotted Seatrout Bay Anchovy 1 Red Drum Lizardfish 1 Spadefish Catfish 1 Parrotfish Eels 1 Mackerel Toadfish 1 Filefishes Brotalus 1 Puffer Halfbeaks 1 Loon Needlefish 1 Greeb Goldspotted killifish 1 Pelican Rainwater killifish 1 Comorant Other Pelagic Fishes 1 Big Herons and Egrets Other Demersal Fishes 1 Small Herons and Egrets Benthic Particulate Organic Carbon (Benthic POC) 1 Ibis Free Bacteria 2 Roseate Spoonbill Water Flagellates 2 Herbivorous Ducks Water Cilitaes 2 Omnivorous Ducks Acartia Tonsa 2 Predatory Ducks Oithona nana 2 Raptors Paracalanus 2 Gruiformes Other Copepoda 2 Small Shorebirds Meroplankton 2 Gulls and Terns Other Zooplankton 2 Kingfisher Sponges 2 Crocodiles Water Particulate Organic Carbon (Water POC) 2 Loggerhead Turtle Input 2 Green Turtle\nManatee Dissolved Organic Carbon (DOC) Output Respiration\nS52\nTable S9: Ecological classification of nodes in the Florida Bay foodweb. Colors correspond to the colors in the clustering of Figure S11.\nCompartment (node) Classification 1 Classification 2 Assignment Free Bacteria Microbial microfauna Microfauna Green Water Flagellates Microbial microfauna Microfauna Green Water Cilitaes Microbial microfauna Microfauna Green Acartia Tonsa Zooplankton microfauna Microfauna Green Oithona nana Zooplankton microfauna Microfauna Green Paracalanus Zooplankton microfauna Microfauna Green Other Copepoda Zooplankton microfauna Microfauna Green Meroplankton Zooplankton microfauna Microfauna Green Other Zooplankton Zooplankton microfauna Microfauna Green Sponges Macroinvertebrates Macroinvertebrates Green Water POC Detritus Detritus Green Input Detritus Detritus Green Sardines Pelagic Fishes Pelagic Fishes Yellow Anchovy Pelagic Fishes Pelagic Fishes Yellow Bay Anchovy Pelagic Fishes Pelagic Fishes Yellow Halfbeaks Pelagic Fishes Pelagic Fishes Yellow Needlefish Pelagic Fishes Pelagic Fishes Yellow Goldspotted killifish Fishes Demersal Fishes Demersal Yellow Rainwater killifish Fishes Demersal Fishes Demersal Yellow Silverside Pelagic Fishes Pelagic Fishes Yellow Other Pelagic Fishes Pelagic Fishes Pelagic Fishes Yellow Detritivorous Crabs Macroinvertebrates Macroinvertebrates Red Predatory Crabs Macroinvertebrates Macroinvertebrates Red Callinectus sapidus Macroinvertebrates Macroinvertebrates Red Lizardfish Benthic Fishes Benthic Fishes Red Eels Fishes Demersal Fishes Demersal Red Code Goby Benthic Fishes Benthic Fishes Red Clown Goby Benthic Fishes Benthic Fishes Red Herbivorous Shrimp Macroinvertebrates Macroinvertebrates Red Benthic Phytoplankton Producer Demersal Producer Blue Thalassia Producer Seagrass Producer Blue Halodule Producer Seagrass Producer Blue Syringodium Producer Seagrass Producer Blue Drift Algae Producer Algae Producer Blue Epiphytes Producer Algae Producer Blue Benthic Flagellates Sediment Organism microfauna Microfauna Blue Benthic Ciliates Sediment Organism microfauna Microfauna Blue Meiofauna Sediment Organism microfauna Microfauna Blue Other Cnidaridae Macroinvertebrates Macroinvertebrates Blue Echinoderma Macroinvertebrates Macroinvertebrates Blue Bivalves Macroinvertebrates Macroinvertebrates Blue Detritivorous Gastropods Macroinvertebrates Macroinvertebrates Blue Predatory Gastropods Macroinvertebrates Macroinvertebrates Blue Detritivorous Polychaetes Macroinvertebrates Macroinvertebrates Blue Predatory Polychaetes Macroinvertebrates Macroinvertebrates Blue Suspension Feeding Polych Macroinvertebrates Macroinvertebrates Blue Macrobenthos Macroinvertebrates Macroinvertebrates Blue Benthic Crustaceans Macroinvertebrates Macroinvertebrates Blue Detritivorous Amphipods Macroinvertebrates Macroinvertebrates Blue Herbivorous Amphipods Macroinvertebrates Macroinvertebrates Blue Isopods Macroinvertebrates Macroinvertebrates Blue Predatory Shrimp Macroinvertebrates Macroinvertebrates Blue Pink Shrimp Macroinvertebrates Macroinvertebrates Blue Omnivorous Crabs Macroinvertebrates Macroinvertebrates Blue Catfish Benthic Fishes Benthic Fishes Blue Mullet Pelagic Fishes Pelagic Fishes Blue Benthic POC Detritus Detritus Blue Toadfish Benthic Fishes Benthic Fishes Blue Brotalus Fishes Demersal Fishes Demersal Blue Blennies Benthic Fishes Benthic Fishes Blue Flatfish Benthic Fishes Benthic Fishes Blue Other Demersal Fishes Fishes Demersal Fishes Demersal Blue\nS53\nTable S10: Comparison of motif-based algorithms against other methods in finding ground truth structure in the Florida Bay food web (64). Performance for identifying the two classifications provided in Table S9 was evaluated based on Adjusted Rand Index (ARI), F1 score, Normalized Mutual Information (NMI), and Purity. In all cases, the motif-based methods have the best performance.\nEvaluation Motif embedding Motif recursive Edge embedding Edge recursive Infomap Louvain\n+ k-means bi-partitioning + k-means bi-partitioning\nC la\nss ifi\nca tio\nn 1 ARI 0.3005 0.2156 0.1564 0.1226 0.1423 0.2207\nF1 0.4437 0.3853 0.3180 0.2888 0.3100 0.4068 NMI 0.5040 0.4468 0.4112 0.3879 0.4035 0.4220 Purity 0.5645 0.5323 0.4032 0.4194 0.4194 0.5323\nC la\nss ifi\nca tio\nn 2 ARI 0.3265 0.2356 0.1814 0.1190 0.1592 0.2207\nF1 0.4802 0.4214 0.3550 0.3035 0.3416 0.4068 NMI 0.4822 0.4185 0.3533 0.3034 0.3471 0.4220 Purity 0.6129 0.5806 0.4839 0.4355 0.4677 0.5323\nS7.2 Coherent feedforward loops in the S. cerevisiae transcriptional regulation network\nIn this network, each node is an operon (a group of genes in a mRNA molecule), and a directed edge from operon i to operon j means that i is regulated by a transcriptional factor encoded by j (29). Edges are directed and signed. A positive sign represents activation and a negative sign represents repression. The network data was downloaded from http://www.weizmann.\nac.il/mcb/UriAlon/sites/mcb.UriAlon/files/uploads/NMpaper/yeastdata. mat and http://www.weizmann.ac.il/mcb/UriAlon/sites/mcb.UriAlon/files/ uploads/DownloadableData/list_of_ffls.pdf.\nFor this case study, we examine the coherent feedforward loop motif (see Figure S12), which act as sign-sensitive delay elements in transcriptional regulation networks (2, 9). Formally, the feedforward loop is represented by the following signed motifs\nB1 = 0 + +0 0 + 0 0 0  , B2 = 0 \u2212 \u22120 0 + 0 0 0  , B3 = 0 + \u22120 0 \u2212 0 0 0  , B4 = 0 \u2212 +0 0 \u2212 0 0 0  . (S30) S54\nTable S11: Connected components of size greater than one for the motif adjacency matrix in the S. cerevisiae network for the coherent feedforward loop.\nSize operons\n18 ALPHA1, CLN1, CLN2, GAL11, HO, MCM1, MFALPHA1, PHO5, SIN3, SPT16, STA1, STA2, STE3, STE6, SWI1, SWI4/SWI6, TUP1, SNF2/SWI1\n9 HXT11, HXT9, IPT1, PDR1, PDR3, PDR5, SNQ2, YOR1, YRR1 9 GCN4, ILV1, ILV2, ILV5, LEU3, LEU4, MET16, MET17, MET4 6 CHO1, CHO2, INO2, INO2/INO4, OPI3, UME6 6 DAL80, DAL80/GZF3, GAP1, GAT1, GLN1, GLN3 5 CYC1, GAL1, GAL4, MIG1, HAP2/3/4/5 3 ADH2, CCR4, SPT6 3 CDC19, RAP1, REB1 3 DIT1, IME1, RIM101\nThese motifs have the same edge pattern and only differ in sign. All of the motifs are simple (A = {1, 2, 3}). For our analysis, we consider all coherent feedforward loops that are subgraphs on the induced subgraph of any three nodes. However, there is only one instance where the coherent feedforward loop itself is a subgraph but not an induced subgraph on three nodes. Specifically, the induced subgraph by DAL80, GAT1, and GLN3 contains a bi-directional edge between DAL80 and GAT1, unidirectional edges from DAL80 and GAT1 to GLN3.\nS7.2.1 Connected components of the adjacency matrices\nAgain, we analyze the component structure of the motif adjacency matrix as a pre-processing step. The original network consists of 690 nodes and 1082 edges, and its largest weakly connected component consists of 664 nodes and 1066 edges. Every coherent feedforward loop in the network resides in the largest weakly connected component, so we subsequently consider this sub-network in the following analysis. Of the 664 nodes in the network, only 62 participate in a coherent feedforward loop. Forming the motif adjacency matrix results in nine connected components, of sizes 18, 9, 9, 6, 6, 5, 3, 3, and 3. The operons for the connected components consisting of more than one node is listed in Table S11.\nS55\nS7.2.2 Comparison against existing methods\nWe note that, although the original network is connected, the motif adjacency matrix corresponds to a disconnected graph. This already reveals much of the structure in the network (Figure S12). Indeed, this \u201cshattering\u201d of the graph into components for the feedforward loop has previously been observed in transcriptional regulation networks (70). We additionally used Algorithm 1 to partition the largest connected component of the motif adjacency matrix (consisting of 18 nodes). This revealed the cluster {CLN2, CLN1, SWI4/SWI6, SPT16, HO}, which contains three coherent feedforward loops (Figure S12). All three instances of the motif correspond to the function \u201ccell cycle and mating type switch\u201d. The motifs in this cluster are the only feedforward loops for which the function is described in Reference (9). Using the same procedure on the undirected version of the induced subgraph of the 18 nodes (i.e., using motif Medge) results in the cluster {CLN1, CLN2, SPT16, SWI4/SWI6 }. This cluster breaks the coherent feedforward loop formed by HO, SWI4/SWI6, and SPT16.\nWe also evaluated our method based on the classification of motif functionality (9).3 In total, there are 12 different functionalities and 29 instances of labeled coherent feedforward loops. We considered the motif-based clustering of the graph to be the connected components of the motif adjacency matrix with the additional partition of the largest connected component. To form an edge-based clustering, we used the embedding + k-means algorithm on the undirected graph (i.e., motif Medge) with k = 12 clusters. We also clustered the graph using Infomap and the Louvain method. Table S12 summarizes the results. We see that the motif-based clustering coherently labels all 29 motifs in the sense that the three nodes in every instance of a labeled motif is placed in the same cluster. The edge-based spectral, Infomap, and Louvain clustering coherently labeled 25, 23, and 23 motifs, respectively.\n3The functionalities may be downloaded from our project web page: http://snap.stanford.edu/ higher-order/.\nS56\nWe measured the accuracy of each clustering method as the rand index (69) on the coherently labeled motifs, multiplied by the fraction of coherently labeled motifs. The motif-based clustering had the highest accuracy. We conclude that motif-based clustering provides an advantage over edge-based clustering methods in identifying functionalities of coherent feedforward loops in the the S. cerevisiae transcriptional regulation network.\nS57\nFigure S12: Higher-order organization of the S. cerevisiae transcriptional regulation network. A: The four higher-order structures used by our higher-order clustering method, which can model signed motifs. These are coherent feedfoward loop motifs, which act as sign-sensitive delay elements in transcriptional regulation networks (2). The edge signs refer to activation (positive) or repression (negative). B: Six higher-order clusters revealed by the motifs in (A). Clusters show functional modules consisting of several motifs (coherent feedforward loops), which were previously studied individually (9). The higher-order clustering framework identifies the functional modules with higher accuracy (97%) than existing methods (68\u201382%). C\u2013D: Two higher-order clusters from (B). In these clusters, all edges have positive sign. The functionality of the motifs in the modules correspond to drug resistance (C) or cell cycle and mating type match (D). The clustering suggests that coherent feedforward loops function together as a single processing unit rather than as independent elements.\nS58\nTable S12: Classification of coherent feedforward loop motifs by several clustering methods. In a given motif instance, we say that it is coherently labeled if the nodes comprising the motif are in the same cluster. If a motif is not coherently labeled, a \u201c-1\u201d is listed. The accuracy is the rand index on the labels and motif functionality on coherently labeled motifs, multiplied by the fraction of coherently labeled motifs.\nMotif nodes Function Class label Motif-based Edge-based Infomap Louvain\nGAL11 ALPHA1 MFALPHA1 pheromone response 1 1 -1 -1 GCN4 MET4 MET16 Metionine biosynthesis 2 2 1 -1 GCN4 MET4 MET17 Metionine biosynthesis 2 2 1 -1 GCN4 LEU3 ILV1 Leucine and branched amino acid biosynthesis 2 2 1 1 GCN4 LEU3 ILV2 Leucine and branched amino acid biosynthesis 2 2 1 1 GCN4 LEU3 ILV5 Leucine and branched amino acid biosynthesis 2 2 1 1 GCN4 LEU3 LEU4 Leucine and branched amino acid biosynthesis 2 2 1 1 GLN3 GAT1 GAP1 Nitrogen utilization 3 3 1 2 GLN3 GAT1 DAL80 Nitrogen utilization 3 3 1 2 GLN3 GAT1 DAL80/GZF3 Glutamate synthetase 3 3 1 2 GLN3 GAT1 GLN1 Glutamate synthetase 3 3 1 2 MIG1 HAP2/3/4/5 CYC1 formation of apocytochromes 4 4 -1 -1 MIG1 GAL4 GAL1 Galactokinase 4 -1 -1 -1 PDR1 YRR1 SNQ2 Drug resistance 5 5 2 3 PDR1 YRR1 YOR1 Drug resistance 5 5 2 3 PDR1 PDR3 HXT11 Drug resistance 5 5 2 3 PDR1 PDR3 HXT9 Drug resistance 5 5 2 3 PDR1 PDR3 PDR5 Drug resistance 5 5 2 3 PDR1 PDR3 IPT1 Drug resistance 5 5 2 3 PDR1 PDR3 SNQ2 Drug resistance 5 5 2 3 PDR1 PDR3 YOR1 Drug resistance 5 5 2 3 RIM101 IME1 DIT1 sporulation-specific 6 6 3 4 SPT16 SWI4/SWI6 CLN1 Cell cycle and mating type switch 7 -1 4 5 SPT16 SWI4/SWI6 CLN2 Cell cycle and mating type switch 7 -1 -1 5 SPT16 SWI4/SWI6 HO Cell cycle and mating type switch 7 -1 -1 -1 TUP1 ALPHA1 MFALPHA1 Mating factor alpha 1 1 -1 5 UME6 INO2/INO4 CHO1 Phospholipid biosynthesis 8 6 5 4 UME6 INO2/INO4 CHO2 Phospholipid biosynthesis 8 6 5 4 UME6 INO2/INO4 OPI3 Phospholipid biosynthesis 8 6 5 4\nFrac. coherently labeled 29 / 29 25 / 29 23 / 29 23 / 29 Accuracy 0.97 0.82 0.68 0.76\nS7.3 Motif M6 in the English Wikipedia article network\nThe English Wikipedia network (57\u201359) consists of 4.21 million nodes (representing articles) and 101.31 million edges, where an edge from node i to node j means that there is a hyperlink from the ith article to the jth article. The network data was downloaded from http://law. di.unimi.it/webdata/enwiki-2013/.\nWe used Algorithm 1 to find a motif-based cluster for motif M6 and Medge (the algorithm was run on the largest connected component of the motif adjacency matrix). The clusters are shown in Figure S13. The nodes in the motif-based cluster are cities and barangays (small\nS59\nadministrative divisions) in the Philippines. The cluster has a set of nodes with many outgoing links that form the source node in motif M6. In total, the cluster consists of 22 nodes and 338 edges. The linking pattern appears anomalous and suggests that perhaps the pages uplinking should receive reciprocated links. On the other hand, the edge-based cluster is much larger cluster and does not have too much structure. The cluster consists of several high-degree nodes and their neighbors.\nS60\nFigure S13: Clusters from the English Wikipedia hyperlink network (57\u201359). A\u2013C: Motifbased cluster (A) for motif M6 (B). The cluster consists of cities and small administrative divisions in the Philippines. The green nodes have many bi-direction links with each other and many incoming links from orange nodes at the bottom of the figure. The spy plot illustrates this network structure (C). D\u2013F: Cluster (D) for undirected edges (E). The cluster has a few very high-degree nodes, as evidenced by the spy plot (F).\nS61\nS7.4 Motif M6 in the Twitter follower network\nWe also analyzed the complete 2010 Twitter follower graph (58, 59, 71). The graph consists 41.65 million nodes (users) and 1.47 billion edges, where an edge from node i to node j signifies that user i is followed by user j on the social network. The network data was downloaded from http://law.di.unimi.it/webdata/twitter-2010/.\nWe used Algorithm 1 to find a motif-based cluster for motif M6 (the algorithm was run on the largest connected component of the motif adjacency matrix). The cluster contains 151 nodes and consists of two disconnected components. Here, we consider the smaller of the two components, which consists of 38 nodes. We also found an edge-based cluster on the undirected graph (using Algorithm 1 with motif Medge). This cluster consists of 44 nodes.\nFigure S14 illustrates the motif-based and edge-based clusters. Both clusters capture anomalies in the graph. The motif-based cluster consists of holding accounts for a photography company. The nodes that form bi-directional links have completed profiles (contain a profile picture) while several nodes with incomplete profiles (without a profile picture) are followed by the completed accounts. The edge-based cluster is a near clique, where the user screen names all begin with \u201cLC \u201d. We suspect that the similar usernames are either true social communities, holding accounts, or bots. (For the most part, their tweets are protected, so we could not verify if any of these scenarios are true). Interestingly, both M6 and Medge find anomalous clusters. However, their structures are quite different. We conclude that M6 can lead to the detection of new anomalous clusters in social networks.\nS62\nFigure S14: Clusters in the 2010 Twitter follower network (58, 59, 71). A\u2013C: Motif-based cluster (A) for motif M6 (B). All accounts are holding accounts for a photography company. The green nodes correspond to accounts that have completed profiles, while the orange accounts have incomplete profiles. The spy plot illustrates how the cluster is formed around this motif (C). D\u2013F: Cluster (D) for edge-based clustering (E). The cluster consists of a near-clique (F) where all users have the prefix \u201cLC \u201d.\nS63\nS7.5 Motif M7 in the Stanford web graph\nThe Stanford web graph (7, 56) consists of 281,903 nodes and 2,312,497 edges, where an edge from node i to node j means that there is a hyperlink from the ith web page to the jth web page. Here, all of the web pages come from the Stanford domain. The network data was downloaded from http://snap.stanford.edu/data/web-Stanford.html.\nWe used Algorithm 1 to find a motif-based cluster for motif M7, a motif that is overexpressed in web graphs (1). An illustration of the cluster and an edge-based cluster (i.e., using Algorithm 1 with Medge) are in Figure S15. Interestingly, both clusters exhibits a coreperiphery structure, albeit markedly different ones. The motif-based cluster contains several core nodes with large in-degree. Such core nodes comprise the sink node in motif M7. On the periphery are several clusters within which are many bi-directional links (as illustrated by the spy plot in Figure S15). The nodes in these clusters then up-link to the core nodes. This type of organizational unit suggests an explanation for why motif M7 is over-expressed: clusters of similar pages tend to uplink to more central pages. The edge-based cluster also has a few nodes with large in-degree, serving as a small core. On the periphery are the neighbors of these nodes, which themselves tend not to be connected (as illustrated by the spy plot).\nS64\nFigure S15: Clusters in the Stanford web graph (7). A\u2013C: Motif-based cluster (A) for motif M7 (B). The cluster has a core group of nodes with many incoming links (serving as the sink node in M7; shown in orange) and several periphery groups that are tied together (the bi-directional link in M7; shown in green) and also up-link to the core. This is evident from the spy plot (C). D\u2013F: Cluster (C) for undirected edges (B). The cluster contains a few high-degree nodes and their neighbors, and the neighbors tend to not be connected, as illustrated by the splot (F).\nS65\nS7.6 Semi-cliques in collaboration networks\nWe used Algorithm 1 to identify clusters of a four-node motif (the semi-clique) that has been studied in conjunction with researcher productivity in collaboration networks (72) (see Figure S16). We found a motif-based cluster in two different collaboration networks. Each one is derived from co-authorship in papers submitted to the arXiv under a certain category; here, we analyze the \u201dHigh Energy Physics\u2013Theory\u201d (HepTh) and \u201dCondensed Matter Physics\u201d (CondMat) categories (56,73). The HepTh network has 23,133 nodes and 93,497 edges and the CondMat network has 9,877 nodes and 25,998 edges. The HepTh network data was downloaded from http://snap.stanford.edu/data/ca-HepTh.html and the CondMat network data was downloaded from http://snap.stanford.edu/data/ca-CondMat. html.\nFigure S16 shows the two clusters for each of the collaboration networks. In both networks, the motif-based cluster consists of a core group of nodes and similarly-sized groups on the periphery. The core group of nodes correspond to the nodes of degree 3 in the motif and the periphery group nodes correspond to the nodes of degree 2. One explanation for this organization is that there is a small small group of authors that writes papers with different research groups. Alternatively, the co-authorship could come from a single research group, where senior authors are included on all of the papers and junior authors on a subset of the papers.\nOn the other hand, the edge-based clusters (i.e., result of Algorithm 1 for Medge) are a clique in the HepTh netowork and a clique with a few dangling nodes in the CondMat network. The dense clusters are quite different from the sparser clusters based on the semi-clique. Such dense clusters are not that surprising. For example, a clique could arise from a single paper published by a group of authors.\nS66\nFigure S16: Clusters in co-authorship networks (73). A\u2013E: Best motif-based cluster for the semi-clique motif (E) in the High Energy Physics\u2013Theory collaboration network (A) and the Condensed Matter Physics collaboration network (C). Corresponding spy plots are shown in (B) and (D). F\u2013I: Best edge-based (I) cluster in the High Energy Physics\u2013Theory collaboration network (F) and the Condensed Matter Physics collaboration network (H). Corresponding spy plots are shown in (G) and (I).\nS67"
        },
        {
            "heading": "S8 Data availability",
            "text": "All data is available at our project web site at http://snap.stanford.edu/higher-order/. The web site includes links to datasets used for experiments throughout the supplementary material (7, 56, 58\u201360, 74\u201383).\nS68\nReferences and Notes\n1. R. Milo, et al., Science 298, 824 (2002).\n2. S. Mangan, A. Zaslaver, U. Alon, Journal of molecular biology 334, 197 (2003).\n3. J. Yang, J. Leskovec, Proceedings of the IEEE 102, 1892 (2014).\n4. P. W. Holland, S. Leinhardt, American Journal of Sociology pp. 492\u2013513 (1970).\n5. M. Rosvall, A. V. Esquivel, A. Lancichinetti, J. D. West, R. Lambiotte, Nature communi-\ncations 5 (2014).\n6. N. Prz\u030culj, D. G. Corneil, I. Jurisica, Bioinformatics 20, 3508 (2004).\n7. J. Leskovec, K. J. Lang, A. Dasgupta, M. W. Mahoney, Internet Mathematics 6, 29 (2009).\n8. O\u0308. N. Yaverog\u0306lu, et al., Scientific reports 4 (2014).\n9. S. Mangan, U. Alon, Proceedings of the National Academy of Sciences 100, 11980 (2003).\n10. C. J. Honey, R. Ko\u0308tter, M. Breakspear, O. Sporns, Proceedings of the National Academy of\nSciences 104, 10240 (2007).\n11. S. E. Schaeffer, Computer Science Review 1, 27 (2007).\n12. Minimizing \u03c6M(S) is NP-hard, which follows from the NP-hardness of the traditional def-\ninition of conductance (68).\n13. See the Supplementary Material.\n14. Formally, when the motif has three nodes, the selected cluster S satisfies \u03c6M(S) \u2264 4 \u221a \u03c6\u2217M \u2264 1, where \u03c6\u2217M is the smallest motif conductance of any possible node set S.\nThis inequality is proved in the Supplementary Material.\nS69\n15. The normalized motif Laplacian matrix is LM = D\u22121/2(D\u2212WM)D\u22121/2, where D is a di-\nagonal matrix with the row-sums of WM on the diagonal (Dii = \u2211 j(WM)ij), and D \u22121/2 is\nthe same matrix with the inverse square-roots on the diagonal (D\u22121/2ii = 1/ \u221a\u2211 j(WM)ij). The spectral ordering \u03c3 is the by-value ordering of D\u22121/2z, where z is the eigenvector corresponding to the second smallest eigenvalue of LM , i.e., \u03c3i is the index of D\u22121/2z with the ith smallest value.\n16. C. Seshadhri, A. Pinar, T. G. Kolda, Statistical Analysis and Data Mining: The ASA Data\nScience Journal 7, 294 (2014).\n17. R. Andersen, F. Chung, K. Lang, Proceedings of the 47th Annual IEEE Symposium on\nFoundations of Computer Science (2006), pp. 475\u2013486.\n18. J. J. Whang, I. S. Dhillon, D. F. Gleich, SIAM Data Mining (2015).\n19. A. Y. Ng, M. I. Jordan, Y. Weiss, Advances in Neural Information Processing Systems 14\n(2002), pp. 849\u2013856.\n20. D. Boley, Data Mining and Knowledge Discovery 2, 325 (1998).\n21. D. L. Riddle, T. Blumenthal, B. J. Meyer, et al., eds., C. elegans II (Cold Spring Harbor\nLaboratory Press, 1997), second edn.\n22. H. Lee, et al., Nature neuroscience 15, 107 (2012).\n23. B. J. Frey, D. Dueck, Science 315, 972 (2007).\n24. B. Serrour, A. Arenas, S. Go\u0301mez, Computer Communications 34, 629 (2011).\n25. T. Michoel, A. Joshi, B. Nachtergaele, Y. Van de Peer, Molecular BioSystems 7, 2769\n(2011).\nS70\n26. A. R. Benson, D. F. Gleich, J. Leskovec, SIAM Data Mining (2015).\n27. F. Krzakala, et al., Proceedings of the National Academy of Sciences 110, 20935 (2013).\n28. M. Kaiser, C. C. Hilgetag, PLoS Computational Biology 2, e95 (2006).\n29. U. Alon, Nature Reviews Genetics 8, 450 (2007).\n30. O. Sporns, R. Ko\u0308tter, PLoS Biology 2, e369 (2004).\n31. A. Inokuchi, T. Washio, H. Motoda, Principles of Data Mining and Knowledge Discovery\n(Springer, 2000), pp. 13\u201323.\n32. F. R. Chung, Proceedings of ICCM (Citeseer, 2007), vol. 2, p. 378.\n33. J. R. Lee, S. O. Gharan, L. Trevisan, Journal of the ACM 61, 37 (2014).\n34. F. Chung, Annals of Combinatorics 9, 1 (2005).\n35. D. Boley, G. Ranjan, Z.-L. Zhang, Linear Algebra and its Applications 435, 224 (2011).\n36. F. D. Malliaros, M. Vazirgiannis, Physics Reports 533, 95 (2013).\n37. G. Karypis, R. Aggarwal, V. Kumar, S. Shekhar, Very Large Scale Integration (VLSI) Sys-\ntems, IEEE Transactions on 7, 69 (1999).\n38. S. Agarwal, K. Branson, S. Belongie, Proceedings of the 23rd International Conference on\nMachine Learning (ACM, 2006), pp. 17\u201324.\n39. D. Zhou, J. Huang, B. Scho\u0308lkopf, Advances in Neural Information Processing Systems 19\n(MIT Press, 2006), pp. 1601\u20131608.\n40. J. Rodr\u0131\u0301guez, Linear and Multilinear Algebra 50, 1 (2002).\nS71\n41. L. Trevisan, Lecture notes on expansion, sparsest cut, and spectral graph theory, http://\nwww.eecs.berkeley.edu/\u02dcluca/books/expanders.pdf. Accessed June 28, 2015.\n42. S. Demeyer, et al., PloS ONE 8, e61183 (2013).\n43. M. Houbraken, et al., PLoS ONE 9, e97896 (2014).\n44. S. Wernicke, IEEE/ACM Transactions on Computational Biology and Bioinformatics 3,\n347 (2006).\n45. S. Wernicke, F. Rasche, Bioinformatics 22, 1152 (2006).\n46. C. R. Aberger, A. No\u0308tzli, K. Olukotun, C. Re\u0301, arXiv preprint arXiv:1503.02368 (2015).\n47. M. Latapy, Theoretical Computer Science 407, 458 (2008).\n48. J. W. Berry, et al., Proceedings of the 5th Conference on Innovations in Theoretical Com-\nputer Science (ACM, New York, NY, USA, 2014), pp. 225\u2013234.\n49. D. Marcus, Y. Shavitt, IEEE 30th International Conference on Distributed Computing Sys-\ntems Workshops (2010), pp. 92\u201398.\n50. N. Chiba, T. Nishizeki, SIAM Journal on Computing 14, 210 (1985).\n51. T. Schank, D. Wagner, Experimental and Efficient Algorithms (Springer, 2005), pp. 606\u2013\n609.\n52. L. Becchetti, P. Boldi, C. Castillo, A. Gionis, Proceedings of the 14th ACM SIGKDD inter-\nnational conference on Knowledge discovery and data mining (ACM, 2008), pp. 16\u201324.\n53. J. Cohen, Computing in Science & Engineering 11, 29 (2009).\nS72\n54. B. N. Parlett, The Symmetric Eigenvalue Problem, vol. 7 (SIAM, 1980).\n55. K. J. Maschhoff, D. C. Sorensen, Applied Parallel Computing Industrial Computation and\nOptimization (Springer, 1996), pp. 478\u2013486.\n56. J. Leskovec, A. Krevl, SNAP Datasets: Stanford large network dataset collection, http:\n//snap.stanford.edu/data (2014).\n57. P. Boldi, B. Codenotti, M. Santini, S. Vigna, Software: Practice and Experience 34, 711\n(2004).\n58. P. Boldi, S. Vigna, Proceedings of the 13th International Conference on World Wide Web\n(ACM, 2004), pp. 595\u2013602.\n59. P. Boldi, M. Rosa, M. Santini, S. Vigna, Proceedings of the 20th International Conference\non World Wide Web (ACM, 2011), pp. 587\u2013596.\n60. P. Boldi, A. Marino, M. Santini, S. Vigna, Proceedings of the companion publication of the\n23rd international conference on World wide web companion (International World Wide Web Conferences Steering Committee, 2014), pp. 227\u2013228.\n61. A. Azad, A. Buluc\u0327, J. R. Gilbert, Proceedings of the IPDPSW, Workshop on Graph Algo-\nrithm Building Blocks (GABB) (2015), pp. 804\u2013811.\n62. M. Rosvall, C. T. Bergstrom, Proceedings of the National Academy of Sciences 105, 1118\n(2008).\n63. V. D. Blondel, J.-L. Guillaume, R. Lambiotte, E. Lefebvre, Journal of statistical mechanics:\ntheory and experiment 2008, P10008 (2008).\nS73\n64. R. E. Ulanowicz, C. Bondavalli, M. S. Egnotovich, Trophic Dynamics in South Florida\nEcosystem, FY 97: The Florida Bay Ecosystem, Tech. Rep. CBL 98-123, Chesapeake Biological Laboratory, Solomons, MD (1998).\n65. J. Bascompte, C. J. Melia\u0301n, E. Sala, Proceedings of the National Academy of Sciences of\nthe United States of America 102, 5443 (2005).\n66. J. Bascompte, et al., Science 325, 416 (2009).\n67. D. B. Stouffer, J. Camacho, W. Jiang, L. A. N. Amaral, Proceedings of the Royal Society of\nLondon B: Biological Sciences 274, 1931 (2007).\n68. D. Wagner, F. Wagner, Proceedings of the 18th International Symposium on Mathematical\nFoundations of Computer Science (1993), pp. 744\u2013750.\n69. C. D. Manning, P. Raghavan, H. Schu\u0308tze, et al., Introduction to Information Retrieval,\nvol. 1 (Cambridge university press Cambridge, 2008).\n70. R. Dobrin, Q. K. Beg, A.-L. Baraba\u0301si, Z. N. Oltvai, BMC bioinformatics 5, 10 (2004).\n71. H. Kwak, C. Lee, H. Park, S. Moon, Proceedings of the 19th International Conference on\nWorld Wide Web (ACM, 2010), pp. 591\u2013600.\n72. T. Chakraborty, N. Ganguly, A. Mukherjee, Advances in Social Networks Analysis and\nMining (ASONAM), 2014 IEEE/ACM International Conference on (IEEE, 2014), pp. 130\u2013 137.\n73. J. Leskovec, J. Kleinberg, C. Faloutsos, ACM Transactions on Knowledge Discovery from\nData (TKDD) 1, 2 (2007).\nS74\n74. R. West, H. S. Paskov, J. Leskovec, C. Potts, Transactions of the Association for Computa-\ntional Linguistics 2, 297 (2014).\n75. J. Leskovec, J. Kleinberg, C. Faloutsos, Proceedings of the eleventh ACM SIGKDD inter-\nnational conference on Knowledge discovery in data mining (ACM, 2005), pp. 177\u2013187.\n76. J. Gehrke, P. Ginsparg, J. Kleinberg, ACM SIGKDD Explorations Newsletter 5, 149 (2003).\n77. R. Albert, H. Jeong, A.-L. Baraba\u0301si, Nature 401, 130 (1999).\n78. J. Leskovec, L. A. Adamic, B. A. Huberman, ACM Transactions on the Web (TWEB) 1, 5\n(2007).\n79. J. Leskovec, D. P. Huttenlocher, J. M. Kleinberg, ICWSM (2010).\n80. J. Leskovec, J. J. Mcauley, Advances in neural information processing systems (2012), pp.\n539\u2013547.\n81. L. Takac, M. Zabovsky, International Scientific Conference and International Workshop\nPresent Day Trends of Innovations (2012), pp. 1\u20136.\n82. L. Backstrom, D. Huttenlocher, J. Kleinberg, X. Lan, Proceedings of the 12th ACM\nSIGKDD international conference on Knowledge discovery and data mining (ACM, 2006), pp. 44\u201354.\n83. J. Yang, J. Leskovec, 2012 IEEE 12th International Conference on Data Mining (IEEE,\n2012), pp. 745\u2013754.\nS75\nAuthors would like to thank Rok Sosic\u030c for insightful comments. ARB acknowledges the support of a Stanford Graduate Fellowship. DFG acknowledges the support of NSF CCF1149756 and IIS-1422918 and DARPA SIMPLEX. JL acknowledges the support of NSF IIS1149837 and CNS-1010921, NIH BD2K, DARPA XDATA and SIMPLEX, Boeing, Lightspeed, and Volkswagen.\nS76"
        }
    ],
    "title": "Higher-order organization of complex networks",
    "year": 2016
}