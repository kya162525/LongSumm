{
    "abstractText": "We present a new family of subgradient methods that dynamica lly incorporate knowledge of the geometry of the data observed in earlier iterations to perfo rm more informative gradient-based learning. Metaphorically, the adaptation allows us to find n eedles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems fro m recent advances in stochastic optimization and online learning which employ proximal funct ions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adap tively modifying the proximal function, which significantly simplifies setting a learning rate nd results in regret guarantees that are provably as good as the best proximal function that can be cho sen in hindsight. We give several efficient algorithms for empirical risk minimization probl ems with common and important regularization functions and domain constraints. We experimen tally study our theoretical analysis and show that adaptive subgradient methods outperform state-o f-the-art, yet non-adaptive, subgradient algorithms.",
    "authors": [
        {
            "affiliations": [],
            "name": "John Duchi"
        },
        {
            "affiliations": [],
            "name": "Elad Hazan"
        },
        {
            "affiliations": [],
            "name": "Tong Zhang"
        },
        {
            "affiliations": [],
            "name": "Yoram Singer"
        },
        {
            "affiliations": [],
            "name": "DUCHI"
        },
        {
            "affiliations": [],
            "name": "HAZAN"
        }
    ],
    "id": "SP:413c1142de9d91804d6d11c67ff3fed59c9fc279",
    "references": [
        {
            "authors": [
                "T. Ando"
            ],
            "title": "Concavity of certain maps on positive definite matrices and applicatio",
            "year": 2008
        },
        {
            "authors": [
                "N. Cesa-Bianchi",
                "A. Conconi",
                "C. Gentile"
            ],
            "title": "A second-order perce",
            "year": 2004
        }
    ],
    "sections": [
        {
            "text": "Keywords: subgradient methods, adaptivity, online learning, stochastic convex optimization"
        },
        {
            "heading": "1. Introduction",
            "text": "In many applications of online and stochastic learning, the input instances are of very high dimension, yet within any particular instance only a few features are non-zer . It is often the case, however, that infrequently occurring features are highly informative and discriminative. The informativeness of rare features has led practitioners to craft domain-specific f ature weightings, such as TF-IDF (Salton and Buckley, 1988), which pre-emphasize infrequentlyoccurring features. We use this old idea as a motivation for applying modern learning-theoretic techniquesto the problem of online and stochastic learning, focusing concretely on (sub)gradient methods.\n\u2217. A preliminary version of this work was published in COLT 2010.\nc\u00a92011 John Duchi, Elad Hazan and Yoram Singer.\nStandard stochastic subgradient methods largely follow a predetermined procedu al scheme that is oblivious to the characteristics of the data being observed. In contrast,our algorithms dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Informally, our procedures give frequently occurring features very low learning rates and infrequent features high learning rates, where t e intuition is that each time an infrequent feature is seen, the learner should \u201ctake notice.\u201d Thus, the adaptation facilitates finding and identifying very predictive but comparatively rare features."
        },
        {
            "heading": "1.1 The Adaptive Gradient Algorithm",
            "text": "Before introducing our adaptive gradient algorithm, which we term ADAGRAD, we establish notation. Vectors and scalars are lower case italic letters, such asx \u2208 X . We denote a sequence of vectors by subscripts, that is,xt ,xt+1, . . ., and entries of each vector by an additional subscript, for example,xt, j . The subdifferential set of a functionf evaluated atx is denoted\u2202 f (x), and a particular vector in the subdifferential set is denoted byf \u2032(x) \u2208 \u2202 f (x) or gt \u2208 \u2202 ft(xt). When a function is differentiable, we write\u2207 f (x). We use\u3008x,y\u3009 to denote the inner product betweenx andy. The Bregman divergence associated with a strongly convex and differentiable function\u03c8 is\nB\u03c8(x,y) = \u03c8(x)\u2212\u03c8(y)\u2212\u3008\u2207\u03c8(y),x\u2212y\u3009 .\nWe also make frequent use of the following two matrices. Letg1:t = [g1 \u00b7 \u00b7 \u00b7 gt ] denote the matrix obtained by concatenating the subgradient sequence. We denote theith row of this matrix, which amounts to the concatenation of theith component of each subgradient we observe, byg1:t,i . We also define the outer product matrixGt = \u2211t\u03c4=1g\u03c4g\u03c4\u22a4.\nOnline learning and stochastic optimization are closely related and basically interchangeable (Cesa-Bianchi et al., 2004). In order to keep our presentation simple, we confine our discussion and algorithmic descriptions to the online setting with the regret bound model. In onlinelearning, the learner repeatedly predicts a pointxt \u2208 X \u2286 Rd, which often represents a weight vector assigning importance values to various features. The learner\u2019s goal is to achieve low r gret with respect to a static predictorx\u2217 in the (closed) convex setX \u2286 Rd (possiblyX = Rd) on a sequence of functions ft(x), measured as\nR(T) = T\n\u2211 t=1 ft(xt)\u2212 inf x\u2208X\nT\n\u2211 t=1 ft(x) .\nAt every timestept, the learner receives the (sub)gradient informationgt \u2208 \u2202 ft(xt). Standard subgradient algorithms then move the predictorxt in the opposite direction ofgt while maintaining xt+1 \u2208 X via the projected gradient update (e.g., Zinkevich, 2003)\nxt+1 = \u03a0X (xt \u2212\u03b7gt) = argmin x\u2208X \u2016x\u2212 (xt \u2212\u03b7gt)\u201622 .\nIn contrast, let the Mahalanobis norm\u2016\u00b7\u2016A = \u221a \u3008\u00b7,A\u00b7\u3009 and denote the projection of a pointy ontoX according toA by \u03a0A\nX (y) = argminx\u2208X \u2016x\u2212y\u2016A = argminx\u2208X \u3008x\u2212y,A(x\u2212y)\u3009. Using this notation,\nour generalization of standard gradient descent employs the update\nxt+1 = \u03a0 G1/2t X ( xt \u2212\u03b7G\u22121/2t gt ) .\nThe above algorithm is computationally impractical in high dimensions since it requires computation of the root of the matrixGt , the outer product matrix. Thus we specialize the update to\nxt+1 = \u03a0 diag(Gt)1/2\nX\n( xt \u2212\u03b7diag(Gt)\u22121/2gt ) . (1)\nBoth the inverse and root of diag(Gt) can be computed in linear time. Moreover, as we discuss later, when the gradient vectors are sparse the update above can often be performed in time proportional to the support of the gradient. We now elaborate and give a more formal discuss on of our setting.\nIn this paper we consider several different online learning algorithms and their stochastic convex optimization counterparts. Formally, we consider online learning with a sequence of composite functions\u03c6t . Each function is of the form\u03c6t(x) = ft(x)+\u03d5(x) where ft and\u03d5 are (closed) convex functions. In the learning settings we study,ft is either an instantaneous loss or a stochastic estimate of the objective function in an optimization task. The function\u03d5 serves as a fixed regularization function and is typically used to control the complexity ofx. At each round the algorithm makes a predictionxt \u2208 X and then receives the functionft . We define the regret with respect to the fixed (optimal) predictorx\u2217 as\nR\u03c6(T), T\n\u2211 t=1\n[\u03c6t(xt)\u2212\u03c6t(x\u2217)] = T\n\u2211 t=1\n[ ft(xt)+\u03d5(xt)\u2212 ft(x\u2217)\u2212\u03d5(x\u2217)] . (2)\nOur goal is to devise algorithms which are guaranteed to suffer asymptoticallysub-linear regret, namely,R\u03c6(T) = o(T).\nOur analysis applies to related, yet different, methods for minimizing the regret (2). The first is Nesterov\u2019s primal-dual subgradient method (2009), and in particular Xi o\u2019s (2010) extension, regularized dual averaging, and the follow-the-regularized-leader (FTRL) family of algorithms (see for instance Kalai and Vempala, 2003; Hazan et al., 2006). In the primal-dual subgradient method the algorithm makes a predictionxt on roundt using the average gradient \u00afgt = 1t \u2211 t \u03c4=1g\u03c4. The update encompasses a trade-off between a gradient-dependent linear term, theegularizer\u03d5, and a stronglyconvex term\u03c8t for well-conditioned predictions. Here\u03c8t is theproximalterm. The update amounts to solving\nxt+1 = argmin x\u2208X\n{\n\u03b7\u3008g\u0304t ,x\u3009+\u03b7\u03d5(x)+ 1 t \u03c8t(x) } , (3)\nwhere\u03b7 is a fixed step-size andx1 = argminx\u2208X \u03d5(x). The second method similarly has numerous names, including proximal gradient, forward-backward splitting, andcomposite mirror descent (Tseng, 2008; Duchi et al., 2010). We use the term composite mirror descent. The composite mirror descent method employs a more immediate trade-off between the current gradientgt , \u03d5, and staying close toxt using the proximal function\u03c8,\nxt+1 = argmin x\u2208X\n{ \u03b7\u3008gt ,x\u3009+\u03b7\u03d5(x)+B\u03c8t (x,xt) } . (4)\nOur work focuses on temporal adaptation of the proximal function in a data driven way, while previous work simply sets\u03c8t \u2261 \u03c8, \u03c8t(\u00b7) = \u221a t\u03c8(\u00b7), or \u03c8t(\u00b7) = t\u03c8(\u00b7) for some fixed\u03c8.\nWe provide formal analyses equally applicable to the above two updates andshow how to automatically choose the function\u03c8t so as to achieve asymptotically small regret. We describe and analyze two algorithms. Both algorithms use squared Mahalanobis norms as their proximal functions, setting\u03c8t(x) = \u3008x,Htx\u3009 for a symmetric matrixHt 0. The first uses diagonal matrices while\nthe second constructs full dimensional matrices. Concretely, for some smallfixed \u03b4 \u2265 0 (specified later, though in practice\u03b4 can be set to 0) we set\nHt = \u03b4I +diag(Gt)1/2 (Diagonal) and Ht = \u03b4I +G 1/2 t (Full) . (5)\nPlugging the appropriate matrix from the above equation into\u03c8t in (3) or (4) gives rise to our ADAGRAD family of algorithms. Informally, we obtain algorithms which are similar to secondorder gradient descent by constructing approximations to the Hessian ofthe functionsft , though we use roots of the matrices."
        },
        {
            "heading": "1.2 Outline of Results",
            "text": "We now outline our results, deferring formal statements of the theorems to latersections. Recall the definitions ofg1:t as the matrix of concatenated subgradients andGt as the outer product matrix in the prequel. The ADAGRAD algorithm with full matrix divergences entertains bounds of the form\nR\u03c6(T) = O ( \u2016x\u2217\u20162 tr(G 1/2 T ) ) and R\u03c6(T) = O\n(\nmax t\u2264T\n\u2016xt \u2212x\u2217\u20162 tr(G 1/2 T )\n)\n.\nWe further show that\ntr ( G1/2T ) = d1/2\n\u221a \u221a \u221a \u221ainf S { T\n\u2211 t=1\n\u3008gt ,S\u22121gt\u3009 : S 0, tr(S)\u2264 d } .\nThese results are formally given in Theorem 7 and its corollaries. When our pr ximal function \u03c8t(x) = \u2329 x,diag(Gt)1/2x \u232a\nwe have bounds attainable in time at most linear in the dimensiond f our problems of the form\nR\u03c6(T) = O\n(\n\u2016x\u2217\u2016\u221e d\n\u2211 i=1 \u2016g1:T,i\u20162\n)\nand R\u03c6(T) = O\n(\nmax t\u2264T\n\u2016xt \u2212x\u2217\u2016\u221e d\n\u2211 i=1 \u2016g1:T,i\u20162\n)\n.\nSimilar to the above, we will show that\nd\n\u2211 i=1\n\u2016g1:T,i\u20162 = d1/2 \u221a \u221a \u221a\n\u221ainf s\n{\nT\n\u2211 t=1\n\u3008gt ,diag(s)\u22121gt\u3009 : s 0,\u30081,s\u3009 \u2264 d } .\nWe formally state the above two regret bounds in Theorem 5 and its corollaries. Following are a simple example and corollary to Theorem 5 to illustrate one regime inwhich we expect substantial improvements (see also the next subsection). Let\u03d5 \u2261 0 and consider Zinkevich\u2019s online gradient descent algorithm. Given a compact convex setX \u2286 Rd and sequence of convex functionsft , Zinkevich\u2019s algorithm makes the sequence of predictionsx1, . . . ,xT with xt+1 = \u03a0X (xt \u2212 (\u03b7/ \u221a t)gt). If the diameter ofX is bounded, thus supx,y\u2208X \u2016x\u2212y\u20162 \u2264 D2, then online gradient descent, with the optimal choice inhi dsightfor the stepsize\u03b7 (see the bound (7) in Section 1.4), achieves a regret bound of\nT\n\u2211 t=1 ft(xt)\u2212 inf x\u2208X\nT\n\u2211 t=1\nft(x)\u2264 \u221a 2D2\n\u221a\nT\n\u2211 t=1 \u2016gt\u201622 . (6)\nWhenX is bounded via supx,y\u2208X \u2016x\u2212y\u2016\u221e \u2264 D\u221e, the following corollary is a simple consequence of our Theorem 5.\nCorollary 1 Let the sequence{xt} \u2282 Rd be generated by the update (4) and assume that maxt \u2016x\u2217\u2212xt\u2016\u221e \u2264 D\u221e. Using stepsize\u03b7 = D\u221e/ \u221a 2, for any x\u2217, the following bound holds.\nR\u03c6(T)\u2264 \u221a 2dD\u221e\n\u221a\ninf s 0,\u30081,s\u3009\u2264d\nT\n\u2211 t=1\n\u2016gt\u20162diag(s)\u22121 = \u221a 2D\u221e d\n\u2211 i=1 \u2016g1:T,i\u20162 .\nThe important feature of the bound above is the infimum under the square root, which allows us to perform better than simply using the identity matrix, and the fact that the stepsizeis easy to set a priori. For example, if the setX = {x : \u2016x\u2016\u221e \u2264 1}, thenD2 = 2 \u221a d while D\u221e = 2, which suggests that if we are learning a dense predictor over a box, the adaptive method should perform well. Indeed, in this case we are guaranteed that the bound in Corollary 1 is better than (6)s the identity matrix belongs to the set over which we take the infimum.\nTo conclude the outline of results, we would like to point to two relevant reseach papers. First, Zinkevich\u2019s regret bound is tight and cannot be improved in a minimax sense(Ab rnethy et al., 2008). Therefore, improving the regret bound requires further reasonable assumptions on the input space. Second, in a independent work, performed concurrently to theresearch presented in this paper, McMahan and Streeter (2010) studycompetitive ratios, showing guaranteed improvements of the above bounds relative to families of online algorithms."
        },
        {
            "heading": "1.3 Improvements and Motivating Example",
            "text": "As mentioned in the prequel, we expect our adaptive methods to outperformstandard online learning methods when the gradient vectors are sparse. We give empirical evidence supporting the improved performance of the adaptive methods in Section 6. Here we give a few abstract examples that show that for sparse data (input sequences wheregt has many zeros) the adaptive methods herein have better performance than non-adaptive methods. In our examples we use the hinge loss, that is,\nft(x) = [1\u2212yt \u3008zt ,x\u3009]+ ,\nwhereyt is the label of examplet andzt \u2208 Rd is the data vector. For our first example, which was also given by McMahan and Streeter (2010), consider the following sparse random data scenario, where the vectorszt \u2208 {\u22121,0,1}d. Assume that at in each roundt, featurei appears with probabilitypi = min{1,ci\u2212\u03b1} for some\u03b1 \u2208 (1,\u221e) and a dimensionindependent constantc. Then taking the expectation of the gradient terms in the bound in Corollary 1, we have\nE\nd\n\u2211 i=1\n\u2016g1:T,i\u20162 = d\n\u2211 i=1 E\n[\n\u221a |{t : |gt,i |= 1}| ] \u2264 d\n\u2211 i=1\n\u221a E|{t : |gt,i |= 1}|= d\n\u2211 i=1\n\u221a\npiT\nby Jensen\u2019s inequality. In the rightmost sum, we havec\u2211di=1 i\u2212\u03b1/2 = O(logd) for \u03b1 \u2265 2, and \u2211di=1 i\u2212\u03b1/2 =O(d1\u2212\u03b1/2) for \u03b1 \u2208 (1,2). If the domainX is a hypercube, sayX = {x : \u2016x\u2016\u221e \u2264 1}, then in Corollary 1D\u221e = 2, and the regret of ADAGRAD is O(max{logd,d1\u2212\u03b1/2} \u221a T). For contrast, the\nstandard regret bound (6) for online gradient descent hasD2 = 2 \u221a\nd and\u2016gt\u201622 \u2265 1, yielding best case regretO( \u221a dT). So we see that in this sparse yet heavy tailed feature setting, ADAGRAD\u2019s regret guarantee can be exponentially smaller in the dimensiond than the non-adaptive regret bound. Our remaining examples construct a sparse sequence for which there is ap rfect predictor that the adaptive methods learn afterd iterations, while standard online gradient descent (Zinkevich,\n2003) suffers significantly higher loss. We assume the domainX s compact, so that for online gradient descent we set\u03b7t = \u03b7/ \u221a t, which gives the optimalO( \u221a T) regret (the setting of\u03b7 does not matter to the adversary we construct)."
        },
        {
            "heading": "1.3.1 DIAGONAL ADAPTATION",
            "text": "Consider the diagonal version of our proposed update (4) withX = {x : \u2016x\u2016\u221e \u2264 1}. Evidently, we can takeD\u221e = 2, and this choice simply results in the updatext+1 = xt \u2212 \u221a 2diag(Gt)\u22121/2gt followed by projection (1) ontoX for ADAGRAD (we use a pseudo-inverse if the inverse does not exist). Letei denote theith unit basis vector, and assume that for eacht, zt = \u00b1ei for somei. Also let yt = sign(\u30081,zt\u3009) so that there exists a perfect classifierx\u2217 = 1\u2208 X \u2282 Rd. We initializex1 to be the zero vector. Fix some\u03b5 > 0, and on rounds roundst = 1, . . . ,\u03b72/\u03b52, setzt = e1. After these rounds, simply choosezt =\u00b1ei for index i \u2208 {2, . . . ,d} chosen at random. It is clear that the update to parameterxi at these iterations is different, and amounts to\nxt+1 = xt +ei ADAGRAD xt+1 =\n[\nxt + \u03b7\u221a t\n]\n[\u22121,1]d (Gradient Descent).\n(Here[\u00b7][\u22121,1]d denotes the truncation of the vector to[\u22121,1]d). In particular, after sufferingd\u22121 more losses, ADAGRAD has a perfect classifier. However, on the remaining iterations gradient descent has\u03b7/ \u221a t \u2264 \u03b5 and thus evidently suffers loss at leastd/(2\u03b5). Of course, for small\u03b5, we haved/(2\u03b5)\u226b d. In short, ADAGRAD achieves constant regret per dimension while online gradient descent can suffer arbitrary loss (for unboundedt). It seems quite silly, then, to use a global learning rate rather than one for each feature. Full Matrix Adaptation. We use a similar construction to the diagonal case to show a situation in which the full matrix update from (5) gives substantially lower regret thanstochastic gradient descent. For full divergences we setX = {x : \u2016x\u20162 \u2264 \u221a d}. Let V = [v1 . . . vd] \u2208 Rd\u00d7d be an orthonormal matrix. Instead of havingzt cycle through the unit vectors, we makezt cycle through thevi so thatzt = \u00b1vi . We let the labelyt = sign( \u2329 1,V\u22a4zt \u232a ) = sign ( \u2211di=1\u3008vi ,zt\u3009 )\n. We provide an elaborated explanation in Appendix A. Intuitively, with\u03c8t(x) = \u3008x,Htx\u3009 andHt set to be the full matrix from (5), ADAGRAD again needs to observe each orthonormal vectorvi nly once while stochastic gradient descent\u2019s loss can be made\u2126(d/\u03b5) for any\u03b5 > 0."
        },
        {
            "heading": "1.4 Related Work",
            "text": "Many successful algorithms have been developed over the past few years to minimize regret in the online learning setting. A modern view of these algorithms casts the problem as the task of following the (regularized) leader (see Rakhlin, 2009, and the referenc s therein) or FTRL in short. Informally, FTRL methods choose the best decision in hindsight at every iteration. Verbatim usage of the FTRL approach fails to achieve low regret, however, adding a proximal1 term to the past predictions leads to numerous low regret algorithms (Kalai and Vempala, 2003; Hazan and Kale, 2008; Rakhlin, 2009). The proximal term strongly affects the performance of the learning algorithm. Therefore, adapting the proximal function to the characteristics of the problem at hand is desirable.\nOur approach is thus motivated by two goals. The first is to generalize the agnostic online learning paradigm to the meta-task of specializing an algorithm to fit a particular data set. Specifically,\n1. The proximal term is also referred to as regularization in the online learning literature. We use the phrase proximal term in order to avoid confusion with the statistical regularization function\u03d5.\nwe change the proximal function to achieve performance guarantees which are ompetitive with the best proximal term found in hindsight. The second, as alluded to earlier, isto automatically adjust the learning rates for online learning and stochastic gradient descent ona per-feature basis. The latter can be very useful when our gradient vectorsgt are sparse, for example, in a classification setting where examples may have only a small number of non-zero features.As we demonstrated in the examples above, it is rather deficient to employ exactly the same learning rate fo a feature seen hundreds of times and for a feature seen only once or twice.\nOur techniques stem from a variety of research directions, and as a byproduct we also extend a few well-known algorithms. In particular, we consider variants of the follow-the-regularized leader (FTRL) algorithms mentioned above, which are kin to Zinkevich\u2019s lazy projection algorithm. We use Xiao\u2019s recently analyzed regularized dual averaging (RDA) algorithm (2010), which builds upon Nesterov\u2019s (2009) primal-dual subgradient method. We also consider forward-backward splitting (FOBOS) (Duchi and Singer, 2009) and its composite mirror-descent (proximal gradient) generalizations (Tseng, 2008; Duchi et al., 2010), which in turn include as special cases projected gradients (Zinkevich, 2003) and mirror descent (Nemirovski and Yudin, 1983; Beck and Teboulle, 2003). Recent work by several authors (Nemirovski et al., 2009; Juditsky et al.,2008; Lan, 2010; Xiao, 2010) considered efficient and robust methods for stochastic optimization, especially in the case when the expected objectivef is smooth. It may be interesting to investigate adaptive metric approaches in smooth stochastic optimization.\nThe idea of adapting first order optimization methods is by no means new and can be traced back at least to the 1970s with the work on space dilation methods of Shor (1972) and variable metric methods, such as the BFGS family of algorithms (e.g., Fletcher, 1970). This prior work often assumed that the function to be minimized was differentiable and, to our knwledge, did not consider stochastic, online, or composite optimization. In her thesis, Nedic\u0301 (2002) studied variable metric subgradient methods, though it seems difficult to derive explicit ratesof convergence from the results there, and the algorithms apply only when the constraint setX = Rd. More recently, Bordes et al. (2009) proposed a Quasi-Newton stochastic gradient-descent procedure, which is similar in spirit to our methods. However, their convergence results assume a smooth objective with positive definite Hessian bounded away from 0. Our results apply more generally.\nPrior to the analysis presented in this paper for online and stochastic optimization, the strongly convex function\u03c8 in the update equations (3) and (4) either remained intact or was simply multiplied by a time-dependent scalar throughout the run of the algorithm. Zinkevich\u2019s projected gradient, for example, uses\u03c8t(x) = \u2016x\u201622, while RDA (Xiao, 2010) employs\u03c8t(x) = \u221a t\u03c8(x) where\u03c8 is a strongly convex function. The bounds for both types of algorithms are similar, and both rely on the norm\u2016\u00b7\u2016 (and its associated dual\u2016\u00b7\u2016\u2217) with respect to which\u03c8 is strongly convex. Mirror-descent type first order algorithms, such as projected gradient methods, attain regret bounds of the form (Zinkevich, 2003; Bartlett et al., 2007; Duchi et al., 2010)\nR\u03c6(T)\u2264 1 \u03b7 B\u03c8(x \u2217,x1)+ \u03b7 2\nT\n\u2211 t=1\n\u2225 \u2225 f \u2032t (xt) \u2225 \u2225 2 \u2217 . (7)\nChoosing\u03b7 \u221d 1/ \u221a T givesR\u03c6(T) = O( \u221a\nT). WhenB\u03c8(x,x\u2217) is bounded for allx\u2208 X , we choose step sizes\u03b7t \u221d 1/ \u221a t which is equivalent to setting\u03c8t(x) = \u221a t\u03c8(x). Therefore, no assumption on the time horizon is necessary. For RDA and follow-the-leader algorithms, thebounds are similar\n(Xiao, 2010, Theorem 3):\nR\u03c6(T)\u2264 \u221a T\u03c8(x\u2217)+ 1\n2 \u221a T\nT\n\u2211 t=1\n\u2225 \u2225 f \u2032t (xt) \u2225 \u2225 2 \u2217 . (8)\nThe problem of adapting to data and obtaining tighter data-dependent bounds for algorithms such as those above is a natural one and has been studied in the mistake-bound setting for online learning in the past. A framework that is somewhat related to ours is the confidence weighted learning scheme by Crammer et al. (2008) and the adaptive regularization of weights algorithm (AROW) of Crammer et al. (2009). These papers provide mistake-boundanalyses for secondorder algorithms, which in turn are similar in spirit to the second-order Perceptron algorithm (CesaBianchi et al., 2005). The analyses by Crammer and colleagues, however, yield mistake bounds dependent on the runs of the individual algorithms and are thus difficult tocompare with our regret bounds.\nAROW maintains a mean prediction vector\u00b5t \u2208 Rd and a covariance matrix\u03a3t \u2208 Rd\u00d7d over\u00b5t as well. At every step of the algorithm, the learner receives a pair(zt ,yt) wherezt \u2208 Rd is thetth example andyt \u2208 {\u22121,+1} is the label. Whenever the predictor\u00b5t attains a margin value smaller than 1, AROW performs the update\n\u03b2t = 1\n\u3008zt ,\u03a3tzt\u3009+\u03bb , \u03b1t = [1\u2212yt \u3008zt ,\u00b5t\u3009]+ ,\n\u00b5t+1 = \u00b5t +\u03b1t\u03a3tytzt , \u03a3t+1 = \u03a3t \u2212\u03b2t\u03a3txtx\u22a4t \u03a3t . (9)\nIn the above scheme, one can force\u03a3t to be diagonal, which reduces the run-time and storage requirements of the algorithm but still gives good performance (Crammer etal., 2009). In contrast to AROW, the ADAGRAD algorithm uses theroot of the inverse covariance matrix, a consequence of our formal analysis. Crammer et al.\u2019s algorithm and our algorithms have similar run times, generally linear in the dimensiond, when using diagonal matrices. However, when using full matrices the runtime of AROW algorithm isO(d2), which is faster than ours as it requires computing the root of a matrix.\nIn concurrent work, McMahan and Streeter (2010) propose and analyze an algorithm which is very similar to some of the algorithms presented in this paper. Our analysis buildon recent advances in online learning and stochastic optimization (Duchi et al., 2010; Xiao, 2010), whereas McMahan and Streeter use first-principles to derive their regret bounds. As a consequence of our approach, we are able to apply our analysis to algorithms for composite minimization wi h a known additional objective term\u03d5. We are also able to generalize and analyze both the mirror descent and dual-averaging family of algorithms. McMahan and Streeter focus on whatthey term thecompetitive ratio, which is the ratio of the worst case regret of the adaptive algorithm to the worst case regret of a non-adaptive algorithm with the best proximal term\u03c8 chosen in hindsight. We touch on this issue briefly in the sequel, but refer the interested reader to McMahanand Streeter (2010) for this alternative elegant perspective. We believe that both analyses shedin ights into the problems studied in this paper and complement each other.\nThere are also other lines of work on adaptive gradient methods that arenot directly related to our work but nonetheless relevant. Tighter regret bounds using the variation of the cost functionsft were proposed by Cesa-Bianchi et al. (2007) and derived by Hazan nd Kale (2008). Bartlett et al. (2007) explore another adaptation technique for\u03b7t where they adapt the step size to accommodate\nboth strongly and weakly convex functions. Our approach differs from previous approaches as it does not focus on a particular loss function or mistake bound. Instead, we vie the problem of adapting the proximal function as a meta-learning problem. We then obtain a bound c mparable to the bound obtained using the best proximal function chosen in hindsight."
        },
        {
            "heading": "2. Adaptive Proximal Functions",
            "text": "Examining the bounds (7) and (8), we see that most of the regret depends on dual norms of \u2032t (xt), and the dual norms in turn depend on the choice of\u03c8. This naturally leads to the question of whether we can modify the proximal term\u03c8 along the run of the algorithm in order to lower the contribution of the aforementioned norms. We achieve this goal by keeping second order information about the sequenceft and allow\u03c8 to vary on each round of the algorithms.\nWe begin by providing two corollaries based on previous work that give the regret of our base algorithms when the proximal function\u03c8t is allowed to change. These corollaries are used in the sequel in our regret analysis. We assume that\u03c8t is monotonically non-decreasing, that is, \u03c8t+1(x) \u2265 \u03c8t(x). We also assume that\u03c8t is 1-strongly convex with respect to a time-dependent semi-norm\u2016\u00b7\u2016\u03c8t . Formally,\u03c8 is 1-strongly convex with respect to\u2016\u00b7\u2016\u03c8 if\n\u03c8(y)\u2265 \u03c8(x)+ \u3008\u2207\u03c8(x),y\u2212x\u3009+ 1 2 \u2016x\u2212y\u20162\u03c8 .\nStrong convexity is guaranteed if and only ifB\u03c8t (x,y)\u2265 12 \u2016x\u2212y\u2016 2 \u03c8t . We also denote the dual norm of \u2016\u00b7\u2016\u03c8t by \u2016\u00b7\u2016\u03c8\u2217t . For completeness, we provide the proofs of following two results in Appendix F, as they build straightforwardly on work by Duchi et al. (2010) and Xiao (2010). For the primal-dual subgradient update, the following bound holds.\nProposition 2 Let the sequence{xt} be defined by the update (3). For any x\u2217 \u2208 X ,\nT\n\u2211 t=1 ft(xt)+\u03d5(xt)\u2212 ft(x\u2217)\u2212\u03d5(x\u2217)\u2264 1 \u03b7 \u03c8T(x\u2217)+ \u03b7 2\nT\n\u2211 t=1\n\u2225 \u2225 f \u2032t (xt) \u2225 \u2225 2 \u03c8\u2217t\u22121 . (10)\nFor composite mirror descent algorithms a similar result holds.\nProposition 3 Let the sequence{xt} be defined by the update (4). Assume w.l.o.g. that\u03d5(x1) = 0. For any x\u2217 \u2208 X ,\nT\n\u2211 t=1\nft(xt)+\u03d5(xt)\u2212 ft(x\u2217)\u2212\u03d5(x\u2217)\n\u2264 1 \u03b7 B\u03c81(x \u2217,x1)+ 1 \u03b7 T\u22121 \u2211 t=1 [ B\u03c8t+1(x \u2217,xt+1)\u2212B\u03c8t (x\u2217,xt+1) ] + \u03b7 2 T \u2211 t=1 \u2225 \u2225 f \u2032t (xt) \u2225 \u2225 2 \u03c8\u2217t . (11)\nThe above corollaries allow us to prove regret bounds for a family of algorithms that iteratively modify the proximal functions\u03c8t in attempt to lower the regret bounds."
        },
        {
            "heading": "3. Diagonal Matrix Proximal Functions",
            "text": "We begin by restricting ourselves to using diagonal matrices to define matrix proximal functions and (semi)norms. This restriction serves a two-fold purpose. First, the analysis for the general case is somewhat complicated and thus the analysis of the diagonal restriction serves as a proxy for better understanding. Second, in problems with high dimension where we expect this type of modification to help, maintaining more complicated proximal functions is likely to be prohibitively expensive. Whereas earlier analysis requires a learning rate to slow changes between predictorsxt andxt+1, we will instead automatically grow the proximal function we use to achieve asymptotically low regret. To remind the reader,g1:t,i is theith row of the matrix obtained by concatenating the subgradients from iteration 1 throught in the online algorithm.\nTo provide some intuition for the algorithm we show in Algorithm 1, let us examine the problem\nmin s\nT\n\u2211 t=1\nd\n\u2211 i=1 g2t,i si s.t. s 0, \u30081,s\u3009 \u2264 c .\nThis problem is solved by settingsi = \u2016g1:T,i\u20162 and scalings so that\u3008s,1\u3009= c. To see this, we can write the Lagrangian of the minimization problem by introducing multipliers\u03bb 0 and\u03b8 \u2265 0 to get\nL(s,\u03bb,\u03b8) = d\n\u2211 i=1 \u2016g1:T,i\u201622 si \u2212\u3008\u03bb,s\u3009+\u03b8(\u30081,s\u3009\u2212c).\nTaking partial derivatives to find the infimum ofL , we see that\u2212\u2016g1:T,i\u201622/s2i \u2212\u03bbi +\u03b8= 0, and complementarity conditions on\u03bbisi (Boyd and Vandenberghe, 2004) imply that\u03bbi = 0. Thus we have si = \u03b8\u2212 1 2 \u2016g1:T,i\u20162, and normalizing appropriately using\u03b8 gives thatsi = c\u2016g1:T,i\u20162/\u2211dj=1 \u2225 \u2225g1:T, j \u2225 \u2225 2.\nAs a final note, we can plugsi into the objective above to see\ninf s\n{\nT\n\u2211 t=1\nd\n\u2211 i=1 g2t,i si : s 0,\u30081,s\u3009 \u2264 c } = 1 c ( d \u2211 i=1 \u2016g1:T,i\u20162\n)2\n. (12)\nLet diag(v) denote the diagonal matrix with diagonalv. It is natural to suspect that fors achieving the infimum in Equation (12), if we use a proximal function similar to\u03c8(x) = \u3008x,diag(s)x\u3009 with associated squared dual norm\u2016x\u20162\u03c8\u2217 = \u2329 x,diag(s)\u22121x \u232a\n, we should do well lowering the gradient terms in the regret bounds (10) and (11).\nTo prove a regret bound for our Algorithm 1, we note that both types of updates suffer losses that include a term depending solely on the gradients obtained along their run. The following lemma is applicable to both updates, and was originally proved by Auer and Gentile (2000), though we provide a proof in Appendix C. McMahan and Streeter (2010) also givean identical lemma.\nLemma 4 Let gt = f \u2032t (xt) and g1:t and st be defined as in Algorithm 1. Then\nT\n\u2211 t=1\n\u2329 gt ,diag(st) \u22121gt \u232a \u2264 2 d\n\u2211 i=1 \u2016g1:T,i\u20162 .\nTo obtain a regret bound, we need to consider the terms consisting of the dual-norm of the subgradient in the regret bounds (10) and (11), which is\u2016 f \u2032t (xt)\u20162\u03c8\u2217t . When\u03c8t(x) = \u3008x,(\u03b4I +diag(st))x\u3009, it is easy to see that the associated dual-norm is\n\u2016g\u20162\u03c8\u2217t = \u2329 g,(\u03b4I +diag(st))\u22121g \u232a .\nFrom the definition ofst in Algorithm 1, we clearly have\u2016 f \u2032t (xt)\u20162\u03c8\u2217t \u2264 \u2329 gt ,diag(st)\u22121gt \u232a\n. Note that if st,i = 0 thengt,i = 0 by definition ofst,i . Thus, for any\u03b4 \u2265 0, Lemma 4 implies\nT\n\u2211 t=1\n\u2225 \u2225 f \u2032t (xt) \u2225 \u2225 2 \u03c8\u2217t\n\u2264 2 d\n\u2211 i=1 \u2016g1:T,i\u20162 . (13)\nTo obtain a bound for a primal-dual subgradient method, we set\u03b4 \u2265 maxt \u2016gt\u2016\u221e, in which case \u2016gt\u20162\u03c8\u2217t\u22121 \u2264 \u2329 gt ,diag(st)\u22121gt \u232a\n, and we follow the same lines of reasoning to achieve the inequality (13).\nIt remains to bound the various Bregman divergence terms for Corollary 3nd the term\u03c8T(x\u2217) for Corollary 2. We focus first on the composite mirror-descent update.Examining the bound (11) and Algorithm 1, we notice that\nB\u03c8t+1(x \u2217,xt+1)\u2212B\u03c8t (x\u2217,xt+1) = 1 2 \u3008x\u2217\u2212xt+1,diag(st+1\u2212st)(x\u2217\u2212xt+1)\u3009\n\u2264 1 2 max i (x\u2217i \u2212xt+1,i)2\u2016st+1\u2212st\u20161 .\nSince\u2016st+1\u2212st\u20161 = \u3008st+1\u2212st ,1\u3009 and\u3008sT ,1\u3009= \u2211di=1\u2016g1:T,i\u20162, we have T\u22121 \u2211 t=1 B\u03c8t+1(x \u2217,xt+1)\u2212B\u03c8t (x\u2217,xt+1) \u2264 1 2 T\u22121 \u2211 t=1 \u2016x\u2217\u2212xt+1\u20162\u221e \u3008st+1\u2212st ,1\u3009\n\u2264 1 2 max t\u2264T \u2016x\u2217\u2212xt\u20162\u221e d \u2211 i=1 \u2016g1:T,i\u20162\u2212 1 2 \u2016x\u2217\u2212x1\u20162\u221e \u3008s1,1\u3009 . (14)\nWe also have\n\u03c8T(x\u2217) = \u03b4\u2016x\u2217\u201622+ \u3008x\u2217,diag(sT)x\u2217\u3009 \u2264 \u03b4\u2016x\u2217\u2016 2 2+\u2016x\u2217\u2016 2 \u221e\nd\n\u2211 i=1 \u2016g1:T,i\u20162 .\nCombining the above arguments with Corollaries 2 and 3, and using (14) with thefact thatB\u03c81(x \u2217,x1)\u2264 1 2 \u2016x\u2217\u2212x1\u2016 2 \u221e \u30081,s1\u3009, we have proved the following theorem.\nTheorem 5 Let the sequence{xt} be defined by Algorithm 1. For xt generated using the primaldual subgradient update (3) with\u03b4 \u2265 maxt \u2016gt\u2016\u221e, for any x\u2217 \u2208 X ,\nR\u03c6(T)\u2264 \u03b4 \u03b7 \u2016x\u2217\u201622+ 1 \u03b7 \u2016x\u2217\u20162\u221e\nd\n\u2211 i=1\n\u2016g1:T,i\u20162+\u03b7 d\n\u2211 i=1 \u2016g1:T,i\u20162 .\nFor xt generated using the composite mirror-descent update (4), for any x\u2217 \u2208 X\nR\u03c6(T)\u2264 1\n2\u03b7 max t\u2264T\n\u2016x\u2217\u2212xt\u20162\u221e d\n\u2211 i=1\n\u2016g1:T,i\u20162+\u03b7 d\n\u2211 i=1 \u2016g1:T,i\u20162 .\nThe above theorem is a bit unwieldy. We thus perform a few algebraic simplifications to get the next corollary, which has a more intuitive form. Let us assume thatX is compact and setD\u221e = supx\u2208X \u2016x\u2212x\u2217\u2016\u221e. Furthermore, define\n\u03b3T , d\n\u2211 i=1 \u2016g1:T,i\u20162 = infs\n{\nT\n\u2211 t=1\n\u2329 gt ,diag(s) \u22121gt \u232a : \u30081,s\u3009 \u2264 d\n\u2211 i=1\n\u2016g1:T,i\u20162 , s 0 } .\nAlso w.l.o.g. let 0\u2208 X . The following corollary is immediate (this is equivalent to Corollary 1, though we have moved the \u221a d term in the earlier bound).\nCorollary 6 Assume that D\u221e and\u03b3T are defined as above. For{xt} generated by Algorithm 1 using the primal-dual subgradient update (3) with\u03b7 = \u2016x\u2217\u2016\u221e, for any x\u2217 \u2208 X we have\nR\u03c6(T)\u2264 2\u2016x\u2217\u2016\u221e \u03b3T +\u03b4 \u2016x\u2217\u201622 \u2016x\u2217\u2016\u221e \u2264 2\u2016x\u2217\u2016\u221e \u03b3T +\u03b4\u2016x\u2217\u20161 .\nUsing the composite mirror descent update (4) to generate{xt} and setting\u03b7 = D\u221e/ \u221a 2, we have\nR\u03c6(T)\u2264 \u221a 2D\u221e d\n\u2211 i=1\n\u2016g1:T,i\u20162 = \u221a 2D\u221e\u03b3T .\nWe now give a short derivation of Corollary 1 from the introduction: useTh orem 5, Corollary 6, and the fact that\ninf s\n{\nT\n\u2211 t=1\nd\n\u2211 i=1 g2t,i si : s 0,\u30081,s\u3009 \u2264 d } = 1 d ( d \u2211 i=1 \u2016g1:T,i\u20162\n)2\n.\nas in (12) in the beginning of Section 3. Plugging the\u03b3T term in from Corollary 6 and multiplying D\u221e by \u221a d completes the proof of the corollary.\nAs discussed in the introduction, Algorithm 1 should have lower regret thannon-adaptive algorithms on sparse data, though this depends on the geometry of the underlyingoptimization space X . For example, suppose that our learning problem is a logistic regression with 0/1-valued features. Then the gradient terms are likewise based on 0/1-valued features and sparse, so the gradient terms in the bound\u2211di=1\u2016g1:T,i\u20162 should all be much smaller than \u221a T. If some features appear much more frequently than others, then the infimal representation of\u03b3T and the infimal equality in Corollary 1 show that we have significantly lower regret by using higher learning rates for infrequent features and lower learning rates on commonly appearing features. Further, if the op imal predictor is relatively dense, as is often the case in predictions problems with sparse inputs, then\u2016x\u2217\u2016\u221e is the best p-norm we can have in the regret.\nMore precisely, McMahan and Streeter (2010) show that ifX s contained within an\u2113\u221e ball of radiusR and contains an\u2113\u221e ball of radiusr, then the bound in the above corollary is within a factor of \u221a 2R/r of the regret of the best diagonal proximal matrix, chosen in hindsight. So, for example, ifX = {x\u2208 Rd : \u2016x\u2016p \u2264C}, thenR/r = d1/p, which shows that the domainX does effect the guarantees we can give on optimality of ADAGRAD."
        },
        {
            "heading": "4. Full Matrix Proximal Functions",
            "text": "In this section we derive and analyze new updates when we estimate a full matrix for he divergence \u03c8t instead of a diagonal one. In this generalized case, we use the root of the ma rix of outer products of the gradients that we have observed to update our parameters. As in thediagonal case, we build on intuition garnered from an optimization problem, and in particular, we seek amatrix Swhich is the solution to the following minimization problem:\nmin S\nT\n\u2211 t=1\n\u2329 gt ,S \u22121gt \u232a s.t. S 0, tr(S)\u2264 c . (15)\nThe solution is obtained by definingGt = \u2211t\u03c4=1g\u03c4g\u03c4\u22a4 and settingS to be a normalized version of the root ofGT , that is,S= cG 1/2 T / tr(G 1/2 T ). For a proof, see Lemma 15 in Appendix E, which also shows that whenGT is not full rank we can instead use its pseudo-inverse. If we iteratively use divergences of the form\u03c8t(x) = \u2329 x,G1/2t x \u232a , we might expect as in the diagonal case to attain low regret by collecting gradient information. We achieve our low regret goal by employing a similar doubling lemma to Lemma 4 and bounding the gradient norm terms. The resulting algorithm is given in Algorithm 2, and the next theorem provides a quantitative analysisof the brief motivation above.\nTheorem 7 Let Gt be the outer product matrix defined above and the sequence{xt} be defined by Algorithm 2. For xt generated using the primal-dual subgradient update of (3) and\u03b4 \u2265 maxt \u2016gt\u20162, for any x\u2217 \u2208 X\nR\u03c6(T)\u2264 \u03b4 \u03b7 \u2016x\u2217\u201622+ 1 \u03b7 \u2016x\u2217\u201622 tr(G 1/2 T )+\u03b7 tr(G 1/2 T ).\nFor xt generated with the composite mirror-descent update of (4), if x\u2217 \u2208 X and\u03b4 \u2265 0\nR\u03c6(T)\u2264 \u03b4 \u03b7 \u2016x\u2217\u201622+ 1 2\u03b7 max t\u2264T \u2016x\u2217\u2212xt\u201622 tr(G 1/2 T )+\u03b7 tr(G 1/2 T ).\nProof To begin, we consider the difference between the divergence terms at timet +1 and timet from the regret (11) in Corollary 3. Let\u03bbmax(M) denote the largest eigenvalue of a matrixM. We have\nB\u03c8t+1(x \u2217,xt+1)\u2212B\u03c8t (x\u2217,xt+1) = 1 2 \u2329 x\u2217\u2212xt+1,(Gt+11/2\u2212Gt1/2)(x\u2217\u2212xt+1) \u232a\n\u2264 1 2 \u2016x\u2217\u2212xt+1\u201622 \u03bbmax(G 1/2 t+1\u2212G 1/2 t ) \u2264 1 2 \u2016x\u2217\u2212xt+1\u201622 tr(G 1/2 t+1\u2212G 1/2 t ) .\nFor the last inequality we used the fact that the trace of a matrix is equal to the sum of its eigenvalues along with the propertyGt+11/2\u2212Gt1/2 0 (see Lemma 13 in Appendix B) and therefore tr(G1/2t+1\u2212 G1/2t )\u2265 \u03bbmax(G1/2t+1\u2212G 1/2 t ). Thus, we get\nT\u22121 \u2211 t=1 B\u03c8t+1(x \u2217,xt+1)\u2212B\u03c8t (x\u2217,xt+1)\u2264 1 2 T\u22121 \u2211 t=1 \u2016x\u2217\u2212xt+1\u201622 ( tr(G1/2t+1)\u2212 tr(G 1/2 t ) ) .\nNow we use the fact thatG1 is a rank 1 PSD matrix with non-negative trace to see that\nT\u22121 \u2211 t=1 \u2016x\u2217\u2212xt+1\u201622 ( tr(G1/2t+1)\u2212 tr(G 1/2 t ) )\n\u2264 max t\u2264T \u2016x\u2217\u2212xt\u201622 tr(GT1/2)\u2212\u2016x\u2217\u2212x1\u2016 2 2 tr(G 1/2 1 ) . (16)\nIt remains to bound the gradient terms common to all our bounds. We use the following three lemmas, which essentially directly applicable. We prove the first two in AppendixD.\nLemma 8 Let B 0 and B\u22121/2 denote the root of the inverse of B when B\u227b 0 and the root of the pseudo-inverse of B otherwise. For any\u03bd such that B\u2212\u03bdgg\u22a4 0 the following inequality holds.\n2tr((B\u2212\u03bdgg\u22a4)1/2)\u2264 2tr(B1/2)\u2212\u03bd tr(B\u22121/2gg\u22a4) .\nLemma 9 Let \u03b4 \u2265 \u2016g\u20162 and A 0, then \u2329 g,(\u03b4I +A1/2)\u22121g \u232a\n\u2264 \u2329 g, ( (A+gg\u22a4)\u2020 )1/2 g \u232a .\nLemma 10 Let St = Gt1/2 be as defined in Algorithm 2 and A\u2020 denote the pseudo-inverse of A. Then\nT\n\u2211 t=1\n\u2329 gt ,S \u2020 t gt \u232a \u2264 2 T\n\u2211 t=1\n\u2329 gt ,S \u2020 Tgt \u232a = 2tr(GT 1/2) .\nProof We prove the lemma by induction. The base case is immediate, since we have\n\u2329\ng1,(G \u2020 1) 1/2g1 \u232a = \u3008g1,g1\u3009 \u2016g1\u20162 = \u2016g1\u20162 \u2264 2\u2016g1\u20162 .\nNow, assume the lemma is true forT \u22121, so from the inductive assumption we get T\n\u2211 t=1\n\u2329 gt ,S \u2020 t gt \u232a \u2264 2 T\u22121 \u2211 t=1 \u2329 gt ,S \u2020 T\u22121gt \u232a + \u2329 gT ,S \u2020 TgT \u232a .\nSinceST\u22121 does not depend ont we can rewrite\u2211T\u22121t=1 \u2329 gt ,S \u2020 T\u22121gt \u232a as\ntr\n(\nS\u2020T\u22121, T\u22121 \u2211 t=1 gtg \u22a4 t\n)\n= tr((G\u2020T\u22121) 1/2GT\u22121) ,\nwhere the right-most equality follows from the definitions ofSt andGt . Therefore, we get\nT\n\u2211 t=1\n\u2329 gt ,S \u2020 t gt \u232a \u2264 2tr((G\u2020T\u22121)1/2GT\u22121)+ \u2329 gT ,(G \u2020 T) 1/2gT \u232a\n= 2tr(G1/2T\u22121)+ \u2329 gT ,(G \u2020 T) 1/2gT \u232a .\nUsing Lemma 8 with the substitutionB= GT , \u03bd = 1, andg= gt lets us exploit the concavity of the function tr(A1/2) to bound the above sum by 2tr(G1/2T ). N\nWe can now finalize our proof of the theorem. As in the diagonal case, we have t at the squared dual norm (seminorm when\u03b4 = 0) associated with\u03c8t is\n\u2016x\u20162\u03c8\u2217t = \u2329 x,(\u03b4I +St)\u22121x \u232a .\nThus it is clear that\u2016gt\u20162\u03c8\u2217t \u2264 \u2329 gt ,S \u2020 t gt \u232a . For the dual-averaging algorithms, we use Lemma 9 above show that\u2016gt\u20162\u03c8\u2217t\u22121 \u2264 \u2329 gt ,S \u2020 t gt \u232a\nso long as\u03b4 \u2265 \u2016gt\u20162. Lemma 10\u2019s doubling inequality then implies that\nT\n\u2211 t=1\n\u2225 \u2225 f \u2032t (xt) \u2225 \u2225 2 \u03c8\u2217t \u2264 2tr(G1/2T ) and T\n\u2211 t=1\n\u2225 \u2225 f \u2032t (xt) \u2225 \u2225 2 \u03c8\u2217t\u22121 \u2264 2tr(G1/2T ) (17)\nfor the mirror-descent and primal-dual subgradient algorithm, respectively. To finish the proof, Note thatB\u03c81(x \u2217,x1)\u2264 12 \u2016x\u2217\u2212x1\u2016 2 2 tr(G 1/2 1 ) when\u03b4 = 0. By combining this with the first of the bounds (17) and the bound (16) on\u2211T\u22121t=1 B\u03c8t+1(x \u2217,xt+1)\u2212B\u03c8t (x\u2217,xt+1), Corollary 3 gives the theorem\u2019s statement for the mirror-descent family of algorithms. Combining the\nfact that\u2211Tt=1\u2016 f \u2032t (xt)\u20162\u03c8\u2217t\u22121 \u2264 2tr(G 1/2 T ) and the bound (16) with Corollary 2 gives the desired bound onR\u03c6(T) for the primal-dual subgradient algorithms, which completes the proof of thetheorem.\nAs before, we can give a corollary that simplifies the bound implied by Theorem 7. The infimal equality in the corollary uses Lemma 15 in Appendix B. The corollary underscore that for learning problems in which there is a rotationU of the space for which the gradient vectorsgt have small inner products\u3008gt ,Ugt\u3009 (essentially a sparse basis for thegt) then using full-matrix proximal functions can attain significantly lower regret.\nCorollary 11 Assume that\u03d5(x1)= 0. Then the regret of the sequence{xt} generated by Algorithm 2 when using the primal-dual subgradient update with\u03b7 = \u2016x\u2217\u20162 is\nR\u03c6(T)\u2264 2\u2016x\u2217\u20162 tr(G 1/2 T )+\u03b4\u2016x\u2217\u20162 .\nLet X be compact set so thatsupx\u2208X \u2016x\u2212x\u2217\u20162 \u2264 D. Taking\u03b7 = D/ \u221a\n2 and using the composite mirror descent update with\u03b4 = 0, we have\nR\u03c6(T)\u2264 \u221a 2D tr(G1/2T ) = \u221a 2dD\n\u221a \u221a \u221a \u221ainf S { T\n\u2211 t=1\ng\u22a4t S\u22121gt : S 0, tr(S)\u2264 d } ."
        },
        {
            "heading": "5. Derived Algorithms",
            "text": "In this section, we derive updates using concrete regularization functions \u03d5 and settings of the domainX for the ADAGRAD framework. We focus on showing how to solve Equations (3) and (4) with the diagonal matrix version of the algorithms we have presented. We focus on the diagonal case for two reasons. First, the updates often take closed-form in this case and carry some intuition. Second, the diagonal case is feasible to implement in very high dimensions, whereas the full matrix version is likely to be confined to a few thousand dimensions. We also discusshow to efficiently compute the updates when the gradient vectors are sparse.\nWe begin by noting a simple but useful fact. LetGt denote either the outer product matrix of gradients or its diagonal counterpart and letHt = \u03b4I +G 1/2 t , as usual. Simple algebraic manipulations yield that each of the updates (3) and (4) in the prequel can be writtenin he following form (omitting the stepsize\u03b7):\nxt+1 = argmin x\u2208X\n{\n\u3008u,x\u3009+\u03d5(x)+ 1 2 \u3008x,Htx\u3009\n}\n. (18)\nIn particular, at timet for the RDA update, we haveu= \u03b7tg\u0304t . For the composite gradient update (4),\n\u03b7\u3008gt ,x\u3009+ 1 2 \u3008x\u2212xt ,Ht(x\u2212xt)\u3009= \u3008\u03b7gt \u2212Htxt ,x\u3009+ 1 2 \u3008x,Htx\u3009+ 1 2 \u3008xt ,Htxt\u3009\nso thatu= \u03b7gt \u2212Htxt . We now derive algorithms for solving the general update (18). Since most of the derivations are known, we generally provide only the closed-form solutions or algorithms for the solutions in the remainder of the subsection, deferring detailed derivations to Appendix G for the interested reader."
        },
        {
            "heading": "5.1 \u21131-regularization",
            "text": "We begin by considering how to solve the minimization problems necessary for Alg ithm 1 with diagonal matrix divergences and\u03d5(x) = \u03bb\u2016x\u20161. We consider the two updates we proposed and denote theith diagonal element of the matrixHt = \u03b4I +diag(st) from Algorithm 1 byHt,ii = \u03b4+ \u2016g1:t,i\u20162. For the primal-dual subgradient update, the solution to (3) amounts to the following simple update forxt+1,i :\nxt+1,i = sign(\u2212g\u0304t,i) \u03b7t\nHt,ii [|g\u0304t,i |\u2212\u03bb]+ . (19)\nComparing the update (19) to the standard dual averaging update (Xiao, 2010), which is\nxt+1,i = sign(\u2212g\u0304t,i)\u03b7 \u221a t [|g\u0304t,i |\u2212\u03bb]+ ,\nit is clear that the difference distills to the step size employed for each coordinate. Our generalization of RDA yields a dedicated step size for each coordinate inversely proporti nal to the time-based norm of the coordinate in the sequence of gradients. Due to the normalizationby this term the step size scaleslinearly with t, so whenHt,ii is small, gradient information on coordinatei is quickly incorporated.\nThe composite mirror-descent update (4) has a similar form that essentially amounts to iterative shrinkage and thresholding, where the shrinkage differs per coordinate:\nxt+1,i = sign\n(\nxt,i \u2212 \u03b7\nHt,ii gt,i\n)[\u2223\n\u2223 \u2223 \u2223 xt,i \u2212 \u03b7\nHt,ii gt,i\n\u2223 \u2223 \u2223 \u2223 \u2212 \u03bb\u03b7 Ht,ii ]\n+\n.\nWe compare the actual performance of the newly derived algorithms to previously studied versions in the next section.\nFor both updates it is clear that we can perform \u201clazy\u201d computation when thegradient vectors are sparse, a frequently occurring setting when learning for instance from text corpora. Suppose that from time stept0 throught, the ith component of the gradient is 0. Then we can evaluate the above updates on demand sinceHt,ii remains intact. For composite mirror-descent, at timet when xt,i is needed, we update\nxt,i = sign(xt0,i)\n[\n|xt0,i |\u2212 \u03bb\u03b7\nHt0,ii (t \u2212 t0)\n]\n+\n.\nEven simpler just in time evaluation can be performed for the the primal-dual subgradient update. Here we need to keep an unnormalized version of the average \u00afgt . Concretely, we keep track of ut = tg\u0304t = \u2211t\u03c4=1g\u03c4 = ut\u22121+gt , then use the update (19):\nxt,i = sign(\u2212ut,i) \u03b7t\nHt,ii\n[ |ut,i | t \u2212\u03bb ]\n+\n,\nwhereHt can clearly be updated lazily in a similar fashion."
        },
        {
            "heading": "5.2 \u21131-ball Projections",
            "text": "We next consider the setting in which\u03d5 \u2261 0 andX = {x : \u2016x\u20161 \u2264 c}, for which it is straightforward to adapt efficient solutions to continuous quadratic knapsack problems (Brucker, 1984). We\nuse the matrixHt = \u03b4I +diag(Gt)1/2 from Algorithm 1. We provide a brief derivation sketch and anO(d logd) algorithm in this section. First, we convert the problem (18) into a projection pr blem onto a scaled\u21131-ball. By making the substitutionsz= H1/2x andA = H\u22121/2, it is clear that problem (18) is equivalent to\nmin z\n\u2225 \u2225\n\u2225 z+H\u22121/2u\n\u2225 \u2225 \u2225\n2 2 s.t. \u2016Az\u20161 \u2264 c .\nNow, by appropriate choice ofv = \u2212H\u22121/2u = \u2212\u03b7tH\u22121/2t g\u0304t for the primal-dual update (3) and v= H1/2t xt \u2212\u03b7H\u22121/2t gt for the mirror-descent update (4), we arrive at the problem\nmin z 1 2 \u2016z\u2212v\u201622 s.t.\nd\n\u2211 i=1 ai |zi | \u2264 c . (20)\nWe can clearly recoverxt+1 from the solutionz\u2217 to the projection (20) viaxt+1 = H \u22121/2 t z \u2217. By the symmetry of the objective (20), we can assume without loss of generality th t v 0 and constrainz 0, and a bit of manipulation with the Lagrangian (see Appendix G) for the problem shows that the solutionz\u2217 has the form\nz\u2217i =\n{ vi \u2212\u03b8\u2217ai if vi \u2265 \u03b8\u2217ai 0 otherwise\nfor some\u03b8\u2217 \u2265 0. The algorithm in Figure 3 constructs the optimal\u03b8 and returnsz\u2217."
        },
        {
            "heading": "5.3 \u21132 Regularization",
            "text": "We now turn to the case where\u03d5(x) = \u03bb\u2016x\u20162 while X = Rd. This type of regularization is useful for zeroing multiple weights in a group, for example in multi-task or multiclass learning (Obozinski et al., 2007). Recalling the general proximal step (18), we must solve\nmin x \u3008u,x\u3009+ 1 2 \u3008x,Hx\u3009+\u03bb\u2016x\u20162 . (21)\nThere is no closed form solution for this problem, but we give an efficientbisection-based procedure for solving (21). We start by deriving the dual. Introducing a variablez= x, we get the equivalent problem of minimizing\u3008u,x\u3009+ 12 \u3008x,Hx\u3009+\u03bb\u2016z\u20162 subject tox= z. With Lagrange multipliers\u03b1 for the equality constraint, we obtain the Lagrangian\nL(x,z,\u03b1) = \u3008u,x\u3009+ 1 2 \u3008x,Hx\u3009+\u03bb\u2016z\u20162+ \u3008\u03b1,x\u2212z\u3009 .\nTaking the infimum ofL with respect to the primal variablesx andz, we see that the infimum is attained atx=\u2212H\u22121(u+\u03b1). Coupled with the fact that infz\u03bb\u2016z\u20162\u2212\u3008\u03b1,z\u3009=\u2212\u221e unless\u2016\u03b1\u20162 \u2264 \u03bb, in which case the infimum is 0, we arrive at the dual form\ninf x,z L(x,z,\u03b1) =\n{ \u221212 \u2329 u+\u03b1,H\u22121(u+\u03b1) \u232a if \u2016\u03b1\u20162 \u2264 \u03bb \u2212\u221e otherwise.\nSettingv= H\u22121u, we further distill the dual to\nmin \u03b1 \u3008v,\u03b1\u3009+ 1 2 \u2329 \u03b1,H\u22121\u03b1 \u232a s.t. \u2016\u03b1\u20162 \u2264 \u03bb . (22)\nWe can solve problem (22) efficiently using a bisection search of its equivalent representation in Lagrange form,\nmin \u03b1 \u3008v,\u03b1\u3009+ 1 2 \u2329 \u03b1,H\u22121\u03b1 \u232a + \u03b8 2 \u2016\u03b1\u201622 ,\nwhere\u03b8 > 0 is an unknown scalar. The solution to the latter as a function of\u03b8 is clearly\u03b1(\u03b8) = \u2212(H\u22121+\u03b8I)\u22121v=\u2212(H\u22121+\u03b8I)\u22121H\u22121u. Since\u2016\u03b1(\u03b8)\u20162 is monotonically decreasing in\u03b8 (consider the the eigen-decomposition of the positive definiteH\u22121), we can simply perform a bisection search over\u03b8, checking at each point whether\u2016\u03b1(\u03b8)\u20162 \u2277 \u03bb.\nTo find initial upper and lower bounds on\u03b8, we note that\n(1/\u03c3max(H)+\u03b8)\u22121\u2016v\u20162 \u2264 \u2016\u03b1(\u03b8)\u20162 \u2264 (1/\u03c3min(H)+\u03b8)\u22121\u2016v\u20162\nwhere\u03c3max(H) denotes the maximum singular value ofH and\u03c3min(H) the minimum. To guarantee \u2016\u03b1(\u03b8max)\u20162 \u2264 \u03bb, we thus set\u03b8max= \u2016v\u20162/\u03bb\u22121/\u03c3max(H). Similarly, for\u03b8min we see that so long as \u03b8 \u2265 \u2016v\u20162/\u03bb\u22121/\u03c3min(H) we have\u2016\u03b1(\u03b8)\u20162 \u2265 \u03bb. The fact that\u2202\u2016x\u20162 = {z : \u2016z\u20162 \u2264 1} whenx= 0 implies that the solution for the original problem (21) isx= 0 if and only if \u2016u\u20162 \u2264 \u03bb. We provide pseudocode for solving (21) in Algorithm 4."
        },
        {
            "heading": "5.4 \u2113\u221e Regularization",
            "text": "We again letX = Rd but now choose\u03d5(x) = \u03bb\u2016x\u2016\u221e. This type of update, similarly to\u21132, zeroes groups of variables, which is handy in finding structurally sparse solutions f r multitask or multiclass problems. Solving the\u2113\u221e regularized problem amounts to\nmin x \u3008u,x\u3009+ 1 2 \u3008x,Hx\u3009+\u03bb\u2016x\u2016\u221e . (23)\nThe dual of this problem is a modified\u21131-projection problem. As in the case of\u21132 regularization, we introduce an equality constrained variablez= x with associated Lagrange multipliers\u03b1 \u2208 Rd to obtain\nL(x,z,\u03b1) = \u3008u,x\u3009+ 1 2 \u3008x,Hx\u3009+\u03bb\u2016z\u2016\u221e + \u3008\u03b1,x\u2212z\u3009 .\nPerforming identical manipulations to the\u21132 case, we take derivatives and get thatx=\u2212H\u22121(u+\u03b1) and, similarly, unless\u2016\u03b1\u20161 \u2264 \u03bb, infzL(x,z,\u03b1) =\u2212\u221e. Thus the dual problem for (23) is\nmax \u03b1 \u2212 1 2 (u+\u03b1)H\u22121(u+\u03b1) s.t. \u2016\u03b1\u20161 \u2264 \u03bb .\nWhenH is diagonal we can find the optimal\u03b1\u2217 using the generalized\u21131-projection in Algorithm 3, then reconstruct the optimalx via x=\u2212H\u22121(u+\u03b1\u2217)."
        },
        {
            "heading": "5.5 Mixed-norm Regularization",
            "text": "Finally, we combine the above results to show how to solve problems with matrix-valued inputs X \u2208 Rd\u00d7k, whereX = [x1 \u00b7 \u00b7 \u00b7 xd]\u22a4. We consider mixed-norm regularization, which is very useful for encouraging sparsity across several tasks (Obozinski et al., 2007). Now\u03d5 is an\u21131/\u2113p norm, that is, \u03d5(X) = \u03bb\u2211di=1\u2016xi\u2016p. By imposing an\u21131-norm overp-norms of the rows ofX, entire rows are nulled at once.\nWhenp\u2208 {2,\u221e} and the proximalH in (18) is diagonal, the previous algorithms can be readily used to solve the mixed norm problems. We simply maintain diagonal matrix informationfor each of the rows \u00afxi of X separately, then solve one of the previous updates for each row independently. We use this form of regularization in our experiments with multiclass prediction prblems in the next section."
        },
        {
            "heading": "6. Experiments",
            "text": "We performed experiments with several real world data sets with differentcharacteristics: the ImageNet image database (Deng et al., 2009), the Reuters RCV1 text classifiation data set (Lewis et al., 2004), the MNIST multiclass digit recognition problem, and the census income data set from the UCI repository (Asuncion and Newman, 2007). For uniformity acrossexperiments, we focus on the completely online (fully stochastic) optimization setting, in which at each iterationthe learning algorithm receives a single example. We measure performance using two metrics: the online loss or error and the test set performance of the predictor the learning algorithm outputs at the end of a single pass through the training data. We also give some results that show how imp sing sparsity constraints (in the form of\u21131 and mixed-norm regularization) affects the learning algorithm\u2019s performance. One benefit of the ADAGRAD framework is its ability to straightforwardly generalize to\ndomain constraintsX 6=Rd and arbitrary regularization functions\u03d5, in contrast to previous adaptive online algorithms.\nWe experiment with RDA (Xiao, 2010), FOBOS(Duchi and Singer, 2009), adaptive RDA, adaptive FOBOS, the Passive-Aggressive (PA) algorithm (Crammer et al., 2006), and AROW (Crammer et al., 2009). To remind the reader, PA is an online learning procedure withthe update\nxt+1 = argmin x [1\u2212yt \u3008zt ,x\u3009]++ \u03bb 2 \u2016x\u2212xt\u201622 ,\nwhere\u03bb is a regularization parameter. PA\u2019s update is similar to the update employed by AROW (see (9)), but the latter maintains second order information onx. By using a representer theorem it is also possible to derive efficient updates for PA and AROW when the loss is the logistic loss, log(1+ exp(\u2212yt \u3008zt ,xt\u3009)). We thus we compare the above six algorithms using both hinge and logistic loss."
        },
        {
            "heading": "6.1 Text Classification",
            "text": "The Reuters RCV1 data set consists of a collection of approximately 800,000text articles, each of which is assigned multiple labels. There are 4 high-level categories, Economics, Commerce, Medical, and Government (ECAT, CCAT, MCAT, GCAT), and multiple more spcific categories. We focus on training binary classifiers for each of the four major categori s. The input features we use are 0/1 bigram features, which, post word stemming, give data of approximately 2 million dimensions. The feature vectors are very sparse, however, and mostexamples have fewer than 5000 non-zero features.\nWe compare the twelve different algorithms mentioned in the prequel as well asvariants of FOBOS and RDA with\u21131-regularization. We summarize the results of the\u21131-regularized runs as well as AROW and PA in Table 1. The results for both hinge and logistic lossesare qualitatively and quantitatively very similar, so we report results only for training with the hinge loss in Table 1. Each row in the table represents the average of four different experiments in which we hold out 25% of the data for a test set and perform an online pass on the remaining 75% of the data. For RDA and FOBOS, we cross-validate the stepsize parameter\u03b7 by simply running multiple passes and then choosing the output of the learner that had the fewest mistakes during training. For PA and AROW we choose\u03bb using the same approach. We use the same regularization multiplier on the\u21131 term for RDA and FOBOS, selected so that RDA achieved approximately 10% non-zero predictors.\nIt is evident from the results presented in Table 1 that the adaptive algorithms (AROW and ADAGRAD) are far superior to non-adaptive algorithms in terms of error rate on testdata. The ADAGRAD algorithms naturally incorporate sparsity as well since they are run with\u21131-regularization, though RDA has significantly higher sparsity levels (PA and AROW do not have any sparsity). Furthermore, although omitted from the table to avoid clutter, ineverytest with the RCV1 corpus, the\nadaptive algorithms outperformed the non-adaptive algorithms. Moreover, both ADAGRAD-RDA and ADAGRAD-Fobos outperform AROW on all the classification tasks. Unregularized RDA and FOBOS attained similar results as did the\u21131-regularized variants (of course without sparsity), but we omit the results to avoid clutter and because they do not give much more understanding."
        },
        {
            "heading": "6.2 Image Ranking",
            "text": "ImageNet (Deng et al., 2009) consists of images organized according to the nouns in the WordNet hierarchy, where each noun is associated on average with more than 500images collected from the web. We selected 15,000 important nouns from the hierarchy and conducted a large scale image ranking task foreachnoun. This approach is identical to the task tackled by Grangier and Bengio (2008) using the Passive-Aggressive algorithm. To solve this problem, we train 15,000 ranking machines using Grangier and Bengio\u2019s visterms features, which represent patches in an image with 79-dimensional sparse vectors. There are approximately 120 patches per image, resulting in a 10,000-dimensional feature space.\nBased on the results in the previous section, we focus on four algorithms for solving this task: AROW, ADAGRAD with RDA updates and\u21131-regularization, vanilla RDA with\u21131, and PassiveAggressive. We use the ranking hinge loss, which is[1\u2212\u3008x,z1\u2212z2\u3009]+ whenz1 is ranked above z2. We train a rankerxc for each of the image classes individually, cross-validating the choice of initial stepsize for each algorithm on a small held-out set. To train an individual ranker for class c, at each step of the algorithm we randomly sample a positive imagez1 for the categoryc and an imagez2 from the training set (which with high probability is a negative example for class c) and perform an update on the examplez1\u2212 z2. We let each algorithm take 100,000 such steps for each image category, we train four sets of rankers with each algorithm, andthe training set includes approximately 2 million images.\nFor evaluation, we use a distinct test set of approximately 1 million images. To evaluate a set of rankers, we iterate through all 15,000 classes in the data set. For each class we take all the positive image examples in the test set and sample 10 times as many negative image examples.Following Grangier and Bengio, we then rank the set of positive and negative images and compute precisionat-k for k = {1, . . . ,10} and the average precision for each category. The precision-at-k is defined as the proportion of examples ranked in the topk for a categoryc that actually belong toc, and the average precision is the average of the precisions at each position in which a relevant picture appears. Letting Pos(c) denote the positive examples for categoryc andp(i) denote the position of the ith returned picture in list of images sorted by inner product withxc, the average precision is\n1 |Pos(c)|\n|Pos(c)|\n\u2211 i=1 i p(i) .\nWe compute the mean of each measurement across all classes, performing this twelve times for each of the sets of rankers trained. Table 2 summarizes our results. We donot report variance as the variance was on the order of 10\u22125 for each algorithm. One apparent characteristic to note from the table is that ADAGRAD RDA achieves higher levels of sparsity than the other algorithms\u2014using only 73% of the input features it achieves very high performance. Moreover, it outperforms all the algorithms in average precision. AROW has better results than the other algorithms in terms of precision-at-k for k\u2264 10, though ADAGRAD\u2019s performance catches up to and eventually surpasses AROW\u2019s ask grows."
        },
        {
            "heading": "6.3 Multiclass Optical Character Recognition",
            "text": "In the well-known MNIST multiclass classification data set, we are given 28\u00d7 pixel imagesai , and the learner\u2019s task is to classify each image as a digit in{0, . . . ,9}. Linear classifiers do not work well on a simple pixel-based representation. Thus we learn classifierbuilt on top of a kernel machine with Gaussian kernels, as do Duchi and Singer (2009), which gives a different (and nonsparse) structure to the feature space in contrast to our previous experiments. In particular, for the ith example andjth feature, the feature value iszi j = K(ai ,a j), exp ( \u2212 12\u03c32 \u2225 \u2225ai \u2212a j \u2225 \u2225 2 2 ) . We use a support set of approximately 3000 images to compute the kernels and trainedmulticlass predictors, which consist of one vectorxc \u2208 R3000 for each classc, giving a 30,000 dimensional problem. There is no known multiclass AROW algorithm. We therefore compare adaptiveRDA with and without mixed-norm\u21131/\u21132 and\u21131/\u2113\u221e regularization (see Section 5.5), RDA, and multiclass Passive Aggressive to one another using the multiclass hinge loss (Crammer et al., 2006). For each algorithm we used the first 5000 of 60,000 training examples to choose the stepsize\u03b7 (for RDA) and\u03bb (for PA).\nIn Figure 5, we plot the learning curves (cumulative mistakes made) of multiclass PA, RDA, RDA with \u21131/\u21132 regularization, adaptive RDA, and adaptive RDA with\u21131/\u21132 regularization (\u21131/\u2113\u221e\nis similar). From the curves, we see that Adaptive RDA seems to have similar perfo mance to PA, and the adaptive versions of RDA are vastly superior to their non-adaptive counterparts. Table 3 further supports this, where we see that the adaptive RDA algorithms outperform their non-adaptive counterparts both in terms of sparsity (the proportion of non-zero rows)and test set error rates."
        },
        {
            "heading": "6.4 Income Prediction",
            "text": "The KDD census income data set from the UCI repository (Asuncion and Newman, 2007) contains census data extracted from 1994 and 1995 population surveys conducted by the U.S. Census Bureau. The data consists of 40 demographic and employment related variables whichare used to predict whether a respondent has income above or below $50,000. We quantize each feature into bins (5 per feature for continuous features) and take products of features togive a 4001 dimensional feature space with 0/1 features. The data is divided into a training set of 199,523 instances and test set of 99,762 test instances.\nAs in the prequel, we compare AROW, PA, RDA, and adaptive RDA with and without \u21131regularization on this data set. We use the first 10,000 examples of the trainingset to select the step size parameters\u03bb for AROW and PA and\u03b7 for RDA. We perform ten experiments on random shuffles of the training data. Each experiment consists of a training pass through some proportion of the data (.05, .1, .25, .5, or the entire training set) and computing the test seterror rate of the learned predictor. Table 4 and Figure 6 summarize the results of these experiments. The variance of the test error rates is on the order of 10\u22126 so we do not report it. As earlier, the table and figure make it clear that the adaptive methods (AROW and ADAGRAD-RDA) give better performance than non-adaptive methods. Further, as detailed in the table, the ADAGRAD methods can give extremely sparse predictors that still give excellent test set performance. This is consistent with the experiments we have seen to this point, where ADAGRAD gives sparse but highly accurate predictors."
        },
        {
            "heading": "6.5 Experiments with Sparsity-Accuracy Tradeoffs",
            "text": "In our final set of experiments, we investigate the tradeoff between the level of sparsity and the classification accuracy for the ADAGRAD-RDA algorithms. Using the same experimental setup as for the initial text classification experiments described in Section 6.1, we record the average test-set performance of ADAGRAD-RDA versus the proportion of features that are non-zero in the predictor ADAGRAD outputs after a single pass through the training data. To achieve this, we run\nADAGRAD with \u21131-regularization, and we sweep the regularization multiplier\u03bb from 10\u22128 to 10\u22121. These values result in predictors ranging from a completely dense predictor to an all-zeros predictor, respectively.\nWe summarize our results in Figure 7, which shows the test set performanceof ADAGRAD for each of the four categories ECAT, CCAT, GCAT, and MCAT. Within each plot, the horizontal black line labeled AROW designates the baseline performance of AROW on thetext classification task, though we would like to note that AROW generates fully dense predictors. The plots all portray a similar story. With high regularization values, ADAGRAD exhibits, as expected, poor performance as it retains no predictive information from the learning task.Put another way, when the regularization value is high ADAGRAD is confined to an overly sparse predictor which exhibits poor generalization. However, as the regularization multiplier\u03bb decreases, the learned predictor becomes less sparse and eventually the accuracy of ADAGRAD exceeds AROW\u2019s accuracy. It is interesting to note that for these experiments, as soon as the predictor resulting from asinglepass\nthrough the data has more than 1% non-zero coefficients, ADAGRAD\u2019s performance matches that of AROW. We also would like to note that the variance in the test-set error rates for these experiments is on the order of 10\u22126, and we thus do not draw error bars in the graphs. The performance of ADAGRAD as a function of regularization for other sparse data sets, especially in reation to that of AROW, was qualitatively similar to this experiment."
        },
        {
            "heading": "7. Conclusions",
            "text": "We presented a paradigm that adapts subgradient methods to the geometry of the pr blem at hand. The adaptation allows us to derive strong regret guarantees, which forsome natural data distributions achieve better performance guarantees than previous algorithms. Our online regret bounds can be naturally converted into rate of convergence and generalization bounds(Cesa-Bianchi et al., 2004). Our experiments show that adaptive methods, specifically ADAGRAD-FOBOS, ADAGRAD-RDA, and AROW clearly outperform their non-adaptive counterparts. Furthermore, the ADAGRAD fam-\nily of algorithms naturally incorporates regularization and gives very sparse solutions with similar performance to dense solutions. Our experiments with adaptive methods usea diagonal approximation to the matrix obtained by taking outer products of subgradients computed along the run of the algorithm. It remains to be tested whether using the full outer product matrix can further improve performance.\nTo conclude we would like to underscore a possible elegant generalizationthat interpolates between full-matrix proximal functions and diagonal approximations using block diagonal matrices. Specifically, forv\u2208 Rd let v= [v\u22a4[1] \u00b7 \u00b7 \u00b7 v\u22a4[k]]\u22a4 wherev[i] \u2208 Rdi are subvectors ofv with \u2211ki=1di = d. We can define the associated block-diagonal approximation to the outer product matrix\u2211t\u03c4=1g\u03c4g\u22a4\u03c4 by\nGt = t\n\u2211 \u03c4=1\n\n     \ng\u03c4,[1]g \u22a4 \u03c4,[1] 0 \u00b7 \u00b7 \u00b7 0\n0 g\u03c4,[2]g \u22a4 \u03c4,[2] . . . 0 ...\n... .. . 0 0 \u00b7 \u00b7 \u00b7 0 g\u03c4,[k]g\u22a4\u03c4,[k]\n\n      .\nIn this case, a combination of Theorems 5 and 7 gives the next corollary.\nCorollary 12 Let Gt be the block-diagonal outer product matrix defined above and the sequence {xt} be defined by the RDA update of (3) with\u03c8t(x) = \u2329 x,G1/2t x \u232a . Then, for any x\u2217 \u2208 X ,\nR\u03c6(T)\u2264 1 \u03b7 max i \u2225 \u2225 \u2225 x\u2217[i] \u2225 \u2225 \u2225 2 2 tr(G1/2T )+\u03b7 tr(G 1/2 T ).\nA similar bound holds for composite mirror-descent updates, and it is straightforward to get infimal equalities similar to those in Corollary 11 with the infimum taken over block-diagonal matrices. Such an algorithm can interpolate between the computational simplicity of the diagonal proximal functions and the ability of full matrices to capture correlation in the gradient vctors.\nA few open questions stem from this line of research. The first is whetherwe canefficiently use full matrices in the proximal functions, as in Section 4. A second open issue whether nonEuclidean proximal functions, such as the relative entropy, can be used. W also think that the strongly convex case\u2014whenft or \u03d5 is strongly convex\u2014presents interesting challenges that we have not completely resolved. We hope to investigate both empirical and formal extensions of this work in the near future."
        },
        {
            "heading": "Acknowledgments",
            "text": "There are many people to whom we owe our sincere thanks for this research. Fernando Pereira helped push us in the direction of working on adaptive online methods and has been a constant source of discussion and helpful feedback. Samy Bengio provided uswith a processed version of the ImageNet data set and was instrumental in helping to get our experiments run ing, and Adam Sadovsky gave many indispensable coding suggestions. The anonymousreviewers also gave several suggestions that improved the quality of the paper. Lastly, Sam Roweis was asounding board for some of our earlier ideas on the subject, and we will miss him dearly."
        },
        {
            "heading": "Appendix A. Full Matrix Motivating Example",
            "text": "As in the diagonal case, as the adversary we choose\u03b5 > 0 and on roundst = 1, . . . ,\u03b72/\u03b52 play the vector\u00b1v1. After the first\u03b72/\u03b52 rounds, the adversary simply cycles through the vectorsv2, . . . ,vd. Thus, for Zinkevich\u2019s projected gradient, we havext = \u03b1t,1v1 for some multiplier\u03b1t,1 > 0 when t \u2264 \u03b72/\u03b52. After the first\u03b72/\u03b52 rounds, we perform the updates\nxt+1 = \u03a0\u2016x\u20162\u2264 \u221a d\n(\nxt + \u03b7\u221a t vi\n)\nfor some indexi, but as in the diagonal case,\u03b7/ \u221a\nt \u2264 \u03b5, and by orthogonality ofvi ,v j , we have xt = V\u03b1t for some\u03b1t 0, and the projection step can only shrink the multiplier\u03b1t,i for index i. Thus, each coordinate incurs loss at least 1/(2\u03b5), and projected gradient descent suffers losses \u2126(d/\u03b5).\nOn the other hand, ADAGRAD suffers loss at mostd. Indeed, sinceg1 = v1 and\u2016v1\u20162 = 1, we haveG21 = v1v \u22a4 1 v1v \u22a4 1 = v1v \u22a4 1 = G1, soG1 = G \u2020 1 = G 1 2 1 , and\nx2 = x1+G \u2020 1 = x1+v1v \u22a4 1 v1 = x1+v1.\nSince\u3008x2,v1\u3009= 1, we see that ADAGRAD suffers no loss (andGt = G1) until a vectorzt =\u00b1vi for i 6= 1 is played by the adversary. However, an identical argument shows that Gt is simply updated to v1v\u22a41 + viv \u22a4 i , in which casext = v1+ vi . Indeed, an inductive argument shows that until all the\nvectorsvi are seen, we have\u2016xt\u20162 < \u221a d by orthogonality, and eventually we have\nxt = d\n\u2211 i=1\nvi and \u2016xt\u20162 = \u221a d\n\u2211 i=1\n\u2016vi\u201622 = \u221a d\nso thatxt \u2208 X = {x : \u2016x\u20162 \u2264 \u221a\nd} for ADAGRAD for all t. All future predictions thus achieve margin 1 and suffer no loss."
        },
        {
            "heading": "Appendix B. Technical Lemmas",
            "text": "Lemma 13 Let A B 0 be symmetric d\u00d7d PSD matrices. Then A1/2 B1/2.\nProof This is Example 3 of Davis (1963). We include a proof for convenience ofthe reader. Let \u03bb be any eigenvalue (with corresponding eigenvectorx) of A1/2\u2212B1/2; we show that\u03bb \u2265 0. ClearlyA1/2x\u2212\u03bbx= B1/2x. Taking the inner product of both sides withA1/2x, we have \u2225 \u2225A1/2x \u2225 \u2225\n2 2\u2212\n\u03bb \u2329 A1/2x,x \u232a = \u2329 A1/2x,B1/2x \u232a\n. We use the Cauchy-Schwarz inequality: \u2223 \u2223 \u2223 \u2223 \u2225 \u2225 \u2225 A1/2x \u2225 \u2225 \u2225 2 2 \u2212\u03bb \u2329 A1/2x,x \u232a \u2223 \u2223 \u2223 \u2223 \u2264 \u2225 \u2225 \u2225 A1/2x \u2225 \u2225 \u2225 2 \u2225 \u2225 \u2225 B1/2x \u2225 \u2225 \u2225 2 = \u221a \u3008Ax,x\u3009\u3008Bx,x\u3009 \u2264 \u3008Ax,x\u3009= \u2225 \u2225 \u2225 A1/2x \u2225 \u2225 \u2225 2 2\nwhere the last inequality follows from the assumption thatA B. Thus we must have\u03bb \u2329 A1/2x,x \u232a \u2265 0, which implies\u03bb \u2265 0. The gradient of the function tr(Xp) is easy to compute for integer values ofp. However, whenp is real we need the following lemma. The lemma tacitly uses the fact that there is a unique positive semidefiniteXp whenX 0 (Horn and Johnson, 1985, Theorem 7.2.6).\nLemma 14 Let p\u2208 R and X\u227b 0. Then\u2207Xtr(Xp) = pXp\u22121.\nProof We do a first order expansion of(X+A)p whenX \u227b 0 andA is symmetric. LetX =U\u039bU\u22a4 be the symmetric eigen-decomposition ofX andVDV\u22a4 be the decomposition of\u039b\u22121/2U\u22a4AU\u039b\u22121/2. Then\n(X+A)p = (U\u039bU\u22a4+A)p =U(\u039b+U\u22a4AU)pU\u22a4 =U\u039bp/2(I +\u039b\u22121/2U\u22a4AU\u039b\u22121/2)p\u039bp/2U\u22a4\n=U\u039bp/2V\u22a4(I +D)pV\u039bp/2U\u22a4 =U\u039bp/2V\u22a4(I + pD+o(D))V\u039bp/2U\u22a4\n=U\u039bpU\u22a4+ pU\u039bp/2V\u22a4DV\u039bp/2U\u22a4+o(U\u039b\u2212/2V\u22a4DV\u039bp/2U\u22a4)\n= Xp+U\u039b(p\u22121)/2U\u22a4AU\u039b(p\u22121)/2U\u22a4+o(A) = Xp+ pX(p\u22121)/2AX(p\u22121)/2+o(A).\nIn the above,o(A) is a matrix that goes to zero faster thanA\u2192 0, and the second line follows via a first-order Taylor expansion of(1+di)p. From the above, we immediately have\ntr((X+A)p) = trXp+ ptr(Xp\u22121A)+o(trA),\nwhich completes the proof."
        },
        {
            "heading": "Appendix C. Proof of Lemma 4",
            "text": "We prove the lemma by considering an arbitrary real-valued sequence{ai} and its vector representationa1:i = [a1 \u00b7 \u00b7 \u00b7 ai ]. We are next going to show that\nT\n\u2211 t=1 a2t \u2016a1:t\u20162 \u2264 2\u2016a1:T\u20162 , (24)\nwhere we define00 = 0. We use induction onT to prove inequality (24). ForT = 1, the inequality trivially holds. Assume the bound (24) holds true forT \u22121, in which case\nT\n\u2211 t=1 a2t \u2016a1:t\u20162 = T\u22121 \u2211 t=1 a2t \u2016a1:t\u20162 + a2T \u2016a1:T\u20162 \u2264 2\u2016a1:T\u22121\u20162+ a2T \u2016a1:T\u20162 ,\nwhere the inequality follows from the inductive hypothesis. We definebT = \u2211Tt=1a2t and use concavity to obtain that \u221a bT \u2212a2T \u2264 \u221a bT \u2212a2T 12\u221abT so long asbT \u2212a 2 T \u2265 0.2 Thus,\n2\u2016a1:T\u22121\u20162+ a2T \u2016a1:T\u20162 = 2 \u221a bT \u2212a2T + a2T\u221a bT \u2264 2 \u221a bT = 2\u2016a1:T\u20162 .\nHaving proved the bound (24), we note that by construction thatst,i = \u2016g1:t,i\u20162, so\nT\n\u2211 t=1\n\u2329 gt ,diag(st) \u22121gt \u232a = T\n\u2211 t=1\nd\n\u2211 i=1 g2t,i \u2016g1:t,i\u20162 \u2264 2 d \u2211 i=1 \u2016g1:T,i\u20162 .\n2. We note that we use an identical technique in the full-matrix case. See Lemma 8."
        },
        {
            "heading": "Appendix D. Proof of Lemmas 8 and 9",
            "text": "We begin with the more difficult proof of Lemma 8. Proof of Lemma 8 The core of the proof is based on the concavity of the function tr(A1/2). However, careful analysis is required asA might not be strictly positive definite. We also use the previous lemma which implies that the gradient of tr(A1/2) is 12A\n\u22121/2 whenA\u227b 0. First, Ap is matrix-concave forA\u227b 0 and 0\u2264 p\u2264 1 (see, for example, Corollary 4.1 in Ando,\n1979 or Theorem 16.1 in Bondar, 1994). That is, forA,B\u227b 0 and\u03b1 \u2208 [0,1] we have\n(\u03b1A+(1\u2212\u03b1)B)p \u03b1Ap+(1\u2212\u03b1)Bp . (25)\nNow suppose simplyA,B 0 (but neither is necessarily strict). Then for any\u03b4 > 0, we have A+\u03b4I \u227b 0 andB+\u03b4I \u227b 0 and therefore\n(\u03b1(A+\u03b4I)+(1\u2212\u03b1)(B+\u03b4I))p \u03b1(A+\u03b4I)p+(1\u2212\u03b1)(B+\u03b4I)p \u03b1Ap+(1\u2212\u03b1)Bp ,\nwhere we used Lemma 13 for the second matrix inequality. Moreover,\u03b1A+ (1\u2212 \u03b1)B+ \u03b4I \u2192 \u03b1A+(1\u2212\u03b1)B as\u03b4 \u2192 0. SinceAp is continuous (when we use the unique PSD root), this line of reasoning proves that (25) holds forA,B 0. Thus, we proved that\ntr((\u03b1A+(1\u2212\u03b1)B)p)\u2265 \u03b1 tr(Ap)+(1\u2212\u03b1) tr(Bp) for 0\u2264 p\u2264 1 .\nRecall now that Lemma 14 implies that the gradient of tr(A1/2) is 12A \u22121/2 whenA\u227b 0. There-\nfore, from the concavity ofA1/2 and the form of its gradient, we can use the standard first-order inequality for concave functions so that for anyA,B\u227b 0,\ntr(A1/2)\u2264 tr(B1/2)+ 1 2 tr(B\u22121/2(A\u2212B)) . (26)\nLet A = B\u2212 \u03bdgg\u22a4 0 and suppose only thatB 0. We must take some care sinceB\u22121/2 may not necessarily exist, and the above inequality does not hold true in the pseudo-inverse sense when B 6\u227b 0. However, for any\u03b4> 0 we know that 2\u2207B tr((B+\u03b4I)1/2)= (B+\u03b4I)\u22121/2, andA\u2212B=\u2212\u03bdgg\u22a4. From (26) and Lemma 13, we have\n2tr(B\u2212 tgg\u22a4)1/2 = 2tr(A1/2) \u2264 2tr((A+\u03b4I)1/2) \u2264 2tr(B+\u03b4I)1/2\u2212\u03bd tr((B+\u03b4I)\u22121/2gg\u22a4) . (27)\nNote thatg\u2208 Range(B), because if it were not, we could choose someu with Bu= 0 and\u3008g,u\u3009 6= 0, which would give \u2329 u,(B\u2212cgg\u22a4)u \u232a\n=\u2212c\u3008g,u\u30092 < 0, a contradiction. Now letB=V diag(\u03bb)V\u22a4 be the eigen-decomposition ofB. Sinceg\u2208 Range(B),\ng\u22a4(B+\u03b4I)\u22121/2g = g\u22a4V diag ( 1/ \u221a \u03bbi +\u03b4 ) V\u22a4g\n= \u2211 i:\u03bbi>0 1\u221a \u03bbi +\u03b4 (g\u22a4vi) 2 \u2212\u2192 \u03b4\u21930 \u2211i:\u03bbi>0 \u03bb\u22121/2i (g \u22a4vi) 2 = g\u22a4(B\u2020)1/2g .\nThus, by taking\u03b4 \u2193 0 in (27), and since both tr(B+ \u03b4I)1/2 and tr((B+ \u03b4I)\u22121/2gg\u22a4) are evidently continuous in\u03b4, we complete the proof.\nProof of Lemma 9 We begin by noting that\u03b42I gg\u22a4, so from Lemma 13 we get(A+gg\u22a4)1/2 (A+ \u03b42I)1/2. SinceA and I are simultaneously diagonalizable, we can generalize the inequality\u221a\na+b\u2264\u221aa+ \u221a b, which holds fora,b\u2265 0, to positive semi-definite matrices, thus,\n(A+\u03b42I)1/2 A1/2+\u03b4I .\nTherefore, ifA+gg\u22a4 is of full rank, we have(A+gg\u22a4)\u22121/2 (A1/2+ \u03b4I)\u22121 (Horn and Johnson, 1985, Corollary 7.7.4(a)). Sinceg\u2208 Range((A+gg\u22a4)1/2), we can apply an analogous limiting argument to the one used in the proof of Lemma 8 and discard all zero eigenvalus ofA+gg\u22a4, which completes the lemma."
        },
        {
            "heading": "Appendix E. Solution to Problem (15)",
            "text": "We prove here a technical lemma that is useful in characterizing the solution of the ptimization problem below. Note that the second part of the lemma implies that we can treat the inverse of the solution matrixS\u22121 asS\u2020. We consider solving\nmin S\ntr(S\u22121A) subject to S 0, tr(S)\u2264 c where A 0 . (28)\nLemma 15 If A is of full rank, then the minimizer of (28) is S= cA 1 2/ tr(A 1 2 ). If A is not of full rank, then setting S= cA 1 2/ tr(A 1 2 ) gives\ntr(S\u2020A) = inf S\n{ tr(S\u22121A) : S 0, tr(S)\u2264 c } .\nIn either case,tr(S\u2020A) = tr(A 1 2 )2/c.\nProof Both proofs rely on constructing the Lagrangian for (28). We introduce\u03b8 \u2208 R+ for the trace constraint andZ 0 for the positive semidefinite constraint onS. In this case, the Lagrangian is\nL(S,\u03b8,Z) = tr(S\u22121A)+\u03b8(tr(S)\u2212c)\u2212 tr(SZ).\nThe derivative ofL with respect toS is\n\u2212S\u22121AS\u22121+\u03b8I \u2212Z. (29)\nIf S is full rank, then to satisfy the generalized complementarity conditions for the problem (Boyd and Vandenberghe, 2004), we must haveZ = 0. Therefore, we getS\u22121AS\u22121 = \u03b8I . We now can multiply by Son the right and the left to get thatA= \u03b8S2, which implies thatS\u221d A12 . If A is of full rank, the optimal solution forS\u227b 0 forces\u03b8 to be positive so that tr(S) = c. This yields the solution S= cA 1 2/ tr(A 1 2 ). In order to verify optimality of this solution, we setZ = 0 and\u03b8 = c\u22122 tr(A1/2)2 which gives\u2207SL(S,\u03b8,Z) = 0, as is indeed required. Suppose now thatA is not full rank and that\nA= Q\n[\n\u039b 0 0 0\n]\nQ\u22a4\nis the eigen-decomposition ofA. Let n be the dimension of the null-space ofA (so the rank ofA is d\u2212n). Define the variables\nZ(\u03b8) = [ 0 0 0 \u03b8I ] , S(\u03b8,\u03b4) = 1\u221a \u03b8 Q [ \u039b 12 0 0 \u03b4I ] Q\u22a4, S(\u03b4) = c tr(A 1 2 )+\u03b4n Q [ \u039b 12 0 0 \u03b4I ] Q\u22a4.\nIt is easy to see that trS(\u03b4) = c, and\nlim \u03b4\u21920\ntr(S(\u03b4)\u22121A) = tr(S(0)\u2020A) = tr(A 1 2 ) tr(\u039b 1 2 )/c= tr(A 1 2 )2/c.\nFurther, letg(\u03b8) = infSL(S,\u03b8,Z(\u03b8)) be the dual of (28). From the above analysis and (29), it is evident that\n\u2212S(\u03b8,\u03b4)\u22121AS(\u03b8,\u03b4)\u22121+\u03b8I \u2212Z(\u03b8) =\u2212\u03b8Q [ \u039b\u2212 12 \u039b\u039b\u2212 12 0 0 \u03b4\u22122I \u00b70 ] Q\u22a4+\u03b8I \u2212 [ 0 0 0 \u03b8I ] = 0.\nSoS(\u03b8,\u03b4) achieves the infimum in the dual forany\u03b4 > 0, tr(S(0)Z(\u03b8)) = 0, and\ng(\u03b8) = \u221a \u03b8 tr(\u039b 1 2 )+ \u221a \u03b8 tr(\u039b 1 2 )+ \u221a \u03b8\u03b4n\u2212\u03b8c.\nSetting\u03b8= tr(\u039b 12 )2/c2 givesg(\u03b8) = tr(\u039b 12 )2/c\u2212\u03b4ntr(\u039b 12 )/c. Taking\u03b4\u2192 0 givesg(\u03b8) = tr(A12 )2/c, which means that lim\u03b4\u21920 tr(S(\u03b4)\u22121A) = tr(A 1 2 )2/c = g(\u03b8). Thus the duality gap for the original problem is 0 soS(0) is the limiting solution. The last statement of the lemma is simply pluggingS\u2020 = (A\u2020) 1 2 tr(A 1 2 )/c in to the objective being minimized."
        },
        {
            "heading": "Appendix F. Proofs of Propositions 2 and 3",
            "text": "We begin with the proof of Proposition 2. The proof essentially builds upon Xiao (2010) and Nesterov (2009), with some modification to deal with the indexing of\u03c8t . We include the proof for completeness. Proof of Proposition 2Define\u03c8\u2217t to be the conjugate dual oft\u03d5(x)+\u03c8t(x)/\u03b7:\n\u03c8\u2217t (g) = sup x\u2208X\n{ \u3008g,x\u3009\u2212 t\u03d5(x)\u2212 1 \u03b7 \u03c8t(x) } .\nSince\u03c8t/\u03b7 is 1/\u03b7-strongly convex with respect to the norm\u2016\u00b7\u2016\u03c8t , the function\u03c8 \u2217 t has\u03b7-Lipschitz continuous gradients with respect to\u2016\u00b7\u2016\u03c8\u2217t :\n\u2016\u2207\u03c8\u2217t (g1)\u2212\u2207\u03c8\u2217t (g2)\u2016\u03c8t \u2264 \u03b7\u2016g1\u2212g2\u2016\u03c8\u2217t (30)\nfor anyg1,g2 (see, e.g., Nesterov, 2005, Theorem 1 or Hiriart-Urruty and Lemar\u00b4 chal, 1996, Chapter X). Further, a simple argument with the fundamental theorem of calculus give that if f has L-Lipschitz gradients,f (y)\u2264 f (x)+ \u3008\u2207 f (x),y\u2212x\u3009+(L/2)\u2016y\u2212x\u20162, and\n\u2207\u03c8\u2217t (g) = argmin x\u2208X\n{\n\u2212\u3008g,x\u3009+ t\u03d5(x)+ 1 \u03b7 \u03c8t(x) } . (31)\nUsing the bound (30) and identity (31), we can give the proof of the corollary. Indeed, letting gt \u2208 \u2202 ft(xt) and definingzt = \u2211t\u03c4=1g\u03c4, we have\nT\n\u2211 t=1\nft(xt)+\u03d5(xt)\u2212 ft(x\u2217)\u2212\u03d5(x\u2217)\n\u2264 T\n\u2211 t=1\n\u3008gt ,xt \u2212x\u2217\u3009\u2212\u03d5(x\u2217)+\u03d5(xt)\n\u2264 T\n\u2211 t=1 \u3008gt ,xt\u3009+\u03d5(xt)+sup x\u2208X\n{\n\u2212 T\n\u2211 t=1 \u3008gt ,x\u3009\u2212T\u03d5(x)\u2212 1 \u03b7 \u03c8T(x)\n}\n+\u03c8T(x\u2217)\n= 1 \u03b7 \u03c8T(x\u2217)+ T \u2211 t=1 \u3008gt ,xt\u3009+\u03d5(xt)+\u03c8\u2217T (\u2212zT) .\nSince\u03c8t+1 \u2265 \u03c8t , it is clear that\n\u03c8\u2217T(\u2212zT) =\u2212 T\n\u2211 t=1 \u3008gt ,xT+1\u3009\u2212T\u03d5(xT+1)\u2212 1 \u03b7 \u03c8T(xT+1)\n\u2264\u2212 T\n\u2211 t=1 \u3008gt ,xT+1\u3009\u2212 (T \u22121)\u03d5(xT+1)\u2212\u03d5(xT+1)\u2212 1 \u03b7 \u03c8T\u22121(xT+1)\n\u2264 sup x\u2208X\n(\n\u2212\u3008zT ,x\u3009\u2212 (T \u22121)\u03d5(x)\u2212 1 \u03b7 \u03c8T\u22121(x) ) \u2212\u03d5(xT+1) = \u03c8\u2217T\u22121(\u2212zT)\u2212\u03d5(xT+1).\nThe Lipschitz continuity of\u2207\u03c8\u2217t , the identity (31), and the fact thatzT \u2212zT\u22121 =\u2212gT give T\n\u2211 t=1\nft(xt)+\u03d5(xt+1)\u2212 ft(x\u2217)\u2212\u03d5(x\u2217)\n\u2264 1 \u03b7 \u03c8T(x\u2217)+ T \u2211 t=1 \u3008gt ,xt\u3009+\u03d5(xt+1)+\u03c8\u2217T\u22121(\u2212zT)\u2212\u03d5(xT+1)\n\u2264 1 \u03b7 \u03c8T(x\u2217)+ T \u2211 t=1 \u3008gt ,xt\u3009+\u03d5(xt+1)\u2212\u03d5(xT+1)\n+\u03c8\u2217T\u22121(\u2212zT\u22121)\u2212 \u2329 \u2207\u03c8\u2217T\u22121(zT\u22121),gT \u232a + \u03b7 2 \u2016gT\u20162\u03c8\u2217T\u22121\n= 1 \u03b7 \u03c8T(x\u2217)+ T\u22121 \u2211 t=1 \u3008gt ,xt\u3009+\u03d5(xt+1)+\u03c8\u2217T\u22121(\u2212zT\u22121)+ \u03b7 2 \u2016gT\u20162\u03c8\u2217T\u22121 .\nWe can repeat the same sequence of steps that gave the last equality to seethat\nT\n\u2211 t=1 ft(xt)+\u03d5(xt+1)\u2212 ft(x\u2217)\u2212\u03d5(x\u2217)\u2264 1 \u03b7 \u03c8T(x\u2217)+ \u03b7 2\nT\n\u2211 t=1 \u2016gt\u20162\u03c8\u2217t\u22121 +\u03c8 \u2217 0(\u2212z0).\nRecalling thatx1 = argminx\u2208X {\u03d5(x)} and that\u03c8\u22170(0) = 0 completes the proof.\nWe now turn to the proof of Proposition 3. We begin by stating and fully proving a (essentially) immediate corollary to Lemma 2.3 of Duchi et al. (2010).\nLemma 16 Let{xt} be the sequence defined by the update (4) and assume that B\u03c8t (\u00b7, \u00b7) is strongly convex with respect to a norm\u2016\u00b7\u2016\u03c8t . Let\u2016\u00b7\u2016\u03c8\u2217t be the associated dual norm. Then for any x \u2217,\n\u03b7( ft(xt)\u2212 ft(x\u2217))+\u03b7(\u03d5(xt+1)\u2212\u03d5(x\u2217))\u2264 B\u03c8t (x\u2217,xt)\u2212B\u03c8t (x\u2217,xt+1)+ \u03b72\n2\n\u2225 \u2225 f \u2032t (xt) \u2225 \u2225 2 \u03c8\u2217t\nProof The optimality ofxt+1 for (4) implies for allx\u2208 X and\u03d5\u2032(xt+1) \u2208 \u2202\u03d5(xt+1) \u2329\nx\u2212xt+1,\u03b7 f \u2032(xt)+\u2207\u03c8t(xt+1)\u2212\u2207\u03c8t(xt)+\u03b7\u03d5\u2032(xt+1) \u232a \u2265 0. (32)\nIn particular, this obtains forx= x\u2217. From the subgradient inequality for convex functions, we have ft(x\u2217)\u2265 ft(xt)+ \u3008 f \u2032t (xt),x\u2217\u2212xt\u3009, or ft(xt)\u2212 ft(x\u2217)\u2264 \u3008 f \u2032t (xt),xt \u2212x\u2217\u3009, and likewise for\u03d5(xt+1). We thus have\n\u03b7 [ ft(xt)+\u03d5(xt+1)\u2212 ft(x\u2217)\u2212\u03d5(x\u2217)] \u2264 \u03b7 \u2329\nxt \u2212x\u2217, f \u2032t (xt) \u232a +\u03b7 \u2329 xt+1\u2212x\u2217,\u03d5\u2032(xt+1) \u232a\n= \u03b7 \u2329 xt+1\u2212x\u2217, f \u2032t (xt) \u232a +\u03b7 \u2329 xt+1\u2212x\u2217,\u03d5\u2032(xt+1) \u232a +\u03b7 \u2329 xt \u2212xt+1, f \u2032t (xt) \u232a = \u2329\nx\u2217\u2212xt+1,\u2207\u03c8t(xt)\u2212\u2207\u03c8t(xt+1)\u2212\u03b7 f \u2032t (xt)\u2212\u03b7\u03d5\u2032(xt+1) \u232a + \u3008x\u2217\u2212xt+1,\u2207\u03c8t(xt+1)\u2212\u2207\u03c8t(xt)\u3009+\u03b7 \u2329 xt \u2212xt+1, f \u2032t (xt) \u232a .\nNow, by (32), the first term in the last equation is non-positive. Thus we hav that\n\u03b7 [ ft(xt)+\u03d5(xt+1)\u2212 ft(x\u2217)\u2212\u03d5(x\u2217)] \u2264 \u3008x\u2217\u2212xt+1,\u2207\u03c8t(xt+1)\u2212\u2207\u03c8t(xt)\u3009+\u03b7 \u2329 xt \u2212xt+1, f \u2032t (xt) \u232a\n= B\u03c8t (x \u2217,xt)\u2212B\u03c8t (xt+1,xt)\u2212B\u03c8t (x\u2217,xt+1)+\u03b7 \u2329 xt \u2212xt+1, f \u2032t (xt) \u232a = B\u03c8t (x \u2217,xt)\u2212B\u03c8t (xt+1,xt)\u2212B\u03c8t (x\u2217,xt+1)+\u03b7 \u2329 \u03b7\u2212 1 2 (xt \u2212xt+1), \u221a \u03b7 f \u2032t (xt) \u232a\n\u2264 B\u03c8t (x\u2217,xt)\u2212B\u03c8t (xt+1,xt)\u2212B\u03c8t (x\u2217,xt+1)+ 1 2 \u2016xt \u2212xt+1\u20162\u03c8t +\n\u03b72\n2\n\u2225 \u2225 f \u2032t (xt) \u2225 \u2225 2 \u03c8\u2217t\n\u2264 B\u03c8t (x\u2217,xt)\u2212B\u03c8t (x\u2217,xt+1)+ \u03b72\n2\n\u2225 \u2225 f \u2032t (xt) \u2225 \u2225 2 \u03c8\u2217t .\nIn the above, the first equality follows from simple algebra with Bregman divergences, the second to last inequality follows from Fenchel\u2019s inequality applied to the conjugate functio s 12 \u2016\u00b7\u2016 2 \u03c8t and 1 2 \u2016\u00b7\u2016 2 \u03c8\u2217t (Boyd and Vandenberghe, 2004, Example 3.27), and the last inequality fol ows from the assumed strong convexity ofB\u03c8t with respect to the norm\u2016\u00b7\u2016\u03c8t .\nProof of Proposition 3Sum the equation in the conclusion of Lemma 16."
        },
        {
            "heading": "Appendix G. Derivations of Algorithms",
            "text": "In this appendix, we give the formal derivations of the solution to the ADAGRAD update for\u21131regularization and projection to an\u21131-ball, as described originally in Section 5.\nG.1 \u21131-regularization\nWe give the derivation for the primal-dual subgradient update, as composite mirror-descent is entirely similar. We need to solve update (3), which amounts to\nmin x \u03b7\u3008g\u0304t ,x\u3009+ 1 2t \u03b4\u2016x\u201622+ 1 2t \u3008x,diag(st)x\u3009+\u03b7\u03bb\u2016x\u20161 .\nLet x\u0302 denote the optimal solution of the above optimization problem. Standard subgradient calculus implies that when|g\u0304t,i | \u2264 \u03bb the solution is \u02c6xi = 0. Similarly, when \u00afgt,i < \u2212\u03bb, then x\u0302i > 0, the objective is differentiable, and the solution is obtained by setting the gradientto zero:\n\u03b7g\u0304t,i + Ht,ii\nt x\u0302i +\u03b7\u03bb = 0 , so that \u02c6xi = \u03b7t Ht,ii (\u2212g\u0304t,i \u2212\u03bb) .\nLikewise, when \u00afgt,i > \u03bb then x\u0302i < 0, and the solution is \u02c6xi = \u03b7tHt,ii (\u2212g\u0304t,i +\u03bb). Combining the three cases, we obtain the simple update (19) forxt+1,i .\nG.2 \u21131-ball projections\nThe derivation we give is somewhat terse, and we refer the interested reader to Brucker (1984) or Pardalos and Rosen (1990) for more depth. Recall that our original problem (20) is symmetric in its objective and constraints, so we assume without loss of generality thatv 0 (otherwise, we reverse the sign of each negative component inv, then flip the sign of the corresponding component in the solution vector). This gives\nmin z 1 2 \u2016z\u2212v\u201622 s.t. \u3008a,z\u3009 \u2264 c, z 0 .\nClearly, if\u3008a,v\u3009\u2264 c the optimalz\u2217= v, hence we assume that\u3008 ,v\u3009> c. We also assume without loss of generality thatvi/ai \u2265 vi+1/ai+1 for simplicity of our derivation. (We revisit this assumption at the end of the derivation.) Introducing Lagrange multipliers\u03b8 \u2208R+ for the constraint that\u3008a,z\u3009 \u2264 c and\u03b1 \u2208 Rd+ for the positivity constraint onz, we get\nL(z,\u03b1,\u03b8) = 1 2 \u2016z\u2212v\u201622+\u03b8(\u3008a,z\u3009\u2212c)\u2212\u3008\u03b1,z\u3009 .\nComputing the gradient ofL , we have\u2207zL(z,\u03b1,\u03b8) = z\u2212 v+ \u03b8a\u2212\u03b1. Suppose that we knew the optimal \u03b8\u2217 \u2265 0. Using the complementarity conditions onz and\u03b1 for optimality of z (Boyd and Vandenberghe, 2004), we see that the solutionz\u2217i satisfies\nz\u2217i =\n{ vi \u2212\u03b8\u2217ai if vi \u2265 \u03b8\u2217ai 0 otherwise.\nAnalogously, the complimentary conditions on\u3008a,z\u3009 \u2264 c show that given\u03b8\u2217, we have d\n\u2211 i=1\nai [vi \u2212\u03b8\u2217ai ]+ = c or d\n\u2211 i=1 a2i\n[\nvi ai \u2212\u03b8\u2217\n]\n+\n= c .\nConversely, had we obtained a value\u03b8 \u2265 0 satisfying the above equation, then\u03b8 would evidently induce the optimalz\u2217 through the equationzi = [vi \u2212\u03b8ai ]+.\nNow, let \u03c1 be the largest index in{1, . . . ,d} such thatvi \u2212\u03b8\u2217ai > 0 for i \u2264 \u03c1 andvi \u2212\u03b8\u2217ai \u2264 0 for i > \u03c1. From the assumption thatvi/ai \u2264 vi+1/ai+1, we havev\u03c1+1/a\u03c1+1 \u2264 \u03b8\u2217 < v\u03c1/a\u03c1. Thus, had we known the last non-zero index\u03c1, we would have obtained\n\u03c1\n\u2211 i=1 aivi \u2212 v\u03c1 a\u03c1\n\u03c1\n\u2211 i=1\na2i = \u03c1\n\u2211 i=1 a2i\n(\nvi ai \u2212 v\u03c1 a\u03c1\n)\n< c ,\n\u03c1\n\u2211 i=1 aivi \u2212 v\u03c1+1 a\u03c1+1\n\u03c1\n\u2211 i=1\na2i = \u03c1+1\n\u2211 i=1 a2i\n(\nvi ai \u2212 v\u03c1+1 a\u03c1+1\n)\n\u2265 c .\nGiven\u03c1 satisfying the above inequalities, we can reconstruct the optimal\u03b8\u2217 by noting that the latter inequality should equalc exactly when we replacev\u03c1/a\u03c1 with \u03b8, that is,\n\u03b8\u2217 = \u2211 \u03c1 i=1aivi \u2212c \u2211\u03c1i=1a2i . (33)\nThe above derivation results in the following procedure (when\u3008a,v\u3009 > c). We sortv in descending order ofvi/ai and find the largest index\u03c1 such that\u2211 \u03c1 i=1aivi \u2212 (v\u03c1/a\u03c1)\u2211 \u03c1\u22121 i=1 a 2 i < c. We then reconstruct\u03b8\u2217 using equality (33) and return the soft-thresholded values ofvi (see Algorithm 3). It is easy to verify that the algorithm can be implemented inO(d logd) time. A randomized search with bookkeeping (Pardalos and Rosen, 1990) can be straightforwardly used to derive a linear time algorithm."
        }
    ],
    "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization\u2217",
    "year": 2011
}