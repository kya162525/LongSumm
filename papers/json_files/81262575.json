{
    "abstractText": "Residual networks (ResNets) have recently achieved state-of-the-art on challenging computer vision tasks. We introduce Resnet in Resnet (RiR): a deep dualstream architecture that generalizes ResNets and standard CNNs and is easily implemented with no computational overhead. RiR consistently improves performance over ResNets, outperforms architectures with similar amounts of augmentation on CIFAR-10, and establishes a new state-of-the-art on CIFAR-100.",
    "authors": [
        {
            "affiliations": [],
            "name": "Sasha Targ"
        },
        {
            "affiliations": [],
            "name": "Diogo Almeida"
        },
        {
            "affiliations": [],
            "name": "Kevin Lyman"
        }
    ],
    "id": "SP:dad7630b79e60f49424a63feea4e671e6d2fbb9a",
    "references": [
        {
            "authors": [
                "Monica Bianchini",
                "Franco Scarselli"
            ],
            "title": "On the complexity of neural network classifiers: A comparison between shallow and deep architectures",
            "venue": "Neural Networks and Learning Systems, IEEE Transactions on,",
            "year": 2014
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "In Computer Vision and Pattern Recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Felix A Gers",
                "J\u00fcrgen Schmidhuber",
                "Fred Cummins"
            ],
            "title": "Learning to forget: Continual prediction with lstm",
            "venue": "Neural computation,",
            "year": 2000
        },
        {
            "authors": [
                "Xavier Glorot",
                "Yoshua Bengio"
            ],
            "title": "Understanding the difficulty of training deep feedforward neural networks",
            "venue": "In International conference on artificial intelligence and statistics,",
            "year": 2010
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision,",
            "year": 2015
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "arXiv preprint arXiv:1512.03385,",
            "year": 2015
        },
        {
            "authors": [
                "Sepp Hochreiter",
                "J\u00fcrgen Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural computation,",
            "year": 1997
        },
        {
            "authors": [
                "Sergey Ioffe",
                "Christian Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "arXiv preprint arXiv:1502.03167,",
            "year": 2015
        },
        {
            "authors": [
                "Nal Kalchbrenner",
                "Ivo Danihelka",
                "Alex Graves"
            ],
            "title": "Grid long short-term memory",
            "venue": "arXiv preprint arXiv:1507.01526,",
            "year": 2015
        },
        {
            "authors": [
                "Diederik Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny",
            "year": 2009
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Armand Joulin",
                "Sumit Chopra",
                "Michael Mathieu",
                "Marc\u2019Aurelio Ranzato"
            ],
            "title": "Learning longer memory in recurrent neural networks",
            "venue": "arXiv preprint arXiv:1412.7753,",
            "year": 2014
        },
        {
            "authors": [
                "Andrew M Saxe",
                "James L McClelland",
                "Surya Ganguli"
            ],
            "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
            "venue": "arXiv preprint arXiv:1312.6120,",
            "year": 2013
        },
        {
            "authors": [
                "Jost Tobias Springenberg",
                "Alexey Dosovitskiy",
                "Thomas Brox",
                "Martin A. Riedmiller"
            ],
            "title": "Striving for simplicity: The all convolutional net",
            "venue": "CoRR, abs/1412.6806,",
            "year": 2014
        },
        {
            "authors": [
                "Rupesh K Srivastava",
                "Klaus Greff",
                "J\u00fcrgen Schmidhuber"
            ],
            "title": "Training very deep networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2015
        },
        {
            "authors": [
                "Ilya Sutskever",
                "James Martens",
                "George Dahl",
                "Geoffrey Hinton"
            ],
            "title": "On the importance of initialization and momentum in deep learning",
            "venue": "In Proceedings of the 30th international conference on machine learning",
            "year": 2013
        },
        {
            "authors": [
                "Tijmen Tieleman",
                "Geoffrey Hinton"
            ],
            "title": "Lecture 6.5-rmsprop. COURSERA: Neural networks for machine",
            "year": 2012
        }
    ],
    "sections": [
        {
            "text": "Residual networks (ResNets) have recently achieved state-of-the-art on challenging computer vision tasks. We introduce Resnet in Resnet (RiR): a deep dualstream architecture that generalizes ResNets and standard CNNs and is easily implemented with no computational overhead. RiR consistently improves performance over ResNets, outperforms architectures with similar amounts of augmentation on CIFAR-10, and establishes a new state-of-the-art on CIFAR-100."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Recently proposed residual networks (ResNets) get state-of-the-art performance on the ILSVRC 2015 classification task (Deng et al., 2009) and allow training of extremely deep networks up to more than 1000 layers (He et al., 2015b). Similar to highway networks, residual networks make use of identity shortcut connections that enable flow of information across layers without attenuation that would be caused by multiple stacked non-linear transformations, resulting in improved optimization (Srivastava et al., 2015). In residual networks, shortcut connections are not gated and untransformed input is always transmitted. While the empirical performance of ResNets in He et al. (2015b) is very impressive, current residual network architectures have several potential limitations: identity connections as implemented in the current ResNet leads to accumulation of a mix of levels of feature representations at each layer, even though in a deep network some features learned by earlier layers may no longer provide useful information in later layers (Gers et al., 2000).\nA hypothesis of the ResNet architecture is that learning identity weights is difficult, but by the same argument, it is difficult to learn the additive inverse of identity weights needed to remove information from the representation at any given layer. The fixed size layer structure of the residual block modules also enforces that residuals must be learned by shallow subnetworks, despite evidence that deeper networks are more expressive (Bianchini & Scarselli, 2014). We introduce a generalized residual architecture that combines residual networks and standard convolutional networks in parallel residual and non-residual streams. We show architectures using generalized residual blocks retain the optimization benefits of identity shortcut connections while improving expressivity and the ease of removing unneeded information. We then present a novel architecture, ResNet in ResNet (RiR), which incorporates these generalized residual blocks within the framework of a ResNet and demonstrate state-of-the-art performance of the RiR architecture on CIFAR-100."
        },
        {
            "heading": "2 GENERALIZING RESIDUAL NETWORK ARCHITECTURES",
            "text": "The modular unit of the generalized residual network architecture is a generalized residual block consisting of parallel states for a residual stream, r, which contains identity shortcut connections and is similar to the structure of a residual block from the original ResNet with a single convolutional layer (parameters Wl,r\u2192r), and a transient stream, t, which is a standard convolutional layer (Wl,t\u2192t). Two additional sets of convolutional filters in each block (Wl,r\u2192t, Wl,t\u2192r) also transfer\n\u2217Equal contribution. Author ordering determined by coin flip.\nar X\niv :1\n60 3.\n08 02\n9v 1\n[ cs\n.L G\n] 2\n5 M\nar 2\n01 6\ninformation across streams.\nrl+1 = \u03c3(conv(rl,Wl,r\u2192r) + conv(tl,Wl,t\u2192r) + shortcut(rl)) (1) tl+1 = \u03c3(conv(rl,Wl,r\u2192t) + conv(tl,Wl,t\u2192r))\nSame-stream and cross-stream activations are summed (along with the shortcut connection for the residual stream) before applying batch normalization and ReLU nonlinearities (together \u03c3) to get the output states of the block (Equation 1) (Ioffe & Szegedy, 2015). The function of the residual stream r resembles that of the original structure of the ResNet (He et al., 2015b) with shortcut connections between each unit of processing, while the transient stream t adds the ability to process information from either stream in a nonlinear manner without shortcut connections, allowing information from earlier states to be discarded. The form of the shortcut connection can be an identity function with the appropriate padding or a projection as in He et al. (2015b). We implement the generalized residual block as a single convolutional layer with a modified initialization, which we call ResNet Init (see Appendix 6.1 for detail).\nThe generalized residual block can act as either a standard CNN layer (by learning to zero the residual stream) or a single-layer ResNet block (by learning to zero the transient stream). By repeating the generalized residual block several times, the generalized residual architecture has the expressivity to learn anything in between, including the standard 2-layer ResNet block (Figure 1c). This architecture allows the network to learn residuals with a variable effective number of processing steps before addition back into the residual stream, which we investigate by visualizations. The generalized residual block is not specific to CNNs, and can be applied to standard fully connected layers and other feedforward layers. Replacing each of the convolutional layers within a residual block from the original ResNet (Figure 1a) with a generalized residual block (Figure 1b) leads us to a new architecture we call ResNet in ResNet (RiR) (Figure 1d). In Figure 2, we summarize the relationship between standard CNN, ResNet Init, ResNet, and RiR architectures."
        },
        {
            "heading": "3 EXPERIMENTS",
            "text": "We evaluate our architectures on CIFAR-10 and CIFAR-100 datasets (Krizhevsky & Hinton, 2009) and report the best results found after a grid search on hyperparameters including learning rate, L2 penalty, initialization among Xavier (Glorot & Bengio, 2010), MSR (He et al., 2015a), and orthogonal (Saxe et al., 2013), optimizer among SGD with momentum, SGD with Nesterov momentum (Sutskever et al., 2013), Adam (Kingma & Ba, 2014), and RMSProp (Tieleman & Hinton, 2012), and the type of shortcut connections in the residual blocks. We optimize the hyperparameters for the original ResNet architecture and use SGD with momentum of 0.9, a minibatch size of 500, L2 penalty of 0.0001, and train for 82 epochs. The learning rate was scaled by 0.1 after epochs 42 and 62 and MSR initialization was used for all weight tensors. Test time batch normalization statistics were approximated using an exponential moving average of training batch normalization statistics. A projection with a 3x3 convolution was used for residual blocks that increase dimensionality, and all other shortcut connections were identity. We use equal numbers of filters for the residual and transient streams of the generalized residual network, but optimizing this hyperparameter could lead to further potential improvements.\nIn our experiments, the ResNet Init architecture shows consistent improvement over standard CNN architectures, and the RiR architecture outperforms the original ResNet (Table 3). We find the RiR architecture performs well across a range of numbers of blocks and layers in each block and that ResNet Init applied to existing architectures, such as ALL-CNN-C (Springenberg et al., 2014), yields improvement over standard initialization (Tables 4, 5). Because each stream uses only half the total filters, we investigate the effect of our architectures on a wider 18-layer network (Tables 1, 2). We find this RiR architecture is remarkably effective, obtaining competitive results on CIFAR-10 with only standard augmentation by random crops and horizontal flips, and state-of-the-art results on CIFAR-100. We visualize the effect of zeroing learned connections of each stream in a trained ResNet Init model a single layer at a time, which shows both streams contribute to accuracy and relative use of residual and transient streams changes at different stages of processing (Figure 3). In Figure 4, we show performance of the RiR architecture is robust to increasing depth of residual blocks and the RiR architecture allows training of deeper residuals compared to the original ResNet."
        },
        {
            "heading": "4 RELATED WORK",
            "text": "Interacting transformation streams in which only one stream includes shortcut connections are also used in blocks of LSTM and Grid-LSTM networks (Hochreiter & Schmidhuber, 1997; Kalchbrenner et al., 2015). However, in contrast to highway networks which control flow through shortcut connections via input-dependent carry and transform gates, and to memory and hidden states of LSTM and Grid-LSTM blocks, flow of information between the residual and transient states of the generalized residual block does not use gates and can thus be implemented with no additional parameters over a standard feedforward network (Srivastava et al., 2015). Another difference between previous architectures and the generalized residual block we present is that while memory (m) and hidden (h) states in an LSTM or Grid-LSTM block are calculated sequentially (with hl = ol tanh(ml)), transformation of residual and memory streams of the generalized residual block occurs in parallel and depends only on the learned convolutional filters at each layer without further constraints on their relation. The SCRN architecture of Mikolov et al. (2014) also uses hidden and context units together within a single layer to learn longer term information, which behave similarly to the transient and residual streams, but SCRN only allows unidirectional flow from context to hidden units and connections between context units are fixed, in contrast to bidirectional flow between streams and learned connections for both transient and residual streams in our generalized residual architecture."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "We present a generalized residual architecture which be simply implemented by a modified initialization scheme, ResNet Init, and apply it to the original ResNet to create a novel RiR architecture which achieves state-of-the-art results. Future work includes additional study of RiR and related residual models to further determine the cause of their beneficial effects."
        },
        {
            "heading": "6 APPENDIX",
            "text": ""
        },
        {
            "heading": "6.1 GENERALIZED RESIDUAL BLOCK IMPLEMENTATION",
            "text": "We implement the generalized residual block with a modified initialization of a standard convolutional or fully connected (FC) layer that combines the identity shortcut with the desired linear transformation (convolution or matrix multiplication), which we call ResNet Init, and concatenating tensors for the residual r and transient t streams to form a single tensor x. Because the identity shortcut and the results of the same-stream and cross-stream transformations are summed to give the output for each stream, linear operations on r and t can be composed into a single linear operation on x (see Equation 2 for an example of ResNet Init applied to an FC layer).\nxl+l = \u03c3(W \u2032 lxl)\u21d4 [ rl+1 tl+1 ] = \u03c3(( [ Wl,r\u2192r Wl,t\u2192r Wl,r\u2192t Wl,t\u2192t ] + [ I 0 0 0 ] )\u00d7 [ rl tl ] ) (2)\nTo implement ResNet Init in a single FC layer, we concatenate weight matrices initialized by any existing scheme and then add a partial identity matrix (with 1s only on the first half of the diagonal) to the concatenated weight matrix. To implement ResNet Init in a single convolutional layer, we first concatenate convolutional kernels along the input dimension (Wl,x\u2192r =Wl,r\u2192r +Wl,t\u2192r and Wl,x\u2192t = Wl,r\u2192t + Wl,t\u2192t) and the filter dimension (Wl,x\u2192x = Wl,x\u2192r + Wl,x\u2192t), and then similarly add half of an identity kernel. The output of the generalized residual block implemented\nas a standard layer with modified initialization (W \u2032l ) is exactly equivalent to the output if it were implemented as separate linear operations (Wl,r\u2192r,Wl,t\u2192r,Wl,r\u2192t,Wl,t\u2192t, and I).\nThe generalized residual block implemented using modified initialization will perform differently from one implemented as separate linear operations when weight regularization that pulls all weights towards zero is present, such as L2 regularization. To maintain equivalence of implementation, we subtract the partial identity from the weights prior to application of weight decay."
        },
        {
            "heading": "6.2 ARCHITECTURES",
            "text": ""
        }
    ],
    "title": "RESNET IN RESNET: GENERALIZING RESIDUAL ARCHITECTURES",
    "year": 2016
}