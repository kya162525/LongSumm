{
    "abstractText": "Robust face detection in the wild is one of the ultimate components to support various facial related problems, i.e. unconstrained face recognition, facial periocular recognition, facial landmarking and pose estimation, facial expression recognition, 3D facial model construction, etc. Although the face detection problem has been intensely studied for decades with various commercial applications, it still meets problems in some real-world scenarios due to numerous challenges, e.g. heavy facial occlusions, extremely low resolutions, strong illumination, exceptionally pose variations, image or video compression artifacts, etc. In this paper, we present a face detection approach named Contextual Multi-Scale Region-based Convolution Neural Network (CMS-RCNN) to robustly solve the problems mentioned above. Similar to the region-based CNNs, our proposed network consists of the region proposal component and the region-of-interest (RoI) detection component. However, far apart of that network, there are two main contributions in our proposed network that play a significant role to achieve the state-of-theart performance in face detection. Firstly, the multi-scale information is grouped both in region proposal and RoI detection to deal with tiny face regions. Secondly, our proposed network allows explicit body contextual reasoning in the network inspired from the intuition of human vision system. The proposed approach is benchmarked on two recent challenging face detection databases, i.e. the WIDER FACE Dataset which contains high degree of variability, as well as the Face Detection Dataset and Benchmark (FDDB). The experimental results show that our proposed approach trained on WIDER FACE Dataset outperforms strong baselines on WIDER FACE Dataset by a large margin, and consistently achieves competitive results on FDDB against the recent state-of-the-art face detection methods.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chenchen Zhu"
        },
        {
            "affiliations": [],
            "name": "Yutong Zheng"
        }
    ],
    "id": "SP:e8b2a98f87b7b2593b4a046464c1ec63bfd13b51",
    "references": [
        {
            "authors": [
                "S. Yang",
                "P. Luo",
                "C.C. Loy",
                "X. Tang"
            ],
            "title": "Wider face: A face detection benchmark",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 1, 2, 7, 8, 10",
            "year": 2016
        },
        {
            "authors": [
                "P. Viola",
                "M. Jones"
            ],
            "title": "Rapid object detection using a boosted cascade of simple features",
            "venue": "Computer Vision and Pattern Recognition, 2001. CVPR 2001. Proceedings of the 2001 IEEE Computer Society Conference on, vol. 1. IEEE, 2001, pp. I\u2013511. 1, 2",
            "year": 2001
        },
        {
            "authors": [
                "C. Zhang",
                "Z. Zhang"
            ],
            "title": "A survey of recent advances in face detection",
            "venue": "Tech. Rep. MSR-TR-2010-66, June 2010. [Online]. Available: http://research.microsoft.com/apps/pubs/default.aspx?id= 132077 1, 2",
            "year": 2010
        },
        {
            "authors": [
                "X. Zhu",
                "D. Ramanan"
            ],
            "title": "Face detection, pose estimation, and landmark localization in the wild",
            "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEE, 2012, pp. 2879\u20132886. 1, 2",
            "year": 2012
        },
        {
            "authors": [
                "J. Li",
                "Y. Zhang"
            ],
            "title": "Learning surf cascade for fast and accurate object detection",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2013, pp. 3468\u20133475. 1, 2",
            "year": 2013
        },
        {
            "authors": [
                "H. Li",
                "G. Hua",
                "Z. Lin",
                "J. Brandt",
                "J. Yang"
            ],
            "title": "Probabilistic elastic part model for unsupervised face detector adaptation",
            "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2013, pp. 793\u2013800. 1, 7, 9",
            "year": 2013
        },
        {
            "authors": [
                "N. Marku\u0161",
                "M. Frljak",
                "I.S. Pand\u017ei\u0107",
                "J. Ahlberg",
                "R. Forchheimer"
            ],
            "title": "A method for object detection based on pixel intensity comparisons organized in decision trees",
            "venue": "arXiv preprint arXiv:1305.4537, 2013. 1, 7, 9",
            "year": 2013
        },
        {
            "authors": [
                "H. Li",
                "Z. Lin",
                "J. Brandt",
                "X. Shen",
                "G. Hua"
            ],
            "title": "Efficient boosted exemplar-based face detection",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 1843\u2013 1850. 1, 7, 9",
            "year": 2014
        },
        {
            "authors": [
                "M. Mathias",
                "R. Benenson",
                "M. Pedersoli",
                "L. Van Gool"
            ],
            "title": "Face detection without bells and whistles",
            "venue": "Computer Vision\u2013ECCV 2014. Springer, 2014, pp. 720\u2013735. 1, 3, 7, 9",
            "year": 2014
        },
        {
            "authors": [
                "D. Chen",
                "S. Ren",
                "Y. Wei",
                "X. Cao",
                "J. Sun"
            ],
            "title": "Joint cascade face detection and alignment",
            "venue": "Computer Vision\u2013ECCV 2014. Springer, 2014, pp. 109\u2013122. 1, 3, 7, 9",
            "year": 2014
        },
        {
            "authors": [
                "B. Yang",
                "J. Yan",
                "Z. Lei",
                "S.Z. Li"
            ],
            "title": "Aggregate channel features for multi-view face detection",
            "venue": "Biometrics (IJCB), 2014 IEEE International Joint Conference on. IEEE, 2014, pp. 1\u20138. 1, 2, 7, 8, 9",
            "year": 2014
        },
        {
            "authors": [
                "G. Ghiasi",
                "C.C. Fowlkes"
            ],
            "title": "Occlusion coherence: Detecting and 10 Fig. 8. Some examples of face detection results using our proposed CMS-RCNN method on WIDER FACE database[1]. Fig. 9. Examples of the top 20 false positives from our CMS-RCNN model tested on the WIDER FACE validation set. In fact these false positives include many human faces not in the dataset due to mislabeling, which means that our method is robust to the noise in the data. localizing occluded faces",
            "venue": "arXiv preprint arXiv:1506.08347, 2015. 1, 3, 7, 9",
            "year": 2015
        },
        {
            "authors": [
                "S. Liao",
                "A. Jain",
                "S. Li"
            ],
            "title": "A fast and accurate unconstrained face detector",
            "venue": "2014. 1, 7, 9",
            "year": 2014
        },
        {
            "authors": [
                "H. Li",
                "Z. Lin",
                "X. Shen",
                "J. Brandt",
                "G. Hua"
            ],
            "title": "A convolutional neural network cascade for face detection",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 5325\u20135334. 1, 3, 7, 9",
            "year": 2015
        },
        {
            "authors": [
                "S.S. Farfade",
                "M.J. Saberian",
                "L.-J. Li"
            ],
            "title": "Multi-view face detection using deep convolutional neural networks",
            "venue": "Proceedings of the 5th ACM on International Conference on Multimedia Retrieval. ACM, 2015, pp. 643\u2013650. 1, 7, 9",
            "year": 2015
        },
        {
            "authors": [
                "S. Yang",
                "P. Luo",
                "C.-C. Loy",
                "X. Tang"
            ],
            "title": "From facial parts responses to face detection: A deep learning approach",
            "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 3676\u2013 3684. 1, 2, 3, 4, 7, 8, 9",
            "year": 2015
        },
        {
            "authors": [
                "R. Ranjan",
                "V.M. Patel",
                "R. Chellappa"
            ],
            "title": "A deep pyramid deformable part model for face detection",
            "venue": "Biometrics Theory, Applications and Systems (BTAS), 2015 IEEE 7th International Conference on. IEEE, 2015, pp. 1\u20138. 1, 7, 9",
            "year": 2015
        },
        {
            "authors": [
                "B. Yang",
                "J. Yan",
                "Z. Lei",
                "S.Z. Li"
            ],
            "title": "Convolutional channel features",
            "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 82\u201390. 1, 7, 9",
            "year": 2015
        },
        {
            "authors": [
                "R. Ranjan",
                "V.M. Patel",
                "R. Chellappa"
            ],
            "title": "Hyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition",
            "venue": "arXiv preprint arXiv:1603.01249, 2016. 1, 7, 9",
            "year": 2016
        },
        {
            "authors": [
                "V. Jain",
                "E. Learned-Miller"
            ],
            "title": "Fddb: A benchmark for face detection in unconstrained settings",
            "venue": "University of Massachusetts, Amherst, Tech. Rep. UM-CS-2010-009, 2010. 2, 7, 8, 9, 11",
            "year": 2010
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv preprint arXiv:1409.1556, 2014. 2, 3, 4",
            "year": 2014
        },
        {
            "authors": [
                "P. Felzenszwalb",
                "R. Girshick",
                "D. McAllester",
                "D. Ramanan"
            ],
            "title": "Object detection with discriminatively trained part-based models",
            "venue": "IEEE Trans. on PAMI, vol. 32, no. 9, pp. 1627\u20131645, Sept 2010. 2",
            "year": 2010
        },
        {
            "authors": [
                "X. Yu",
                "J. Huang",
                "S. Zhang",
                "W. Yan",
                "D. Metaxas"
            ],
            "title": "Pose-free facial landmark fitting via optimized part mixtures and cascaded deformable shape model",
            "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2013, pp. 1944\u20131951. 3",
            "year": 2013
        },
        {
            "authors": [
                "S.K. Divvala",
                "D. Hoiem",
                "J.H. Hays",
                "A.A. Efros",
                "M. Hebert"
            ],
            "title": "An empirical study of context in object detection",
            "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. IEEE, 2009, pp. 1271\u20131278. 3, 5",
            "year": 2009
        },
        {
            "authors": [
                "S. Bell",
                "C.L. Zitnick",
                "K. Bala",
                "R. Girshick"
            ],
            "title": "Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks",
            "venue": "arXiv preprint arXiv:1512.04143, 2015. 3",
            "year": 2015
        },
        {
            "authors": [
                "S. Zagoruyko",
                "A. Lerer",
                "T.-Y. Lin",
                "P.O. Pinheiro",
                "S. Gross",
                "S. Chintala",
                "P. Doll\u00e1r"
            ],
            "title": "A multipath network for object detection",
            "venue": "arXiv preprint arXiv:1604.02135, 2016. 3",
            "year": 2016
        },
        {
            "authors": [
                "A. Krizhevsky",
                "I. Sutskever",
                "G.E. Hinton"
            ],
            "title": "Imagenet classifica- 11 Fig. 10. Some examples of face detection results using our proposed CMS-RCNN method on FDDB database [20]. tion with deep convolutional neural networks",
            "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105. 3",
            "year": 2012
        },
        {
            "authors": [
                "R. Girshick",
                "J. Donahue",
                "T. Darrell",
                "J. Malik"
            ],
            "title": "Region-based convolutional networks for accurate object detection and segmentation",
            "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 38, no. 1, pp. 142\u2013158, 2016. 3",
            "year": 2016
        },
        {
            "authors": [
                "R. Girshick"
            ],
            "title": "Fast r-cnn",
            "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 1440\u20131448. 3, 4",
            "year": 2015
        },
        {
            "authors": [
                "S. Ren",
                "K. He",
                "R. Girshick",
                "J. Sun"
            ],
            "title": "Faster r-cnn: Towards realtime object detection with region proposal networks",
            "venue": "Advances in Neural Information Processing Systems, 2015, pp. 91\u201399. 4, 8",
            "year": 2015
        },
        {
            "authors": [
                "M.D. Zeiler",
                "R. Fergus"
            ],
            "title": "Visualizing and understanding convolutional networks",
            "venue": "Computer vision\u2013ECCV 2014. Springer, 2014, pp. 818\u2013833. 4",
            "year": 2014
        },
        {
            "authors": [
                "T.-Y. Lin",
                "M. Maire",
                "S. Belongie",
                "J. Hays",
                "P. Perona",
                "D. Ramanan",
                "P. Doll\u00e1r",
                "C.L. Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "ECCV, 2014, pp. 740\u2013755. 4",
            "year": 2014
        },
        {
            "authors": [
                "B. Hariharan",
                "P. Arbel\u00e1ez",
                "R. Girshick",
                "J. Malik"
            ],
            "title": "Hypercolumns for object segmentation and fine-grained localization",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 447\u2013456. 5",
            "year": 2015
        },
        {
            "authors": [
                "W. Liu",
                "A. Rabinovich",
                "A.C. Berg"
            ],
            "title": "Parsenet: Looking wider to see better",
            "venue": "arXiv preprint arXiv:1506.04579, 2015. 5, 6",
            "year": 2015
        },
        {
            "authors": [
                "Y. Jia",
                "E. Shelhamer",
                "J. Donahue",
                "S. Karayev",
                "J. Long",
                "R. Girshick",
                "S. Guadarrama",
                "T. Darrell"
            ],
            "title": "Caffe: Convolutional architecture for fast feature embedding",
            "venue": "Proceedings of the ACM International Conference on Multimedia. ACM, 2014, pp. 675\u2013678. 6",
            "year": 2014
        },
        {
            "authors": [
                "C.L. Zitnick",
                "P. Doll\u00e1r"
            ],
            "title": "Edge boxes: Locating object proposals from edges",
            "venue": "ECCV. Springer, 2014, pp. 391\u2013405. 7",
            "year": 2014
        },
        {
            "authors": [
                "M. Everingham",
                "L. Van Gool",
                "C.K. Williams",
                "J. Winn",
                "A. Zisserman"
            ],
            "title": "The pascal visual object classes (voc) challenge",
            "venue": "International journal of computer vision, vol. 88, no. 2, pp. 303\u2013 338, 2010. 8 12 Fig. 11. More results of unconstrained face detection under challenging conditions using our proposed CMS- RCNN.",
            "year": 2010
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Robust Face Detection, Multi-Scale Information, Contextual Reasoning, Convolutional Neural Network, Regionbased CNN\nF"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Detection and analysis on human subjects using facial feature based biometrics for access control, surveillance systems and other security applications have gained popularity over the past few years. Several such biometrics systems are deployed in security checkpoints across the globe with more being deployed every day. Particularly, face recognition has been one of the most popular biometrics modalities attractive to security departments. Indeed, the uniqueness of facial features across individuals can be captured much more easily than other biometrics. In order to take into account a face recognition algorithm, however, face detection usually needs to be done first.\nThe problem of face detection has been intensely studied for decades with the aim of ensuring the generalization of robust algorithms to unseen face images [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13]. Although the detection accuracy in recent face detection algorithms [14], [15], [16], [17], [18], [19] has been highly improved due to the advancement of deep Convolutional Neural Networks (CNN), they are still far from achieving the same detection capabilities as a human due to a number of challenges\nCyLab Biometrics Center and the Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA, USA. Emails: {chenchez, yutongzh, kluu}@andrew.cmu.edu, msavvid@ri.cmu.edu\n* indicates equal contribution.\nin practice. For example, off-angle faces, large occlusions, low-resolutions and strong lighting conditions, as shown in\nar X\niv :1\n60 6.\n05 41\n3v 1\n[ cs\n.C V\n] 1\n7 Ju\nn 20\n16\n2 Figure 1, are always the important factors that need to be considered.\nThis paper presents an advanced CNN based approach named Contextual Multi-Scale Region-based CNN (CMSRCNN) to handle the problem of face detection in digital face images collected under numerous challenging conditions, e.g. heavy facial occlusion, illumination, extreme offangle, low-resolution, scale difference, etc. Our designed region-based CNN architecture allows the network to simultaneously look at multi-scale features, as well as to explicitly look outside facial regions as the potential body regions. In other words, this process tries to mimic the way of face detection by human in a sense that when humans are not sure about a face, seeing the body will increase our confidence. Additionally this architecture also helps to synchronize both the global semantic features in high level layers and the localization features in low level layers for facial representation. Therefore, it is able to robustly deal with the challenges in the problem of unconstrained face detection.\nOur CMS-RCNN method introduces the Multi-Scale Region Proposal Network (MS-RPN) to generate a set of region candidates and the Contextual Multi-Scale Convolution Neural Network (CMS-CNN) to do inference on the region candidates of facial regions. A confidence score and bounding box regression are computed for every candidate. In the end, the face detection system is able to decide the quality of the detection results by thresholding these generated confidence scores in given face images. The architecture of our proposed CMS-RCNN network for unconstrained face detection is illustrated in Figure 2.\nOur approach is evaluated on two challenging face detection databases and compared against numerous recent face detection methods. Firstly, the proposed CMS-RCNN method is compared against four strong baselines [11], [16], [1] on the WIDER FACE Dataset [1], a large scale face detection benchmark database. This experiment shows its capability to detect face images in the wild, e.g. under occlusions, illumination, facial poses, low-resolution conditions, etc. Our method outperforms the baselines by a huge margin in all easy, medium, and hard partitions. It is also benchmarked on the Face Detection Data Set and Benchmark (FDDB) [20], a dataset of face regions designed for studying the problem of unconstrained face detection. The experimental results show that the proposed CMS-RCNN approach consistently achieves highly competitive results against the other state-of-the-art face detection methods.\nThe rest of this paper is organized as follows. In section 2, we summarize prior work in face detection. Section 3 reviews a general deep learning framework, the background as well as the limitations of the Faster R-CNN in the problem of face detection. In Section 4, we introduce our proposed CMS-RCNN approach for the problem of unconstrained face detection. Section 5 presents the experimental face detection results and comparisons obtained using our proposed approach on two challenging face detection databases, i.e. the WIDER FACE Dataset and the FDDB database. Finally, our conclusions in this work are presented\nFig. 2. Our proposed Contextual Multi-Scale Regionbased CNN model. It is based on the VGG-16 model [21], with 5 sets of convolution layers in the middle. The upper part is the Multi-Scale Region Proposal Network (MS-RPN) and the lower part is the Contextual MultiScale Convolution Neural Network (CMS-CNN). In the CMS-CNN, the face features labeled as blue blocks and the body context features labeled as red blocks are processed in parallel and combined in the end for final outputs, i.e. confidence score and bounding box.\nin Section 6."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Face detection has been a well studied area of computer vision. One of the first well performing approaches to the problem was the Viola-Jones face detector [2]. It was capable of performing real time face detection using a cascade of boosted simple Haar classifiers. The concepts of boosting and using simple features has been the basis for many different approaches [3] since the Viola-Jones face detector. These early detectors tended to work well on frontal face images but not very well on faces in different poses. As time has passed, many of these methods have been able to deal with off-angle face detection by utilizing multiple models for the various poses of the face. This increases the model size but does afford more practical uses of the methods. Some approaches have moved away from the idea of simple features but continued to use the boosted learning framework. Li and Zhang [5] used SURF cascades for general object detection but also showed good results on face detection.\nMore recent work on face detection has tended to focus on using different models such as a Deformable Parts Model (DPM) [4], [22]. Zhu and Ramanan\u2019s work was an interesting approach to the problem of face detection\n3 where they combined the problems of face detection, pose estimation, and facial landmarking into one framework. By utilizing all three aspects in one framework, they were able to outperform the state-of-the-art at the time on real world images. Yu et al. [23] extended this work by incorporating group sparsity in learning which landmarks are the most salient for face detection as well as incorporating 3D models of the landmarks in order to deal with pose. Chen et al. [10] have combined ideas from both of these approaches by utilizing a cascade detection framework while simultaneously localizing features on the face for alignment of the detectors. Similarly, Ghiasi and Fowlkes [12] have been able to use heirarchical DPMs not only to achieve good face detection in the presence of occlusion but also landmark localization. However, Mathias et al. [9] were able to show that both DPM models and rigid template detectors similar to the Viola-Jones detector have a lot of potential that has not been adequately explored. By retraining these models with appropriately controlled training data, they were able to create face detectors that perform similarly to other, more complex state-of-the-art face detectors.\nAll of these approaches to face detection were based on selecting a feature extractor beforehand. However, there has been work done in using a ConvNet to learn which features are used to detect faces. Neural Networks have been around for a long time but have been experiencing a resurgence in popularity due to hardware improvements and new techniques resulting in the capability to train these networks on large amounts of training data. Li et al. [14] utilized a cascade of CNNs to perform face detection. The cascading networks allowed them to process different scales of faces at different levels of the cascade while also allowing for false positives from previous networks to be removed at later layers in a similar approach to other cascade detectors. Yang et al. [16] approached the problem from a different perspective more similar to a DPM approach. In their method, the face is broken into several facial parts such as hair, eyes, nose, mouth, and beard. By training a detector on each part and combining the score maps intelligently, they were able to achieve accurate face detection even under occlusions. Both of these methods require training several networks in order to achieve their high accuracy. Our method, on the other hand, can be trained as a single network, end-to-end, allowing for less annotation of training data needed while maintaining highly accurate face detection.\nThe ideas of using contextual information in object detection have been studied in several recent work with very high detection accuracy. Divvala et al. [24] reviewed the the role of context in a contemporary, challenging object detection in their empirical evaluation analysis. In their conclusions, the context information not only reduces the overall detection errors, but also the remaining errors made by the detector are more reasonable. Bell et al. [25] introduced an advanced object detector method named Inside-Outside Network (ION) to exploit information both inside and outside the region of interest. In their approach, the contextual information outside the region of interest\nis incorporated using spatial recurrent neural networks. Inside the network, skip pooling is used to extract information at multiple scales and levels of abstraction. Recently, Zagoruyko et al. [26] have presented the MultiPath network with three modifications to the standard Fast R-CNN object detector, i.e. skip connections that give the detector access to features at multiple network layers, a foveal structure to exploit object context at multiple object resolutions, and an integral loss function and corresponding network adjustment that improve localization. The information in their proposed network can flow along multiple paths. Their MultiPath network is combined with DeepMask object proposals to solve the object detection problem.\nUnlike all the previous approaches that select a feature extractor beforehand and incorporate a linear classifier with the depth descriptor beside RGB channels, our method solves the problem under a deep learning framework where the global and the local context features, i.e. multi scaling, are synchronized to Faster Region-based Convolutional Neural Networks in order to robustly achieve semantic detection."
        },
        {
            "heading": "3 BACKGROUND",
            "text": "The recent studies in deep ConvNets have achieved significant results in object detection, classification and modeling [27]. In this section, we review various well-known Deep ConvNets. Then, we show the current limitations of the Faster R-CNN, one of the state-of-the-art deep ConvNet methods in object detection, in the defined context of the face detection."
        },
        {
            "heading": "3.1 Region-based Convolution Neural Networks",
            "text": "One of the most important approaches for the object detection task is the family of Region-based Convolution Neural Networks (R-CNN).\nR-CNN [28], the first generation of this family, applies the high-capacity deep ConvNet to classify given bottomup region proposals. Due to the lack of labeled training data, it adopts a strategy of supervised pre-training for an auxiliary task followed by domain-specific fine-tuning. Then the ConvNet is used as a feature extractor and the system is further trained for object detection with Support Vector Machines (SVM). Finally, it performs boundingbox regression. The method achieves high accuracy but is very time-consuming. The system takes a long time to generate region proposals, extract features from each image, and store these features in a hard disk, which also takes up a large amount of space. At testing time, the detection process takes 47s per image using VGG-16 network [21] implemented in GPU due to the slowness of feature extraction. In other words, R-CNN is slow because it processes each object proposal independently without sharing computation.\nFast R-CNN [29] solves this problem by sharing the features between proposals. The network is designed to only compute a feature map once per image in a fully convolutional style, and to use ROI-pooling to dynamically\n4 sample features from the feature map for each object proposal. The network also adopts a multi-task loss, i.e. classification loss and bounding-box regression loss. Based on the two improvements, the framework is trained endto-end. The processing time for each image significantly reduced to 0.3s. Fast R-CNN accelerates the detection network using the ROI-pooling layer. However the region proposal step is designed out of the network hence still remains a bottleneck, which results in sub-optimal solution and dependence on the external region proposal methods.\nFaster R-CNN [30] addresses the problem with fast RCNN by introducing the Region Proposal Network (RPN). An RPN is implemented in a fully convolutional style to predict the object bounding boxes and the objectness scores. In addition, the anchors are defined with different scales and ratios to achieve the translation invariance. The RPN shares the full-image convolution features with the detection network. Therefore the whole system is able to complete both proposal generation and detection computation within 0.2s using very deep VGG-16 model [21]. With a smaller ZF model [31], it can reach the level of real-time processing."
        },
        {
            "heading": "3.2 Limitations of Faster R-CNN",
            "text": "The Region-based CNN family, e.g. Faster R-CNN and its variants [29], achieves the state-of-the-art performance results in object detection on the PASCAL VOC dataset. These methods can detect objects such as vehicles, animals, people, chairs, and etc. with very high accuracy. In general, the defined objects often occupy the majority of a given image. However, when these methods are tested on the challenging Microsoft COCO dataset [32], the performance drops a lot, since images contain more small, occluded and incomplete objects. Similar situations happen in the problem of face detection. We focus on detecting only facial regions that are sometimes small, heavily occluded and of low resolution (as shown in Figure 1).\nThe detection network in designed Faster R-CNN is unable to robustly detect such tiny faces. The intuition point is that the Regions of Interest pooling layer, i.e. ROIpooling layer, builds features only from the last single high level feature map. For example, the global stride of the \u2019conv5\u2019 layer in the VGG-16 model is 16. Therefore, given a facial region with the sizes less than 16\u00d7 16 pixels in an image, the projected ROI-pooling region for that location will be less than 1 pixel in the \u2019conv5\u2019 layer, even if the proposed region is correct. Thus, the detector will have much difficulty to predict the object class and the bounding box location based on information from only one pixel."
        },
        {
            "heading": "3.3 Other Face Detection Method Limitations",
            "text": "Other challenges in object detection in the wild include occlusion and low-resolution. For face detection, it is very common for people to wear stuffs like sunglasses, scarf and hats, which occlude the face. In such cases, the methods that only extract features from faces do not work well. For example, Faceness [16] consider finding faces through scoring facial parts responses by their spatial structure and\narrangement, which works well on clear faces. But when facial parts are missing due to occlusion or when face itself is too small, facial parts become more hard to detect. Therefore, the body context information plays its role. As an example of context-dependent objects, faces often come together with human body. Even though the faces are occluded, we can still locate it only by seeing the whole human body. Similar advantages for faces at low-resolution, i.e. tiny faces. The deep features can not tell much about tiny faces since their receptive field is too small to be informative. Introducing context information can extend the area to extract features and make them meaningful. On the other hand, the context information also helped with reducing false detection as discussed previously, since context information tells the difference between real faces with bodies and face-like patterns without bodies."
        },
        {
            "heading": "4 CONTEXTUAL MULTI-SCALE R-CNN",
            "text": "Our goal is to detect human faces captured under various challenging conditions such as strong illumination, heavily occlusion, extreme off-angles, and low resolution. Under these conditions, the current CNN-based detection systems suffer from two major problems, i.e. 1) tiny faces are hard to identify; 2) only face region is taken into consideration for classification. In this section, we show why these problems hinder the ability of a face detection system. Then, our proposed network is presented to address these problems by using the Multi-Scale Region Proposal Network (MSRPN) and the Contextual Multi-Scale Convolution Neural Network (CMS-CNN), as illustrated in Figure 2. Similar to Faster R-CNN, the MS-RPN outputs several region candidates and the CMS-CNN computes the confidence score and bounding box for each candidate."
        },
        {
            "heading": "4.1 Identifying Tiny Faces",
            "text": "Why tiny faces are hard to be robustly detected by the previous region-based CNNs? The reason is that in these networks both the proposed region and the classification score are produced from one single high-level convolution feature map. This representation doesn\u2019t have enough information for the multiple tasks, i.e. region proposal and RoI detection. For example, Faster R-CNN generates region candidates and does RoI-pooling from the \u2019conv5\u2019 layer of the VGG-16 model, which has a overall stride of 16. One issue is that the reception field in this layer is quite large. When the face size is less than 16-by-16 pixels, the corresponding output in \u2019conv5\u2019 layer is less than 1 pixel, which is insufficient to encode informative features. The other issue is that as the convolution layers go deeper, each pixel in the feature map gather more and more information outside the original input region so that it contains lower proportion of information for the region of interest. These two issues together make the last convolution layer less representative for tiny faces.\n5"
        },
        {
            "heading": "4.1.1 Multiple Scale Faster-RCNN",
            "text": "Our solution for this problem is a combination of both global and local features, i.e. multiple scales. In this architecture, the feature maps are incorporated from lower level convolution layers with the last convolution layer for both MS-RPN and CMS-CNN. Features from lower convolution layer help get more information for the tiny faces, because stride in lower convolution layer will not be too small. Another benefit is that both low-level feature with localization capability and high-level feature with semantic information are fused together [33], since face detection needs to localize the face as well as to identify the face. In the MS-RPN, the whole lower level feature maps are down-sampled to the size of high level feature map and then concatenated with it to form a unified feature map. Then we reduce the dimension of the unified feature map and use it to generate region candidates. In the CMS-CNN, the region proposal is projected into feature maps from multiple convolution layers. And RoI-pooling is performed in each layer, resulting in a fixed-size feature tensor. All feature tensors are normalized, concatenated and dimension-reduced to a single feature blob, which is forwarded to two fully connected layers to compute a representation of the region candidate."
        },
        {
            "heading": "4.1.2 L2 Normalization",
            "text": "In both MS-RPN and CMS-CNN, concatenation of feature maps is done with L2 normalization layer [34], shown in Fig. 2, since the feature maps from different layer have generally different properties in terms of numbers of channels, scale of value and norm of feature map pixels. Generally, comparing with values in shallower layers, the values in deeper layers are usually too small, which leads to the dominance of shallower layers. In practice, it is impossible for the system to readjust and tune value from each layer for best performance. Therefore, L2 normalization layers before concatenation are crucial for the robustness of the system because it keeps the value from each layer in roughly the same scale.\nThe normalization is performed within each pixel, and all feature map is treated independently:\nx\u0302 = x\n\u2016x\u20162\n\u2016x\u20162 = ( d\u2211\ni=1\n|xi|) 1 2\nwhere the x and x\u0302 stand for the original pixel vector and the normalized pixel vector respectively. d stands for the number of channels in each feature map tensor.\nDuring training, scaling factors \u03b3i will be updated to readjust the scale of the normalized features. For each channel i, the scaling factor follows:\nyi = \u03b3ix\u0302i\nwhere yi stand for the re-scaled feature value.\nFollowing the back-propagation and chain rule, the update for scaling factor \u03b3 is:\n\u2202l \u2202x\u0302 = \u2202l \u2202y \u00b7 \u03b3\n\u2202l \u2202x = \u2202l \u2202x\u0302\n( I\n\u2016x\u20162 \u2212 xx\nT\n\u2016x\u201632 ) \u2202l\n\u2202\u03b3i = \u2211 yi \u2202l \u2202yi x\u0302i\nwhere y = [y1, y2, ..., yd] T ."
        },
        {
            "heading": "4.1.3 New Layer in Deep Learning Caffe Framework",
            "text": "The system integrate information from lower layer feature maps, i.e. third and fourth convolution layers, to extract determinant features for tiny faces. For both parts of our system, i.e. MS-RPN and CMS-CNN, the L2 normalization layers are inserted before concatenation of feature maps from the three layers. The features were re-scaled to proper values and concatenated to a single feature map. We set the initial scaling factor in a special way, following two rules. First, the average scale for each feature map is roughly identical; second, after the following 1\u00d71 convolution, the resulting tensor should have the same average scale as the conv5 layer in the work of Faster R-CNN. As implied, after the following 1 \u00d7 1 convolution, the tensor should be the same as the original architecture in Faster R-CNN, in terms of its size, scale of values and function for the downstream process."
        },
        {
            "heading": "4.2 Integrating Body Context",
            "text": "When humans are searching for faces, they try to look for not only the facial patterns, e.g. eyes, nose, mouth, but also the human bodies. Sometimes a human body makes us more convinced about the existence of a face. In addition, sometimes human body helps to reject false positives. If we only look at face regions, we may make mistakes identifying them. For example, Figure 3 shows two cases where body region plays a significant role for correct detection. This intuition is not only true for human but also valid in computer vision. Previous research has shown that contextual reasoning is a critical piece of the object recognition puzzle, and that context not only reduces the overall detection errors, but, more importantly, the remaining errors made by the detector are more reasonable [24]. Based on this intuition, our network is designed to make explicit reference to the human body context information in the RoI detection.\nIn our proposed network, the contextual body reasoning is implemented by explicitly grouping body information from convolution feature maps shown as the red blocks in Figure 2. Specifically, additional RoI-pooling operations are performed for each region proposal in convolution feature maps to represent the body context features. Then same as the face feature tensors, these body feature tensors are normalized, concatenated and dimension-reduced to a single feature blob. After two fully connected layers the\n6\nfinal body representation is concatenated with the face representation. They together contribute to the computation of confidence score and bounding box regression.\nWith projected region proposal as the face region, the additional RoI-pooling region represents the body region and satisfies a pre-defined spatial relation with the face region. In order to model this spatial relation, we make a simple hypothesis that if there is a face, there must exist a body, and the spatial relation between each face and body is fixed. This assumption may not be true all the time but should cover most of the scenarios since most people we see in the real world are either standing or sitting. Therefore, the spatial relation is roughly fixed between the face and the vertical body. Mathematically, this spatial relation can be represented by four parameters presented in Equation 1.\ntx = (xb \u2212 xf )/wf ty = (yb \u2212 yf )/hf tw = log(wb/wf )\nth = log(hb/hf )\n(1)\nwhere x(\u2217), y(\u2217), w(\u2217), and h(\u2217) denote the two coordinates of the box center, width, and height respectively. And b and f stand for body and face respectively. tx, ty , tw, and th are the parameters. Through out this paper, we fix the for parameters such that the two projected RoI regions of face and body satisfies a certain spatial ratio illustrated in the famous drawing in Figure 4."
        },
        {
            "heading": "4.3 Information Fusion",
            "text": "It\u2019s worth noticing that in our deep network architecture we have multiple face feature maps and body context feature maps for each proposed region. A critical issue is how we effectively fuse these information, i.e. what computation to apply and in which stage.\nIn our network, features extracted from different convolution layers need to be fused together to get a uniform representation. They cannot be naively concatenated due to the overall differences of the numbers of channels, scales of values and norms of feature map pixels among these layers. The detailed research shows that the deeper layers often contain smaller values than the shallower layers. Therefore,\nthe larger values will dominate the smaller ones, making the system rely too much on shallower features rather than a combination of multiple scale features causing the system to no longer be robust. We adopt the normalization layer from [34] to address this problem. The system takes the multiple scale features and apply L2 normalization along the channel axis of each feature map. Then, since the channel size is different among layers, the normalized feature map from each layer needed to be re-weighted, so that their values are at the same scale. After that, the feature maps are concatenated to one single feature map tensor. This modification helps to stabilize the system and increase the accuracy. Finally, the channel size of the concatenated feature map is shrunk to fit right in the original architecture for the downstream fully-connected layers.\nAnother crucial question is whether to fuse the face information and the body information at a early stage or at the very end of the network. Here we choose the late fusion strategy in which face features and body context features are extracted in two parallel pipelines. At the very end of the network two representations for face and body context are concatenated together to form a long feature vector. Then this feature vector is forwarded to compute confidence score and bounding box regression. The other strategy is the early fusion, in which face feature maps and body context feature maps get concatenated right after RoI pooling and normalization. These two strategies both combine the information from face and body context, but we prefer the late fusion. The reason is that we want the network to make decisions in a more semantic space. We care more about the existence of the face and the body. The localization information is already encoded in the predefined spatial relation mentioned in Section 4.2. Moreover empirical experiments also show that late fusion strategy works better."
        },
        {
            "heading": "4.4 Implementation Details",
            "text": "Our CMS-RCNN is implemented in the Caffe deep learning framework [35]. The first 5 sets of convolution layers have\n7 the same architecture as the deep VGG-16 model, and during training their parameters are initialized from the pre-trained VGG-16. For simplicity we refer to the last convolution layers in set 3, 4 and 5 as \u2019conv3\u2019, \u2019conv4\u2019, and \u2019conv5\u2019 respectively. All the following layers are connected exclusively to these three layers. In the MS-RPN, we want \u2019conv3\u2019, \u2019conv4\u2019, and \u2019conv5\u2019 to be synchronized to the same size so that concatenation can be applied. So \u2019conv3\u2019 is followed by pooling layer to perform down-sampling. Then \u2019conv3\u2019, \u2019conv4\u2019, and \u2019conv5\u2019 are normalized along the channel axis to a learnable re-weighting scale and concatenated together. To ensure training convergence, the initial re-weighting scale needs to be carefully set. Here we set the initial scale of \u2019conv3\u2019, \u2019conv4\u2019, and \u2019conv5\u2019 to be 66.84, 94.52, and 94.52 respectively. In the CMS-CNN, the RoI pooling layer already ensure that the pooled feature maps have the same size. Again we normalize the pooled features to make sure the downstream values are at reasonable scales when training is initialized. Specifically, features pooled from \u2019conv3\u2019, \u2019conv4\u2019, and \u2019conv5\u2019 are initialized with scale to be 57.75, 81.67, and 81.67 respectively, for both face and body pipelines. The MS-RPN and the CMSCNN share the same parameters for all convolution layers so that computation can be done once, resulting in higher efficiency. Additionally, in order to shrink the channel size of the concatenated feature map, a 1\u00d71 convolution layer is then employed. Therefore the channel size of final feature map is at the same size as the original fifth convolution layer in Faster R-CNN."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "This section presents the face detection bechmarking using our proposed CMS-RCNN approach on the WIDER FACE dataset [1] and the Face Detection Data Set and Benchmark (FDDB) [20] database. The WIDER FACE dataset is experimented with high degree of variability. Using this database, our proposed approach robustly outperforms strong baseline methods, including Two-stage CNN [1], Multi-scale Cascade CNN [1], Faceness [16] and Aggregate Channel Features (ACF) [11], by a large margin. We also show that our model trained on WIDER FACE dataset generalizes well enough to the FDDB database. The trained model consistently achieves competitive results against the recent state-of-the-art face detection methods on this database, including HyperFace [19], DP2MFD [17], CCF [18], Faceness [16], NPDFace [13], MultiresHPM [12], DDFD [15], CascadeCNN [14], ACF-multiscale [11], Pico [7], HeadHunter [9], Joint Cascade [10], Boosted Exemplar [8], and PEP-Adapt [6]."
        },
        {
            "heading": "5.1 Experiments on WIDER FACE Dataset",
            "text": "Data description WIDER FACE is a public face detection benchmark dataset. It contains 393,703 labeled human faces from 32,203 images collected based on 61 event classes from internet. The database has many human faces with a high degree of pose variation, large occlusions, low-resolutions and\nstrong lighting conditions. The images in this database are organized and split into three subsets, i.e. training, validation and testing. Each contains 40%, 10% and 50% respectively of the original databases. The images and the ground-truth labels of the training and the validation sets are available online for experiments. However, in the testing set, only the testing images (not the ground-truth labels) are available online. All detection results are sent to the database server for evaluating and receiving the PrecisionRecall curves.\nIn our experiments, the proposed CMS-RCNN is trained on the training set of the WIDER FACE dataset containing 159,424 annotated faces collected in 12,880 images. The trained model on this database are used in testing of all databases.\nTesting and Comparison During the testing phase, the face images in the testing set are divided into three parts based on their detection rates on EdgeBox [36]. In other words, face images are divided into three levels according to the difficulties of the detection, i.e. Easy, Medium and Hard [1]. The proposed CMS-RCNN model is compared against recent strong face detection methods, i.e. Two-stage CNN [1], Multiscale Cascade CNN [1], Faceness [16], and Aggregate Channel Features (ACF) [11]. All these methods are trained on the same training set and tested on the same testing set.\nThe Precision-Recall curves and AP values are shown in Figure 5. Our method outperforms those strong baselines by a large margin. It achieves the best average precision in all level faces, i.e. AP = 0.902 (Easy), 0.874 (Medium) and 0.643 (Hard), and outperforms the second best baseline by 26.0% (Easy), 37.4% (Medium) and 60.8% (Hard). These results suggest that as the difficulty level goes up, CMSRCNN can detect challenging faces better. So it has the ability to handle difficult conditions hence is more closed to human detection level. Figure 8 shows some examples of face detection results using the proposed CMS-RCNN on this database.\nWith Context v.s. Without Context As we show in Section 4.2 that human vision can benefit from additional context information for better detection and recognition, we show in this section how does explicit contextual reasoning in the network help improve the model performance.\nTo prove this, we test our models with and without body context information on the validation set of WIDER FACE dataset. The model without body context is implemented by removing the context pipeline and only use the representation from face pipeline to compute the confidence score and the bounding box regression. We compare their performances as illustrated in Figure 6. The Faster R-CNN method is setup as a baseline.\nStarting from 0 in recall, two curves of our models are overlapped at first, which means that two models perform as well as each other on some easy faces. Then the curve of model without context starts to drop quicker than the\nmodel with context, suggesting the model with context can handle the challenging conditions better when faces become more and more difficult. Thus eventually the model with context achieves a higher recall value. Additionally, the context model produces a longer PR curve, which means that contextual reasoning can help finding more faces.\nVisualization of False Positives As it is well known that precision-recall curves get dropped due to the false positives, we are interested in the false\npositives produced by our CMS-RCNN model. We are curious about what object can fool our model to treat it as a face. Is it due to over-fitting, data bias, or miss labeling?\nIn order to visualize the false positives, we test the CMSRCNN model on the WIDER FACE validation set and pick all the false positives according to the ground truth. Then those positives are sorted by the confidence score in a descending order. We choose the top 20 false positives as illustrated in Figure 9 Because their confidence scores are high, they are the objects most likely to cause our model making mistakes. It turns out that most of the false positives are actually human faces caused by miss labeling, which is a problem of the dataset itself. For other false positives, we find the errors made by our model are rather reasonable. They all have the pattern of human face as well as the shape of human body."
        },
        {
            "heading": "5.2 Experiments on FDDB Face Database",
            "text": "To show that our method generalizes well to other database, the proposed CMS-RCNN is also benchmarked on the FDDB database [20]. It is a standard database for testing and evaluation of face detection algorithms. It contains annotations for 5,171 faces in a set of 2,845 images taken from the Faces in the Wild dataset. Most of the images in the FDDB database contain less than 3 faces that are clear or slightly occluded. The faces generally have large sizes and high resolutions compared to WIDER FACE. We use the same model trained on WIDER FACE training set presented in Section 5.1 to perform the evaluation on the FDDB database.\nThe evaluation is performed based on the discrete criterion following the same rules in PASCAL VOC Challenge [37], i.e. if the ratio of the intersection of a detected region with an annotated face region is greater than 0.5, it is considered as a true positive detection. The evaluation is proceeded following the FDDB evaluation protocol and\n9\ncompared against the published methods provided in the protocol, i.e. HyperFace [19], DP2MFD [17], CCF [18], Faceness [16], NPDFace [13], MultiresHPM [12], DDFD [15], CascadeCNN [14], ACF-multiscale [11], Pico [7], HeadHunter [9], Joint Cascade [10], Boosted Exemplar [8], and PEP-Adapt [6]. The proposed CMS-RCNN approach outperforms most of the published face detection methods and achieves a very high recall rate comparing against all other methods (as shown Figure 7). This is concrete evidence to demonstrate that CMS-RCNN robustly detects unconstrained faces. Figure 10 shows some examples of the face detection results using the proposed CMS-RCNN on the FDDB dataset."
        },
        {
            "heading": "6 CONCLUSION AND FUTURE WORK",
            "text": "This paper has presented our proposed CMS-RCNN approach to robustly detect human facial regions from images collected under various challenging conditions, e.g. highly occlusions, low resolutions, facial expressions, illumination variations, etc. The approach is benchmarked on two challenging face detection databases, i.e. the WIDER FACE Dataset and the FDDB, and compared against recent other face detection methods. The experimental results show that our proposed approach outperforms strong baselines on the WIDER FACE and consistently achieves very competitive results against state-of-the-art methods on the FDDB.\nIn our implementation, the proposed CMS-RCNN consists of the MS-RPN and the CMS-CNN. During training, they are merged together in an approximate joint training style for each SGD iteration, in which the derivatives w.r.t. the proposal boxes\u2019 coordinates are ignored. In the future we want to go to the fully joint training so that the network can be trained in end-to-end fashion."
        }
    ],
    "title": "CMS-RCNN: Contextual Multi-Scale Region-based CNN for Unconstrained Face Detection",
    "year": 2016
}