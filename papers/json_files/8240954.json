{
    "abstractText": "Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are \u201cscaled exponential linear units\u201d (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance \u2014 even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization schemes, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs, and other machine learning methods such as random forests and support vector machines. For FNNs we considered (i) ReLU networks without normalization, (ii) batch normalization, (iii) layer normalization, (iv) weight normalization, (v) highway networks, and (vi) residual networks. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs.",
    "authors": [
        {
            "affiliations": [],
            "name": "G\u00fcnter Klambauer"
        },
        {
            "affiliations": [],
            "name": "Thomas Unterthiner"
        },
        {
            "affiliations": [],
            "name": "Andreas Mayr"
        },
        {
            "affiliations": [],
            "name": "Sepp Hochreiter"
        }
    ],
    "id": "SP:75403a36af130afbfdbfdb06de9b285d07728df3",
    "references": [
        {
            "authors": [
                "M. Abramowitz",
                "I. Stegun"
            ],
            "title": "Handbook of Mathematical Functions, volume 55 of Applied Mathematics Series",
            "venue": "National Bureau of Standards,",
            "year": 1964
        },
        {
            "authors": [
                "Y. Bengio"
            ],
            "title": "Deep learning of representations: Looking forward",
            "venue": "In Proceedings of the First International Conference on Statistical Language and Speech Processing,",
            "year": 2013
        },
        {
            "authors": [
                "J. Blinn"
            ],
            "title": "Consider the lowly 2\u00d72 matrix",
            "venue": "IEEE Computer Graphics and Applications,",
            "year": 1996
        },
        {
            "authors": [
                "R.C. Bradley"
            ],
            "title": "Central limit theorems under weak dependence",
            "venue": "Journal of Multivariate Analysis,",
            "year": 1981
        },
        {
            "authors": [
                "D. Cire\u015fan",
                "U. Meier"
            ],
            "title": "Multi-column deep neural networks for offline handwritten chinese character classification",
            "venue": "In 2015 International Joint Conference on Neural Networks (IJCNN),",
            "year": 2015
        },
        {
            "authors": [
                "Clevert",
                "D.-A",
                "T. Unterthiner",
                "S. Hochreiter"
            ],
            "title": "Fast and accurate deep network learning by exponential linear units (ELUs)",
            "venue": "5th International Conference on Learning Representations,",
            "year": 2015
        },
        {
            "authors": [
                "P. Dugan",
                "C. Clark",
                "Y. LeCun",
                "S. Van Parijs"
            ],
            "title": "Phase 4: Dcl system using deep learning approaches for land-based or ship-based real-time recognition and localization of marine mammals-distributed processing and big data applications",
            "venue": "arXiv preprint arXiv:1605.00982",
            "year": 2016
        },
        {
            "authors": [
                "A. Esteva",
                "B. Kuprel",
                "R. Novoa",
                "J. Ko",
                "S. Swetter",
                "H. Blau",
                "S. Thrun"
            ],
            "title": "Dermatologist-level classification of skin cancer",
            "year": 2017
        },
        {
            "authors": [
                "M. Fern\u00e1ndez-Delgado",
                "E. Cernadas",
                "S. Barro",
                "D. Amorim"
            ],
            "title": "Do we need hundreds of classifiers to solve real world classification problems",
            "venue": "Journal of Machine Learning Research,",
            "year": 2014
        },
        {
            "authors": [
                "D. Goldberg"
            ],
            "title": "What every computer scientist should know about floating-point arithmetic",
            "venue": "ACM Comput. Surv.,",
            "year": 1991
        },
        {
            "authors": [
                "A. Graves",
                "A. Mohamed",
                "G. Hinton"
            ],
            "title": "Speech recognition with deep recurrent neural networks. In IEEE International conference on acoustics, speech and signal processing",
            "year": 2013
        },
        {
            "authors": [
                "A. Graves",
                "J. Schmidhuber"
            ],
            "title": "Offline handwriting recognition with multidimensional recurrent neural networks",
            "venue": "In Advances in neural information processing systems,",
            "year": 2009
        },
        {
            "authors": [
                "V. Gulshan",
                "L. Peng",
                "M. Coram",
                "M.C. Stumpe",
                "D. Wu",
                "A. Narayanaswamy",
                "S. Venugopalan",
                "K. Widner",
                "T. Madams",
                "J Cuadros"
            ],
            "title": "Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs",
            "year": 2016
        },
        {
            "authors": [
                "J. Harrison"
            ],
            "title": "A machine-checked theory of floating point arithmetic",
            "venue": "editors, Theorem Proving in Higher Order Logics: 12th International Conference, TPHOLs\u201999,",
            "year": 1999
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2015
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision (ICCV),",
            "year": 2015
        },
        {
            "authors": [
                "S. Hochreiter",
                "J. Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural Computation,",
            "year": 1997
        },
        {
            "authors": [
                "B. Huval",
                "T. Wang",
                "S Tandon"
            ],
            "title": "An empirical evaluation of deep learning on highway driving",
            "venue": "arXiv preprint arXiv:1504.01716",
            "year": 2015
        },
        {
            "authors": [
                "S. Ioffe",
                "C. Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "In Proceedings of The 32nd International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "W. Kahan"
            ],
            "title": "A logarithm too clever by half",
            "venue": "Technical report,",
            "year": 2004
        },
        {
            "authors": [
                "V. Korolev",
                "I. Shevtsova"
            ],
            "title": "An improvement of the Berry\u2013Esseen inequality with applications to Poisson and mixed Poisson random sums",
            "venue": "Scandinavian Actuarial Journal,",
            "year": 2012
        },
        {
            "authors": [
                "A. Krizhevsky",
                "I. Sutskever",
                "G. Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2012
        },
        {
            "authors": [
                "Y. LeCun",
                "Y. Bengio"
            ],
            "title": "Convolutional networks for images, speech, and time series",
            "venue": "The handbook of brain theory and neural networks,",
            "year": 1995
        },
        {
            "authors": [
                "S. Loosemore",
                "R.M. Stallman",
                "R. McGrath",
                "A. Oram",
                "U. Drepper"
            ],
            "title": "The GNU C Library: Application Fundamentals",
            "venue": "GNU Press, Free Software Foundation,",
            "year": 2016
        },
        {
            "authors": [
                "R. Lyon",
                "B. Stappers",
                "S. Cooper",
                "J. Brooke",
                "J. Knowles"
            ],
            "title": "Fifty years of pulsar candidate selection: From simple filters to a new principled real-time classification approach",
            "venue": "Monthly Notices of the Royal Astronomical Society,",
            "year": 2016
        },
        {
            "authors": [
                "A. Mayr",
                "G. Klambauer",
                "T. Unterthiner",
                "S. Hochreiter"
            ],
            "title": "DeepTox: Toxicity prediction using deep learning",
            "venue": "Frontiers in Environmental Science,",
            "year": 2016
        },
        {
            "authors": [
                "Muller",
                "J.-M"
            ],
            "title": "On the definition of ulp(x)",
            "venue": "Technical Report Research report RR2005-09, Laboratoire de l\u2019Informatique du Paralle\u0301lisme",
            "year": 2005
        },
        {
            "authors": [
                "C. Ren",
                "A.R. MacKenzie"
            ],
            "title": "Closed-form approximations to the error and complementary error functions and their applications in atmospheric science",
            "venue": "Atmos. Sci. Let.,",
            "year": 2007
        },
        {
            "authors": [
                "H. Sak",
                "A. Senior",
                "K. Rao",
                "F. Beaufays"
            ],
            "title": "Fast and accurate recurrent neural network acoustic models for speech recognition",
            "venue": "arXiv preprint arXiv:1507.06947",
            "year": 2015
        },
        {
            "authors": [
                "T. Salimans",
                "D.P. Kingma"
            ],
            "title": "Weight normalization: A simple reparameterization to accelerate training of deep neural networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "J. Schmidhuber"
            ],
            "title": "Deep learning in neural networks: An overview",
            "venue": "Neural Networks,",
            "year": 2015
        },
        {
            "authors": [
                "D. Silver",
                "A. Huang",
                "C Maddison"
            ],
            "title": "Mastering the game of Go with deep neural networks and tree search",
            "year": 2016
        },
        {
            "authors": [
                "R.K. Srivastava",
                "K. Greff",
                "J. Schmidhuber"
            ],
            "title": "Training very deep networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2015
        },
        {
            "authors": [
                "I. Sutskever",
                "O. Vinyals",
                "Q.V. Le"
            ],
            "title": "Sequence to sequence learning with neural networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2014
        }
    ],
    "sections": [
        {
            "text": "Introduction\nDeep Learning has set new records at different benchmarks and led to various commercial applications [25, 33]. Recurrent neural networks (RNNs) [18] achieved new levels at speech and natural language processing, for example at the TIMIT benchmark [12] or at language translation [36], and are already employed in mobile devices [31]. RNNs have won handwriting recognition challenges (Chinese and Arabic handwriting) [33, 13, 6] and Kaggle challenges, such as the \u201cGrasp-and Lift EEG\u201d competition. Their counterparts, convolutional neural networks (CNNs) [24] excel at vision and video tasks. CNNs are on par with human dermatologists at the visual detection of skin cancer [9]. The visual processing for self-driving cars is based on CNNs [19], as is the visual input to AlphaGo which has beaten one of the best human GO players [34]. At vision challenges, CNNs are constantly winning, for example\nar X\niv :1\n70 6.\n02 51\n5v 1\n[ cs\n.L G\n] 8\nJ un\nat the large ImageNet competition [23, 16], but also almost all Kaggle vision challenges, such as the \u201cDiabetic Retinopathy\u201d and the \u201cRight Whale\u201d challenges [8, 14].\nHowever, looking at Kaggle challenges that are not related to vision or sequential tasks, gradient boosting, random forests, or support vector machines (SVMs) are winning most of the competitions. Deep Learning is notably absent, and for the few cases where FNNs won, they are shallow. For example, the HIGGS challenge, the Merck Molecular Activity challenge, and the Tox21 Data challenge were all won by FNNs with at most four hidden layers. Surprisingly, it is hard to find success stories with FNNs that have many hidden layers, though they would allow for different levels of abstract representations of the input [3].\nTo robustly train very deep CNNs, batch normalization evolved into a standard to normalize neuron activations to zero mean and unit variance [20]. Layer normalization [2] also ensures zero mean and unit variance, while weight normalization [32] ensures zero mean and unit variance if in the previous layer the activations have zero mean and unit variance. However, training with normalization techniques is perturbed by stochastic gradient descent (SGD), stochastic regularization (like dropout), and the estimation of the normalization parameters. Both RNNs and CNNs can stabilize learning via weight sharing, therefore they are less prone to these perturbations. In contrast, FNNs trained with normalization techniques suffer from these perturbations and have high variance in the training error (see Figure 1). This high variance hinders learning and slows it down. Furthermore, strong regularization, such as dropout, is not possible as it would further increase the variance which in turn would lead to divergence of the learning process. We believe that this sensitivity to perturbations is the reason that FNNs are less successful than RNNs and CNNs.\nSelf-normalizing neural networks (SNNs) are robust to perturbations and do not have high variance in their training errors (see Figure 1). SNNs push neuron activations to zero mean and unit variance thereby leading to the same effect as batch normalization, which enables to robustly learn many layers. SNNs are based on scaled exponential linear units \u201cSELUs\u201d which induce self-normalizing properties like variance stabilization which in turn avoids exploding and vanishing gradients.\nSelf-normalizing Neural Networks (SNNs)\nNormalization and SNNs. For a neural network with activation function f , we consider two consecutive layers that are connected by a weight matrix W . Since the input to a neural network is a random variable, the activations x in the lower layer, the network inputs z = Wx, and the activations y = f(z) in the higher layer are random variables as well. We assume that all activations xi of the lower layer have mean \u00b5 := E(xi) and variance \u03bd := Var(xi). An activation y in the higher layer has mean \u00b5\u0303 := E(y) and variance \u03bd\u0303 := Var(y). Here E(.) denotes the expectation and Var(.) the variance of a random variable. A single activation y = f(z) has net input z = wTx. For n units with activation xi, 1 6 i 6 n in the lower layer, we define n times the mean of the weight vector w \u2208 Rn as \u03c9 := \u2211n i=1 wi and n times the second moment as \u03c4 := \u2211n i=1 w 2 i .\nWe consider the mapping g that maps mean and variance of the activations from one layer to mean and variance of the activations in the next layer\n( \u00b5 \u03bd ) 7\u2192 ( \u00b5\u0303 \u03bd\u0303 ) : ( \u00b5\u0303 \u03bd\u0303 ) = g ( \u00b5 \u03bd ) . (1)\nNormalization techniques like batch, layer, or weight normalization ensure a mapping g that keeps (\u00b5, \u03bd) and (\u00b5\u0303, \u03bd\u0303) close to predefined values, typically (0, 1). Definition 1 (Self-normalizing neural net). A neural network is self-normalizing if it possesses a mapping g : \u2126 7\u2192 \u2126 for each activation y that maps mean and variance from one layer to the next and has a stable and attracting fixed point depending on (\u03c9, \u03c4) in \u2126. Furthermore, the mean and the variance remain in the domain \u2126, that is g(\u2126) \u2286 \u2126, where \u2126 = {(\u00b5, \u03bd) | \u00b5 \u2208 [\u00b5min, \u00b5max], \u03bd \u2208 [\u03bdmin, \u03bdmax]}. When iteratively applying the mapping g, each point within \u2126 converges to this fixed point.\nTherefore, we consider activations of a neural network to be normalized, if both their mean and their variance across samples are within predefined intervals. If mean and variance of x are already within\nthese intervals, then also mean and variance of y remain in these intervals, i.e., the normalization is transitive across layers. Within these intervals, the mean and variance both converge to a fixed point if the mapping g is applied iteratively.\nTherefore, SNNs keep normalization of activations when propagating them through layers of the network. The normalization effect is observed across layers of a network: in each layer the activations are getting closer to the fixed point. The normalization effect can also observed be for two fixed layers across learning steps: perturbations of lower layer activations or weights are damped in the higher layer by drawing the activations towards the fixed point. If for all y in the higher layer, \u03c9 and \u03c4 of the corresponding weight vector are the same, then the fixed points are also the same. In this case we have a unique fixed point for all activations y. Otherwise, in the more general case, \u03c9 and \u03c4 differ for different y but the mean activations are drawn into [\u00b5min, \u00b5max] and the variances are drawn into [\u03bdmin, \u03bdmax].\nConstructing Self-Normalizing Neural Networks. We aim at constructing self-normalizing neural networks by adjusting the properties of the function g. Only two design choices are available for the function g: (1) the activation function and (2) the initialization of the weights.\nFor the activation function, we propose \u201cscaled exponential linear units\u201d (SELUs) to render a FNN as self-normalizing. The SELU activation function is given by\nselu(x) = \u03bb { x if x > 0 \u03b1ex \u2212 \u03b1 if x 6 0 . (2)\nSELUs allow to construct a mapping g with properties that lead to SNNs. SNNs cannot be derived with (scaled) rectified linear units (ReLUs), sigmoid units, tanh units, and leaky ReLUs. The activation function is required to have (1) negative and positive values for controlling the mean, (2) saturation regions (derivatives approaching zero) to dampen the variance if it is too large in the lower layer, (3) a slope larger than one to increase the variance if it is too small in the lower layer, (4) a continuous curve. The latter ensures a fixed point, where variance damping is equalized by variance increasing. We met these properties of the activation function by multiplying the exponential linear unit (ELU) [7] with \u03bb > 1 to ensure a slope larger than one for positive net inputs.\nFor the weight initialization, we propose \u03c9 = 0 and \u03c4 = 1 for all units in the higher layer. The next paragraphs will show the advantages of this initialization. Of course, during learning these assumptions on the weight vector will be violated. However, we can prove the self-normalizing property even for weight vectors that are not normalized, therefore, the self-normalizing property can be kept during learning and weight changes.\nDeriving the Mean and Variance Mapping Function g. We assume that the xi are independent from each other but share the same mean \u00b5 and variance \u03bd. Of course, the independence assumptions is not fulfilled in general. We will elaborate on the independence assumption below. The network input z in the higher layer is z = wTx for which we can infer the following moments E(z) =\u2211n i=1 wi E(xi) = \u00b5 \u03c9 and Var(z) = Var( \u2211n i=1 wi xi) = \u03bd \u03c4 , where we used the independence of the xi. The net input z is a weighted sum of independent, but not necessarily identically distributed variables xi, for which the central limit theorem (CLT) states that z approaches a normal distribution: z \u223c N (\u00b5\u03c9, \u221a \u03bd\u03c4) with density pN(z;\u00b5\u03c9, \u221a \u03bd\u03c4). According to the CLT, the larger n, the closer is z to a normal distribution. For Deep Learning, broad layers with hundreds of neurons xi are common. Therefore the assumption that z is normally distributed is met well for most currently used neural networks (see Figure A8). The function g maps the mean and variance of activations in the lower layer to the mean \u00b5\u0303 = E(y) and variance \u03bd\u0303 = Var(y) of the activations y in the next layer:\ng : ( \u00b5 \u03bd ) 7\u2192 ( \u00b5\u0303 \u03bd\u0303 ) : \u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4) = \u222b \u221e \u2212\u221e selu(z) pN(z;\u00b5\u03c9, \u221a \u03bd\u03c4) dz (3)\n\u03bd\u0303(\u00b5, \u03c9, \u03bd, \u03c4) = \u222b \u221e \u2212\u221e selu(z)2 pN(z;\u00b5\u03c9, \u221a \u03bd\u03c4) dz \u2212 (\u00b5\u0303)2 .\nThese integrals can be analytically computed and lead to following mappings of the moments:\n\u00b5\u0303 = 1\n2 \u03bb\n( (\u00b5\u03c9) erf ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + (4)\n\u03b1 e\u00b5\u03c9+ \u03bd\u03c4 2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 \u03b1 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + \u221a 2 \u03c0 \u221a \u03bd\u03c4e\u2212 (\u00b5\u03c9)2 2(\u03bd\u03c4) + \u00b5\u03c9 )\n\u03bd\u0303 = 1 2 \u03bb2 (( (\u00b5\u03c9)2 + \u03bd\u03c4 )( 2\u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 )) + \u03b12 ( \u22122e\u00b5\u03c9+ \u03bd\u03c42 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) (5)\n+e2(\u00b5\u03c9+\u03bd\u03c4) erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) + erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 )) + \u221a 2 \u03c0 (\u00b5\u03c9) \u221a \u03bd\u03c4e\u2212 (\u00b5\u03c9)2 2(\u03bd\u03c4) ) \u2212 (\u00b5\u0303)2\nStable and Attracting Fixed Point (0,1) for Normalized Weights. We assume a normalized weight vector w with \u03c9 = 0 and \u03c4 = 1. Given a fixed point (\u00b5, \u03bd), we can solve equations Eq. (4) and Eq. (5) for \u03b1 and \u03bb. We chose the fixed point (\u00b5, \u03bd) = (0, 1), which is typical for activation normalization. We obtain the fixed point equations \u00b5\u0303 = \u00b5 = 0 and \u03bd\u0303 = \u03bd = 1 that we solve for \u03b1 and \u03bb and obtain the solutions \u03b101 \u2248 1.6733 and \u03bb01 \u2248 1.0507, where the subscript 01 indicates that these are the parameters for fixed point (0, 1). The analytical expressions for \u03b101 and \u03bb01 are given in Eq. (14). We are interested whether the fixed point (\u00b5, \u03bd) = (0, 1) is stable and attracting. If the Jacobian of g has a norm smaller than 1 at the fixed point, then g is a contraction mapping and the fixed point is stable. The (2x2)-Jacobian J (\u00b5, \u03bd) of g : (\u00b5, \u03bd) 7\u2192 (\u00b5\u0303, \u03bd\u0303) evaluated at the fixed point (0, 1) with \u03b101 and \u03bb01 is\nJ (\u00b5, \u03bd) = \u2202 \u00b5 new(\u00b5,\u03bd) \u2202\u00b5 \u2202 \u00b5new(\u00b5,\u03bd) \u2202\u03bd\n\u2202 \u03bd new(\u00b5,\u03bd) \u2202\u00b5 \u2202 \u03bdnew(\u00b5,\u03bd) \u2202\u03bd\n , J (0, 1) = (0.0 0.0888340.0 0.782648 ) . (6)\nThe spectral norm of J (0, 1) (its largest singular value) is 0.7877 < 1. That means g is a contraction mapping around the fixed point (0, 1) (the mapping is depicted in Figure 2). Therefore, (0, 1) is a stable fixed point of the mapping g.\nStable and Attracting Fixed Points for Unnormalized Weights. A normalized weight vector w cannot be ensured during learning. For SELU parameters \u03b1 = \u03b101 and \u03bb = \u03bb01, we show in the next theorem that if (\u03c9, \u03c4) is close to (0, 1), then g still has an attracting and stable fixed point that is close to (0, 1). Thus, in the general case there still exists a stable fixed point which, however, depends on (\u03c9, \u03c4). If we restrict (\u00b5, \u03bd, \u03c9, \u03c4) to certain intervals, then we can show that (\u00b5, \u03bd) is mapped to\nthe respective intervals. Next we present the central theorem of this paper, from which follows that SELU networks are self-normalizing under mild conditions on the weights.\nTheorem 1 (Stable and Attracting Fixed Points). We assume \u03b1 = \u03b101 and \u03bb = \u03bb01. We restrict the range of the variables to the following intervals \u00b5 \u2208 [\u22120.1, 0.1], \u03c9 \u2208 [\u22120.1, 0.1], \u03bd \u2208 [0.8, 1.5], and \u03c4 \u2208 [0.95, 1.1], that define the functions\u2019 domain \u2126. For \u03c9 = 0 and \u03c4 = 1, the mapping Eq. (3) has the stable fixed point (\u00b5, \u03bd) = (0, 1), whereas for other \u03c9 and \u03c4 the mapping Eq. (3) has a stable and attracting fixed point depending on (\u03c9, \u03c4) in the (\u00b5, \u03bd)-domain: \u00b5 \u2208 [\u22120.03106, 0.06773] and \u03bd \u2208 [0.80009, 1.48617]. All points within the (\u00b5, \u03bd)-domain converge when iteratively applying the mapping Eq. (3) to this fixed point.\nProof. We provide a proof sketch (see detailed proof in Appendix Section A3). With the Banach fixed point theorem we show that there exists a unique attracting and stable fixed point. To this end, we have to prove that a) g is a contraction mapping and b) that the mapping stays in the domain, that is, g(\u2126) \u2286 \u2126. The spectral norm of the Jacobian of g can be obtained via an explicit formula for the largest singular value for a 2\u00d7 2 matrix. g is a contraction mapping if its spectral norm is smaller than 1. We perform a computer-assisted proof to evaluate the largest singular value on a fine grid and ensure the precision of the computer evaluation by an error propagation analysis of the implemented algorithms on the according hardware. Singular values between grid points are upper bounded by the mean value theorem. To this end, we bound the derivatives of the formula for the largest singular value with respect to \u03c9, \u03c4, \u00b5, \u03bd. Then we apply the mean value theorem to pairs of points, where one is on the grid and the other is off the grid. This shows that for all values of \u03c9, \u03c4, \u00b5, \u03bd in the domain \u2126, the spectral norm of g is smaller than one. Therefore, g is a contraction mapping on the domain \u2126. Finally, we show that the mapping g stays in the domain \u2126 by deriving bounds on \u00b5\u0303 and \u03bd\u0303. Hence, the Banach fixed-point theorem holds and there exists a unique fixed point in \u2126 that is attained.\nConsequently, feed-forward neural networks with many units in each layer and with the SELU activation function are self-normalizing (see definition 1), which readily follows from Theorem 1. To give an intuition, the main property of SELUs is that they damp the variance for negative net inputs and increase the variance for positive net inputs. The variance damping is stronger if net inputs are further away from zero while the variance increase is stronger if net inputs are close to zero. Thus, for large variance of the activations in the lower layer the damping effect is dominant and the variance decreases in the higher layer. Vice versa, for small variance the variance increase is dominant and the variance increases in the higher layer.\nHowever, we cannot guarantee that mean and variance remain in the domain \u2126. Therefore, we next treat the case where (\u00b5, \u03bd) are outside \u2126. It is especially crucial to consider \u03bd because this variable has much stronger influence than \u00b5. Mapping \u03bd across layers to a high value corresponds to an exploding gradient, since the Jacobian of the activation of high layers with respect to activations in lower layers has large singular values. Analogously, mapping \u03bd across layers to a low value corresponds to an vanishing gradient. Bounding the mapping of \u03bd from above and below would avoid both exploding and vanishing gradients. Theorem 2 states that the variance of neuron activations of\nSNNs is bounded from above, and therefore ensures that SNNs learn robustly and do not suffer from exploding gradients.\nTheorem 2 (Decreasing \u03bd). For \u03bb = \u03bb01, \u03b1 = \u03b101 and the domain \u2126+: \u22121 6 \u00b5 6 1, \u22120.1 6 \u03c9 6 0.1, 3 6 \u03bd 6 16, and 0.8 6 \u03c4 6 1.25, we have for the mapping of the variance \u03bd\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) given in Eq. (5): \u03bd\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) < \u03bd.\nThe proof can be found in the Appendix Section A3. Thus, when mapped across many layers, the variance in the interval [3, 16] is mapped to a value below 3. Consequently, all fixed points (\u00b5, \u03bd) of the mapping g (Eq. (3)) have \u03bd < 3. Analogously, Theorem 3 states that the variance of neuron activations of SNNs is bounded from below, and therefore ensures that SNNs do not suffer from vanishing gradients.\nTheorem 3 (Increasing \u03bd). We consider \u03bb = \u03bb01, \u03b1 = \u03b101 and the domain \u2126\u2212: \u22120.1 6 \u00b5 6 0.1, and \u22120.1 6 \u03c9 6 0.1. For the domain 0.02 6 \u03bd 6 0.16 and 0.8 6 \u03c4 6 1.25 as well as for the domain 0.02 6 \u03bd 6 0.24 and 0.9 6 \u03c4 6 1.25, the mapping of the variance \u03bd\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) given in Eq. (5) increases: \u03bd\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) > \u03bd.\nThe proof can be found in the Appendix Section A3. All fixed points (\u00b5, \u03bd) of the mapping g (Eq. (3)) ensure for 0.8 6 \u03c4 that \u03bd\u0303 > 0.16 and for 0.9 6 \u03c4 that \u03bd\u0303 > 0.24. Consequently, the variance mapping Eq. (5) ensures a lower bound on the variance \u03bd. Therefore SELU networks control the variance of the activations and push it into an interval, whereafter the mean and variance move toward the fixed point. Thus, SELU networks are steadily normalizing the variance and subsequently normalizing the mean, too. In all experiments, we observed that self-normalizing neural networks push the mean and variance of activations into the domain \u2126 .\nInitialization. Since SNNs have a fixed point at zero mean and unit variance for normalized weights \u03c9 = \u2211n i=1 wi = 0 and \u03c4 = \u2211n i=1 w 2 i = 1 (see above), we initialize SNNs such that these constraints are fulfilled in expectation. We draw the weights from a Gaussian distribution with E(wi) = 0 and variance Var(wi) = 1/n. Uniform and truncated Gaussian distributions with these moments led to networks with similar behavior. The \u201cMSRA initialization\u201d is similar since it uses zero mean and variance 2/n to initialize the weights [17]. The additional factor 2 counters the effect of rectified linear units.\nNew Dropout Technique. Standard dropout randomly sets an activation x to zero with probability 1 \u2212 q for 0 < q 6 1. In order to preserve the mean, the activations are scaled by 1/q during training. If x has mean E(x) = \u00b5 and variance Var(x) = \u03bd, and the dropout variable d follows a binomial distribution B(1, q), then the mean E(1/qdx) = \u00b5 is kept. Dropout fits well to rectified linear units, since zero is in the low variance region and corresponds to the default value. For scaled exponential linear units, the default and low variance value is limx\u2192\u2212\u221e selu(x) = \u2212\u03bb\u03b1 = \u03b1\u2032. Therefore, we propose \u201calpha dropout\u201d, that randomly sets inputs to \u03b1\u2032. The new mean and new variance is E(xd + \u03b1\u2032(1 \u2212 d)) = q\u00b5 + (1 \u2212 q)\u03b1\u2032, and Var(xd + \u03b1\u2032(1 \u2212 d)) = q((1 \u2212 q)(\u03b1\u2032 \u2212 \u00b5)2 + \u03bd). We aim at keeping mean and variance to their original values after \u201calpha dropout\u201d, in order to ensure the self-normalizing property even for \u201calpha dropout\u201d. The affine transformation a(xd + \u03b1\u2032(1 \u2212 d)) + b allows to determine parameters a and b such that mean and variance are kept to their values: E(a(xd + \u03b1\u2032(1 \u2212 d)) + b) = \u00b5 and Var(a(xd + \u03b1\u2032(1 \u2212 d)) + b) = \u03bd . In contrast to dropout, a and b will depend on \u00b5 and \u03bd, however our SNNs converge to activations with zero mean and unit variance. With \u00b5 = 0 and \u03bd = 1, we obtain a = ( q + \u03b1\u20322q(1\u2212 q) )\u22121/2 and\nb = \u2212 ( q + \u03b1\u20322q(1\u2212 q) )\u22121/2 ((1\u2212 q)\u03b1\u2032). The parameters a and b only depend on the dropout rate 1\u2212 q and the most negative activation \u03b1\u2032. Empirically, we found that dropout rates 1\u2212 q = 0.05 or 0.10 lead to models with good performance. \u201cAlpha-dropout\u201d fits well to scaled exponential linear units by randomly setting activations to the negative saturation value.\nApplicability of the central limit theorem and independence assumption. In the derivative of the mapping (Eq. (3)), we used the central limit theorem (CLT) to approximate the network inputs z = \u2211n i=1 wixi with a normal distribution. We justified normality because network inputs represent a weighted sum of the inputs xi, where for Deep Learning n is typically large. The Berry-Esseen\ntheorem states that the convergence rate to normality is n\u22121/2 [22]. In the classical version of the CLT, the random variables have to be independent and identically distributed, which typically does not hold for neural networks. However, the Lyapunov CLT does not require the variable to be identically distributed anymore. Furthermore, even under weak dependence, sums of random variables converge in distribution to a Gaussian distribution [5].\nExperiments\nWe compare SNNs to other deep networks at different benchmarks. Hyperparameters such as number of layers (blocks), neurons per layer, learning rate, and dropout rate, are adjusted by grid-search for each dataset on a separate validation set (see Section A4). We compare the following FNN methods:\n\u2022 \u201cMSRAinit\u201d: FNNs without normalization and with ReLU activations and \u201cMicrosoft weight initialization\u201d [17].\n\u2022 \u201cMSRAinit\u201d: FNNs without normalization and with ReLU activations and \u201cMicrosoft weight initialization\u201d [17].\n\u2022 \u201cBatchNorm\u201d: FNNs with batch normalization [20]. \u2022 \u201cLayerNorm\u201d: FNNs with layer normalization [2]. \u2022 \u201cWeightNorm\u201d: FNNs with weight normalization [32]. \u2022 \u201cHighway\u201d: Highway networks [35]. \u2022 \u201cResNet\u201d: Residual networks [16] adapted to FNNs using residual blocks with 2 or 3 layers\nwith rectangular or diavolo shape.\n\u2022 \u201cSNNs\u201d: Self normalizing networks with SELUs with \u03b1 = \u03b101 and \u03bb = \u03bb01 and the proposed dropout technique and initialization strategy.\n121 UCI Machine Learning Repository datasets. The benchmark comprises 121 classification datasets from the UCI Machine Learning repository [10] from diverse application areas, such as physics, geology, or biology. The size of the datasets ranges between 10 and 130, 000 data points and the number of features from 4 to 250. In abovementioned work [10], there were methodological mistakes [37] which we avoided here. Each compared FNN method was optimized with respect to its architecture and hyperparameters on a validation set that was then removed from the subsequent analysis. The selected hyperparameters served to evaluate the methods in terms of accuracy on the pre-defined test sets (details on the hyperparameter selection are given in Section A4). The accuracies are reported in the Table A11. We ranked the methods by their accuracy for each prediction task and compared their average ranks. SNNs significantly outperform all competing networks in pairwise comparisons (paired Wilcoxon test across datasets) as reported in Table 1 (left panel).\nWe further included 17 machine learning methods representing diverse method groups [10] in the comparison and the grouped the data sets into \u201csmall\u201d and \u201clarge\u201d data sets (for details see Section A4). On 75 small datasets with less than 1000 data points, random forests and SVMs outperform SNNs and other FNNs. On 46 larger datasets with at least 1000 data points, SNNs show the highest performance followed by SVMs and random forests (see right panel of Table 1, for complete results see Tables A12 and A12). Overall, SNNs have outperformed state of the art machine learning methods on UCI datasets with more than 1,000 data points.\nTypically, hyperparameter selection chose SNN architectures that were much deeper than the selected architectures of other FNNs, with an average depth of 10.8 layers, compared to average depths of 6.0 for BatchNorm, 3.8 WeightNorm, 7.0 LayerNorm, 5.9 Highway, and 7.1 for MSRAinit networks. For ResNet, the average number of blocks was 6.35. SNNs with many more than 4 layers often provide the best predictive accuracies across all neural networks.\nDrug discovery: The Tox21 challenge dataset. The Tox21 challenge dataset comprises about 12,000 chemical compounds whose twelve toxic effects have to be predicted based on their chemical structure. We used the validation sets of the challenge winners for hyperparameter selection (see\nSection A4) and the challenge test set for performance comparison. We repeated the whole evaluation procedure 5 times to obtain error bars. The results in terms of average AUC are given in Table 2. In 2015, the challenge organized by the US NIH was won by an ensemble of shallow ReLU FNNs which achieved an AUC of 0.846 [28]. Besides FNNs, this ensemble also contained random forests and SVMs. Single SNNs came close with an AUC of 0.845\u00b10.003. The best performing SNNs have 8 layers, compared to the runner-ups ReLU networks with layer normalization with 2 and 3 layers. Also batchnorm and weightnorm networks, typically perform best with shallow networks of 2 to 4 layers (Table 2). The deeper the networks, the larger the difference in performance between SNNs and other methods (see columns 5\u20138 of Table 2). The best performing method is an SNN with 8 layers.\nSNN 83.7 \u00b1 0.3 84.4 \u00b1 0.5 84.2 \u00b1 0.4 83.9 \u00b1 0.5 84.5 \u00b1 0.2 83.5 \u00b1 0.5 82.5 \u00b1 0.7 Batchnorm 80.0 \u00b1 0.5 79.8 \u00b1 1.6 77.2 \u00b1 1.1 77.0 \u00b1 1.7 75.0 \u00b1 0.9 73.7 \u00b1 2.0 76.0 \u00b1 1.1 WeightNorm 83.7 \u00b1 0.8 82.9 \u00b1 0.8 82.2 \u00b1 0.9 82.5 \u00b1 0.6 81.9 \u00b1 1.2 78.1 \u00b1 1.3 56.6 \u00b1 2.6 LayerNorm 84.3 \u00b1 0.3 84.3 \u00b1 0.5 84.0 \u00b1 0.2 82.5 \u00b1 0.8 80.9 \u00b1 1.8 78.7 \u00b1 2.3 78.8 \u00b1 0.8 Highway 83.3 \u00b1 0.9 83.0 \u00b1 0.5 82.6 \u00b1 0.9 82.4 \u00b1 0.8 80.3 \u00b1 1.4 80.3 \u00b1 2.4 79.6 \u00b1 0.8 MSRAinit 82.7 \u00b1 0.4 81.6 \u00b1 0.9 81.1 \u00b1 1.7 80.6 \u00b1 0.6 80.9 \u00b1 1.1 80.2 \u00b1 1.1 80.4 \u00b1 1.9 ResNet 82.2 \u00b1 1.1 80.0 \u00b1 2.0 80.5 \u00b1 1.2 81.2 \u00b1 0.7 81.8 \u00b1 0.6 81.2 \u00b1 0.6 na\nAstronomy: Prediction of pulsars in the HTRU2 dataset. Since a decade, machine learning methods have been used to identify pulsars in radio wave signals [27]. Recently, the High Time Resolution Universe Survey (HTRU2) dataset has been released with 1,639 real pulsars and 16,259 spurious signals. Currently, the highest AUC value of a 10-fold cross-validation is 0.976 which has been achieved by Naive Bayes classifiers followed by decision tree C4.5 with 0.949 and SVMs with 0.929. We used eight features constructed by the PulsarFeatureLab as used previously [27]. We assessed the performance of FNNs using 10-fold nested cross-validation, where the hyperparameters were selected in the inner loop on a validation set (for details on the hyperparameter selection see\nSection A4). Table 3 reports the results in terms of AUC. SNNs outperform all other methods and have pushed the state-of-the-art to an AUC of 0.98.\nWe have introduced self-normalizing neural networks for which we have proved that neuron activations are pushed towards zero mean and unit variance when propagated through the network. Additionally, for activations not close to unit variance, we have proved an upper and lower bound on the variance mapping. Consequently, SNNs do not face vanishing and exploding gradient problems. Therefore, SNNs work well for architectures with many layers, allowed us to introduce a novel regularization scheme, and learn very robustly. On 121 UCI benchmark datasets, SNNs have outperformed other FNNs with and without normalization techniques, such as batch, layer, and weight normalization, or specialized architectures, such as Highway or Residual networks. SNNs also yielded the best results on drug discovery and astronomy tasks. The best performing SNN architectures are typically very deep in contrast to other FNNs.\nReferences\nThe references are provided in Section A7.\nAppendix\nContents\nA1 Background 10\nA2 Theorems 12\nA2.1 Theorem 1: Stable and Attracting Fixed Points Close to (0,1) . . . . . . . . . . . . 12\nA2.2 Theorem 2: Decreasing Variance from Above . . . . . . . . . . . . . . . . . . . . 12\nA2.3 Theorem 3: Increasing Variance from Below . . . . . . . . . . . . . . . . . . . . . 12\nA3 Proofs of the Theorems 13\nA3.1 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\nA3.2 Proof of Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\nA3.3 Proof of Theorem 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nA3.4 Lemmata and Other Tools Required for the Proofs . . . . . . . . . . . . . . . . . . 19\nA3.4.1 Lemmata for proofing Theorem 1 (part 1): Jacobian norm smaller than one 19\nA3.4.2 Lemmata for proofing Theorem 1 (part 2): Mapping within domain . . . . 28\nA3.4.3 Lemmata for proofing Theorem 2: The variance is contracting . . . . . . . 29\nA3.4.4 Lemmata for proofing Theorem 3: The variance is expanding . . . . . . . 32\nA3.4.5 Computer-assisted proof details for main Lemma 12 in Section A3.4.1. . . 33\nA3.4.6 Intermediate Lemmata and Proofs . . . . . . . . . . . . . . . . . . . . . . 37\nA4 Additional information on experiments 84\nA4.1 121 UCI Machine Learning Repository data sets: Hyperparameters . . . . . . . . . 85\nA4.2 121 UCI Machine Learning Repository data sets: detailed results . . . . . . . . . . 87\nA4.3 Tox21 challenge data set: Hyperparameters . . . . . . . . . . . . . . . . . . . . . 92\nA4.4 HTRU2 data set: Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . 95\nA5 Other fixed points 97\nA6 Bounds determined by numerical methods 97\nA7 References 98\nList of figures 100\nList of tables 100\nBrief index 102\nThis appendix is organized as follows: the first section sets the background, definitions, and formulations. The main theorems are presented in the next section. The following section is devoted to the proofs of these theorems. The next section reports additional results and details on the performed computational experiments, such as hyperparameter selection. The last section shows that our theoretical bounds can be confirmed by numerical methods as a sanity check.\nThe proof of theorem 1 is based on the Banach\u2019s fixed point theorem for which we require (1) a contraction mapping, which is proved in Subsection A3.4.1 and (2) that the mapping stays within its domain, which is proved in Subsection A3.4.2 For part (1), the proof relies on the main Lemma 12, which is a computer-assisted proof, and can be found in Subsection A3.4.1. The validity of the computer-assisted proof is shown in Subsection A3.4.5 by error analysis and the precision of the functions\u2019 implementation. The last Subsection A3.4.6 compiles various lemmata with intermediate results that support the proofs of the main lemmata and theorems.\nA1 Background\nWe consider a neural network with activation function f and two consecutive layers that are connected by weight matrix W . Since samples that serve as input to the neural network are chosen according to a distribution, the activations x in the lower layer, the network inputs z = Wx, and activations y = f(z) in the higher layer are all random variables. We assume that all units xi in the lower layer have mean activation \u00b5 := E(xi) and variance of the activation \u03bd := Var(xi) and a unit y in the higher layer has mean activation \u00b5\u0303 := E(y) and variance \u03bd\u0303 := Var(y). Here E(.)\ndenotes the expectation and Var(.) the variance of a random variable. For activation of unit y, we have net input z = wTx and the scaled exponential linear unit (SELU) activation y = selu(z), with\nselu(x) = \u03bb { x if x > 0 \u03b1ex \u2212 \u03b1 if x 6 0 . (7)\nFor n units xi, 1 6 i 6 n in the lower layer and the weight vector w \u2208 Rn, we define n times the mean by \u03c9 := \u2211n i=1 wi and n times the second moment by \u03c4 := \u2211n i=1 w 2 i .\nWe define a mapping g from mean \u00b5 and variance \u03bd of one layer to the mean \u00b5\u0303 and variance \u03bd\u0303 in the next layer:\ng : (\u00b5, \u03bd) 7\u2192 (\u00b5\u0303, \u03bd\u0303) . (8)\nFor neural networks with scaled exponential linear units, the mean is of the activations in the next layer computed according to\n\u00b5\u0303 = \u222b 0 \u2212\u221e \u03bb\u03b1(exp(z)\u2212 1)pGauss(z;\u00b5\u03c9, \u221a \u03bd\u03c4)dz + \u222b \u221e 0 \u03bbzpGauss(z;\u00b5\u03c9, \u221a \u03bd\u03c4)dz , (9)\nand the second moment of the activations in the next layer is computed according to\n\u03be\u0303 = \u222b 0 \u2212\u221e \u03bb2\u03b12(exp(z)\u2212 1)2pGauss(z;\u00b5\u03c9, \u221a \u03bd\u03c4)dz + \u222b \u221e 0 \u03bb2z2pGauss(z;\u00b5\u03c9, \u221a \u03bd\u03c4)dz . (10)\nTherefore, the expressions \u00b5\u0303 and \u03bd\u0303 have the following form:\n\u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) = 1\n2 \u03bb\n( \u2212(\u03b1+ \u00b5\u03c9) erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + (11)\n\u03b1e\u00b5\u03c9+ \u03bd\u03c4 2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) + \u221a 2\n\u03c0\n\u221a \u03bd\u03c4e\u2212 \u00b52\u03c92 2\u03bd\u03c4 + 2\u00b5\u03c9 ) \u03bd\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) = \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1)\u2212 (\u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1))2 (12)\n\u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) = 1 2 \u03bb2 (( (\u00b5\u03c9)2 + \u03bd\u03c4 )( erf ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 1 ) + (13)\n\u03b12 ( \u22122e\u00b5\u03c9+ \u03bd\u03c42 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) + e2(\u00b5\u03c9+\u03bd\u03c4) erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) +\nerfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 )) + \u221a 2 \u03c0 (\u00b5\u03c9) \u221a \u03bd\u03c4e\u2212 (\u00b5\u03c9)2 2(\u03bd\u03c4) )\nWe solve equations Eq. 4 and Eq. 5 for fixed points \u00b5\u0303 = \u00b5 and \u03bd\u0303 = \u03bd. For a normalized weight vector with \u03c9 = 0 and \u03c4 = 1 and the fixed point (\u00b5, \u03bd) = (0, 1), we can solve equations Eq. 4 and Eq. 5 for \u03b1 and \u03bb. We denote the solutions to fixed point (\u00b5, \u03bd) = (0, 1) by \u03b101 and \u03bb01.\n\u03b101 = \u2212\n\u221a 2 \u03c0\nerfc (\n1\u221a 2\n) exp ( 1 2 ) \u2212 1 \u2248 1.67326 (14)\n\u03bb01 =\n( 1\u2212 erfc ( 1\u221a 2 )\u221a e ) 2\u03c0(\n2 erfc (\u221a 2 ) e2 + \u03c0 erfc ( 1\u221a 2 )2 e\u2212 2(2 + \u03c0) erfc ( 1\u221a 2 )\u221a e+ \u03c0 + 2 )\u22121/2 \u03bb01 \u2248 1.0507 .\nThe parameters \u03b101 and \u03bb01 ensure\n\u00b5\u0303(0, 0, 1, 1, \u03bb01, \u03b101) = 0\n\u03bd\u0303(0, 0, 1, 1, \u03bb01, \u03b101) = 1\nSince we focus on the fixed point (\u00b5, \u03bd) = (0, 1), we assume throughout the analysis that \u03b1 = \u03b101 and \u03bb = \u03bb01. We consider the functions \u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101), \u03bd\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101), and \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) on the domain \u2126 = {(\u00b5, \u03c9, \u03bd, \u03c4) | \u00b5 \u2208 [\u00b5min, \u00b5max] = [\u22120.1, 0.1], \u03c9 \u2208 [\u03c9min, \u03c9max] = [\u22120.1, 0.1], \u03bd \u2208 [\u03bdmin, \u03bdmax] = [0.8, 1.5], \u03c4 \u2208 [\u03c4min, \u03c4max] = [0.95, 1.1]}. Figure 2 visualizes the mapping g for \u03c9 = 0 and \u03c4 = 1 and \u03b101 and \u03bb01 at few pre-selected points. It can be seen that (0, 1) is an attracting fixed point of the mapping g.\nA2 Theorems\nA2.1 Theorem 1: Stable and Attracting Fixed Points Close to (0,1)\nTheorem 1 shows that the mapping g defined by Eq. (4) and Eq. (5) exhibits a stable and attracting fixed point close to zero mean and unit variance. Theorem 1 establishes the self-normalizing property of self-normalizing neural networks (SNNs). The stable and attracting fixed point leads to robust learning through many layers. Theorem 1 (Stable and Attracting Fixed Points). We assume \u03b1 = \u03b101 and \u03bb = \u03bb01. We restrict the range of the variables to the domain \u00b5 \u2208 [\u22120.1, 0.1], \u03c9 \u2208 [\u22120.1, 0.1], \u03bd \u2208 [0.8, 1.5], and \u03c4 \u2208 [0.95, 1.1]. For \u03c9 = 0 and \u03c4 = 1, the mapping Eq. (4) and Eq. (5) has the stable fixed point (\u00b5, \u03bd) = (0, 1). For other \u03c9 and \u03c4 the mapping Eq. (4) and Eq. (5) has a stable and attracting fixed point depending on (\u03c9, \u03c4) in the (\u00b5, \u03bd)-domain: \u00b5 \u2208 [\u22120.03106, 0.06773] and \u03bd \u2208 [0.80009, 1.48617]. All points within the (\u00b5, \u03bd)-domain converge when iteratively applying the mapping Eq. (4) and Eq. (5) to this fixed point.\nA2.2 Theorem 2: Decreasing Variance from Above\nThe next Theorem 2 states that the variance of unit activations does not explode through consecutive layers of self-normalizing networks. Even more, a large variance of unit activations decreases when propagated through the network. In particular this ensures that exploding gradients will never be observed. In contrast to the domain in previous subsection, in which \u03bd \u2208 [0.8, 1.5], we now consider a domain in which the variance of the inputs is higher \u03bd \u2208 [3, 16] and even the range of the mean is increased \u00b5 \u2208 [\u22121, 1]. We denote this new domain with the symbol \u2126++ to indicate that the variance lies above the variance of the original domain \u2126. In \u2126++, we can show that the variance \u03bd\u0303 in the next layer is always smaller then the original variance \u03bd. Concretely, this theorem states that: Theorem 2 (Decreasing \u03bd). For \u03bb = \u03bb01, \u03b1 = \u03b101 and the domain \u2126++: \u22121 6 \u00b5 6 1, \u22120.1 6 \u03c9 6 0.1, 3 6 \u03bd 6 16, and 0.8 6 \u03c4 6 1.25 we have for the mapping of the variance \u03bd\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) given in Eq. (5)\n\u03bd\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) < \u03bd . (15) The variance decreases in [3, 16] and all fixed points (\u00b5, \u03bd) of mapping Eq. (5) and Eq. (4) have \u03bd < 3.\nA2.3 Theorem 3: Increasing Variance from Below\nThe next Theorem 3 states that the variance of unit activations does not vanish through consecutive layers of self-normalizing networks. Even more, a small variance of unit activations increases when propagated through the network. In particular this ensures that vanishing gradients will never be observed. In contrast to the first domain, in which \u03bd \u2208 [0.8, 1.5], we now consider two domains \u2126\u22121 and \u2126\u22122 in which the variance of the inputs is lower 0.05 6 \u03bd 6 0.16 and 0.05 6 \u03bd 6 0.24, and even the parameter \u03c4 is different 0.9 6 \u03c4 6 1.25 to the original \u2126. We denote this new domain with the symbol \u2126\u2212i to indicate that the variance lies below the variance of the original domain \u2126. In \u2126 \u2212 1 and \u2126\u22122 , we can show that the variance \u03bd\u0303 in the next layer is always larger then the original variance \u03bd, which means that the variance does not vanish through consecutive layers of self-normalizing networks. Concretely, this theorem states that:\nTheorem 3 (Increasing \u03bd). We consider \u03bb = \u03bb01, \u03b1 = \u03b101 and the two domains \u2126\u22121 = {(\u00b5, \u03c9, \u03bd, \u03c4) | \u2212 0.1 6 \u00b5 6 0.1,\u22120.1 6 \u03c9 6 0.1, 0.05 6 \u03bd 6 0.16, 0.8 6 \u03c4 6 1.25} and \u2126\u22122 = {(\u00b5, \u03c9, \u03bd, \u03c4) | \u2212 0.1 6 \u00b5 6 0.1,\u22120.1 6 \u03c9 6 0.1, 0.05 6 \u03bd 6 0.24, 0.9 6 \u03c4 6 1.25}. The mapping of the variance \u03bd\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) given in Eq. (5) increases\n\u03bd\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) > \u03bd (16)\nin both \u2126\u22121 and \u2126 \u2212 2 . All fixed points (\u00b5, \u03bd) of mapping Eq. (5) and Eq. (4) ensure for 0.8 6 \u03c4 that \u03bd\u0303 > 0.16 and for 0.9 6 \u03c4 that \u03bd\u0303 > 0.24. Consequently, the variance mapping Eq. (5) and Eq. (4) ensures a lower bound on the variance \u03bd.\nA3 Proofs of the Theorems\nA3.1 Proof of Theorem 1\nWe have to show that the mapping g defined by Eq. (4) and Eq. (5) has a stable and attracting fixed point close to (0, 1). To proof this statement and Theorem 1, we apply the Banach fixed point theorem which requires (1) that g is a contraction mapping and (2) that g does not map outside the function\u2019s domain, concretely:\nTheorem 4 (Banach Fixed Point Theorem). Let (X, d) be a non-empty complete metric space with a contraction mapping f : X \u2192 X . Then f has a unique fixed-point xf \u2208 X with f(xf ) = xf . Every sequence xn = f(xn\u22121) with starting element x0 \u2208 X converges to the fixed point: xn \u2212\u2212\u2212\u2212\u2192\nn\u2192\u221e xf .\nContraction mappings are functions that map two points such that their distance is decreasing:\nDefinition 2 (Contraction mapping). A function f : X \u2192 X on a metric space X with distance d is a contraction mapping, if there is a 0 6 \u03b4 < 1, such that for all points u and v in X: d(f(u), f(v)) 6 \u03b4d(u,v).\nTo show that g is a contraction mapping in \u2126 with distance \u2016.\u20162, we use the Mean Value Theorem for u, v \u2208 \u2126\n\u2016g(u)\u2212 g(v)\u20162 6M \u2016u\u2212 v\u20162, (17)\nin which M is an upper bound on the spectral norm the JacobianH of g. The spectral norm is given by the largest singular value of the Jacobian of g. If the largest singular value of the Jacobian is smaller than 1, the mapping g of the mean and variance to the mean and variance in the next layer is contracting. We show that the largest singular value is smaller than 1 by evaluating the function for the singular value S(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) on a grid. Then we use the Mean Value Theorem to bound the deviation of the function S between grid points. To this end, we have to bound the gradient of S with respect to (\u00b5, \u03c9, \u03bd, \u03c4). If all function values plus gradient times the deltas (differences between grid points and evaluated points) is still smaller than 1, then we have proofed that the function is below 1 (Lemma 12). To show that the mapping does not map outside the function\u2019s domain, we derive bounds on the expressions for the mean and the variance (Lemma 13). Section A3.4.1 and Section A3.4.2 are concerned with the contraction mapping and the image of the function domain of g, respectively.\nWith the results that the largest singular value of the Jacobian is smaller than one (Lemma 12) and that the mapping stays in the domain \u2126 (Lemma 13), we can prove Theorem 1. We first recall Theorem 1:\nTheorem (Stable and Attracting Fixed Points). We assume \u03b1 = \u03b101 and \u03bb = \u03bb01. We restrict the range of the variables to the domain \u00b5 \u2208 [\u22120.1, 0.1], \u03c9 \u2208 [\u22120.1, 0.1], \u03bd \u2208 [0.8, 1.5], and \u03c4 \u2208 [0.95, 1.1]. For \u03c9 = 0 and \u03c4 = 1, the mapping Eq. (4) and Eq. (5) has the stable fixed point (\u00b5, \u03bd) = (0, 1). For other \u03c9 and \u03c4 the mapping Eq. (4) and Eq. (5) has a stable and attracting fixed point depending on (\u03c9, \u03c4) in the (\u00b5, \u03bd)-domain: \u00b5 \u2208 [\u22120.03106, 0.06773] and \u03bd \u2208 [0.80009, 1.48617]. All points within the (\u00b5, \u03bd)-domain converge when iteratively applying the mapping Eq. (4) and Eq. (5) to this fixed point.\nProof. According to Lemma 12 the mapping g (Eq. (4) and Eq. (5)) is a contraction mapping in the given domain, that is, it has a Lipschitz constant smaller than one. We showed that (\u00b5, \u03bd) = (0, 1) is a fixed point of the mapping for (\u03c9, \u03c4) = (0, 1).\nThe domain is compact (bounded and closed), therefore it is a complete metric space. We further have to make sure the mapping g does not map outside its domain \u2126. According to Lemma 13, the mapping maps into the domain \u00b5 \u2208 [\u22120.03106, 0.06773] and \u03bd \u2208 [0.80009, 1.48617]. Now we can apply the Banach fixed point theorem given in Theorem 4 from which the statement of the theorem follows.\nA3.2 Proof of Theorem 2\nFirst we recall Theorem 2: Theorem (Decreasing \u03bd). For \u03bb = \u03bb01, \u03b1 = \u03b101 and the domain \u2126++: \u22121 6 \u00b5 6 1, \u22120.1 6 \u03c9 6 0.1, 3 6 \u03bd 6 16, and 0.8 6 \u03c4 6 1.25 we have for the mapping of the variance \u03bd\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) given in Eq. (5)\n\u03bd\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) < \u03bd . (18)\nThe variance decreases in [3, 16] and all fixed points (\u00b5, \u03bd) of mapping Eq. (5) and Eq. (4) have \u03bd < 3.\nProof. We start to consider an even larger domain \u22121 6 \u00b5 6 1, \u22120.1 6 \u03c9 6 0.1, 1.5 6 \u03bd 6 16, and 0.8 6 \u03c4 6 1.25. We prove facts for this domain and later restrict to 3 6 \u03bd 6 16, i.e. \u2126++. We consider the function g of the difference between the second moment \u03be\u0303 in the next layer and the variance \u03bd in the lower layer:\ng(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) = \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) \u2212 \u03bd . (19)\nIf we can show that g(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) < 0 for all (\u00b5, \u03c9, \u03bd, \u03c4) \u2208 \u2126++, then we would obtain our desired result \u03bd\u0303 6 \u03be\u0303 < \u03bd. The derivative with respect to \u03bd is according to Theorem 16:\n\u2202\n\u2202\u03bd g(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) =\n\u2202\n\u2202\u03bd \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) \u2212 1 < 0 . (20)\nTherefore g is strictly monotonically decreasing in \u03bd. Since \u03be\u0303 is a function in \u03bd\u03c4 (these variables only appear as this product), we have for x = \u03bd\u03c4\n\u2202\n\u2202\u03bd \u03be\u0303 =\n\u2202 \u2202x \u03be\u0303 \u2202x \u2202\u03bd = \u2202 \u2202x \u03be\u0303 \u03c4 (21)\nand \u2202\n\u2202\u03c4 \u03be\u0303 =\n\u2202 \u2202x \u03be\u0303 \u2202x \u2202\u03c4 = \u2202 \u2202x \u03be\u0303 \u03bd . (22)\nTherefore we have according to Theorem 16:\n\u2202\n\u2202\u03c4 \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) =\n\u03bd\n\u03c4\n\u2202\n\u2202\u03bd \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) > 0 . (23)\nTherefore \u2202\n\u2202\u03c4 g(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) =\n\u2202\n\u2202\u03c4 \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) > 0 . (24)\nConsequently, g is strictly monotonically increasing in \u03c4 . Now we consider the derivative with respect to \u00b5 and \u03c9. We start with \u2202\u2202\u00b5 \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1), which is\n\u2202\n\u2202\u00b5 \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) = (25)\n\u03bb2\u03c9 ( \u03b12 ( \u2212e\u00b5\u03c9+ \u03bd\u03c42 ) erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) +\n\u03b12e2\u00b5\u03c9+2\u03bd\u03c4 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) + \u00b5\u03c9 ( 2\u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 )) + \u221a 2 \u03c0 \u221a \u03bd\u03c4e\u2212 \u00b52\u03c92 2\u03bd\u03c4 ) .\nWe consider the sub-function\u221a 2\n\u03c0\n\u221a \u03bd\u03c4 \u2212 \u03b12 ( e ( \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 e ( \u00b5\u03c9+2\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n)) . (26)\nWe set x = \u03bd\u03c4 and y = \u00b5\u03c9 and obtain\u221a 2\n\u03c0\n\u221a x\u2212 \u03b12 ( e ( x+y\u221a 2 \u221a x )2 erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 e ( 2x+y\u221a 2 \u221a x )2 erfc ( 2x+ y\u221a\n2 \u221a x\n)) . (27)\nThe derivative to this sub-function with respect to y is \u03b12 ( e (2x+y)2 2x (2x+ y) erfc (\n2x+y\u221a 2 \u221a x\n) \u2212 e (x+y)2 2x (x+ y) erfc ( x+y\u221a 2 \u221a x )) x = (28)\n\u221a 2\u03b12 \u221a x  e (2x+y)22x (x+y) erfc( x+y\u221a2\u221ax)\u221a 2 \u221a x \u2212 e (x+y)2 2x (x+y) erfc ( x+y\u221a 2 \u221a x ) \u221a 2 \u221a x  x > 0 .\nThe inequality follows from Lemma 24, which states that zez 2\nerfc(z) is monotonically increasing in z. Therefore the sub-function is increasing in y. The derivative to this sub-function with respect to x is\n1 2 \u221a \u03c0x2 \u221a \u03c0\u03b12\n( e (2x+y)2 2x ( 4x2 \u2212 y2 ) erfc ( 2x+ y\u221a\n2 \u221a x\n) (29)\n\u2212e (x+y)2 2x (x\u2212 y)(x+ y) erfc ( x+ y\u221a\n2 \u221a x\n)) \u2212 \u221a 2 ( \u03b12 \u2212 1 ) x3/2.\nThe sub-function is increasing in x, since the derivative is larger than zero: \u221a \u03c0\u03b12 ( e (2x+y)2 2x ( 4x2 \u2212 y2 ) erfc ( 2x+y\u221a\n2 \u221a x\n) \u2212 e (x+y)2 2x (x\u2212 y)(x+ y) erfc ( x+y\u221a 2 \u221a x )) \u2212 \u221a 2x3/2 ( \u03b12 \u2212 1 ) 2 \u221a \u03c0x2 >\n(30)\n\u221a \u03c0\u03b12  (2x\u2212y)(2x+y)2\u221a \u03c0 ( 2x+y\u221a 2 \u221a x + \u221a( 2x+y\u221a 2 \u221a x )2 +2 ) \u2212 (x\u2212y)(x+y)2 \u221a \u03c0 ( x+y\u221a 2 \u221a x + \u221a( x+y\u221a 2 \u221a x )2 + 4\u03c0 ) \u2212\u221a2x3/2 (\u03b12 \u2212 1)\n2 \u221a \u03c0x2\n=\n\u221a \u03c0\u03b12\n( (2x\u2212y)(2x+y)2( \u221a 2 \u221a x)\n\u221a \u03c0 ( 2x+y+ \u221a (2x+y)2+4x ) \u2212 (x\u2212y)(x+y)2(\u221a2\u221ax)\u221a \u03c0 ( x+y+ \u221a (x+y)2+ 8x\u03c0 ))\u2212\u221a2x3/2 (\u03b12 \u2212 1) 2 \u221a \u03c0x2 =\n\u221a \u03c0\u03b12\n( (2x\u2212y)(2x+y)2\n\u221a \u03c0 ( 2x+y+ \u221a (2x+y)2+4x ) \u2212 (x\u2212y)(x+y)2\u221a \u03c0 ( x+y+ \u221a (x+y)2+ 8x\u03c0 ))\u2212 x (\u03b12 \u2212 1) \u221a\n2 \u221a \u03c0x3/2\n>\n\u221a \u03c0\u03b12\n( (2x\u2212y)(2x+y)2\n\u221a \u03c0 ( 2x+y+ \u221a (2x+y)2+2(2x+y)+1 ) \u2212 (x\u2212y)(x+y)2\u221a \u03c0 ( x+y+ \u221a (x+y)2+0.878\u00b72(x+y)+0.8782 ))\u2212 x (\u03b12 \u2212 1) \u221a\n2 \u221a \u03c0x3/2\n=\n\u221a \u03c0\u03b12\n( (2x\u2212y)(2x+y)2\n\u221a \u03c0 ( 2x+y+ \u221a (2x+y+1)2 ) \u2212 (x\u2212y)(x+y)2\u221a \u03c0 ( x+y+ \u221a (x+y+0.878)2 ))\u2212 x (\u03b12 \u2212 1) \u221a\n2 \u221a \u03c0x3/2\n=\n\u221a \u03c0\u03b12 ( (2x\u2212y)(2x+y)2\u221a \u03c0(2(2x+y)+1) \u2212 (x\u2212y)(x+y)2\u221a \u03c0(2(x+y)+0.878) ) \u2212 x ( \u03b12 \u2212 1 ) \u221a\n2 \u221a \u03c0x3/2\n=\n\u221a \u03c0\u03b12 ( (2(x+y)+0.878)(2x\u2212y)(2x+y)2\u221a\n\u03c0 \u2212 (x\u2212y)(x+y)(2(2x+y)+1)2\u221a \u03c0 ) (2(2x+ y) + 1)(2(x+ y) + 0.878) \u221a 2 \u221a \u03c0x3/2 +\n\u221a \u03c0\u03b12 ( \u2212x ( \u03b12 \u2212 1 ) (2(2x+ y) + 1)(2(x+ y) + 0.878) ) (2(2x+ y) + 1)(2(x+ y) + 0.878) \u221a 2 \u221a \u03c0x3/2 =\n8x3 + 12x2y + 4.14569x2 + 4xy2 \u2212 6.76009xy \u2212 1.58023x+ 0.683154y2\n(2(2x+ y) + 1)(2(x+ y) + 0.878) \u221a 2 \u221a \u03c0x3/2\n>\n8x3 \u2212 0.1 \u00b7 12x2 + 4.14569x2 + 4 \u00b7 (0.0)2x\u2212 6.76009 \u00b7 0.1x\u2212 1.58023x+ 0.683154 \u00b7 (0.0)2\n(2(2x+ y) + 1)(2(x+ y) + 0.878) \u221a 2 \u221a \u03c0x3/2\n=\n8x2 + 2.94569x\u2212 2.25624 (2(2x+ y) + 1)(2(x+ y) + 0.878) \u221a 2 \u221a \u03c0 \u221a x =\n8(x\u2212 0.377966)(x+ 0.746178) (2(2x+ y) + 1)(2(x+ y) + 0.878) \u221a 2 \u221a \u03c0 \u221a x > 0 .\nWe explain this chain of inequalities:\n\u2022 First inequality: We applied Lemma 22 two times. \u2022 Equalities factor out \u221a 2 \u221a x and reformulate.\n\u2022 Second inequality part 1: we applied\n0 < 2y =\u21d2 (2x+ y)2 + 4x+ 1 < (2x+ y)2 + 2(2x+ y) + 1 = (2x+ y + 1)2 . (31)\n\u2022 Second inequality part 2: we show that for a = 110 (\u221a 960+169\u03c0 \u03c0 \u2212 13 ) following holds:\n8x \u03c0 \u2212\n( a2 + 2a(x+ y) ) > 0. We have \u2202\u2202x 8x \u03c0 \u2212 ( a2 + 2a(x+ y) ) = 8\u03c0 \u2212 2a > 0 and\n\u2202 \u2202y 8x \u03c0 \u2212\n( a2 + 2a(x+ y) ) = \u22122a < 0. Therefore the minimum is at border for minimal x\nand maximal y:\n8 \u00b7 1.2 \u03c0 \u2212  2 10 (\u221a 960 + 169\u03c0 \u03c0 \u2212 13 ) (1.2 + 0.1) + ( 1 10 (\u221a 960 + 169\u03c0 \u03c0 \u2212 13 ))2 = 0 . (32)\nThus\n8x\n\u03c0 > a2 + 2a(x+ y) . (33)\nfor a = 110 (\u221a 960+169\u03c0 \u03c0 \u2212 13 ) > 0.878.\n\u2022 Equalities only solve square root and factor out the resulting terms (2(2x + y) + 1) and (2(x+ y) + 0.878).\n\u2022 We set \u03b1 = \u03b101 and multiplied out. Thereafter we also factored out x in the numerator. Finally a quadratic equations was solved.\nThe sub-function has its minimal value for minimal x = \u03bd\u03c4 = 1.5 \u00b7 0.8 = 1.2 and minimal y = \u00b5\u03c9 = \u22121 \u00b7 0.1 = \u22120.1. We further minimize the function\n\u00b5\u03c9e \u00b52\u03c92 2\u03bd\u03c4 ( 2\u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 )) > \u22120.1e 0.1 2 2\u00b71.2 ( 2\u2212 erfc ( 0.1\u221a 2 \u221a 1.2 )) . (34)\nWe compute the minimum of the term in brackets of \u2202\u2202\u00b5 \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) in Eq. (25):\n\u00b5\u03c9e \u00b52\u03c92 2\u03bd\u03c4 ( 2\u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 )) + (35)\n\u03b1201\n( \u2212 ( e ( \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 e ( \u00b5\u03c9+2\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n))) + \u221a 2\n\u03c0\n\u221a \u03bd\u03c4 >\n\u03b1201\n( \u2212 ( e ( 1.2\u22120.1\u221a 2 \u221a 1.2 )2 erfc ( 1.2\u2212 0.1\u221a\n2 \u221a 1.2\n) \u2212 e ( 2\u00b71.2\u22120.1\u221a 2 \u221a 1.2 )2 erfc ( 2 \u00b7 1.2\u2212 0.1\u221a\n2 \u221a 1.2\n))) \u2212\n0.1e 0.12 2\u00b71.2 ( 2\u2212 erfc ( 0.1\u221a 2 \u221a 1.2 )) + \u221a 1.2 \u221a 2 \u03c0 = 0.212234 .\nTherefore the term in brackets of Eq. (25) is larger than zero. Thus, \u2202\u2202\u00b5 \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) has the sign\nof \u03c9. Since \u03be\u0303 is a function in \u00b5\u03c9 (these variables only appear as this product), we have for x = \u00b5\u03c9 \u2202\n\u2202\u03bd \u03be\u0303 =\n\u2202 \u2202x \u03be\u0303 \u2202x \u2202\u00b5 = \u2202 \u2202x \u03be\u0303 \u03c9 (36)\nand \u2202\n\u2202\u03c9 \u03be\u0303 =\n\u2202 \u2202x \u03be\u0303 \u2202x \u2202\u03c9 = \u2202 \u2202x \u03be\u0303 \u00b5 . (37)\n\u2202\n\u2202\u03c9 \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) =\n\u00b5\n\u03c9\n\u2202\n\u2202\u00b5 \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) . (38)\nSince \u2202\u2202\u00b5 \u03be\u0303 has the sign of \u03c9, \u2202 \u2202\u00b5 \u03be\u0303 has the sign of \u00b5. Therefore\n\u2202\n\u2202\u03c9 g(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) =\n\u2202\n\u2202\u03c9 \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) (39)\nhas the sign of \u00b5.\nWe now divide the \u00b5-domain into \u22121 6 \u00b5 6 0 and 0 6 \u00b5 6 1. Analogously we divide the \u03c9-domain into \u22120.1 6 \u03c9 6 0 and 0 6 \u03c9 6 0.1. In this domains g is strictly monotonically. For all domains g is strictly monotonically decreasing in \u03bd and strictly monotonically increasing in \u03c4 . Note that we now consider the range 3 6 \u03bd 6 16. For the maximal value of g we set \u03bd = 3 (we set it to 3!) and \u03c4 = 1.25.\nWe consider now all combination of these domains:\n\u2022 \u22121 6 \u00b5 6 0 and \u22120.1 6 \u03c9 6 0: g is decreasing in \u00b5 and decreasing in \u03c9. We set \u00b5 = \u22121 and \u03c9 = \u22120.1.\ng(\u22121,\u22120.1, 3, 1.25, \u03bb01, \u03b101) = \u22120.0180173 . (40)\n\u2022 \u22121 6 \u00b5 6 0 and 0 6 \u03c9 6 0.1: g is increasing in \u00b5 and decreasing in \u03c9. We set \u00b5 = 0 and \u03c9 = 0.\ng(0, 0, 3, 1.25, \u03bb01, \u03b101) = \u22120.148532 . (41)\n\u2022 0 6 \u00b5 6 1 and \u22120.1 6 \u03c9 6 0: g is decreasing in \u00b5 and increasing in \u03c9. We set \u00b5 = 0 and \u03c9 = 0.\ng(0, 0, 3, 1.25, \u03bb01, \u03b101) = \u22120.148532 . (42)\n\u2022 0 6 \u00b5 6 1 and 0 6 \u03c9 6 0.1: g is increasing in \u00b5 and increasing in \u03c9. We set \u00b5 = 1 and \u03c9 = 0.1.\ng(1, 0.1, 3, 1.25, \u03bb01, \u03b101) = \u22120.0180173 . (43)\nTherefore the maximal value of g is \u22120.0180173.\nA3.3 Proof of Theorem 3\nFirst we recall Theorem 3:\nTheorem (Increasing \u03bd). We consider \u03bb = \u03bb01, \u03b1 = \u03b101 and the two domains \u2126\u22121 = {(\u00b5, \u03c9, \u03bd, \u03c4) | \u2212 0.1 6 \u00b5 6 0.1,\u22120.1 6 \u03c9 6 0.1, 0.05 6 \u03bd 6 0.16, 0.8 6 \u03c4 6 1.25} and \u2126\u22122 = {(\u00b5, \u03c9, \u03bd, \u03c4) | \u2212 0.1 6 \u00b5 6 0.1,\u22120.1 6 \u03c9 6 0.1, 0.05 6 \u03bd 6 0.24, 0.9 6 \u03c4 6 1.25} . The mapping of the variance \u03bd\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) given in Eq. (5) increases\n\u03bd\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) > \u03bd (44)\nin both \u2126\u22121 and \u2126 \u2212 2 . All fixed points (\u00b5, \u03bd) of mapping Eq. (5) and Eq. (4) ensure for 0.8 6 \u03c4 that \u03bd\u0303 > 0.16 and for 0.9 6 \u03c4 that \u03bd\u0303 > 0.24. Consequently, the variance mapping Eq. (5) and Eq. (4) ensures a lower bound on the variance \u03bd.\nProof. The mean value theorem states that there exists a t \u2208 [0, 1] for which\n\u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) \u2212 \u03be\u0303(\u00b5, \u03c9, \u03bdmin, \u03c4, \u03bb01, \u03b101) = (45) \u2202 \u2202\u03bd \u03be\u0303(\u00b5, \u03c9, \u03bd + t(\u03bdmin \u2212 \u03bd), \u03c4, \u03bb01, \u03b101) (\u03bd \u2212 \u03bdmin) .\nTherefore\n\u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) = \u03be\u0303(\u00b5, \u03c9, \u03bdmin, \u03c4, \u03bb01, \u03b101) + (46) \u2202 \u2202\u03bd \u03be\u0303(\u00b5, \u03c9, \u03bd + t(\u03bdmin \u2212 \u03bd), \u03c4, \u03bb01, \u03b101) (\u03bd \u2212 \u03bdmin) .\nTherefore we are interested to bound the derivative of the \u03be-mapping Eq. (13) with respect to \u03bd:\n\u2202\n\u2202\u03bd \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) = (47)\n1 2 \u03bb2\u03c4e\u2212 \u00b52\u03c92 2\u03bd\u03c4\n( \u03b12 ( \u2212 ( e ( \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 2e ( \u00b5\u03c9+2\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n))) \u2212\nerfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2 ) .\nThe sub-term Eq. (308) enters the derivative Eq. (47) with a negative sign! According to Lemma 18, the minimal value of sub-term Eq. (308) is obtained by the largest largest \u03bd, by the smallest \u03c4 , and the largest y = \u00b5\u03c9 = 0.01. Also the positive term erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2 is multiplied by \u03c4 , which is minimized by using the smallest \u03c4 . Therefore we can use the smallest \u03c4 in whole formula Eq. (47) to lower bound it.\nFirst we consider the domain 0.05 6 \u03bd 6 0.16 and 0.8 6 \u03c4 6 1.25. The factor consisting of the exponential in front of the brackets has its smallest value for e\u2212 0.01\u00b70.01 2\u00b70.05\u00b70.8 . Since erfc is monotonically decreasing we inserted the smallest argument via erfc ( \u2212 0.01\u221a\n2 \u221a 0.05\u00b70.8\n) in order to obtain the maximal\nnegative contribution. Thus, applying Lemma 18, we obtain the lower bound on the derivative:\n1 2 \u03bb2\u03c4e\u2212 \u00b52\u03c92 2\u03bd\u03c4\n( \u03b12 ( \u2212 ( e ( \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 2e ( \u00b5\u03c9+2\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n))) \u2212\n(48)\nerfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2 ) >\n1 2 0.8e\u2212 0.01\u00b70.01 2\u00b70.05\u00b70.8\u03bb201\n( \u03b1201 ( \u2212 ( e ( 0.16\u00b70.8+0.01\u221a 2 \u221a 0.16\u00b70.8 )2 erfc ( 0.16 \u00b7 0.8 + 0.01\u221a\n2 \u221a 0.16 \u00b7 0.8\n) \u2212\n2e\n( 2\u00b70.16\u00b70.8+0.01\u221a\n2 \u221a 0.16\u00b70.8 )2 erfc ( 2 \u00b7 0.16 \u00b7 0.8 + 0.01\u221a\n2 \u221a 0.16 \u00b7 0.8\n))) \u2212 erfc ( \u2212 0.01\u221a\n2 \u221a 0.05 \u00b7 0.8\n) + 2 ) ) > 0.969231 .\nFor applying the mean value theorem, we require the smallest \u03bd\u0303(\u03bd). We follow the proof of Lemma 8, which shows that at the minimum y = \u00b5\u03c9 must be maximal and x = \u03bd\u03c4 must be minimal. Thus, the smallest \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) is \u03be\u0303(0.01, 0.01, 0.05, 0.8, \u03bb01, \u03b101) = 0.0662727 for 0.05 6 \u03bd and 0.8 6 \u03c4 .\nTherefore the mean value theorem and the bound on (\u00b5\u0303)2 (Lemma 43) provide\n\u03bd\u0303 = \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101)\u2212 (\u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101))2 > (49) 0.0662727 + 0.969231(\u03bd \u2212 0.05)\u2212 0.005 = 0.01281115 + 0.969231\u03bd > 0.08006969 \u00b7 0.16 + 0.969231\u03bd > 1.049301\u03bd > \u03bd .\nNext we consider the domain 0.05 6 \u03bd 6 0.24 and 0.9 6 \u03c4 6 1.25. The factor consisting of the exponential in front of the brackets has its smallest value for e\u2212 0.01\u00b70.01 2\u00b70.05\u00b70.9 . Since erfc is monotonically decreasing we inserted the smallest argument via erfc ( \u2212 0.01\u221a\n2 \u221a 0.05\u00b70.9\n) in order to obtain the maximal\nnegative contribution.\nThus, applying Lemma 18, we obtain the lower bound on the derivative: 1 2 \u03bb2\u03c4e\u2212 \u00b52\u03c92 2\u03bd\u03c4 ( \u03b12 ( \u2212 ( e ( \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) \u2212 2e ( \u00b5\u03c9+2\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ))) \u2212\n(50)\nerfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2 ) >\n1 2 0.9e\u2212 0.01\u00b70.01 2\u00b70.05\u00b70.9\u03bb201\n( \u03b1201 ( \u2212 ( e ( 0.24\u00b70.9+0.01\u221a 2 \u221a 0.24\u00b70.9 )2 erfc ( 0.24 \u00b7 0.9 + 0.01\u221a\n2 \u221a 0.24 \u00b7 0.9\n) \u2212\n2e\n( 2\u00b70.24\u00b70.9+0.01\u221a\n2 \u221a 0.24\u00b70.9 )2 erfc ( 2 \u00b7 0.24 \u00b7 0.9 + 0.01\u221a\n2 \u221a 0.24 \u00b7 0.9\n))) \u2212 erfc ( \u2212 0.01\u221a\n2 \u221a 0.05 \u00b7 0.9\n) + 2 ) ) > 0.976952 .\nFor applying the mean value theorem, we require the smallest \u03bd\u0303(\u03bd). We follow the proof of Lemma 8, which shows that at the minimum y = \u00b5\u03c9 must be maximal and x = \u03bd\u03c4 must be minimal. Thus, the smallest \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) is \u03be\u0303(0.01, 0.01, 0.05, 0.9, \u03bb01, \u03b101) = 0.0738404 for 0.05 6 \u03bd and 0.9 6 \u03c4 . Therefore the mean value theorem and the bound on (\u00b5\u0303)2 (Lemma 43) gives\n\u03bd\u0303 = \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101)\u2212 (\u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101))2 > (51) 0.0738404 + 0.976952(\u03bd \u2212 0.05)\u2212 0.005 = 0.0199928 + 0.976952\u03bd > 0.08330333 \u00b7 0.24 + 0.976952\u03bd > 1.060255\u03bd > \u03bd .\nA3.4 Lemmata and Other Tools Required for the Proofs\nA3.4.1 Lemmata for proofing Theorem 1 (part 1): Jacobian norm smaller than one\nIn this section, we show that the largest singular value of the Jacobian of the mapping g is smaller than one. Therefore, g is a contraction mapping. This is even true in a larger domain than the original \u2126. We do not need to restrict \u03c4 \u2208 [0.95, 1.1], but we can extend to \u03c4 \u2208 [0.8, 1.25]. The range of the other variables is unchanged such that we consider the following domain throughout this section: \u00b5 \u2208 [\u22120.1, 0.1], \u03c9 \u2208 [\u22120.1, 0.1], \u03bd \u2208 [0.8, 1.5], and \u03c4 \u2208 [0.8, 1.25].\nJacobian of the mapping. In the following, we denote two Jacobians: (1) the Jacobian J of the mapping h : (\u00b5, \u03bd) 7\u2192 (\u00b5\u0303, \u03be\u0303), and (2) the JacobianH of the mapping g : (\u00b5, \u03bd) 7\u2192 (\u00b5\u0303, \u03bd\u0303) because the influence of \u00b5\u0303 on \u03bd\u0303 is small, and many properties of the system can already be seen on J .\nJ = ( J11 J12 J21 J22 ) = ( \u2202 \u2202\u00b5 \u00b5\u0303 \u2202 \u2202\u03bd \u00b5\u0303\n\u2202 \u2202\u00b5 \u03be\u0303 \u2202 \u2202\u03bd \u03be\u0303\n) (52)\nH = ( H11 H12 H21 H22 ) = ( J11 J12 J21 \u2212 2\u00b5\u0303J11 J22 \u2212 2\u00b5\u0303J12 ) (53)\nThe definition of the entries of the Jacobian J is:\nJ11(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) = \u2202\n\u2202\u00b5 \u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) = (54)\n1 2 \u03bb\u03c9\n( \u03b1e\u00b5\u03c9+ \u03bd\u03c4 2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2 ) J12(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) = \u2202\n\u2202\u03bd \u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) = (55)\n1 4 \u03bb\u03c4\n( \u03b1e\u00b5\u03c9+ \u03bd\u03c4 2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 (\u03b1\u2212 1) \u221a 2\n\u03c0\u03bd\u03c4 e\u2212\n\u00b52\u03c92\n2\u03bd\u03c4\n)\nJ21(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) = \u2202\n\u2202\u00b5 \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) = (56)\n\u03bb2\u03c9 ( \u03b12 ( \u2212e\u00b5\u03c9+ \u03bd\u03c42 ) erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) +\n\u03b12e2\u00b5\u03c9+2\u03bd\u03c4 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) + \u00b5\u03c9 ( 2\u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 )) + \u221a 2 \u03c0 \u221a \u03bd\u03c4e\u2212 \u00b52\u03c92 2\u03bd\u03c4 )\nJ22(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) = \u2202\n\u2202\u03bd \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) = (57)\n1 2 \u03bb2\u03c4\n( \u03b12 ( \u2212e\u00b5\u03c9+ \u03bd\u03c42 ) erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) +\n2\u03b12e2\u00b5\u03c9+2\u03bd\u03c4 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2 )\nProof sketch: Bounding the largest singular value of the Jacobian. If the largest singular value of the Jacobian is smaller than 1, then the spectral norm of the Jacobian is smaller than 1. Then the mapping Eq. (4) and Eq. (5) of the mean and variance to the mean and variance in the next layer is contracting.\nWe show that the largest singular value is smaller than 1 by evaluating the function S(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) on a grid. Then we use the Mean Value Theorem to bound the deviation of the function S between grid points. Toward this end we have to bound the gradient of S with respect to (\u00b5, \u03c9, \u03bd, \u03c4). If all function values plus gradient times the deltas (differences between grid points and evaluated points) is still smaller than 1, then we have proofed that the function is below 1.\nThe singular values of the 2\u00d7 2 matrix\nA = ( a11 a12 a21 a22 ) (58)\nare\ns1 = 1\n2\n(\u221a (a11 + a22)2 + (a21 \u2212 a12)2 + \u221a (a11 \u2212 a22)2 + (a12 + a21)2 ) (59)\ns2 = 1\n2\n(\u221a (a11 + a22)2 + (a21 \u2212 a12)2 \u2212 \u221a (a11 \u2212 a22)2 + (a12 + a21)2 ) . (60)\nWe used an explicit formula for the singular values [4]. We now setH11 = a11,H12 = a12,H21 = a21,H22 = a22 to obtain a formula for the largest singular value of the Jacobian depending on (\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1). The formula for the largest singular value for the Jacobian is: S(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) = (\u221a (H11 +H22)2 + (H21 \u2212H12)2 + \u221a (H11 \u2212H22)2 + (H12 +H21)2 ) =\n(61)\n= 1\n2 (\u221a (J11 + J22 \u2212 2\u00b5\u0303J12)2 + (J21 \u2212 2\u00b5\u0303J11 \u2212 J12)2 +\u221a\n(J11 \u2212 J22 + 2\u00b5\u0303J12)2 + (J12 + J21 \u2212 2\u00b5\u0303J11)2 ) ,\nwhere J are defined in Eq. (54) and we left out the dependencies on (\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) in order to keep the notation uncluttered, e.g. we wrote J11 instead of J11(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1).\nBounds on the derivatives of the Jacobian entries. In order to bound the gradient of the singular value, we have to bound the derivatives of the Jacobian entries J11(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1), J12(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1), J21(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1), and J22(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) with respect to \u00b5, \u03c9, \u03bd, and \u03c4 . The values \u03bb and \u03b1 are fixed to \u03bb01 and \u03b101. The 16 derivatives of the 4 Jacobian entries with respect to the 4 variables are:\n\u2202J11 \u2202\u00b5 = 1 2 \u03bb\u03c92e\u2212 \u00b52\u03c92 2\u03bd\u03c4 \u03b1e (\u00b5\u03c9+\u03bd\u03c4)22\u03bd\u03c4 erfc(\u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) \u2212 \u221a 2 \u03c0 (\u03b1\u2212 1)\u221a \u03bd\u03c4  (62) \u2202J11 \u2202\u03c9 = 1 2 \u03bb \u2212e\u2212\u00b52\u03c922\u03bd\u03c4  \u221a 2 \u03c0 (\u03b1\u2212 1)\u00b5\u03c9\u221a \u03bd\u03c4 \u2212 \u03b1(\u00b5\u03c9 + 1)e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4\n) \u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2\n) \u2202J11 \u2202\u03bd = 1 4 \u03bb\u03c4\u03c9e\u2212 \u00b52\u03c92 2\u03bd\u03c4 ( \u03b1e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) + \u221a 2 \u03c0 ( (\u03b1\u2212 1)\u00b5\u03c9 (\u03bd\u03c4)3/2 \u2212 \u03b1\u221a \u03bd\u03c4\n)) \u2202J11 \u2202\u03c4 = 1 4 \u03bb\u03bd\u03c9e\u2212 \u00b52\u03c92 2\u03bd\u03c4 ( \u03b1e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) + \u221a 2 \u03c0 ( (\u03b1\u2212 1)\u00b5\u03c9 (\u03bd\u03c4)3/2 \u2212 \u03b1\u221a \u03bd\u03c4\n)) \u2202J12 \u2202\u00b5 = \u2202J11 \u2202\u03bd\n\u2202J12 \u2202\u03c9 = 1 4 \u03bb\u00b5\u03c4e\u2212 \u00b52\u03c92 2\u03bd\u03c4\n( \u03b1e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) + \u221a 2\n\u03c0\n( (\u03b1\u2212 1)\u00b5\u03c9\n(\u03bd\u03c4)3/2 \u2212 \u03b1\u221a \u03bd\u03c4 )) \u2202J12 \u2202\u03bd = 1 8 \u03bbe\u2212 \u00b52\u03c92 2\u03bd\u03c4 ( \u03b1\u03c42e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) +\u221a\n2\n\u03c0\n( (\u22121)(\u03b1\u2212 1)\u00b52\u03c92\n\u03bd5/2 \u221a \u03c4\n+\n\u221a \u03c4(\u03b1+ \u03b1\u00b5\u03c9 \u2212 1)\n\u03bd3/2 \u2212 \u03b1\u03c4\n3/2\n\u221a \u03bd )) \u2202J12 \u2202\u03c4 = 1 8 \u03bbe\u2212 \u00b52\u03c92 2\u03bd\u03c4 ( 2\u03b1e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) + \u03b1\u03bd\u03c4e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) +\u221a\n2\n\u03c0\n( (\u22121)(\u03b1\u2212 1)\u00b52\u03c92\n(\u03bd\u03c4)3/2 + \u2212\u03b1+ \u03b1\u00b5\u03c9 + 1\u221a \u03bd\u03c4 \u2212 \u03b1 \u221a \u03bd\u03c4 )) \u2202J21 \u2202\u00b5 = \u03bb2\u03c92 ( \u03b12 ( \u2212e\u2212 \u00b52\u03c92 2\u03bd\u03c4 ) e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) +\n2\u03b12e (\u00b5\u03c9+2\u03bd\u03c4)2 2\u03bd\u03c4 e\u2212 \u00b52\u03c92 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2 ) \u2202J21 \u2202\u03c9 = \u03bb2 ( \u03b12(\u00b5\u03c9 + 1) ( \u2212e\u2212 \u00b52\u03c92 2\u03bd\u03c4 ) e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) +\n\u03b12(2\u00b5\u03c9 + 1)e (\u00b5\u03c9+2\u03bd\u03c4)2 2\u03bd\u03c4 e\u2212 \u00b52\u03c92 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) +\n2\u00b5\u03c9 ( 2\u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 )) + \u221a 2 \u03c0 \u221a \u03bd\u03c4e\u2212 \u00b52\u03c92 2\u03bd\u03c4 ) \u2202J21 \u2202\u03bd = 1 2 \u03bb2\u03c4\u03c9e\u2212 \u00b52\u03c92 2\u03bd\u03c4 ( \u03b12 ( \u2212e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 ) erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) +\n4\u03b12e (\u00b5\u03c9+2\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) + \u221a 2 \u03c0 (\u22121) ( \u03b12 \u2212 1 ) \u221a \u03bd\u03c4  \u2202J21 \u2202\u03c4 = 1 2 \u03bb2\u03bd\u03c9e\u2212 \u00b52\u03c92 2\u03bd\u03c4 ( \u03b12 ( \u2212e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 ) erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) +\n4\u03b12e (\u00b5\u03c9+2\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) + \u221a 2 \u03c0 (\u22121) ( \u03b12 \u2212 1 ) \u221a \u03bd\u03c4  \u2202J22 \u2202\u00b5 = \u2202J21 \u2202\u03bd\n\u2202J22 \u2202\u03c9 = 1 2 \u03bb2\u00b5\u03c4e\u2212 \u00b52\u03c92 2\u03bd\u03c4\n( \u03b12 ( \u2212e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 ) erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) +\n4\u03b12e (\u00b5\u03c9+2\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) + \u221a 2 \u03c0 (\u22121) ( \u03b12 \u2212 1 ) \u221a \u03bd\u03c4  \u2202J22 \u2202\u03bd = 1 4 \u03bb2\u03c42e\u2212 \u00b52\u03c92 2\u03bd\u03c4 ( \u03b12 ( \u2212e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 ) erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) +\n8\u03b12e (\u00b5\u03c9+2\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) + \u221a 2\n\u03c0\n(( \u03b12 \u2212 1 ) \u00b5\u03c9\n(\u03bd\u03c4)3/2 \u2212 3\u03b1\n2 \u221a \u03bd\u03c4 )) \u2202J22 \u2202\u03c4 = 1 4 \u03bb2 ( \u22122\u03b12e\u2212 \u00b52\u03c92 2\u03bd\u03c4 e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) \u2212\n\u03b12\u03bd\u03c4e\u2212 \u00b52\u03c92 2\u03bd\u03c4 e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) + 4\u03b12e (\u00b5\u03c9+2\u03bd\u03c4)2 2\u03bd\u03c4 e\u2212 \u00b52\u03c92 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) +\n8\u03b12\u03bd\u03c4e (\u00b5\u03c9+2\u03bd\u03c4)2 2\u03bd\u03c4 e\u2212 \u00b52\u03c92 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) + 2 ( 2\u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 )) +\u221a\n2 \u03c0 e\u2212 \u00b52\u03c92 2\u03bd\u03c4\n(( \u03b12 \u2212 1 ) \u00b5\u03c9\n\u221a \u03bd\u03c4\n\u2212 3\u03b12 \u221a \u03bd\u03c4 )) Lemma 5 (Bounds on the Derivatives). The following bounds on the absolute values of the derivatives of the Jacobian entries J11(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1), J12(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1), J21(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1), and J22(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) with respect to \u00b5, \u03c9, \u03bd, and \u03c4 hold:\u2223\u2223\u2223\u2223\u2202J11\u2202\u00b5\n\u2223\u2223\u2223\u2223 < 0.0031049101995398316 (63)\u2223\u2223\u2223\u2223\u2202J11\u2202\u03c9 \u2223\u2223\u2223\u2223 < 1.055872374194189\u2223\u2223\u2223\u2223\u2202J11\u2202\u03bd \u2223\u2223\u2223\u2223 < 0.031242911235461816\u2223\u2223\u2223\u2223\u2202J11\u2202\u03c4 \u2223\u2223\u2223\u2223 < 0.03749149348255419\n\u2223\u2223\u2223\u2223\u2202J12\u2202\u00b5 \u2223\u2223\u2223\u2223 < 0.031242911235461816\n\u2223\u2223\u2223\u2223\u2202J12\u2202\u03c9 \u2223\u2223\u2223\u2223 < 0.031242911235461816\u2223\u2223\u2223\u2223\u2202J12\u2202\u03bd \u2223\u2223\u2223\u2223 < 0.21232788238624354\u2223\u2223\u2223\u2223\u2202J12\u2202\u03c4 \u2223\u2223\u2223\u2223 < 0.2124377655377270\n\u2223\u2223\u2223\u2223\u2202J21\u2202\u00b5 \u2223\u2223\u2223\u2223 < 0.02220441024325437\u2223\u2223\u2223\u2223\u2202J21\u2202\u03c9 \u2223\u2223\u2223\u2223 < 1.146955401845684\u2223\u2223\u2223\u2223\u2202J21\u2202\u03bd \u2223\u2223\u2223\u2223 < 0.14983446469110305\u2223\u2223\u2223\u2223\u2202J21\u2202\u03c4 \u2223\u2223\u2223\u2223 < 0.17980135762932363\n\u2223\u2223\u2223\u2223\u2202J22\u2202\u00b5 \u2223\u2223\u2223\u2223 < 0.14983446469110305\u2223\u2223\u2223\u2223\u2202J22\u2202\u03c9 \u2223\u2223\u2223\u2223 < 0.14983446469110305\u2223\u2223\u2223\u2223\u2202J22\u2202\u03bd \u2223\u2223\u2223\u2223 < 1.805740052651535\u2223\u2223\u2223\u2223\u2202J22\u2202\u03c4 \u2223\u2223\u2223\u2223 < 2.396685907216327\nProof. See proof 39.\nBounds on the entries of the Jacobian. Lemma 6 (Bound on J11). The absolute value of the function J11 = 12\u03bb\u03c9 ( \u03b1e\u00b5\u03c9+ \u03bd\u03c4 2 erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) \u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2 )\nis bounded by |J11| 6 0.104497 in the domain \u22120.1 6 \u00b5 6 0.1, \u22120.1 6 \u03c9 6 0.1, 0.8 6 \u03bd 6 1.5, and 0.8 6 \u03c4 6 1.25 for \u03b1 = \u03b101 and \u03bb = \u03bb01.\nProof.\n|J11| = \u2223\u2223\u2223\u222312\u03bb\u03c9 ( \u03b1e\u00b5\u03c9+ \u03bd\u03c4 2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) + 2\u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ))\u2223\u2223\u2223\u2223 6 |1\n2 ||\u03bb||\u03c9| (|\u03b1|0.587622 + 1.00584) 6 0.104497,\n(64) where we used that (a) J11 is strictly monotonically increasing in \u00b5\u03c9 and |2 \u2212 erfc (\n0.01\u221a 2 \u221a \u03bd\u03c4\n) | 6\n1.00584 and (b) Lemma 47 that |e\u00b5\u03c9+ \u03bd\u03c42 erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) | 6 e0.01+ 0.642 erfc ( 0.01+0.64\u221a\n2 \u221a 0.64\n) = 0.587622\nLemma 7 (Bound on J12). The absolute value of the function J12 = 14\u03bb\u03c4 ( \u03b1e\u00b5\u03c9+ \u03bd\u03c4 2 erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) \u2212 (\u03b1\u2212 1) \u221a 2 \u03c0\u03bd\u03c4 e \u2212\u00b5 2\u03c92 2\u03bd\u03c4 ) is bounded by |J12| 6 0.194145 in the domain \u22120.1 6 \u00b5 6 0.1, \u22120.1 6 \u03c9 6 0.1, 0.8 6 \u03bd 6 1.5, and 0.8 6 \u03c4 6 1.25 for \u03b1 = \u03b101 and \u03bb = \u03bb01.\nProof.\n|J12| 6 1\n4 |\u03bb||\u03c4 | \u2223\u2223\u2223\u2223\u2223 ( \u03b1e\u00b5\u03c9+ \u03bd\u03c4 2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) \u2212 (\u03b1\u2212 1) \u221a 2 \u03c0\u03bd\u03c4 e\u2212 \u00b52\u03c92 2\u03bd\u03c4 )\u2223\u2223\u2223\u2223\u2223 6 1 4 |\u03bb||\u03c4 | |0.983247\u2212 0.392294| 6\n0.194035 (65)\nFor the first term we have 0.434947 6 e\u00b5\u03c9+ \u03bd\u03c4 2 erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) 6 0.587622 after Lemma 47 and for\nthe second term 0.582677 6 \u221a\n2 \u03c0\u03bd\u03c4 e\n\u2212\u00b5 2\u03c92\n2\u03bd\u03c4 6 0.997356, which can easily be seen by maximizing or minimizing the arguments of the exponential or the square root function. The first term scaled by \u03b1 is 0.727780 6 \u03b1e\u00b5\u03c9+ \u03bd\u03c4 2 erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) 6 0.983247 and the second term scaled by \u03b1 \u2212 1 is\n0.392294 6 (\u03b1 \u2212 1) \u221a\n2 \u03c0\u03bd\u03c4 e\n\u2212\u00b5 2\u03c92\n2\u03bd\u03c4 6 0.671484. Therefore, the absolute difference between these terms is at most 0.983247\u2212 0.392294 leading to the derived bound.\nBounds on mean, variance and second moment. For deriving bounds on \u00b5\u0303, \u03be\u0303, and \u03bd\u0303, we need the following lemma. Lemma 8 (Derivatives of the Mapping). We assume \u03b1 = \u03b101 and \u03bb = \u03bb01. We restrict the range of the variables to the domain \u00b5 \u2208 [\u22120.1, 0.1], \u03c9 \u2208 [\u22120.1, 0.1], \u03bd \u2208 [0.8, 1.5], and \u03c4 \u2208 [0.8, 1.25].\nThe derivative \u2202\u2202\u00b5 \u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) has the sign of \u03c9.\nThe derivative \u2202\u2202\u03bd \u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) is positive.\nThe derivative \u2202\u2202\u00b5 \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) has the sign of \u03c9.\nThe derivative \u2202\u2202\u03bd \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) is positive.\nProof. See 40.\nLemma 9 (Bounds on mean, variance and second moment). The expressions \u00b5\u0303, \u03be\u0303, and \u03bd\u0303 for \u03b1 = \u03b101 and \u03bb = \u03bb01 are bounded by \u22120.041160 < \u00b5\u0303 < 0.087653, 0.703257 < \u03be\u0303 < 1.643705 and 0.695574 < \u03bd\u0303 < 1.636023 in the domain \u00b5 \u2208 [\u22120.1, 0.1], \u03bd \u2208 [0.8, 15], \u03c9 \u2208 [\u22120.1, 0.1], \u03c4 \u2208 [0.8, 1.25].\nProof. We use Lemma 8 which states that with given sign the derivatives of the mapping Eq. (4) and Eq. (5) with respect to \u03bd and \u00b5 are either positive or have the sign of \u03c9. Therefore with given sign of \u03c9 the mappings are strict monotonic and the their maxima and minima are found at the borders. The minimum of \u00b5\u0303 is obtained at \u00b5\u03c9 = \u22120.01 and its maximum at \u00b5\u03c9 = 0.01 and \u03c3 and \u03c4 at minimal or maximal values, respectively. It follows that \u22120.041160 < \u00b5\u0303(\u22120.1, 0.1, 0.8, 0.8, \u03bb01, \u03b101) 6\u00b5\u0303 6 \u00b5\u0303(0.1, 0.1, 1.5, 1.25, \u03bb01, \u03b101) < 0.087653.\n(66)\nSimilarly, the maximum and minimum of \u03be\u0303 is obtained at the values mentioned above:\n0.703257 < \u03be\u0303(\u22120.1, 0.1, 0.8, 0.8, \u03bb01, \u03b101) 6\u03be\u0303 6 \u03be\u0303(0.1, 0.1, 1.5, 1.25, \u03bb01, \u03b101) < 1.643705. (67)\nHence we obtain the following bounds on \u03bd\u0303:\n0.703257\u2212 \u00b5\u03032 < \u03be\u0303 \u2212 \u00b5\u03032 < 1.643705\u2212 \u00b5\u03032 (68) 0.703257\u2212 0.007683 < \u03bd\u0303 < 1.643705\u2212 0.007682\n0.695574 < \u03bd\u0303 < 1.636023.\nUpper Bounds on the Largest Singular Value of the Jacobian. Lemma 10 (Upper Bounds on Absolute Derivatives of Largest Singular Value). We set \u03b1 = \u03b101 and \u03bb = \u03bb01 and restrict the range of the variables to \u00b5 \u2208 [\u00b5min, \u00b5max] = [\u22120.1, 0.1], \u03c9 \u2208 [\u03c9min, \u03c9max] = [\u22120.1, 0.1], \u03bd \u2208 [\u03bdmin, \u03bdmax] = [0.8, 1.5], and \u03c4 \u2208 [\u03c4min, \u03c4max] = [0.8, 1.25]. The absolute values of derivatives of the largest singular value S(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) given in Eq. (61) with respect to (\u00b5, \u03c9, \u03bd, \u03c4) are bounded as follows:\n\u2223\u2223\u2223\u2223\u2202S\u2202\u00b5 \u2223\u2223\u2223\u2223 < 0.32112 , (69)\u2223\u2223\u2223\u2223\u2202S\u2202\u03c9 \u2223\u2223\u2223\u2223 < 2.63690 , (70)\u2223\u2223\u2223\u2223\u2202S\u2202\u03bd \u2223\u2223\u2223\u2223 < 2.28242 , (71)\u2223\u2223\u2223\u2223\u2202S\u2202\u03c4 \u2223\u2223\u2223\u2223 < 2.98610 . (72)\nProof. The Jacobian of our mapping Eq. (4) and Eq. (5) is defined as\nH = ( H11 H12 H21 H22 ) = ( J11 J12 J21 \u2212 2\u00b5\u0303J11 J22 \u2212 2\u00b5\u0303J12 ) (73)\nand has the largest singular value\nS(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) = 1\n2\n(\u221a (H11 \u2212H22)2 + (H12 +H21)2 + \u221a (H11 +H22)2 + (H12 \u2212H21)2 ) ,\n(74)\naccording to the formula of Blinn [4].\nWe obtain\u2223\u2223\u2223\u2223 \u2202S\u2202H11 \u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u222312 ( H11 \u2212H22\u221a (H11 \u2212H22)2 + (H12 +H21)2 + H11 +H22\u221a (H11 +H22)2 + (H21 \u2212H12)2 )\u2223\u2223\u2223\u2223\u2223 < (75)\n1\n2 \u2223\u2223\u2223\u2223\u2223\u2223 1\u221a (H12+H21)2 (H11\u2212H22)2 + 1 \u2223\u2223\u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2223\u2223 1\u221a (H21\u2212H12)2 (H11+H22)2 + 1 \u2223\u2223\u2223\u2223\u2223\u2223  < 1 + 1 2 = 1\nand analogously\u2223\u2223\u2223\u2223 \u2202S\u2202H12 \u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u222312 ( H12 +H21\u221a (H11 \u2212H22)2 + (H12 +H21)2 \u2212 H21 \u2212H12\u221a (H11 +H22)2 + (H21 \u2212H12)2 )\u2223\u2223\u2223\u2223\u2223 < 1 (76)\nand\u2223\u2223\u2223\u2223 \u2202S\u2202H21 \u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u222312 ( H21 \u2212H12\u221a (H11 +H22)2 + (H21 \u2212H12)2 + H12 +H21\u221a (H11 \u2212H22)2 + (H12 +H21)2 )\u2223\u2223\u2223\u2223\u2223 < 1 (77)\nand\u2223\u2223\u2223\u2223 \u2202S\u2202H22 \u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u222312 ( H11 +H22\u221a (H11 +H22)2 + (H21 \u2212H12)2 \u2212 H11 \u2212H22\u221a (H11 \u2212H22)2 + (H12 +H21)2 )\u2223\u2223\u2223\u2223\u2223 < 1 . (78)\nWe have \u2202S\n\u2202\u00b5 =\n\u2202S \u2202H11 \u2202H11 \u2202\u00b5 + \u2202S \u2202H12 \u2202H12 \u2202\u00b5 + \u2202S \u2202H21 \u2202H21 \u2202\u00b5 + \u2202S \u2202H22 \u2202H22 \u2202\u00b5\n(79)\n\u2202S \u2202\u03c9 = \u2202S \u2202H11 \u2202H11 \u2202\u03c9 + \u2202S \u2202H12 \u2202H12 \u2202\u03c9 + \u2202S \u2202H21 \u2202H21 \u2202\u03c9 + \u2202S \u2202H22 \u2202H22 \u2202\u03c9\n(80)\n\u2202S \u2202\u03bd = \u2202S \u2202H11 \u2202H11 \u2202\u03bd + \u2202S \u2202H12 \u2202H12 \u2202\u03bd + \u2202S \u2202H21 \u2202H21 \u2202\u03bd + \u2202S \u2202H22 \u2202H22 \u2202\u03bd\n(81)\n\u2202S \u2202\u03c4 = \u2202S \u2202H11 \u2202H11 \u2202\u03c4 + \u2202S \u2202H12 \u2202H12 \u2202\u03c4 + \u2202S \u2202H21 \u2202H21 \u2202\u03c4 + \u2202S \u2202H22 \u2202H22 \u2202\u03c4\n(82)\n(83) from which follows using the bounds from Lemma 5: Derivative of the singular value w.r.t. \u00b5:\u2223\u2223\u2223\u2223\u2202S\u2202\u00b5 \u2223\u2223\u2223\u2223 6 (84)\u2223\u2223\u2223\u2223 \u2202S\u2202H11 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2202H11\u2202\u00b5 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223 \u2202S\u2202H12 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2202H12\u2202\u00b5 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223 \u2202S\u2202H21 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2202H21\u2202\u00b5 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223 \u2202S\u2202H22 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2202H22\u2202\u00b5\n\u2223\u2223\u2223\u2223 6\u2223\u2223\u2223\u2223\u2202H11\u2202\u00b5 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202H12\u2202\u00b5 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202H21\u2202\u00b5 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202H22\u2202\u00b5\n\u2223\u2223\u2223\u2223 6\u2223\u2223\u2223\u2223\u2202J11\u2202\u00b5 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J12\u2202\u00b5 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J21 \u2212 2\u00b5\u0303J11\u2202\u00b5 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J22 \u2212 2\u00b5\u0303J12\u2202\u00b5\n\u2223\u2223\u2223\u2223 6\u2223\u2223\u2223\u2223\u2202J11\u2202\u00b5 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J12\u2202\u00b5 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J21\u2202\u00b5 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J22\u2202\u00b5 \u2223\u2223\u2223\u2223+ 2 \u2223\u2223\u2223\u2223\u2202J11\u2202\u00b5 \u2223\u2223\u2223\u2223 |\u00b5\u0303|+ 2 |J11|2 + 2 \u2223\u2223\u2223\u2223\u2202J12\u2202\u00b5\n\u2223\u2223\u2223\u2223 |\u00b5\u0303|+ 2 |J12| |J11| 6 0.0031049101995398316 + 0.031242911235461816 + 0.02220441024325437 + 0.14983446469110305+\n2 \u00b7 0.104497 \u00b7 0.087653 + 2 \u00b7 0.1044972+ 2 \u00b7 0.194035 \u00b7 0.087653 + 2 \u00b7 0.104497 \u00b7 0.194035 < 0.32112,\nwhere we used the results from the lemmata 5, 6, 7, and 9. Derivative of the singular value w.r.t. \u03c9:\u2223\u2223\u2223\u2223\u2202S\u2202\u03c9 \u2223\u2223\u2223\u2223 6 (85)\u2223\u2223\u2223\u2223 \u2202S\u2202H11 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2202H11\u2202\u03c9 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223 \u2202S\u2202H12 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2202H12\u2202\u03c9 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223 \u2202S\u2202H21 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2202H21\u2202\u03c9 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223 \u2202S\u2202H22 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2202H22\u2202\u03c9\n\u2223\u2223\u2223\u2223 6\u2223\u2223\u2223\u2223\u2202H11\u2202\u03c9 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202H12\u2202\u03c9 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202H21\u2202\u03c9 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202H22\u2202\u03c9\n\u2223\u2223\u2223\u2223 6\u2223\u2223\u2223\u2223\u2202J11\u2202\u03c9 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J12\u2202\u03c9 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J21 \u2212 2\u00b5\u0303J11\u2202\u03c9 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J22 \u2212 2\u00b5\u0303J12\u2202\u03c9\n\u2223\u2223\u2223\u2223 6\u2223\u2223\u2223\u2223\u2202J11\u2202\u03c9 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J12\u2202\u03c9 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J21\u2202\u03c9 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J22\u2202\u03c9 \u2223\u2223\u2223\u2223+ 2 \u2223\u2223\u2223\u2223\u2202J11\u2202\u03c9 \u2223\u2223\u2223\u2223 |\u00b5\u0303|+ 2 |J11| \u2223\u2223\u2223\u2223\u2202\u00b5\u0303\u2202\u03c9\n\u2223\u2223\u2223\u2223+ 2 \u2223\u2223\u2223\u2223\u2202J12\u2202\u03c9 \u2223\u2223\u2223\u2223 |\u00b5\u0303|+ 2 |J12| \u2223\u2223\u2223\u2223\u2202\u00b5\u0303\u2202\u03c9\n\u2223\u2223\u2223\u2223 6 (86) 2.38392 + 2 \u00b7 1.055872374194189 \u00b7 0.087653 + 2 \u00b7 0.1044972 + 2 \u00b7 0.031242911235461816 \u00b7 0.087653 + 2 \u00b7 0.194035 \u00b7 0.104497 < 2.63690 ,\nwhere we used the results from the lemmata 5, 6, 7, and 9 and that \u00b5\u0303 is symmetric for \u00b5, \u03c9. Derivative of the singular value w.r.t. \u03bd:\u2223\u2223\u2223\u2223\u2202S\u2202\u03bd \u2223\u2223\u2223\u2223 6 (87)\n\u2223\u2223\u2223\u2223 \u2202S\u2202H11 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2202H11\u2202\u03bd \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223 \u2202S\u2202H12 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2202H12\u2202\u03bd \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223 \u2202S\u2202H21 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2202H21\u2202\u03bd \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223 \u2202S\u2202H22 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2202H22\u2202\u03bd \u2223\u2223\u2223\u2223 6\u2223\u2223\u2223\u2223\u2202H11\u2202\u03bd \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202H12\u2202\u03bd \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202H21\u2202\u03bd \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202H22\u2202\u03bd\n\u2223\u2223\u2223\u2223 6\u2223\u2223\u2223\u2223\u2202J11\u2202\u03bd \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J12\u2202\u03bd \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J21 \u2212 2\u00b5\u0303J11\u2202\u03bd \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J22 \u2212 2\u00b5\u0303J12\u2202\u03bd\n\u2223\u2223\u2223\u2223 6\u2223\u2223\u2223\u2223\u2202J11\u2202\u03bd \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J12\u2202\u03bd \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J21\u2202\u03bd \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J22\u2202\u03bd \u2223\u2223\u2223\u2223+ 2 \u2223\u2223\u2223\u2223\u2202J11\u2202\u03bd \u2223\u2223\u2223\u2223 |\u00b5\u0303|+ 2 |J11| |J12|+ 2 \u2223\u2223\u2223\u2223\u2202J12\u2202\u03bd\n\u2223\u2223\u2223\u2223 |\u00b5\u0303|+ 2 |J12|2 6 2.19916 + 2 \u00b7 0.031242911235461816 \u00b7 0.087653 + 2 \u00b7 0.104497 \u00b7 0.194035+ 2 \u00b7 0.21232788238624354 \u00b7 0.087653 + 2 \u00b7 0.1940352 < 2.28242 ,\nwhere we used the results from the lemmata 5, 6, 7, and 9.\nDerivative of the singular value w.r.t. \u03c4 :\u2223\u2223\u2223\u2223\u2202S\u2202\u03c4 \u2223\u2223\u2223\u2223 6 (88)\u2223\u2223\u2223\u2223 \u2202S\u2202H11 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2202H11\u2202\u03c4 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223 \u2202S\u2202H12 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2202H12\u2202\u03c4 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223 \u2202S\u2202H21 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2202H21\u2202\u03c4 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223 \u2202S\u2202H22 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2202H22\u2202\u03c4\n\u2223\u2223\u2223\u2223 6\u2223\u2223\u2223\u2223\u2202H11\u2202\u03c4 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202H12\u2202\u03c4 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202H21\u2202\u03c4 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202H22\u2202\u03c4\n\u2223\u2223\u2223\u2223 6\u2223\u2223\u2223\u2223\u2202J11\u2202\u03c4 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J12\u2202\u03c4 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J21 \u2212 2\u00b5\u0303J11\u2202\u03c4 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J22 \u2212 2\u00b5\u0303J12\u2202\u03c4\n\u2223\u2223\u2223\u2223 6\u2223\u2223\u2223\u2223\u2202J11\u2202\u03c4 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J12\u2202\u03c4 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J21\u2202\u03c4 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J22\u2202\u03c4 \u2223\u2223\u2223\u2223+ 2 \u2223\u2223\u2223\u2223\u2202J11\u2202\u03c4 \u2223\u2223\u2223\u2223 |\u00b5\u0303|+ 2 |J11| \u2223\u2223\u2223\u2223\u2202\u00b5\u0303\u2202\u03c4\n\u2223\u2223\u2223\u2223+ 2 \u2223\u2223\u2223\u2223\u2202J12\u2202\u03c4 \u2223\u2223\u2223\u2223 |\u00b5\u0303|+ 2 |J12| \u2223\u2223\u2223\u2223\u2202\u00b5\u0303\u2202\u03c4\n\u2223\u2223\u2223\u2223 6 (89) 2.82643 + 2 \u00b7 0.03749149348255419 \u00b7 0.087653 + 2 \u00b7 0.104497 \u00b7 0.194035+ 2 \u00b7 0.2124377655377270 \u00b7 0.087653 + 2 \u00b7 0.1940352 < 2.98610 ,\nwhere we used the results from the lemmata 5, 6, 7, and 9 and that \u00b5\u0303 is symmetric for \u03bd, \u03c4 .\nLemma 11 (Mean Value Theorem Bound on Deviation from Largest Singular Value). We set \u03b1 = \u03b101 and \u03bb = \u03bb01 and restrict the range of the variables to \u00b5 \u2208 [\u00b5min, \u00b5max] = [\u22120.1, 0.1], \u03c9 \u2208 [\u03c9min, \u03c9max] = [\u22120.1, 0.1], \u03bd \u2208 [\u03bdmin, \u03bdmax] = [0.8, 1.5], and \u03c4 \u2208 [\u03c4min, \u03c4max] = [0.8, 1.25]. The distance of the singular value at S(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) and that at S(\u00b5 + \u2206\u00b5, \u03c9 + \u2206\u03c9, \u03bd + \u2206\u03bd, \u03c4 + \u2206\u03c4, \u03bb01, \u03b101) is bounded as follows:\n|S(\u00b5+ \u2206\u00b5, \u03c9 + \u2206\u03c9, \u03bd + \u2206\u03bd, \u03c4 + \u2206\u03c4, \u03bb01, \u03b101) \u2212 S(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101)| < (90) 0.32112 |\u2206\u00b5|+ 2.63690 |\u2206\u03c9|+ 2.28242 |\u2206\u03bd|+ 2.98610 |\u2206\u03c4 | .\nProof. The mean value theorem states that a t \u2208 [0, 1] exists for which S(\u00b5+ \u2206\u00b5, \u03c9 + \u2206\u03c9, \u03bd + \u2206\u03bd, \u03c4 + \u2206\u03c4, \u03bb01, \u03b101) \u2212 S(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) = (91) \u2202S\n\u2202\u00b5 (\u00b5+ t\u2206\u00b5, \u03c9 + t\u2206\u03c9, \u03bd + t\u2206\u03bd, \u03c4 + t\u2206\u03c4, \u03bb01, \u03b101) \u2206\u00b5 +\n\u2202S \u2202\u03c9 (\u00b5+ t\u2206\u00b5, \u03c9 + t\u2206\u03c9, \u03bd + t\u2206\u03bd, \u03c4 + t\u2206\u03c4, \u03bb01, \u03b101) \u2206\u03c9 +\n\u2202S \u2202\u03bd (\u00b5+ t\u2206\u00b5, \u03c9 + t\u2206\u03c9, \u03bd + t\u2206\u03bd, \u03c4 + t\u2206\u03c4, \u03bb01, \u03b101) \u2206\u03bd +\n\u2202S \u2202\u03c4 (\u00b5+ t\u2206\u00b5, \u03c9 + t\u2206\u03c9, \u03bd + t\u2206\u03bd, \u03c4 + t\u2206\u03c4, \u03bb01, \u03b101) \u2206\u03c4\nfrom which immediately follows that\n|S(\u00b5+ \u2206\u00b5, \u03c9 + \u2206\u03c9, \u03bd + \u2206\u03bd, \u03c4 + \u2206\u03c4, \u03bb01, \u03b101) \u2212 S(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101)| 6 (92)\u2223\u2223\u2223\u2223\u2202S\u2202\u00b5 (\u00b5+ t\u2206\u00b5, \u03c9 + t\u2206\u03c9, \u03bd + t\u2206\u03bd, \u03c4 + t\u2206\u03c4, \u03bb01, \u03b101) \u2223\u2223\u2223\u2223 |\u2206\u00b5| +\u2223\u2223\u2223\u2223\u2202S\u2202\u03c9 (\u00b5+ t\u2206\u00b5, \u03c9 + t\u2206\u03c9, \u03bd + t\u2206\u03bd, \u03c4 + t\u2206\u03c4, \u03bb01, \u03b101) \u2223\u2223\u2223\u2223 |\u2206\u03c9| +\u2223\u2223\u2223\u2223\u2202S\u2202\u03bd (\u00b5+ t\u2206\u00b5, \u03c9 + t\u2206\u03c9, \u03bd + t\u2206\u03bd, \u03c4 + t\u2206\u03c4, \u03bb01, \u03b101) \u2223\u2223\u2223\u2223 |\u2206\u03bd| +\u2223\u2223\u2223\u2223\u2202S\u2202\u03c4 (\u00b5+ t\u2206\u00b5, \u03c9 + t\u2206\u03c9, \u03bd + t\u2206\u03bd, \u03c4 + t\u2206\u03c4, \u03bb01, \u03b101) \u2223\u2223\u2223\u2223 |\u2206\u03c4 | .\nWe now apply Lemma 10 which gives bounds on the derivatives, which immediately gives the statement of the lemma.\nLemma 12 (Largest Singular Value Smaller Than One). We set \u03b1 = \u03b101 and \u03bb = \u03bb01 and restrict the range of the variables to \u00b5 \u2208 [\u22120.1, 0.1], \u03c9 \u2208 [\u22120.1, 0.1], \u03bd \u2208 [0.8, 1.5], and \u03c4 \u2208 [0.8, 1.25]. The the largest singular value of the Jacobian is smaller than 1:\nS(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) < 1 . (93)\nTherefore the mapping Eq. (4) and Eq. (5) is a contraction mapping.\nProof. We set \u2206\u00b5 = 0.0068097371, \u2206\u03c9 = 0.0008292885, \u2206\u03bd = 0.0009580840, and \u2206\u03c4 = 0.0007323095.\nAccording to Lemma 11 we have\n|S(\u00b5+ \u2206\u00b5, \u03c9 + \u2206\u03c9, \u03bd + \u2206\u03bd, \u03c4 + \u2206\u03c4, \u03bb01, \u03b101) \u2212 S(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101)| < (94) 0.32112 \u00b7 0.0068097371 + 2.63690 \u00b7 0.0008292885+ 2.28242 \u00b7 0.0009580840 + 2.98610 \u00b7 0.0007323095 < 0.008747 .\nFor a grid with grid length \u2206\u00b5 = 0.0068097371, \u2206\u03c9 = 0.0008292885, \u2206\u03bd = 0.0009580840, and \u2206\u03c4 = 0.0007323095, we evaluated the function Eq. (61) for the largest singular value in the domain \u00b5 \u2208 [\u22120.1, 0.1], \u03c9 \u2208 [\u22120.1, 0.1], \u03bd \u2208 [0.8, 1.5], and \u03c4 \u2208 [0.8, 1.25]. We did this using a computer. According to Subsection A3.4.5 the precision if regarding error propagation and precision of the implemented functions is larger than 10\u221213. We performed the evaluation on different operating systems and different hardware architectures including CPUs and GPUs. In all cases the function Eq. (61) for the largest singular value of the Jacobian is bounded by 0.9912524171058772.\nWe obtain from Eq. (94):\nS(\u00b5+ \u2206\u00b5, \u03c9 + \u2206\u03c9, \u03bd + \u2206\u03bd, \u03c4 + \u2206\u03c4, \u03bb01, \u03b101) 6 0.9912524171058772 + 0.008747 < 1 . (95)\nA3.4.2 Lemmata for proofing Theorem 1 (part 2): Mapping within domain\nWe further have to investigate whether the the mapping Eq. (4) and Eq. (5) maps into a predefined domains. Lemma 13 (Mapping into the domain). The mapping Eq. (4) and Eq. (5) map for \u03b1 = \u03b101 and \u03bb = \u03bb01 into the domain \u00b5 \u2208 [\u22120.03106, 0.06773] and \u03bd \u2208 [0.80009, 1.48617] with \u03c9 \u2208 [\u22120.1, 0.1] and \u03c4 \u2208 [0.95, 1.1].\nProof. We use Lemma 8 which states that with given sign the derivatives of the mapping Eq. (4) and Eq. (5) with respect to \u03b1 = \u03b101 and \u03bb = \u03bb01 are either positive or have the sign of \u03c9. Therefore with given sign of \u03c9 the mappings are strict monotonic and the their maxima and minima are found at the\nborders. The minimum of \u00b5\u0303 is obtained at \u00b5\u03c9 = \u22120.01 and its maximum at \u00b5\u03c9 = 0.01 and \u03c3 and \u03c4 at their minimal and maximal values, respectively. It follows that:\n\u22120.03106 < \u00b5\u0303(\u22120.1, 0.1, 0.8, 0.95, \u03bb01, \u03b101) 6\u00b5\u0303 6 \u00b5\u0303(0.1, 0.1, 1.5, 1.1, \u03bb01, \u03b101) < 0.06773, (96)\nand that \u00b5\u0303 \u2208 [\u22120.1, 0.1].\nSimilarly, the maximum and minimum of \u03be\u0303( is obtained at the values mentioned above:\n0.80467 < \u03be\u0303(\u22120.1, 0.1, 0.8, 0.95, \u03bb01, \u03b101) 6\u03be\u0303 6 \u03be\u0303(0.1, 0.1, 1.5, 1.1, \u03bb01, \u03b101) < 1.48617. (97)\nSince |\u03be\u0303 \u2212 \u03bd\u0303| = |\u00b5\u03032| < 0.004597, we can conclude that 0.80009 < \u03bd\u0303 < 1.48617 and the variance remains in [0.8, 1.5].\nCorollary 14. The image g(\u2126\u2032) of the mapping g : (\u00b5, \u03bd) 7\u2192 (\u00b5\u0303, \u03bd\u0303) (Eq. (8)) and the domain \u2126\u2032 = {(\u00b5, \u03bd)| \u2212 0.1 6 \u00b5 6 0.1, 0.8 6 \u00b5 6 1.5} is a subset of \u2126\u2032:\ng(\u2126\u2032) \u2286 \u2126\u2032, (98)\nfor all \u03c9 \u2208 [\u22120.1, 0.1] and \u03c4 \u2208 [0.95, 1.1].\nProof. Directly follows from Lemma 13.\nA3.4.3 Lemmata for proofing Theorem 2: The variance is contracting\nMain Sub-Function. We consider the main sub-function of the derivate of second moment, J22 (Eq. (54)):\n\u2202\n\u2202\u03bd \u03be\u0303 =\n1 2 \u03bb2\u03c4\n( \u2212\u03b12e\u00b5\u03c9+ \u03bd\u03c42 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) + 2\u03b12e2\u00b5\u03c9+2\u03bd\u03c4 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2 ) (99)\nthat depends on \u00b5\u03c9 and \u03bd\u03c4 , therefore we set x = \u03bd\u03c4 and y = \u00b5\u03c9. Algebraic reformulations provide the formula in the following form:\n\u2202\n\u2202\u03bd \u03be\u0303 =\n1 2 \u03bb2\u03c4\n( \u03b12 ( \u2212e\u2212 y2 2x )( e (x+y)2 2x erfc ( y + x\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( y + 2x\u221a\n2 \u221a x\n)) \u2212 erfc ( y\u221a 2 \u221a x ) + 2 ) (100)\nFor \u03bb = \u03bb01 and \u03b1 = \u03b101, we consider the domain \u22121 6 \u00b5 6 1, \u22120.1 6 \u03c9 6 0.1, 1.5 6 \u03bd 6 16, and, 0.8 6 \u03c4 6 1.25.\nFor x and y we obtain: 0.8 \u00b7 1.5 = 1.2 6 x 6 20 = 1.25 \u00b7 16 and 0.1 \u00b7 (\u22121) = \u22120.1 6 y 6 0.1 = 0.1 \u00b7 1. In the following we assume to remain within this domain. Lemma 15 (Main subfunction). For 1.2 6 x 6 20 and \u22120.1 6 y 6 0.1, the function\ne (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( 2x+ y\u221a\n2 \u221a x\n) (101)\nis smaller than zero, is strictly monotonically increasing in x, and strictly monotonically decreasing in y for the minimal x = 12/10 = 1.2.\nProof. See proof 44.\nThe graph of the subfunction in the specified domain is displayed in Figure A3.\nX y\nf(x,y)\n\u22120.132\n\u22120.130\n\u22120.128\n\u22120.126\n\u22120.10 \u22120.05 0.00 0.05 0.10\nf( 1\n.2 ,y\n)\ny\nFigure A3: Left panel: Graphs of the main subfunction f(x, y) = e (x+y)2 2x erfc ( x+y\u221a 2 \u221a x ) \u2212 2e (2x+y)2 2x erfc (\n2x+y\u221a 2 \u221a x\n) treated in Lemma 15. The function is negative and monotonically increasing\nwith x independent of y. Right panel: Graphs of the main subfunction at minimal x = 1.2. The graph shows that the function f(1.2, y) is strictly monotonically decreasing in y.\nTheorem 16 (Contraction \u03bd-mapping). The mapping of the variance \u03bd\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) given in Eq. (5) is contracting for \u03bb = \u03bb01, \u03b1 = \u03b101 and the domain \u2126+: \u22120.1 6 \u00b5 6 0.1, \u22120.1 6 \u03c9 6 0.1, 1.5 6 \u03bd 6 16, and 0.8 6 \u03c4 6 1.25, that is,\u2223\u2223\u2223\u2223 \u2202\u2202\u03bd \u03bd\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101)\n\u2223\u2223\u2223\u2223 < 1 . (102) Proof. In this domain \u2126+ we have the following three properties (see further below): \u2202\u2202\u03bd \u03be\u0303 < 1, \u00b5\u0303 > 0, and \u2202\u2202\u03bd \u00b5\u0303 > 0. Therefore, we have\n\u2223\u2223\u2223\u2223 \u2202\u2202\u03bd \u03bd\u0303 \u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223 \u2202\u2202\u03bd \u03be\u0303 \u2212 2\u00b5\u0303 \u2202\u2202\u03bd \u00b5\u0303 \u2223\u2223\u2223\u2223 < \u2223\u2223\u2223\u2223 \u2202\u2202\u03bd \u03be\u0303 \u2223\u2223\u2223\u2223 < 1 (103)\n\u2022 We first proof that \u2202\u2202\u03bd \u03be\u0303 < 1 in an even larger domain that fully contains \u2126 +. According to\nEq. (54), the derivative of the mapping Eq. (5) with respect to the variance \u03bd is\n\u2202\n\u2202\u03bd \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) = (104)\n1 2 \u03bb2\u03c4\n( \u03b12 ( \u2212e\u00b5\u03c9+ \u03bd\u03c42 ) erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) +\n2\u03b12e2\u00b5\u03c9+2\u03bd\u03c4 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2 ) .\nFor \u03bb = \u03bb01, \u03b1 = \u03b101, \u22121 6 \u00b5 6 1, \u22120.1 6 \u03c9 6 0.1 1.5 6 \u03bd 6 16, and 0.8 6 \u03c4 6 1.25, we first show that the derivative is positive and then upper bound it.\nAccording to Lemma 15, the expression\ne (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 2e (\u00b5\u03c9+2\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) (105)\nis negative. This expression multiplied by positive factors is subtracted in the derivative Eq. (104), therefore, the whole term is positive. The remaining term\n2\u2212 erfc (\n\u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4\n) (106)\nof the derivative Eq. (104) is also positive according to Lemma 21. All factors outside the brackets in Eq. (104) are positive. Hence, the derivative Eq. (104) is positive.\nThe upper bound of the derivative is:\n1 2 \u03bb201\u03c4\n( \u03b1201 ( \u2212e\u00b5\u03c9+ \u03bd\u03c42 ) erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) + (107)\n2\u03b1201e 2\u00b5\u03c9+2\u03bd\u03c4 erfc\n( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2 ) =\n1 2 \u03bb201\u03c4\n( \u03b1201 ( \u2212e\u2212 \u00b52\u03c92 2\u03bd\u03c4 )( e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212\n2e (\u00b5\u03c9+2\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n)) \u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2 ) 6\n1 2 1.25\u03bb201\n( \u03b1201 ( \u2212e\u2212 \u00b52\u03c92 2\u03bd\u03c4 )( e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212\n2e (\u00b5\u03c9+2\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n)) \u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2 ) 6\n1 2 1.25\u03bb201\n( \u03b1201 ( e ( 1.2+0.1\u221a 2 \u221a 1.2 )2 erfc ( 1.2 + 0.1\u221a\n2 \u221a 1.2\n) \u2212\n2e\n( 2\u00b71.2+0.1\u221a\n2 \u221a 1.2 )2 erfc ( 2 \u00b7 1.2 + 0.1\u221a\n2 \u221a 1.2\n))( \u2212e\u2212 \u00b52\u03c92 2\u03bd\u03c4 ) \u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2 ) 6\n1 2 1.25\u03bb201\n( \u2212e0.0\u03b1201 ( e ( 1.2+0.1\u221a 2 \u221a 1.2 )2 erfc ( 1.2 + 0.1\u221a\n2 \u221a 1.2\n) \u2212\n2e\n( 2\u00b71.2+0.1\u221a\n2 \u221a 1.2 )2 erfc ( 2 \u00b7 1.2 + 0.1\u221a\n2 \u221a 1.2\n)) \u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2 ) 6\n1 2 1.25\u03bb201\n( \u2212e0.0\u03b1201 ( e ( 1.2+0.1\u221a 2 \u221a 1.2 )2 erfc ( 1.2 + 0.1\u221a\n2 \u221a 1.2\n) \u2212\n2e\n( 2\u00b71.2+0.1\u221a\n2 \u221a 1.2 )2 erfc ( 2 \u00b7 1.2 + 0.1\u221a\n2 \u221a 1.2\n)) \u2212 erfc ( 0.1\u221a 2 \u221a 1.2 ) + 2 ) 6\n0.995063 < 1 .\nWe explain the chain of inequalities:\n\u2013 First equality brings the expression into a shape where we can apply Lemma 15 for the the function Eq. (101).\n\u2013 First inequality: The overall factor \u03c4 is bounded by 1.25. \u2013 Second inequality: We apply Lemma 15. According to Lemma 15 the function\nEq. (101) is negative. The largest contribution is to subtract the most negative value of the function Eq. (101), that is, the minimum of function Eq. (101). According to Lemma 15 the function Eq. (101) is strictly monotonically increasing in x and strictly monotonically decreasing in y for x = 1.2. Therefore the function Eq. (101) has its minimum at minimal x = \u03bd\u03c4 = 1.5 \u00b70.8 = 1.2 and maximal y = \u00b5\u03c9 = 1.0 \u00b70.1 = 0.1. We insert these values into the expression.\n\u2013 Third inequality: We use for the whole expression the maximal factor e\u2212 \u00b52\u03c92\n2\u03bd\u03c4 < 1 by setting this factor to 1.\n\u2013 Fourth inequality: erfc is strictly monotonically decreasing. Therefore we maximize its argument to obtain the least value which is subtracted. We use the minimal x = \u03bd\u03c4 = 1.5 \u00b7 0.8 = 1.2 and the maximal y = \u00b5\u03c9 = 1.0 \u00b7 0.1 = 0.1.\n\u2013 Sixth inequality: evaluation of the terms.\n\u2022 We now show that \u00b5\u0303 > 0. The expression \u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4) (Eq. (4)) is strictly monotonically increasing im \u00b5\u03c9 and \u03bd\u03c4 . Therefore, the minimal value in \u2126+ is obtained at \u00b5\u0303(0.01, 0.01, 1.5, 0.8) = 0.008293 > 0.\n\u2022 Last we show that \u2202\u2202\u03bd \u00b5\u0303 > 0. The expression \u2202 \u2202\u03bd \u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4) = J12(\u00b5, \u03c9, \u03bd, \u03c4) (Eq. (54))\ncan we reformulated as follows:\nJ12(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) = \u03bb\u03c4e\u2212\n\u00b52\u03c92\n2\u03bd\u03c4 (\u221a \u03c0\u03b1e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 \u221a\n2(\u03b1\u22121)\u221a \u03bd\u03c4 ) 4 \u221a \u03c0 (108)\nis larger than zero when the term \u221a \u03c0\u03b1e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 \u221a\n2(\u03b1\u22121)\u221a \u03bd\u03c4\nis larger than zero. This term obtains its minimal value at \u00b5\u03c9 = 0.01 and \u03bd\u03c4 = 16 \u00b7 1.25, which can easily be shown using the Abramowitz bounds (Lemma 22) and evaluates to 0.16, therefore J12 > 0 in \u2126+.\nA3.4.4 Lemmata for proofing Theorem 3: The variance is expanding\nMain Sub-Function From Below. We consider functions in \u00b5\u03c9 and \u03bd\u03c4 , therefore we set x = \u00b5\u03c9 and y = \u03bd\u03c4 .\nFor \u03bb = \u03bb01 and \u03b1 = \u03b101, we consider the domain \u22120.1 6 \u00b5 6 0.1, \u22120.1 6 \u03c9 6 0.1 0.00875 6 \u03bd 6 0.7, and 0.8 6 \u03c4 6 1.25.\nFor x and y we obtain: 0.8 \u00b70.00875 = 0.007 6 x 6 0.875 = 1.25 \u00b70.7 and 0.1 \u00b7 (\u22120.1) = \u22120.01 6 y 6 0.01 = 0.1 \u00b7 0.1. In the following we assume to be within this domain. In this domain, we consider the main sub-function of the derivate of second moment in the next layer, J22 (Eq. (54)):\n\u2202\n\u2202\u03bd \u03be\u0303 =\n1 2 \u03bb2\u03c4\n( \u2212\u03b12e\u00b5\u03c9+ \u03bd\u03c42 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) + 2\u03b12e2\u00b5\u03c9+2\u03bd\u03c4 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2 ) (109)\nthat depends on \u00b5\u03c9 and \u03bd\u03c4 , therefore we set x = \u03bd\u03c4 and y = \u00b5\u03c9. Algebraic reformulations provide the formula in the following form:\n\u2202\n\u2202\u03bd \u03be\u0303 = (110)\n1 2 \u03bb2\u03c4\n( \u03b12 ( \u2212e\u2212 y2 2x )( e (x+y)2 2x erfc ( y + x\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( y + 2x\u221a\n2 \u221a x\n)) \u2212 erfc ( y\u221a 2 \u221a x ) + 2 ) Lemma 17 (Main subfunction Below). For 0.007 6 x 6 0.875 and\u22120.01 6 y 6 0.01, the function\ne (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( 2x+ y\u221a\n2 \u221a x\n) (111)\nsmaller than zero, is strictly monotonically increasing in x and strictly monotonically increasing in y for the minimal x = 0.007 = 0.00875 \u00b7 0.8, x = 0.56 = 0.7 \u00b7 0.8, x = 0.128 = 0.16 \u00b7 0.8, and x = 0.216 = 0.24 \u00b7 0.9 (lower bound of 0.9 on \u03c4 ).\nProof. See proof 45.\nLemma 18 (Monotone Derivative). For \u03bb = \u03bb01, \u03b1 = \u03b101 and the domain \u22120.1 6 \u00b5 6 0.1, \u22120.1 6 \u03c9 6 0.1, 0.00875 6 \u03bd 6 0.7, and 0.8 6 \u03c4 6 1.25. We are interested of the derivative of\n\u03c4 ( e ( \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 2e ( \u00b5\u03c9+2\u00b7\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n)) . (112)\nThe derivative of the equation above with respect to\n\u2022 \u03bd is larger than zero;\n\u2022 \u03c4 is smaller than zero for maximal \u03bd = 0.7, \u03bd = 0.16, and \u03bd = 0.24 (with 0.9 6 \u03c4 );\n\u2022 y = \u00b5\u03c9 is larger than zero for \u03bd\u03c4 = 0.008750.8 = 0.007, \u03bd\u03c4 = 0.70.8 = 0.56, \u03bd\u03c4 = 0.160.8 = 0.128, and \u03bd\u03c4 = 0.24 \u00b7 0.9 = 0.216.\nProof. See proof 46.\nA3.4.5 Computer-assisted proof details for main Lemma 12 in Section A3.4.1.\nError Analysis. We investigate the error propagation for the singular value (Eq. (61)) if the function arguments \u00b5, \u03c9, \u03bd, \u03c4 suffer from numerical imprecisions up to . To this end, we first derive error propagation rules based on the mean value theorem and then we apply these rules to the formula for the singular value. Lemma 19 (Mean value theorem). For a real-valued function f which is differentiable in the closed interval [a, b], there exists t \u2208 [0, 1] with\nf(a) \u2212 f(b) = \u2207f(a + t(b\u2212 a)) \u00b7 (a \u2212 b) . (113)\nIt follows that for computation with error \u2206x, there exists a t \u2208 [0, 1] with\n|f(x + \u2206x) \u2212 f(x)| 6 \u2016\u2207f(x + t\u2206x)\u2016 \u2016\u2206x\u2016 . (114)\nTherefore the increase of the norm of the error after applying function f is bounded by the norm of the gradient \u2016\u2207f(x + t\u2206x)\u2016. We now compute for the functions, that we consider their gradient and its 2-norm:\n\u2022 addition: f(x) = x1 + x2 and \u2207f(x) = (1, 1), which gives \u2016\u2207f(x)\u2016 = \u221a 2.\nWe further know that\n|f(x + \u2206x)\u2212 f(x)| = |x1 + x2 + \u2206x1 + \u2206x2 \u2212 x1 \u2212 x2| 6 |\u2206x1|+ |\u2206x2| . (115)\nAdding n terms gives:\u2223\u2223\u2223\u2223\u2223 n\u2211 i=1 xi + \u2206xi \u2212 n\u2211 i=1 xi \u2223\u2223\u2223\u2223\u2223 6 n\u2211 i=1 |\u2206xi| 6 n |\u2206xi|max . (116)\n\u2022 subtraction: f(x) = x1 \u2212 x2 and \u2207f(x) = (1,\u22121), which gives \u2016\u2207f(x)\u2016 = \u221a 2.\nWe further know that\n|f(x + \u2206x)\u2212 f(x)| = |x1 \u2212 x2 + \u2206x1 \u2212\u2206x2 \u2212 x1 + x2| 6 |\u2206x1|+ |\u2206x2| . (117)\nSubtracting n terms gives:\u2223\u2223\u2223\u2223\u2223 n\u2211 i=1 \u2212(xi + \u2206xi) + n\u2211 i=1 xi \u2223\u2223\u2223\u2223\u2223 6 n\u2211 i=1 |\u2206xi| 6 n |\u2206xi|max . (118)\n\u2022 multiplication: f(x) = x1x2 and\u2207f(x) = (x2, x1), which gives \u2016\u2207f(x)\u2016 = \u2016x\u2016. We further know that\n|f(x + \u2206x)\u2212 f(x)| = |x1 \u00b7 x2 + \u2206x1 \u00b7 x2 + \u2206x2 \u00b7 x1 + \u2206x1 \u00b7\u2206xs \u2212 x1 \u00b7 x2| 6 (119)\n|\u2206x1| |x2|+ |\u2206x2| |x1|+O(\u22062) .\nMultiplying n terms gives:\u2223\u2223\u2223\u2223\u2223 n\u220f i=1 (xi + \u2206xi) \u2212 n\u220f i=1 xi \u2223\u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223 n\u220f i=1 xi n\u2211 i=1 \u2206xi xi + O(\u22062) \u2223\u2223\u2223\u2223\u2223 6 (120) n\u220f i=1 |xi| n\u2211 i=1 \u2223\u2223\u2223\u2223\u2206xixi \u2223\u2223\u2223\u2223 + O(\u22062) 6 n n\u220f i=1 |xi| \u2223\u2223\u2223\u2223\u2206xixi \u2223\u2223\u2223\u2223 max + O(\u22062) .\n\u2022 division: f(x) = x1x2 and \u2207f(x) = ( 1 x2 ,\u2212x1\nx22\n) , which gives \u2016\u2207f(x)\u2016 = \u2016x\u2016\nx22 .\nWe further know that |f(x + \u2206x)\u2212 f(x)| = \u2223\u2223\u2223\u2223x1 + \u2206x1x2 + \u2206x2 \u2212 x1x2 \u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223 (x1 + \u2206x1)x2 \u2212 x1(x2 + \u2206x2)(x2 + \u2206x2)x2 \u2223\u2223\u2223\u2223 = (121)\u2223\u2223\u2223\u2223\u2206x1 \u00b7 x2 \u2212\u2206x2 \u00b7 x1x22 + \u2206x2 \u00b7 x2 \u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u2206x1x2 \u2212 \u2206x2 \u00b7 x1x22 \u2223\u2223\u2223\u2223+O(\u22062) .\n\u2022 square root: f(x) = \u221a x and f \u2032(x) = 1\n2 \u221a x , which gives |f \u2032(x)| = 1 2 \u221a x .\n\u2022 exponential function: f(x) = exp(x) and f \u2032(x) = exp(x), which gives |f \u2032(x)| = exp(x).\n\u2022 error function: f(x) = erf(x) and f \u2032(x) = 2\u221a\n\u03c0 exp(\u2212x2), which gives |f \u2032(x)| = 2\u221a \u03c0 exp(\u2212x2).\n\u2022 complementary error function: f(x) = erfc(x) and f \u2032(x) = \u2212 2\u221a\n\u03c0 exp(\u2212x2), which gives |f \u2032(x)| = 2\u221a \u03c0 exp(\u2212x2).\nLemma 20. If the values \u00b5, \u03c9, \u03bd, \u03c4 have a precision of , the singular value (Eq. (61)) evaluated with the formulas given in Eq. (54) and Eq. (61) has a precision better than 292 .\nThis means for a machine with a typical precision of 2\u221252 = 2.220446 \u00b7 10\u221216, we have the rounding error \u2248 10\u221216, the evaluation of the singular value (Eq. (61)) with the formulas given in Eq. (54) and Eq. (61) has a precision better than 10\u221213 > 292 .\nProof. We have the numerical precision of the parameters \u00b5, \u03c9, \u03bd, \u03c4 , that we denote by \u2206\u00b5,\u2206\u03c9,\u2206\u03bd,\u2206\u03c4 together with our domain \u2126.\nWith the error propagation rules that we derived in Subsection A3.4.5, we can obtain bounds for the numerical errors on the following simple expressions:\n\u2206 (\u00b5\u03c9) 6 \u2206\u00b5 |\u03c9|+ \u2206\u03c9 |\u00b5| 6 0.2 (122) \u2206 (\u03bd\u03c4) 6 \u2206\u03bd |\u03c4 |+ \u2206\u03c4 |\u03bd| 6 1.5 + 1.5 = 3\n\u2206 (\u03bd\u03c4\n2\n) 6 (\u2206(\u03bd\u03c4)2 + \u22062 |\u03bd\u03c4 |) 1\n22 6 (6 + 1.25 \u00b7 1.5 )/4 < 2\n\u2206 (\u00b5\u03c9 + \u03bd\u03c4) 6 \u2206 (\u00b5\u03c9) + \u2206 (\u03bd\u03c4) = 3.2 \u2206 ( \u00b5\u03c9 + \u03bd\u03c4\n2\n) 6 \u2206 (\u00b5\u03c9) + \u2206 (\u03bd\u03c4 2 ) < 2.2\n\u2206 (\u221a \u03bd\u03c4 ) 6 \u2206 (\u03bd\u03c4)\n2 \u221a \u03bd\u03c4\n6 3\n2 \u221a 0.64 = 1.875\n\u2206 (\u221a 2 ) 6 \u22062\n2 \u221a 2 6\n1 2 \u221a 2\n\u2206 (\u221a 2 \u221a \u03bd\u03c4 ) 6 \u221a 2\u2206 (\u221a \u03bd\u03c4 ) + \u03bd\u03c4\u2206 (\u221a 2 ) 6 \u221a\n2 \u00b7 1.875 + 1.5 \u00b7 1.25 \u00b7 1 2 \u221a 2 < 3.5\n\u2206 ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) 6 ( \u2206 (\u00b5\u03c9) \u221a 2 \u221a \u03bd\u03c4 + |\u00b5\u03c9|\u2206 (\u221a 2 \u221a \u03bd\u03c4 )) 1(\u221a 2 \u221a \u03bd\u03c4 )2 6\n( 0.2 \u221a 2 \u221a 0.64 + 0.01 \u00b7 3.5 ) 1\n2 \u00b7 0.64 < 0.25\n\u2206 ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) 6 ( \u2206 (\u00b5\u03c9 + \u03bd\u03c4) \u221a 2 \u221a \u03bd\u03c4 + |\u00b5\u03c9 + \u03bd\u03c4 |\u2206 (\u221a 2 \u221a \u03bd\u03c4 )) 1(\u221a\n2 \u221a \u03bd\u03c4 )2 6(\n3.2 \u221a 2 \u221a 0.64 + 1.885 \u00b7 3.5 ) 1\n2 \u00b7 0.64 < 8 .\nUsing these bounds on the simple expressions, we can now calculate bounds on the numerical errors of compound expressions:\n\u2206 ( erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 )) 6 2\u221a \u03c0 e \u2212 ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 )2 \u2206 ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) < (123)\n2\u221a \u03c0 0.25 < 0.3\n\u2206 ( erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n)) 6 2\u221a \u03c0 e \u2212 ( \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 \u2206 ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) < (124)\n2\u221a \u03c0 8 < 10\n\u2206 ( e\u00b5\u03c9+ \u03bd\u03c4 2 ) 6 ( e\u00b5\u03c9+ \u03bd\u03c4 2 ) \u2206 ( e\u00b5\u03c9+ \u03bd\u03c4 2 ) < (125)\ne0.94752.2 < 5.7 (126)\nSubsequently, we can use the above results to get bounds for the numerical errors on the Jacobian entries (Eq. (54)), applying the rules from Subsection A3.4.5 again:\n\u2206 (J11) = \u2206 ( 1\n2 \u03bb\u03c9\n( \u03b1e\u00b5\u03c9+ \u03bd\u03c4 2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2 )) < 6 , (127)\nand we obtain \u2206 (J12) < 78 , \u2206 (J21) < 189 , \u2206 (J22) < 405 and \u2206 (\u00b5\u0303) < 52 . We also have bounds on the absolute values on Jij and \u00b5\u0303 (see Lemma 6, Lemma 7, and Lemma 9), therefore we can propagate the error also through the function that calculates the singular value (Eq. (61)).\n\u2206 (S(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1)) = (128)\n\u2206\n( 1\n2 (\u221a (J11 + J22 \u2212 2\u00b5\u0303J12)2 + (J21 \u2212 2\u00b5\u0303J11 \u2212 J12)2 +\u221a\n(J11 \u2212 J22 + 2\u00b5\u0303J12)2 + (J12 + J21 \u2212 2\u00b5\u0303J11)2 )) < 292 .\nPrecision of Implementations. We will show that our computations are correct up to 3 ulps. For our implementation in GNU C library and the hardware architectures that we used, the precision of all mathematical functions that we used is at least one ulp. The term \u201culp\u201d (acronym for \u201cunit in the last place\u201d) was coined by W. Kahan in 1960. It is the highest precision (up to some factor smaller 1), which can be achieved for the given hardware and floating point representation.\nKahan defined ulp as [21]:\n\u201cUlp(x) is the gap between the two finite floating-point numbers nearest x, even if x is one of them. (But ulp(NaN) is NaN.)\u201d\nHarrison defined ulp as [15]:\n\u201can ulp in x is the distance between the two closest straddling floating point numbers a and b, i.e. those with a 6 x 6 b and a 6= b assuming an unbounded exponent range.\u201d\nIn the literature we find also slightly different definitions [29].\nAccording to [29] who refers to [11]:\n\u201cIEEE-754 mandates four standard rounding modes:\u201d \u201cRound-to-nearest: r(x) is the floating-point value closest to x with the usual distance; if two floating-point value are equally close to x, then r(x) is the one whose least significant bit is equal to zero.\u201d \u201cIEEE-754 standardises 5 operations: addition (which we shall note \u2295 in order to distinguish it from the operation over the reals), subtraction ( ), multiplication (\u2297), division ( ), and also square root.\u201d \u201cIEEE-754 specifies em exact rounding [Goldberg, 1991, \u00a71.5]: the result of a floating-point operation is the same as if the operation were performed on the real numbers with the given inputs, then rounded according to the rules in the preceding section. Thus, x \u2295 y is defined as r(x + y), with x and y taken as elements of R \u222a {\u2212\u221e,+\u221e}; the same applies for the other operators.\u201d\nConsequently, the IEEE-754 standard guarantees that addition, subtraction, multiplication, division, and squared root is precise up to one ulp.\nWe have to consider transcendental functions. First the is the exponential function, and then the complementary error function erfc(x), which can be computed via the error function erf(x).\nIntel states [29]:\n\u201cWith the Intel486 processor and Intel 387 math coprocessor, the worst- case, transcendental function error is typically 3 or 3.5 ulps, but is some- times as large as 4.5 ulps.\u201d\nAccording to https://www.mirbsd.org/htman/i386/man3/exp.htm and http: //man.openbsd.org/OpenBSD-current/man3/exp.3:\n\u201cexp(x), log(x), expm1(x) and log1p(x) are accurate to within an ulp\u201d\nwhich is the same for freebsd https://www.freebsd.org/cgi/man.cgi?query=exp&sektion= 3&apropos=0&manpath=freebsd:\n\u201cThe values of exp(0), expm1(0), exp2(integer), and pow(integer, integer) are exact provided that they are representable. Otherwise the error in these functions is generally below one ulp.\u201d\nThe same holds for \u201cFDLIBM\u201d http://www.netlib.org/fdlibm/readme:\n\u201cFDLIBM is intended to provide a reasonably portable (see assumptions below), reference quality (below one ulp for major functions like sin,cos,exp,log) math library (libm.a).\u201d\nIn http://www.gnu.org/software/libc/manual/html_node/ Errors-in-Math-Functions.html we find that both exp and erf have an error of 1 ulp while erfc has an error up to 3 ulps depending on the architecture. For the most common architectures as used by us, however, the error of erfc is 1 ulp.\nWe implemented the function in the programming language C. We rely on the GNU C Library [26]. According to the GNU C Library manual which can be obtained from http://www.gnu.org/ software/libc/manual/pdf/libc.pdf, the errors of the math functions exp, erf , and erfc are not larger than 3 ulps for all architectures [26, pp. 528]. For the architectures ix86, i386/i686/fpu, and m68k/fpmu68k/m680x0/fpu that we used the error are at least one ulp [26, pp. 528].\n2*exp(\u2212x^2)/(sqrt(pi)*(sqrt(x^2+4/pi)+x))\n2*exp(\u2212x^2)/(sqrt(pi)*(sqrt(x^2+2)+x))\nFunction\nerfc(x)\n0.00\n0.25\n0.50\n0.75\n1.00\n0.0 0.5 1.0 1.5 2.0\nx\ny\nFigure A4: Graphs of the upper and lower bounds on erfc. The lower bound 2e \u2212x2\n\u221a \u03c0( \u221a x2+2+x) (red), the\nupper bound 2e \u2212x2 \u221a \u03c0 (\u221a x2+ 4\u03c0+x ) (green) and the function erfc(x) (blue) as treated in Lemma 22.\nA3.4.6 Intermediate Lemmata and Proofs\nSince we focus on the fixed point (\u00b5, \u03bd) = (0, 1), we assume for our whole analysis that \u03b1 = \u03b101 and \u03bb = \u03bb01. Furthermore, we restrict the range of the variables \u00b5 \u2208 [\u00b5min, \u00b5max] = [\u22120.1, 0.1], \u03c9 \u2208 [\u03c9min, \u03c9max] = [\u22120.1, 0.1], \u03bd \u2208 [\u03bdmin, \u03bdmax] = [0.8, 1.5], and \u03c4 \u2208 [\u03c4min, \u03c4max] = [0.8, 1.25]. For bounding different partial derivatives we need properties of different functions. We will bound a the absolute value of a function by computing an upper bound on its maximum and a lower bound on its minimum. These bounds are computed by upper or lower bounding terms. The bounds get tighter if we can combine terms to a more complex function and bound this function. The following lemmata give some properties of functions that we will use in bounding complex functions.\nThroughout this work, we use the error function erf(x) := 1\u221a \u03c0 \u222b x \u2212x e\n\u2212t2 and the complementary error function erfc(x) = 1\u2212 erf(x). Lemma 21 (Basic functions). exp(x) is strictly monotonically increasing from 0 at \u2212\u221e to\u221e at\u221e and has positive curvature.\nAccording to its definition erfc(x) is strictly monotonically decreasing from 2 at \u2212\u221e to 0 at\u221e.\nNext we introduce a bound on erfc:\nLemma 22 (Erfc bound from Abramowitz).\n2e\u2212x 2 \u221a \u03c0 (\u221a x2 + 2 + x ) < erfc(x) 6 2e\u2212x2\u221a \u03c0 (\u221a x2 + 4\u03c0 + x ) , (129)\nfor x > 0.\nProof. The statement follows immediately from [1] (page 298, formula 7.1.13).\nThese bounds are displayed in figure A4.\nLemma 23 (Function ex 2 erfc(x)). ex 2\nerfc(x) is strictly monotonically decreasing for x > 0 and has positive curvature (positive 2nd order derivative), that is, the decreasing slowes down.\nA graph of the function is displayed in Figure A5.\n0 1\n2\n3\n4\n5\n\u22121 0 1 2 3\ne x p (x\n^2 )*\ne rf\nc (x\n)\nx\n\u22125\n\u22124\n\u22123\n\u22122\n\u22121\n0\n\u22121 0 1 2 3\nx *e\nx p\n(x ^2\n)* e rf\nc (x\n)\nx\nFigure A5: Graphs of the functions ex 2 erfc(x) (left) and xex 2\nerfc(x) (right) treated in Lemma 23 and Lemma 24, respectively.\nProof. The derivative of ex 2 erfc(x) is\n\u2202ex 2 erfc(x)\n\u2202x = 2ex\n2 x erfc(x)\u2212 2\u221a \u03c0 . (130)\nUsing Lemma 22, we get\n\u2202ex 2 erfc(x)\n\u2202x = 2ex\n2 x erfc(x)\u2212 2\u221a \u03c0 < 4x \u221a \u03c0 (\u221a x2 + 4\u03c0 + x ) \u2212 2\u221a \u03c0 =\n2\n( 2\u221a\n4 \u03c0x2\n+1+1 \u2212 1 ) \u221a \u03c0 < 0\n(131)\nThus ex 2 erfc(x) is strictly monotonically decreasing for x > 0.\nThe second order derivative of ex 2 erfc(x) is\n\u22022ex 2 erfc(x)\n\u2202x2 = 4ex\n2 x2 erfc(x) + 2ex 2 erfc(x)\u2212 4x\u221a \u03c0 . (132)\nAgain using Lemma 22 (first inequality), we get\n2 (( 2x2 + 1 ) ex 2\nerfc(x)\u2212 2x\u221a \u03c0\n) > (133)\n4 ( 2x2 + 1 ) \u221a \u03c0 (\u221a x2 + 2 + x ) \u2212 4x\u221a \u03c0 =\n4 ( x2 \u2212 \u221a x2 + 2x+ 1 ) \u221a \u03c0 (\u221a x2 + 2 + x\n) = 4 ( x2 \u2212 \u221a x4 + 2x2 + 1\n) \u221a \u03c0 (\u221a x2 + 2 + x\n) > 4 ( x2 \u2212 \u221a x4 + 2x2 + 1 + 1\n) \u221a \u03c0 (\u221a x2 + 2 + x\n) = 0 For the last inequality we added 1 in the numerator in the square root which is subtracted, that is, making a larger negative term in the numerator.\nLemma 24 (Properties of xex 2 erfc(x)). The function xex 2\nerfc(x) has the sign of x and is monotonically increasing to 1\u221a\n\u03c0 .\nProof. The derivative of xex 2 erfc(x) is\n2ex 2 x2 erfc(x) + ex 2 erfc(x)\u2212 2x\u221a \u03c0 . (134)\nThis derivative is positive since\n2ex 2 x2 erfc(x) + ex 2 erfc(x)\u2212 2x\u221a \u03c0 = (135) ex 2 ( 2x2 + 1 )\nerfc(x)\u2212 2x\u221a \u03c0 >\n2 ( 2x2 + 1 ) \u221a \u03c0 (\u221a x2 + 2 + x ) \u2212 2x\u221a \u03c0 = 2 (( 2x2 + 1 ) \u2212 x (\u221a x2 + 2 + x )) \u221a \u03c0 (\u221a x2 + 2 + x\n) = 2 ( x2 \u2212 x \u221a x2 + 2 + 1\n) \u221a \u03c0 (\u221a x2 + 2 + x ) = 2 (x2 \u2212 x\u221ax2 + 2 + 1)\u221a \u03c0 (\u221a x2 + 2 + x ) > 2 ( x2 \u2212 x \u221a x2 + 1x2 + 2 + 1 ) \u221a \u03c0 (\u221a x2 + 2 + x\n) = 2 ( x2 \u2212 \u221a x4 + 2x2 + 1 + 1\n) \u221a \u03c0 (\u221a x2 + 2 + x ) = 2 ( x2 \u2212 \u221a (x2 + 1) 2 + 1 ) \u221a \u03c0 (\u221a x2 + 2 + x\n) = 0 . We apply Lemma 22 to x erfc(x)ex 2 and divide the terms of the lemma by x, which gives\n2 \u221a \u03c0 (\u221a\n2 x2 + 1 + 1\n) < x erfc(x)ex2 6 2\u221a \u03c0 (\u221a\n4 \u03c0x2 + 1 + 1 ) . (136) For limx\u2192\u221e both the upper and the lower bound go to 1\u221a\u03c0 .\nLemma 25 (Function \u00b5\u03c9). h11(\u00b5, \u03c9) = \u00b5\u03c9 is monotonically increasing in \u00b5\u03c9. It has minimal value t11 = \u22120.01 and maximal value T11 = 0.01.\nProof. Obvious.\nLemma 26 (Function \u03bd\u03c4 ). h22(\u03bd, \u03c4) = \u03bd\u03c4 is monotonically increasing in \u03bd\u03c4 and is positive. It has minimal value t22 = 0.64 and maximal value T22 = 1.875.\nProof. Obvious.\nLemma 27 (Function \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ). h1(\u00b5, \u03c9, \u03bd, \u03c4) = \u00b5\u03c9+\u03bd\u03c4\u221a2\u221a\u03bd\u03c4 is larger than zero and increasing in both \u03bd\u03c4 and \u00b5\u03c9. It has minimal value t1 = 0.5568 and maximal value T1 = 0.9734.\nProof. The derivative of the function \u00b5\u03c9+x\u221a 2 \u221a x with respect to x is\n1\u221a 2 \u221a x \u2212 \u00b5\u03c9 + x 2 \u221a 2x3/2 = 2x\u2212 (\u00b5\u03c9 + x) 2 \u221a 2x3/2 = x\u2212 \u00b5\u03c9 2 \u221a 2x3/2 > 0 , (137)\nsince x > 0.8 \u00b7 0.8 and \u00b5\u03c9 < 0.1 \u00b7 0.1.\nLemma 28 (Function \u00b5\u03c9+2\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ). h2(\u00b5, \u03c9, \u03bd, \u03c4) = \u00b5\u03c9+2\u03bd\u03c4\u221a2\u221a\u03bd\u03c4 is larger than zero and increasing in both \u03bd\u03c4 and \u00b5\u03c9. It has minimal value t2 = 1.1225 and maximal value T2 = 1.9417.\nProof. The derivative of the function \u00b5\u03c9+2x\u221a 2 \u221a x with respect to x is \u221a\n2\u221a x \u2212 \u00b5\u03c9 + 2x 2 \u221a 2x3/2 = 4x\u2212 (\u00b5\u03c9 + 2x) 2 \u221a 2x3/2 = 2x\u2212 \u00b5\u03c9 2 \u221a 2x3/2 > 0 . (138)\nLemma 29 (Function \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ). h3(\u00b5, \u03c9, \u03bd, \u03c4) = \u00b5\u03c9\u221a2\u221a\u03bd\u03c4 monotonically decreasing in \u03bd\u03c4 and monotonically increasing in \u00b5\u03c9. It has minimal value t3 = \u22120.0088388 and maximal value T3 = 0.0088388.\nProof. Obvious. Lemma 30 (Function (\n\u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4\n)2 ). h4(\u00b5, \u03c9, \u03bd, \u03c4) = ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 )2 has a minimum at 0 for \u00b5 = 0 or\n\u03c9 = 0 and has a maximum for the smallest \u03bd\u03c4 and largest |\u00b5\u03c9| and is larger or equal to zero. It has minimal value t4 = 0 and maximal value T4 = 0.000078126.\nProof. Obvious. Lemma 31 (Function \u221a\n2 \u03c0 (\u03b1\u22121)\u221a \u03bd\u03c4\n). \u221a\n2 \u03c0 (\u03b1\u22121)\u221a \u03bd\u03c4 > 0 and decreasing in \u03bd\u03c4 .\nProof. Statements follow directly from elementary functions square root and division. Lemma 32 (Function 2 \u2212 erfc (\n\u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4\n) ). 2 \u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) > 0 and decreasing in \u03bd\u03c4 and\nincreasing in \u00b5\u03c9.\nProof. Statements follow directly from Lemma 21 and erfc. Lemma 33 (Function \u221a\n2 \u03c0 ( (\u03b1\u22121)\u00b5\u03c9 (\u03bd\u03c4)3/2 \u2212 \u03b1\u221a \u03bd\u03c4 ) ). For \u03bb = \u03bb01 and \u03b1 = \u03b101,\u221a\n2 \u03c0 ( (\u03b1\u22121)\u00b5\u03c9 (\u03bd\u03c4)3/2 \u2212 \u03b1\u221a \u03bd\u03c4 ) < 0 and increasing in both \u03bd\u03c4 and \u00b5\u03c9.\nProof. We consider the function \u221a\n2 \u03c0 ( (\u03b1\u22121)\u00b5\u03c9 x3/2 \u2212 \u03b1\u221a x ) , which has the derivative with respect to x:\u221a\n2\n\u03c0\n( \u03b1\n2x3/2 \u2212 3(\u03b1\u2212 1)\u00b5\u03c9 2x5/2\n) . (139)\nThis derivative is larger than zero, since\u221a 2\n\u03c0\n( \u03b1\n2(\u03bd\u03c4)3/2 \u2212 3(\u03b1\u2212 1)\u00b5\u03c9 2(\u03bd\u03c4)5/2\n) >\n\u221a 2 \u03c0 ( \u03b1\u2212 3(\u03b1\u22121)\u00b5\u03c9\u03bd\u03c4 ) 2(\u03bd\u03c4)3/2 > 0 . (140)\nThe last inequality follows from \u03b1\u2212 3\u00b70.1\u00b70.1(\u03b1\u22121)0.8\u00b70.8 > 0 for \u03b1 = \u03b101. We next consider the function \u221a\n2 \u03c0 ( (\u03b1\u22121)x (\u03bd\u03c4)3/2 \u2212 \u03b1\u221a \u03bd\u03c4 ) , which has the derivative with respect to x:\u221a\n2 \u03c0 (\u03b1\u2212 1) (\u03bd\u03c4)3/2 > 0 . (141)\nLemma 34 (Function \u221a\n2 \u03c0\n( (\u22121)(\u03b1\u22121)\u00b52\u03c92\n(\u03bd\u03c4)3/2 + \u2212\u03b1+\u03b1\u00b5\u03c9+1\u221a \u03bd\u03c4 \u2212 \u03b1 \u221a \u03bd\u03c4 )\n). The function\u221a 2 \u03c0 ( (\u22121)(\u03b1\u22121)\u00b52\u03c92 (\u03bd\u03c4)3/2 + \u2212\u03b1+\u03b1\u00b5\u03c9+1\u221a \u03bd\u03c4 \u2212 \u03b1 \u221a \u03bd\u03c4 ) < 0 is decreasing in \u03bd\u03c4 and increasing in \u00b5\u03c9.\nProof. We define the function\u221a 2\n\u03c0\n( (\u22121)(\u03b1\u2212 1)\u00b52\u03c92\nx3/2 + \u2212\u03b1+ \u03b1\u00b5\u03c9 + 1\u221a x \u2212 \u03b1 \u221a x\n) (142)\nwhich has as derivative with respect to x:\u221a 2\n\u03c0\n( 3(\u03b1\u2212 1)\u00b52\u03c92\n2x5/2 \u2212 \u2212\u03b1+ \u03b1\u00b5\u03c9 + 1 2x3/2 \u2212 \u03b1 2 \u221a x\n) = (143)\n1\u221a 2\u03c0x5/2\n( 3(\u03b1\u2212 1)\u00b52\u03c92 \u2212 x(\u2212\u03b1+ \u03b1\u00b5\u03c9 + 1)\u2212 \u03b1x2 ) .\nThe derivative of the term 3(\u03b1 \u2212 1)\u00b52\u03c92 \u2212 x(\u2212\u03b1 + \u03b1\u00b5\u03c9 + 1) \u2212 \u03b1x2 with respect to x is \u22121 + \u03b1\u2212 \u00b5\u03c9\u03b1\u2212 2\u03b1x < 0, since 2\u03b1x > 1.6\u03b1. Therefore the term is maximized with the smallest value for x, which is x = \u03bd\u03c4 = 0.8 \u00b7 0.8. For \u00b5\u03c9 we use for each term the value which gives maximal contribution. We obtain an upper bound for the term:\n3(\u22120.1 \u00b7 0.1)2(\u03b101 \u2212 1)\u2212 (0.8 \u00b7 0.8)2\u03b101 \u2212 0.8 \u00b7 0.8((\u22120.1 \u00b7 0.1)\u03b101 \u2212 \u03b101 + 1) = \u22120.243569 . (144)\nTherefore the derivative with respect to x = \u03bd\u03c4 is smaller than zero and the original function is decreasing in \u03bd\u03c4\nWe now consider the derivative with respect to x = \u00b5\u03c9. The derivative with respect to x of the function \u221a\n2\n\u03c0\n( \u2212\u03b1 \u221a \u03bd\u03c4 \u2212 (\u03b1\u2212 1)x 2\n(\u03bd\u03c4)3/2 + \u2212\u03b1+ \u03b1x+ 1\u221a \u03bd\u03c4\n) (145)\nis \u221a 2 \u03c0 (\u03b1\u03bd\u03c4 \u2212 2(\u03b1\u2212 1)x)\n(\u03bd\u03c4)3/2 . (146)\nSince \u22122x(\u22121 + \u03b1) + \u03bd\u03c4\u03b1 > \u22122 \u00b7 0.01 \u00b7 (\u22121 + \u03b101) + 0.8 \u00b7 0.8\u03b101 > 1.0574 > 0, the derivative is larger than zero. Consequently, the original function is increasing in \u00b5\u03c9.\nThe maximal value is obtained with the minimal \u03bd\u03c4 = 0.8 \u00b7 0.8 and the maximal \u00b5\u03c9 = 0.1 \u00b7 0.1. The maximal value is\u221a\n2\n\u03c0\n( 0.1 \u00b7 0.1\u03b101 \u2212 \u03b101 + 1\u221a\n0.8 \u00b7 0.8 + 0.120.12(\u22121)(\u03b101 \u2212 1) (0.8 \u00b7 0.8)3/2 \u2212 \u221a\n0.8 \u00b7 0.8\u03b101 ) = \u22121.72296 .\n(147)\nTherefore the original function is smaller than zero. Lemma 35 (Function \u221a\n2 \u03c0\n( (\u03b12\u22121)\u00b5\u03c9\n(\u03bd\u03c4)3/2 \u2212 3\u03b1 2 \u221a \u03bd\u03c4 ) ). For \u03bb = \u03bb01 and \u03b1 = \u03b101,\u221a\n2 \u03c0\n( (\u03b12\u22121)\u00b5\u03c9\n(\u03bd\u03c4)3/2 \u2212 3\u03b1 2 \u221a \u03bd\u03c4\n) < 0 and increasing in both \u03bd\u03c4 and \u00b5\u03c9.\nProof. The derivative of the function\u221a 2\n\u03c0\n(( \u03b12 \u2212 1 ) \u00b5\u03c9\nx3/2 \u2212 3\u03b1\n2 \u221a x\n) (148)\nwith respect to x is\u221a 2\n\u03c0\n( 3\u03b12\n2x3/2 \u2212\n3 ( \u03b12 \u2212 1 ) \u00b5\u03c9\n2x5/2\n) = 3 ( \u03b12x\u2212 ( \u03b12 \u2212 1 ) \u00b5\u03c9 )\n\u221a 2\u03c0x5/2\n> 0 , (149)\nsince \u03b12x\u2212 \u00b5\u03c9(\u22121 + \u03b12) > \u03b12010.8 \u00b7 0.8\u2212 0.1 \u00b7 0.1 \u00b7 (\u22121 + \u03b1201) > 1.77387 The derivative of the function \u221a\n2\n\u03c0\n(( \u03b12 \u2212 1 ) x\n(\u03bd\u03c4)3/2 \u2212 3\u03b1\n2 \u221a \u03bd\u03c4\n) (150)\nwith respect to x is \u221a 2 \u03c0 ( \u03b12 \u2212 1 ) (\u03bd\u03c4)3/2 > 0 . (151)\nThe maximal function value is obtained by maximal \u03bd\u03c4 = 1.5 \u00b7 1.25 and the maximal \u00b5\u03c9 = 0.1 \u00b7 0.1. The maximal value is \u221a\n2 \u03c0\n( 0.1\u00b70.1(\u03b1201\u22121)\n(1.5\u00b71.25)3/2 \u2212 3\u03b1201\u221a 1.5\u00b71.25\n) = \u22124.88869. Therefore the function is\nnegative.\nLemma 36 (Function \u221a\n2 \u03c0\n( (\u03b12\u22121)\u00b5\u03c9\u221a\n\u03bd\u03c4 \u2212 3\u03b12\n\u221a \u03bd\u03c4 ) ). The function\u221a\n2 \u03c0\n( (\u03b12\u22121)\u00b5\u03c9\u221a\n\u03bd\u03c4 \u2212 3\u03b12\n\u221a \u03bd\u03c4 ) < 0 is decreasing in \u03bd\u03c4 and increasing in \u00b5\u03c9.\nProof. The derivative of the function\u221a 2\n\u03c0\n(( \u03b12 \u2212 1 ) \u00b5\u03c9\n\u221a x\n\u2212 3\u03b12 \u221a x ) (152)\nwith respect to x is\u221a 2\n\u03c0\n( \u2212 ( \u03b12 \u2212 1 ) \u00b5\u03c9\n2x3/2 \u2212 3\u03b1\n2\n2 \u221a x\n) = \u2212 ( \u03b12 \u2212 1 ) \u00b5\u03c9 \u2212 3\u03b12x\n\u221a 2\u03c0x3/2\n< 0 , (153)\nsince \u22123\u03b12x\u2212 \u00b5\u03c9(\u22121 + \u03b12) < \u22123\u03b12010.8 \u00b7 0.8 + 0.1 \u00b7 0.1(\u22121 + \u03b1201) < \u22125.35764. The derivative of the function \u221a\n2\n\u03c0\n(( \u03b12 \u2212 1 ) x\n\u221a \u03bd\u03c4\n\u2212 3\u03b12 \u221a \u03bd\u03c4 ) (154)\nwith respect to x is \u221a 2 \u03c0 ( \u03b12 \u2212 1 ) \u221a \u03bd\u03c4 > 0 . (155)\nThe maximal function value is obtained for minimal \u03bd\u03c4 = 0.8 \u00b7 0.8 and the maximal \u00b5\u03c9 = 0.1 \u00b7 0.1. The value is \u221a\n2 \u03c0\n( 0.1\u00b70.1(\u03b1201\u22121)\u221a\n0.8\u00b70.8 \u2212 3 \u221a 0.8 \u00b7 0.8\u03b1201 ) = \u22125.34347. Thus, the function is\nnegative.\nLemma 37 (Function \u03bd\u03c4e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) ). The function \u03bd\u03c4e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) > 0 is\nincreasing in \u03bd\u03c4 and decreasing in \u00b5\u03c9.\nProof. The derivative of the function\nxe (\u00b5\u03c9+x)2 2x erfc ( \u00b5\u03c9 + x\u221a\n2 \u221a x\n) (156)\nwith respect to x is\ne (\u00b5\u03c9+x)2 2x ( x(x+ 2)\u2212 \u00b52\u03c92 ) erfc ( \u00b5\u03c9+x\u221a\n2 \u221a x ) 2x + \u00b5\u03c9 \u2212 x\u221a 2\u03c0 \u221a x . (157)\nThis derivative is larger than zero, since\ne (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 ( \u03bd\u03c4(\u03bd\u03c4 + 2)\u2212 \u00b52\u03c92 ) erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4 ) 2\u03bd\u03c4 + \u00b5\u03c9 \u2212 \u03bd\u03c4\u221a 2\u03c0 \u221a \u03bd\u03c4 > (158)\n0.4349 ( \u03bd\u03c4(\u03bd\u03c4 + 2)\u2212 \u00b52\u03c92 ) 2\u03bd\u03c4 + \u00b5\u03c9 \u2212 \u03bd\u03c4\u221a 2\u03c0 \u221a \u03bd\u03c4 >\n0.5 ( \u03bd\u03c4(\u03bd\u03c4 + 2)\u2212 \u00b52\u03c92 ) \u221a\n2\u03c0\u03bd\u03c4 + \u00b5\u03c9 \u2212 \u03bd\u03c4\u221a 2\u03c0 \u221a \u03bd\u03c4 =\n0.5 ( \u03bd\u03c4(\u03bd\u03c4 + 2)\u2212 \u00b52\u03c92 ) + \u221a \u03bd\u03c4(\u00b5\u03c9 \u2212 \u03bd\u03c4)\n\u221a 2\u03c0\u03bd\u03c4\n=\n\u22120.5\u00b52\u03c92 + \u00b5\u03c9 \u221a \u03bd\u03c4 + 0.5(\u03bd\u03c4)2 \u2212 \u03bd\u03c4 \u221a \u03bd\u03c4 + \u03bd\u03c4\u221a\n2\u03c0\u03bd\u03c4 =\n\u22120.5\u00b52\u03c92 + \u00b5\u03c9 \u221a \u03bd\u03c4 + (0.5\u03bd\u03c4 \u2212 \u221a \u03bd\u03c4)\n2 + 0.25(\u03bd\u03c4)2\u221a\n2\u03c0\u03bd\u03c4 > 0 .\nWe explain this chain of inequalities:\n\u2022 The first inequality follows by applying Lemma 23 which says that e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4 ) is strictly monotonically decreasing. The minimal value that is larger than 0.4349 is taken on at the maximal values \u03bd\u03c4 = 1.5 \u00b7 1.25 and \u00b5\u03c9 = 0.1 \u00b7 0.1.\n\u2022 The second inequality uses 120.4349 \u221a 2\u03c0 = 0.545066 > 0.5.\n\u2022 The equalities are just algebraic reformulations. \u2022 The last inequality follows from \u22120.5\u00b52\u03c92 + \u00b5\u03c9 \u221a \u03bd\u03c4 + 0.25(\u03bd\u03c4)2 > 0.25(0.8 \u00b7 0.8)2 \u2212\n0.5 \u00b7 (0.1)2(0.1)2 \u2212 0.1 \u00b7 0.1 \u00b7 \u221a 0.8 \u00b7 0.8 = 0.09435 > 0.\nTherefore the function is increasing in \u03bd\u03c4 .\nDecreasing in \u00b5\u03c9 follows from decreasing of ex 2\nerfc(x) according to Lemma 23. Positivity follows form the fact that erfc and the exponential function are positive and that \u03bd\u03c4 > 0.\nLemma 38 (Function \u03bd\u03c4e (\u00b5\u03c9+2\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9+2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) ). The function \u03bd\u03c4e (\u00b5\u03c9+2\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9+2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) > 0\nis increasing in \u03bd\u03c4 and decreasing in \u00b5\u03c9.\nProof. The derivative of the function\nxe (\u00b5\u03c9+2x)2 2x erfc ( \u00b5\u03c9 + 2x\u221a\n2 \u221a 2x\n) (159)\nis\ne (\u00b5\u03c9+2x)2 4x (\u221a \u03c0e (\u00b5\u03c9+2x)2 4x ( 2x(2x+ 1)\u2212 \u00b52\u03c92 ) erfc ( \u00b5\u03c9+2x\n2 \u221a x\n) + \u221a x(\u00b5\u03c9 \u2212 2x) ) 2 \u221a \u03c0x . (160)\nWe only have to determine the sign of \u221a \u03c0e (\u00b5\u03c9+2x)2 4x ( 2x(2x+ 1)\u2212 \u00b52\u03c92 ) erfc ( \u00b5\u03c9+2x\n2 \u221a x\n) + \u221a x(\u00b5\u03c9\u2212\n2x) since all other factors are obviously larger than zero.\nThis derivative is larger than zero, since\n\u221a \u03c0e (\u00b5\u03c9+2\u03bd\u03c4)2 4\u03bd\u03c4 ( 2\u03bd\u03c4(2\u03bd\u03c4 + 1)\u2212 \u00b52\u03c92 ) erfc\n( \u00b5\u03c9 + 2\u03bd\u03c4\n2 \u221a \u03bd\u03c4\n) + \u221a \u03bd\u03c4(\u00b5\u03c9 \u2212 2\u03bd\u03c4) > (161)\n0.463979 ( 2\u03bd\u03c4(2\u03bd\u03c4 + 1)\u2212 \u00b52\u03c92 ) + \u221a \u03bd\u03c4(\u00b5\u03c9 \u2212 2\u03bd\u03c4) =\n\u2212 0.463979\u00b52\u03c92 + \u00b5\u03c9 \u221a \u03bd\u03c4 + 1.85592(\u03bd\u03c4)2 + 0.927958\u03bd\u03c4 \u2212 2\u03bd\u03c4 \u221a \u03bd\u03c4 =\n\u00b5\u03c9 (\u221a \u03bd\u03c4 \u2212 0.463979\u00b5\u03c9 ) + 0.85592(\u03bd\u03c4)2 + ( \u03bd\u03c4 \u2212 \u221a \u03bd\u03c4 )2 \u2212 0.0720421\u03bd\u03c4 > 0 .\nWe explain this chain of inequalities:\n\u2022 The first inequality follows by applying Lemma 23 which says that e (\u00b5\u03c9+2\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9+2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) is strictly monotonically decreasing. The minimal value\nthat is larger than 0.261772 is taken on at the maximal values \u03bd\u03c4 = 1.5 \u00b7 1.25 and \u00b5\u03c9 = 0.1 \u00b7 0.1. 0.261772 \u221a \u03c0 > 0.463979.\n\u2022 The equalities are just algebraic reformulations.\n\u2022 The last inequality follows from \u00b5\u03c9 ( \u221a \u03bd\u03c4 \u2212 0.463979\u00b5\u03c9) + 0.85592(\u03bd\u03c4)2 \u2212 0.0720421\u03bd\u03c4 > 0.85592 \u00b7 (0.8 \u00b7 0.8)2 \u2212 0.1 \u00b7 0.1 (\u221a 1.5 \u00b7 1.25 + 0.1 \u00b7 0.1 \u00b7 0.463979 ) \u2212\n0.0720421 \u00b7 1.5 \u00b7 1.25 > 0.201766.\nTherefore the function is increasing in \u03bd\u03c4 .\nDecreasing in \u00b5\u03c9 follows from decreasing of ex 2\nerfc(x) according to Lemma 23. Positivity follows from the fact that erfc and the exponential function are positive and that \u03bd\u03c4 > 0.\nLemma 39 (Bounds on the Derivatives). The following bounds on the absolute values of the derivatives of the Jacobian entries J11(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1), J12(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1), J21(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1), and J22(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) with respect to \u00b5, \u03c9, \u03bd, and \u03c4 hold:\u2223\u2223\u2223\u2223\u2202J11\u2202\u00b5\n\u2223\u2223\u2223\u2223 < 0.0031049101995398316 (162)\u2223\u2223\u2223\u2223\u2202J11\u2202\u03c9 \u2223\u2223\u2223\u2223 < 1.055872374194189\u2223\u2223\u2223\u2223\u2202J11\u2202\u03bd \u2223\u2223\u2223\u2223 < 0.031242911235461816\u2223\u2223\u2223\u2223\u2202J11\u2202\u03c4 \u2223\u2223\u2223\u2223 < 0.03749149348255419\n\u2223\u2223\u2223\u2223\u2202J12\u2202\u00b5 \u2223\u2223\u2223\u2223 < 0.031242911235461816\u2223\u2223\u2223\u2223\u2202J12\u2202\u03c9 \u2223\u2223\u2223\u2223 < 0.031242911235461816\u2223\u2223\u2223\u2223\u2202J12\u2202\u03bd \u2223\u2223\u2223\u2223 < 0.21232788238624354\u2223\u2223\u2223\u2223\u2202J12\u2202\u03c4 \u2223\u2223\u2223\u2223 < 0.2124377655377270\n\u2223\u2223\u2223\u2223\u2202J21\u2202\u00b5 \u2223\u2223\u2223\u2223 < 0.02220441024325437\u2223\u2223\u2223\u2223\u2202J21\u2202\u03c9 \u2223\u2223\u2223\u2223 < 1.146955401845684\u2223\u2223\u2223\u2223\u2202J21\u2202\u03bd \u2223\u2223\u2223\u2223 < 0.14983446469110305\u2223\u2223\u2223\u2223\u2202J21\u2202\u03c4 \u2223\u2223\u2223\u2223 < 0.17980135762932363\n\u2223\u2223\u2223\u2223\u2202J22\u2202\u00b5 \u2223\u2223\u2223\u2223 < 0.14983446469110305\u2223\u2223\u2223\u2223\u2202J22\u2202\u03c9 \u2223\u2223\u2223\u2223 < 0.14983446469110305\u2223\u2223\u2223\u2223\u2202J22\u2202\u03bd \u2223\u2223\u2223\u2223 < 1.805740052651535\u2223\u2223\u2223\u2223\u2202J22\u2202\u03c4 \u2223\u2223\u2223\u2223 < 2.396685907216327\nProof. For each derivative we compute a lower and an upper bound and take the maximum of the absolute value. A lower bound is determined by minimizing the single terms of the functions that represents the derivative. An upper bound is determined by maximizing the single terms of the functions that represent the derivative. Terms can be combined to larger terms for which the maximum and the minimum must be known. We apply many previous lemmata which state properties of functions representing single or combined terms. The more terms are combined, the tighter the bounds can be made.\nNext we go through all the derivatives, where we use Lemma 25, Lemma 26, Lemma 27, Lemma 28, Lemma 29, Lemma 30, Lemma 21, and Lemma 23 without citing. Furthermore, we use the bounds on the simple expressions t11,t22, ..., and T4 as defined the aforementioned lemmata:\n\u2022 \u2202J11\u2202\u00b5\nWe use Lemma 31 and consider the expression \u03b1e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 \u221a\n2 \u03c0 (\u03b1\u22121)\u221a \u03bd\u03c4\nin brackets. An upper bound on the maximum of is\n\u03b101e t21 erfc(t1)\u2212 \u221a 2 \u03c0 (\u03b101 \u2212 1)\u221a\nT22 = 0.591017 . (163)\nA lower bound on the minimum is\n\u03b101e T 21 erfc(T1)\u2212 \u221a 2 \u03c0 (\u03b101 \u2212 1)\u221a\nt22 = 0.056318 . (164)\nThus, an upper bound on the maximal absolute value is\n1 2 \u03bb01\u03c9 2 maxe t4\n\u03b101et21 erfc(t1)\u2212 \u221a 2 \u03c0 (\u03b101 \u2212 1)\u221a\nT22\n = 0.0031049101995398316 . (165)\n\u2022 \u2202J11\u2202\u03c9\nWe use Lemma 31 and consider the expression \u221a 2 \u03c0 (\u03b1\u22121)\u00b5\u03c9\u221a\n\u03bd\u03c4 \u2212 \u03b1(\u00b5\u03c9 +\n1)e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) in brackets.\nAn upper bound on the maximum is\u221a 2 \u03c0 (\u03b101 \u2212 1)T11\u221a\nt22 \u2212 \u03b101(t11 + 1)eT 2 1 erfc(T1) = \u22120.713808 . (166)\nA lower bound on the minimum is\u221a 2 \u03c0 (\u03b101 \u2212 1)t11\u221a\nt22 \u2212 \u03b101(T11 + 1)et 2 1 erfc(t1) = \u22120.99987 . (167)\nThis term is subtracted, and 2\u2212 erfc(x) > 0, therefore we have to use the minimum and the maximum for the argument of erfc.\nThus, an upper bound on the maximal absolute value is\n1 2 \u03bb01\n\u2212et4  \u221a 2 \u03c0 (\u03b101 \u2212 1)t11\u221a\nt22 \u2212 \u03b101(T11 + 1)et 2 1 erfc(t1)\n \u2212 erfc(T3) + 2  =\n(168) 1.055872374194189 .\n\u2022 \u2202J11\u2202\u03bd We consider the term in brackets\n\u03b1e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) + \u221a 2\n\u03c0\n( (\u03b1\u2212 1)\u00b5\u03c9\n(\u03bd\u03c4)3/2 \u2212 \u03b1\u221a \u03bd\u03c4\n) . (169)\nWe apply Lemma 33 for the first sub-term. An upper bound on the maximum is\n\u03b101e t21 erfc(t1) +\n\u221a 2\n\u03c0\n( (\u03b101 \u2212 1)T11\nT 3/2 22\n\u2212 \u03b101\u221a T22\n) = 0.0104167 . (170)\nA lower bound on the minimum is\n\u03b101e T 21 erfc(T1) +\n\u221a 2\n\u03c0\n( (\u03b101 \u2212 1)t11\nt 3/2 22\n\u2212 \u03b101\u221a t22\n) = \u22120.95153 . (171)\nThus, an upper bound on the maximal absolute value is\n\u2212 1 4 \u03bb01\u03c4max\u03c9maxe t4\n( \u03b101e T 21 erfc(T1) + \u221a 2\n\u03c0\n( (\u03b101 \u2212 1)t11\nt 3/2 22\n\u2212 \u03b101\u221a t22\n)) = (172)\n0.031242911235461816 .\n\u2022 \u2202J11\u2202\u03c4 We use the results of item \u2202J11\u2202\u03bd were the brackets are only differently scaled. Thus, an upper bound on the maximal absolute value is\n\u2212 1 4 \u03bb01\u03bdmax\u03c9maxe t4\n( \u03b101e T 21 erfc(T1) + \u221a 2\n\u03c0\n( (\u03b101 \u2212 1)t11\nt 3/2 22\n\u2212 \u03b101\u221a t22\n)) = (173)\n0.03749149348255419 .\n\u2022 \u2202J12\u2202\u00b5 Since \u2202J12\u2202\u00b5 = \u2202J11 \u2202\u03bd , an upper bound on the maximal absolute value is\n\u2212 1 4 \u03bb01\u03c4max\u03c9maxe t4\n( \u03b101e T 21 erfc(T1) + \u221a 2\n\u03c0\n( (\u03b101 \u2212 1)t11\nt 3/2 22\n\u2212 \u03b101\u221a t22\n)) = (174)\n0.031242911235461816 .\n\u2022 \u2202J12\u2202\u03c9 We use the results of item \u2202J11\u2202\u03bd were the brackets are only differently scaled. Thus, an upper bound on the maximal absolute value is\n\u2212 1 4 \u03bb01\u00b5max\u03c4maxe t4\n( \u03b101e T 21 erfc(T1) + \u221a 2\n\u03c0\n( (\u03b101 \u2212 1)t11\nt 3/2 22\n\u2212 \u03b101\u221a t22\n)) = (175)\n0.031242911235461816 .\n\u2022 \u2202J12\u2202\u03bd For the second term in brackets, we see that \u03b101\u03c42mine\nT 21 erfc(T1) = 0.465793 and \u03b101\u03c4 2 maxe t21 erfc(t1) = 1.53644. We now check different values for\u221a 2\n\u03c0\n( (\u22121)(\u03b1\u2212 1)\u00b52\u03c92\n\u03bd5/2 \u221a \u03c4\n+\n\u221a \u03c4(\u03b1+ \u03b1\u00b5\u03c9 \u2212 1)\n\u03bd3/2 \u2212 \u03b1\u03c4\n3/2\n\u221a \u03bd\n) , (176)\nwhere we maximize or minimize all single terms.\nA lower bound on the minimum of this expression is\u221a 2\n\u03c0\n( (\u22121)(\u03b101 \u2212 1)\u00b52max\u03c92max\n\u03bd 5/2 min \u221a \u03c4min\n+\n\u221a \u03c4min(\u03b101 + \u03b101t11 \u2212 1)\n\u03bd 3/2 max\n\u2212 \u03b101\u03c4 3/2 max\u221a\n\u03bdmin\n) = (177)\n\u2212 1.83112 .\nAn upper bound on the maximum of this expression is\u221a 2\n\u03c0\n( (\u22121)(\u03b101 \u2212 1)\u00b52min\u03c92min\n\u03bd 5/2 max \u221a \u03c4max\n+\n\u221a \u03c4max(\u03b101 + \u03b101T11 \u2212 1)\n\u03bd 3/2 min\n\u2212 \u03b101\u03c4 3/2 min\u221a\n\u03bdmax\n) = (178)\n0.0802158 .\nAn upper bound on the maximum is\n1 8 \u03bb01e t4\n(\u221a 2\n\u03c0\n( (\u22121)(\u03b101 \u2212 1)\u00b52min\u03c92min\n\u03bd 5/2 max \u221a \u03c4max\n\u2212 \u03b101\u03c4 3/2 min\u221a\n\u03bdmax + (179)\n\u221a \u03c4max(\u03b101 + \u03b101T11 \u2212 1)\n\u03bd 3/2 min\n) + \u03b101\u03c4 2 maxe t21 erfc(t1) ) = 0.212328 .\nA lower bound on the minimum is 1\n8 \u03bb01e\nt4 ( \u03b101\u03c4 2 mine\nT 21 erfc(T1) + (180)\u221a 2\n\u03c0\n( (\u22121)(\u03b101 \u2212 1)\u00b52max\u03c92max\n\u03bd 5/2 min \u221a \u03c4min\n+\n\u221a \u03c4min(\u03b101 + \u03b101t11 \u2212 1)\n\u03bd 3/2 max\n\u2212 \u03b101\u03c4 3/2 max\u221a\n\u03bdmin\n)) =\n\u2212 0.179318 .\nThus, an upper bound on the maximal absolute value is\n1 8 \u03bb01e t4\n(\u221a 2\n\u03c0\n( (\u22121)(\u03b101 \u2212 1)\u00b52min\u03c92min\n\u03bd 5/2 max \u221a \u03c4max\n\u2212 \u03b101\u03c4 3/2 min\u221a\n\u03bdmax + (181)\n\u221a \u03c4max(\u03b101 + \u03b101T11 \u2212 1)\n\u03bd 3/2 min\n) + \u03b101\u03c4 2 maxe t21 erfc(t1) ) = 0.21232788238624354 .\n\u2022 \u2202J12\u2202\u03c4 We use Lemma 34 to obtain an upper bound on the maximum of the expression of the lemma:\u221a\n2\n\u03c0\n( 0.12 \u00b7 0.12(\u22121)(\u03b101 \u2212 1)\n(0.8 \u00b7 0.8)3/2 \u2212 \u221a 0.8 \u00b7 0.8\u03b101 + (0.1 \u00b7 0.1)\u03b101 \u2212 \u03b101 + 1\u221a 0.8 \u00b7 0.8\n) = \u22121.72296 .\n(182)\nWe use Lemma 34 to obtain an lower bound on the minimum of the expression of the lemma:\u221a 2\n\u03c0\n( 0.12 \u00b7 0.12(\u22121)(\u03b101 \u2212 1)\n(1.5 \u00b7 1.25)3/2 \u2212 \u221a 1.5 \u00b7 1.25\u03b101 + (\u22120.1 \u00b7 0.1)\u03b101 \u2212 \u03b101 + 1\u221a 1.5 \u00b7 1.25\n) = \u22122.2302 .\n(183)\nNext we apply Lemma 37 for the expression \u03bd\u03c4e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) . We use Lemma 37\nto obtain an upper bound on the maximum of this expression:\n1.5 \u00b7 1.25e (1.5\u00b71.25\u22120.1\u00b70.1)2\n2\u00b71.5\u00b71.25 \u03b101 erfc\n( 1.5 \u00b7 1.25\u2212 0.1 \u00b7 0.1\u221a\n2 \u221a 1.5 \u00b7 1.25\n) = 1.37381 . (184)\nWe use Lemma 37 to obtain an lower bound on the minimum of this expression:\n0.8 \u00b7 0.8e (0.8\u00b70.8+0.1\u00b70.1)2\n2\u00b70.8\u00b70.8 \u03b101 erfc\n( 0.8 \u00b7 0.8 + 0.1 \u00b7 0.1\u221a\n2 \u221a 0.8 \u00b7 0.8\n) = 0.620462 . (185)\nNext we apply Lemma 23 for 2\u03b1e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) . An upper bound on this expres-\nsion is\n2e (0.8\u00b70.8\u22120.1\u00b70.1)2\n20.8\u00b70.8 \u03b101 erfc\n( 0.8 \u00b7 0.8\u2212 0.1 \u00b7 0.1\u221a\n2 \u221a 0.8 \u00b7 0.8\n) = 1.96664 . (186)\nA lower bound on this expression is\n2e (1.5\u00b71.25+0.1\u00b70.1)2\n2\u00b71.5\u00b71.25 \u03b101 erfc\n( 1.5 \u00b7 1.25 + 0.1 \u00b7 0.1\u221a\n2 \u221a 1.5 \u00b7 1.25\n) = 1.4556 . (187)\nThe sum of the minimal values of the terms is\u22122.23019+0.62046+1.45560 = \u22120.154133. The sum of the maximal values of the terms is \u22121.72295 + 1.37380 + 1.96664 = 1.61749. Thus, an upper bound on the maximal absolute value is\n1 8 \u03bb01e t4\n( \u03b101T22e (t11+T22) 2 2T22 erfc ( t11 + T22\u221a\n2 \u221a T22\n) + (188)\n2\u03b101e t21 erfc(t1) +\n\u221a 2\n\u03c0\n( \u2212 (\u03b101 \u2212 1)T 2 11\nt 3/2 22\n+ \u2212\u03b101 + \u03b101T11 + 1\u221a\nt22 \u2212\n\u03b101 \u221a t22 )) = 0.2124377655377270 .\n\u2022 \u2202J21\u2202\u00b5 An upper bound on the maximum is\n\u03bb201\u03c9 2 max ( \u03b1201e T 21 ( \u2212e\u2212T4 ) erfc(T1) + 2\u03b1 2 01e t22et4 erfc(t2) \u2212 erfc(T3) + 2 ) = (189)\n0.0222044 .\nA upper bound on the absolute minimum is\n\u03bb201\u03c9 2 max ( \u03b1201e t21 ( \u2212e\u2212t4 ) erfc(t1) + 2\u03b1 2 01e T 22 eT4 erfc(T2) \u2212 erfc(t3) + 2 ) = (190)\n0.00894889 .\nThus, an upper bound on the maximal absolute value is\n\u03bb201\u03c9 2 max ( \u03b1201e T 21 ( \u2212e\u2212T4 ) erfc(T1) + 2\u03b1 2 01e t22et4 erfc(t2) \u2212 erfc(T3) + 2 ) = (191)\n0.02220441024325437 .\n\u2022 \u2202J21\u2202\u03c9 An upper bound on the maximum is\n\u03bb201 ( \u03b1201(2T11 + 1)e t22e\u2212t4 erfc(t2) + 2T11(2\u2212 erfc(T3)) + (192)\n\u03b1201(t11 + 1)e T 21 ( \u2212e\u2212T4 ) erfc(T1) +\n\u221a 2\n\u03c0\n\u221a T22e \u2212t4 ) = 1.14696 .\nA lower bound on the minimum is\n\u03bb201 ( \u03b1201(T11 + 1)e t21 ( \u2212e\u2212t4 ) erfc(t1) + (193)\n\u03b1201(2t11 + 1)e T 22 e\u2212T4 erfc(T2) + 2t11(2\u2212 erfc(T3))+\n\u221a 2\n\u03c0\n\u221a t22e \u2212T4 ) = \u22120.359403 .\nThus, an upper bound on the maximal absolute value is\n\u03bb201 ( \u03b1201(2T11 + 1)e t22e\u2212t4 erfc(t2) + 2T11(2\u2212 erfc(T3)) + (194)\n\u03b1201(t11 + 1)e T 21 ( \u2212e\u2212T4 ) erfc(T1) +\n\u221a 2\n\u03c0\n\u221a T22e \u2212t4 ) = 1.146955401845684 .\n\u2022 \u2202J21\u2202\u03bd An upper bound on the maximum is\n1 2 \u03bb201\u03c4max\u03c9maxe \u2212t4\n\u03b1201 (\u2212eT 21 ) erfc(T1) + 4\u03b1201et22 erfc(t2) + \u221a 2 \u03c0 (\u22121) ( \u03b1201 \u2212 1 ) \u221a T22  = (195)\n0.149834 .\nA lower bound on the minimum is\n1 2 \u03bb201\u03c4max\u03c9maxe \u2212t4\n\u03b1201 (\u2212et21) erfc(t1) + 4\u03b1201eT 22 erfc(T2) + \u221a 2 \u03c0 (\u22121) ( \u03b1201 \u2212 1 ) \u221a t22  = (196)\n\u2212 0.0351035 . Thus, an upper bound on the maximal absolute value is\n1 2 \u03bb201\u03c4max\u03c9maxe \u2212t4\n\u03b1201 (\u2212eT 21 ) erfc(T1) + 4\u03b1201et22 erfc(t2) + \u221a 2 \u03c0 (\u22121) ( \u03b1201 \u2212 1 ) \u221a T22  = (197)\n0.14983446469110305 .\n\u2022 \u2202J21\u2202\u03c4 An upper bound on the maximum is\n1 2 \u03bb201\u03bdmax\u03c9maxe \u2212t4\n\u03b1201 (\u2212eT 21 ) erfc(T1) + 4\u03b1201et22 erfc(t2) + \u221a 2 \u03c0 (\u22121) ( \u03b1201 \u2212 1 ) \u221a T22  = (198)\n0.179801 .\nA lower bound on the minimum is\n1 2 \u03bb201\u03bdmax\u03c9maxe \u2212t4\n\u03b1201 (\u2212et21) erfc(t1) + 4\u03b1201eT 22 erfc(T2) + \u221a 2 \u03c0 (\u22121) ( \u03b1201 \u2212 1 ) \u221a t22  = (199)\n\u2212 0.0421242 . Thus, an upper bound on the maximal absolute value is\n1 2 \u03bb201\u03bdmax\u03c9maxe \u2212t4\n\u03b1201 (\u2212eT 21 ) erfc(T1) + 4\u03b1201et22 erfc(t2) + \u221a 2 \u03c0 (\u22121) ( \u03b1201 \u2212 1 ) \u221a T22  = (200)\n0.17980135762932363 .\n\u2022 \u2202J22\u2202\u00b5 We use the fact that \u2202J22\u2202\u00b5 = \u2202J21 \u2202\u03bd . Thus, an upper bound on the maximal absolute value is\n1 2 \u03bb201\u03c4max\u03c9maxe \u2212t4\n\u03b1201 (\u2212eT 21 ) erfc(T1) + 4\u03b1201et22 erfc(t2) + \u221a 2 \u03c0 (\u22121) ( \u03b1201 \u2212 1 ) \u221a T22  = (201)\n0.14983446469110305 .\n\u2022 \u2202J22\u2202\u03c9 An upper bound on the maximum is\n1 2 \u03bb201\u00b5max\u03c4maxe \u2212t4\n\u03b1201 (\u2212eT 21 ) erfc(T1) + 4\u03b1201et22 erfc(t2) + \u221a 2 \u03c0 (\u22121) ( \u03b1201 \u2212 1 ) \u221a T22  = (202)\n0.149834 .\nA lower bound on the minimum is\n1 2 \u03bb201\u00b5max\u03c4maxe \u2212t4\n\u03b1201 (\u2212et21) erfc(t1) + 4\u03b1201eT 22 erfc(T2) + \u221a 2 \u03c0 (\u22121) ( \u03b1201 \u2212 1 ) \u221a t22  = (203)\n\u2212 0.0351035 . Thus, an upper bound on the maximal absolute value is\n1 2 \u03bb201\u00b5max\u03c4maxe \u2212t4\n\u03b1201 (\u2212eT 21 ) erfc(T1) + 4\u03b1201et22 erfc(t2) + \u221a 2 \u03c0 (\u22121) ( \u03b1201 \u2212 1 ) \u221a T22  = (204)\n0.14983446469110305 .\n\u2022 \u2202J22\u2202\u03bd\nWe apply Lemma 35 to the expression \u221a\n2 \u03c0\n( (\u03b12\u22121)\u00b5\u03c9\n(\u03bd\u03c4)3/2 \u2212 3\u03b1 2 \u221a \u03bd\u03c4\n) . Using Lemma 35, an\nupper bound on the maximum is 1\n4 \u03bb201\u03c4 2 maxe\n\u2212t4 ( \u03b1201 ( \u2212eT 2 1 ) erfc(T1) + 8\u03b1 2 01e\nt22 erfc(t2) + (205)\u221a 2\n\u03c0\n(( \u03b1201 \u2212 1 ) T11\nT 3/2 22\n\u2212 3\u03b1 2 01\u221a T22\n)) = 1.19441 .\nUsing Lemma 35, a lower bound on the minimum is 1\n4 \u03bb201\u03c4 2 maxe\n\u2212t4 ( \u03b1201 ( \u2212et 2 1 ) erfc(t1) + 8\u03b1 2 01e\nT 22 erfc(T2) + (206)\u221a 2\n\u03c0\n(( \u03b1201 \u2212 1 ) t11\nt 3/2 22\n\u2212 3\u03b1 2 01\u221a t22\n)) = \u22121.80574 .\nThus, an upper bound on the maximal absolute value is\n\u2212 1 4 \u03bb201\u03c4 2 maxe\n\u2212t4 ( \u03b1201 ( \u2212et 2 1 ) erfc(t1) + 8\u03b1 2 01e\nT 22 erfc(T2) + (207)\u221a 2\n\u03c0\n(( \u03b1201 \u2212 1 ) t11\nt 3/2 22\n\u2212 3\u03b1 2 01\u221a t22\n)) = 1.805740052651535 .\n\u2022 \u2202J22\u2202\u03c4\nWe apply Lemma 36 to the expression \u221a\n2 \u03c0\n( (\u03b12\u22121)\u00b5\u03c9\u221a\n\u03bd\u03c4 \u2212 3\u03b12\n\u221a \u03bd\u03c4 ) .\nWe apply Lemma 37 to the expression \u03bd\u03c4e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) . We apply Lemma 38 to\nthe expression \u03bd\u03c4e (\u00b5\u03c9+2\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9+2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) .\nWe combine the results of these lemmata to obtain an upper bound on the maximum:\n1 4 \u03bb201\n( \u2212\u03b1201t22e\u2212T4e (T11+t22) 2 2t22 erfc ( T11 + t22\u221a\n2 \u221a t22\n) + (208)\n8\u03b1201T22e \u2212t4e\n(t11+2T22) 2\n2T22 erfc ( t11 + 2T22\u221a\n2 \u221a T22\n) \u2212\n2\u03b1201e T 21 e\u2212T4 erfc(T1) + 4\u03b1 2 01e t22e\u2212t4 erfc(t2) + 2(2\u2212 erfc(T3)) +\u221a 2\n\u03c0 e\u2212T4\n(( \u03b1201 \u2212 1 ) T11\u221a\nt22 \u2212 3\u03b1201\n\u221a t22 )) = 2.39669 .\nWe combine the results of these lemmata to obtain an lower bound on the minimum: 1\n4 \u03bb201\n( 8\u03b1201t22e \u2212T4e (T11+2t22) 2 2t22 erfc ( T11 + 2t22\u221a\n2 \u221a t22\n) + (209)\n\u03b1201T22e \u2212t4e\n(t11+T22) 2\n2T22 erfc ( t11 + T22\u221a\n2 \u221a T22\n) \u2212\n2\u03b1201e t21e\u2212t4 erfc(t1) + 4\u03b1 2 01e T 22 e\u2212T4 erfc(T2) + 2(2\u2212 erfc(t3)) + \u221a 2\n\u03c0 e\u2212t4\n(( \u03b1201 \u2212 1 ) t11\u221a\nT22 \u2212 3\u03b1201\n\u221a T22 )) = \u22121.17154 .\nThus, an upper bound on the maximal absolute value is\n1 4 \u03bb201\n( \u2212\u03b1201t22e\u2212T4e (T11+t22) 2 2t22 erfc ( T11 + t22\u221a\n2 \u221a t22\n) + (210)\n8\u03b1201T22e \u2212t4e\n(t11+2T22) 2\n2T22 erfc ( t11 + 2T22\u221a\n2 \u221a T22\n) \u2212\n2\u03b1201e T 21 e\u2212T4 erfc(T1) + 4\u03b1 2 01e t22e\u2212t4 erfc(t2) + 2(2\u2212 erfc(T3)) +\u221a 2\n\u03c0 e\u2212T4\n(( \u03b1201 \u2212 1 ) T11\u221a\nt22 \u2212 3\u03b1201\n\u221a t22 )) = 2.396685907216327 .\nLemma 40 (Derivatives of the Mapping). We assume \u03b1 = \u03b101 and \u03bb = \u03bb01. We restrict the range of the variables to the domain \u00b5 \u2208 [\u22120.1, 0.1], \u03c9 \u2208 [\u22120.1, 0.1], \u03bd \u2208 [0.8, 1.5], and \u03c4 \u2208 [0.8, 1.25].\nThe derivative \u2202\u2202\u00b5 \u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) has the sign of \u03c9.\nThe derivative \u2202\u2202\u03bd \u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) is positive.\nThe derivative \u2202\u2202\u00b5 \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) has the sign of \u03c9.\nThe derivative \u2202\u2202\u03bd \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) is positive.\nProof. \u2022 \u2202\u2202\u00b5 \u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1)\n(2\u2212 erfc(x) > 0 according to Lemma 21 and ex2 erfc(x) is also larger than zero according to Lemma 23. Consequently, has \u2202\u2202\u00b5 \u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) the sign of \u03c9.\n\u2022 \u2202\u2202\u03bd \u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1)\nLemma 23 says ex 2 erfc(x) is decreasing in \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4\n. The first term (negative) is increasing in \u03bd\u03c4 since it is proportional to minus one over the squared root of \u03bd\u03c4 .\nWe obtain a lower bound by setting \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 = 1.5\u00b71.25+0.1\u00b70.1\u221a 2 \u221a 1.5\u00b71.25 for the e x2 erfc(x)\nterm. The term in brackets is larger than e ( 1.5\u00b71.25+0.1\u00b70.1\u221a 2 \u221a 1.5\u00b71.25 )2 \u03b101 erfc ( 1.5\u00b71.25+0.1\u00b70.1\u221a\n2 \u221a 1.5\u00b71.25 ) \u2212\u221a\n2 \u03c00.8\u00b70.8 (\u03b101 \u2212 1) = 0.056 Consequently, the function is larger than zero.\n\u2022 \u2202\u2202\u00b5 \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1)\nWe consider the sub-function\u221a 2\n\u03c0\n\u221a \u03bd\u03c4 \u2212 \u03b12 ( e ( \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 e ( \u00b5\u03c9+2\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n)) .\n(211)\nWe set x = \u03bd\u03c4 and y = \u00b5\u03c9 and obtain\u221a 2\n\u03c0\n\u221a x\u2212 \u03b12 ( e ( x+y\u221a 2 \u221a x )2 erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 e ( 2x+y\u221a 2 \u221a x )2 erfc ( 2x+ y\u221a\n2 \u221a x\n)) . (212)\nThe derivative of this sub-function with respect to y is \u03b12 ( e (2x+y)2 2x (2x+ y) erfc (\n2x+y\u221a 2 \u221a x\n) \u2212 e (x+y)2 2x (x+ y) erfc ( x+y\u221a 2 \u221a x )) x = (213)\n\u221a 2\u03b12 \u221a x  e (2x+y)22x (x+y) erfc( x+y\u221a2\u221ax)\u221a 2 \u221a x \u2212 e (x+y)2 2x (x+y) erfc ( x+y\u221a 2 \u221a x ) \u221a 2 \u221a x  x > 0 .\nThe inequality follows from Lemma 24, which states that zez 2\nerfc(z) is monotonically increasing in z. Therefore the sub-function is increasing in y.\nThe derivative of this sub-function with respect to x is \u221a \u03c0\u03b12 ( e (2x+y)2 2x ( 4x2 \u2212 y2 ) erfc ( 2x+y\u221a\n2 \u221a x\n) \u2212 e (x+y)2 2x (x\u2212 y)(x+ y) erfc ( x+y\u221a 2 \u221a x )) \u2212 \u221a 2 ( \u03b12 \u2212 1 ) x3/2\n2 \u221a \u03c0x2\n.\n(214)\nThe sub-function is increasing in x, since the derivative is larger than zero: \u221a \u03c0\u03b12 ( e (2x+y)2 2x ( 4x2 \u2212 y2 ) erfc ( 2x+y\u221a\n2 \u221a x\n) \u2212 e (x+y)2 2x (x\u2212 y)(x+ y) erfc ( x+y\u221a 2 \u221a x )) \u2212 \u221a 2x3/2 ( \u03b12 \u2212 1 ) 2 \u221a \u03c0x2 >\n(215)\n\u221a \u03c0\u03b12  (2x\u2212y)(2x+y)2\u221a \u03c0 ( 2x+y\u221a 2 \u221a x + \u221a( 2x+y\u221a 2 \u221a x )2 +2 ) \u2212 (x\u2212y)(x+y)2 \u221a \u03c0 ( x+y\u221a 2 \u221a x + \u221a( x+y\u221a 2 \u221a x )2 + 4\u03c0 ) \u2212\u221a2x3/2 (\u03b12 \u2212 1)\n2 \u221a \u03c0x2\n=\n\u221a \u03c0\u03b12\n( (2x\u2212y)(2x+y)2( \u221a 2 \u221a x)\n\u221a \u03c0 ( 2x+y+ \u221a (2x+y)2+4x ) \u2212 (x\u2212y)(x+y)2(\u221a2\u221ax)\u221a \u03c0 ( x+y+ \u221a (x+y)2+ 8x\u03c0 ))\u2212\u221a2x3/2 (\u03b12 \u2212 1) 2 \u221a \u03c0x2 =\n\u221a \u03c0\u03b12\n( (2x\u2212y)(2x+y)2\n\u221a \u03c0 ( 2x+y+ \u221a (2x+y)2+4x ) \u2212 (x\u2212y)(x+y)2\u221a \u03c0 ( x+y+ \u221a (x+y)2+ 8x\u03c0 ))\u2212 x (\u03b12 \u2212 1) \u221a\n2 \u221a \u03c0x3/2\n>\n\u221a \u03c0\u03b12\n( (2x\u2212y)(2x+y)2\n\u221a \u03c0 ( 2x+y+ \u221a (2x+y)2+2(2x+y)+1 ) \u2212 (x\u2212y)(x+y)2\u221a \u03c0 ( x+y+ \u221a (x+y)2+0.782\u00b72(x+y)+0.7822 ))\u2212 x (\u03b12 \u2212 1) \u221a\n2 \u221a \u03c0x3/2\n=\n\u221a \u03c0\u03b12\n( (2x\u2212y)(2x+y)2\n\u221a \u03c0 ( 2x+y+ \u221a (2x+y+1)2 ) \u2212 (x\u2212y)(x+y)2\u221a \u03c0 ( x+y+ \u221a (x+y+0.782)2 ))\u2212 x (\u03b12 \u2212 1) \u221a\n2 \u221a \u03c0x3/2\n=\n\u221a \u03c0\u03b12 ( (2x\u2212y)(2x+y)2\u221a \u03c0(2(2x+y)+1) \u2212 (x\u2212y)(x+y)2\u221a \u03c0(2(x+y)+0.782) ) \u2212 x ( \u03b12 \u2212 1 ) \u221a\n2 \u221a \u03c0x3/2\n=\n\u221a \u03c0\u03b12 ( (2(x+y)+0.782)(2x\u2212y)(2x+y)2\u221a\n\u03c0 \u2212 (x\u2212y)(x+y)(2(2x+y)+1)2\u221a \u03c0 ) (2(2x+ y) + 1)(2(x+ y) + 0.782) \u221a 2 \u221a \u03c0x3/2 +\n\u221a \u03c0\u03b12 ( \u2212x ( \u03b12 \u2212 1 ) (2(2x+ y) + 1)(2(x+ y) + 0.782) ) (2(2x+ y) + 1)(2(x+ y) + 0.782) \u221a 2 \u221a \u03c0x3/2 =\n8x3 + (12y + 2.68657)x2 + (y(4y \u2212 6.41452)\u2212 1.40745)x+ 1.22072y2\n(2(2x+ y) + 1)(2(x+ y) + 0.782) \u221a 2 \u221a \u03c0x3/2\n>\n8x3 + (2.68657\u2212 120.01)x2 + (0.01(\u22126.41452\u2212 40.01)\u2212 1.40745)x+ 1.22072(0.0)2\n(2(2x+ y) + 1)(2(x+ y) + 0.782) \u221a 2 \u221a \u03c0x3/2\n=\n8x2 + 2.56657x\u2212 1.472 (2(2x+ y) + 1)(2(x+ y) + 0.782) \u221a 2 \u221a \u03c0 \u221a x =\n8x2 + 2.56657x\u2212 1.472 (2(2x+ y) + 1)(2(x+ y) + 0.782) \u221a 2 \u221a \u03c0 \u221a x =\n8(x+ 0.618374)(x\u2212 0.297553) (2(2x+ y) + 1)(2(x+ y) + 0.782) \u221a 2 \u221a \u03c0 \u221a x > 0 .\nWe explain this chain of inequalities:\n\u2013 First inequality: We applied Lemma 22 two times. \u2013 Equalities factor out \u221a 2 \u221a x and reformulate.\n\u2013 Second inequality part 1: we applied\n0 < 2y =\u21d2 (2x+ y)2 + 4x+ 1 < (2x+ y)2 + 2(2x+ y) + 1 = (2x+ y + 1)2 . (216)\n\u2013 Second inequality part 2: we show that for a = 120 (\u221a 2048+169\u03c0 \u03c0 \u2212 13 ) following\nholds: 8x\u03c0 \u2212 ( a2 + 2a(x+ y) ) > 0. We have \u2202\u2202x 8x \u03c0 \u2212 ( a2 + 2a(x+ y) ) = 8\u03c0\u22122a > 0 and \u2202\u2202y 8x \u03c0 \u2212 ( a2 + 2a(x+ y) ) = \u22122a > 0. Therefore the minimum is at border for minimal x and maximal y:\n8 \u00b7 0.64 \u03c0 \u2212  2 20 (\u221a 2048 + 169\u03c0 \u03c0 \u2212 13 ) (0.64 + 0.01) + ( 1 20 (\u221a 2048 + 169\u03c0 \u03c0 \u2212 13 ))2 = 0 . (217)\nThus 8x\n\u03c0 > a2 + 2a(x+ y) . (218)\nfor a = 120 (\u221a 2048+169\u03c0 \u03c0 \u2212 13 ) > 0.782.\n\u2013 Equalities only solve square root and factor out the resulting terms (2(2x + y) + 1) and (2(x+ y) + 0.782).\n\u2013 We set \u03b1 = \u03b101 and multiplied out. Thereafter we also factored out x in the numerator. Finally a quadratic equations was solved.\nThe sub-function has its minimal value for minimal x and minimal y x = \u03bd\u03c4 = 0.8 \u00b7 0.8 = 0.64 and y = \u00b5\u03c9 = \u22120.1 \u00b7 0.1 = \u22120.01. We further minimize the function\n\u00b5\u03c9e \u00b52\u03c92 2\u03bd\u03c4 ( 2\u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 )) > \u22120.01e 0.01 2 20.64 ( 2\u2212 erfc ( 0.01\u221a 2 \u221a 0.64 )) . (219)\nWe compute the minimum of the term in brackets of \u2202\u2202\u00b5 \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1):\n\u00b5\u03c9e \u00b52\u03c92 2\u03bd\u03c4 ( 2\u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 )) + (220)\n\u03b1201\n( \u2212 ( e ( \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 e ( \u00b5\u03c9+2\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n))) + \u221a 2\n\u03c0\n\u221a \u03bd\u03c4 >\n\u03b1201\n( \u2212 ( e ( 0.64\u22120.01\u221a 2 \u221a 0.64 )2 erfc ( 0.64\u2212 0.01\u221a\n2 \u221a 0.64\n) \u2212 e ( 20.64\u22120.01\u221a 2 \u221a 0.64 )2 erfc ( 2 \u00b7 0.64\u2212 0.01\u221a\n2 \u221a 0.64\n))) \u2212\n0.01e 0.012 20.64 ( 2\u2212 erfc ( 0.01\u221a 2 \u221a 0.64 )) + \u221a 0.64 \u221a 2 \u03c0 = 0.0923765 .\nTherefore the term in brackets is larger than zero.\nThus, \u2202\u2202\u00b5 \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) has the sign of \u03c9.\n\u2022 \u2202\u2202\u03bd \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) We look at the sub-term\n2e\n( 2x+y\u221a 2 \u221a x )2 erfc ( 2x+ y\u221a\n2 \u221a x\n) \u2212 e ( x+y\u221a 2 \u221a x )2 erfc ( x+ y\u221a\n2 \u221a x\n) . (221)\nWe obtain a chain of inequalities:\n2e\n( 2x+y\u221a 2 \u221a x )2 erfc ( 2x+ y\u221a\n2 \u221a x\n) \u2212 e ( x+y\u221a 2 \u221a x )2 erfc ( x+ y\u221a\n2 \u221a x\n) > (222)\n2 \u00b7 2 \u221a \u03c0 ( 2x+y\u221a\n2 \u221a x +\n\u221a( 2x+y\u221a\n2 \u221a x\n)2 + 2 ) \u2212 2 \u221a \u03c0 ( x+y\u221a 2 \u221a x + \u221a( x+y\u221a 2 \u221a x )2 + 4\u03c0 ) =\n2 \u221a 2 \u221a x ( 2\u221a\n(2x+y)2+4x+2x+y \u2212 1\u221a\n(x+y)2+ 8x\u03c0 +x+y ) \u221a \u03c0 >\n2 \u221a 2 \u221a x ( 2\u221a\n(2x+y)2+2(2x+y)+1+2x+y \u2212 1\u221a (x+y)2+0.782\u00b72(x+y)+0.7822+x+y ) \u221a \u03c0 =\n2 \u221a 2 \u221a x (\n2 2(2x+y)+1 \u2212 1 2(x+y)+0.782 ) \u221a \u03c0\n=( 2 \u221a 2 \u221a x )\n(2(2(x+ y) + 0.782)\u2212 (2(2x+ y) + 1)) \u221a \u03c0((2(x+ y) + 0.782)(2(2x+ y) + 1))\n=( 2 \u221a 2 \u221a x )\n(2y + 0.782 \u00b7 2\u2212 1) \u221a \u03c0((2(x+ y) + 0.782)(2(2x+ y) + 1)) > 0 .\nWe explain this chain of inequalities:\n\u2013 First inequality: We applied Lemma 22 two times. \u2013 Equalities factor out \u221a 2 \u221a x and reformulate.\n\u2013 Second inequality part 1: we applied 0 < 2y =\u21d2 (2x+ y)2 + 4x+ 1 < (2x+ y)2 + 2(2x+ y) + 1 = (2x+ y + 1)2 .\n(223)\n\u2013 Second inequality part 2: we show that for a = 120 (\u221a 2048+169\u03c0 \u03c0 \u2212 13 ) following\nholds: 8x\u03c0 \u2212 ( a2 + 2a(x+ y) ) > 0. We have \u2202\u2202x 8x \u03c0 \u2212 ( a2 + 2a(x+ y) ) = 8\u03c0\u22122a > 0 and \u2202\u2202y 8x \u03c0 \u2212 ( a2 + 2a(x+ y) ) = \u22122a < 0. Therefore the minimum is at border for minimal x and maximal y:\n8 \u00b7 0.64 \u03c0 \u2212  2 20 (\u221a 2048 + 169\u03c0 \u03c0 \u2212 13 ) (0.64 + 0.01) + ( 1 20 (\u221a 2048 + 169\u03c0 \u03c0 \u2212 13 ))2 = 0 . (224)\nThus 8x\n\u03c0 > a2 + 2a(x+ y) . (225)\nfor a = 120 (\u221a 2048+169\u03c0 \u03c0 \u2212 13 ) > 0.782.\n\u2013 Equalities only solve square root and factor out the resulting terms (2(2x + y) + 1) and (2(x+ y) + 0.782).\nWe know that (2\u2212 erfc(x) > 0 according to Lemma 21. For the sub-term we derived\n2e\n( 2x+y\u221a 2 \u221a x )2 erfc ( 2x+ y\u221a\n2 \u221a x\n) \u2212 e ( x+y\u221a 2 \u221a x )2 erfc ( x+ y\u221a\n2 \u221a x\n) > 0 . (226)\nConsequently, both terms in the brackets of \u2202\u2202\u03bd \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) are larger than zero. Therefore \u2202\u2202\u03bd \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) is larger than zero.\nLemma 41 (Mean at low variance). The mapping of the mean \u00b5\u0303 (Eq. (4))\n\u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) = 1\n2 \u03bb\n( \u2212(\u03b1+ \u00b5\u03c9) erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + (227)\n\u03b1e\u00b5\u03c9+ \u03bd\u03c4 2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) + \u221a 2\n\u03c0\n\u221a \u03bd\u03c4e\u2212 \u00b52\u03c92 2\u03bd\u03c4 + 2\u00b5\u03c9 ) in the domain \u22120.1 6 \u00b5 6 \u22120.1, \u22120.1 6 \u03c9 6 \u22120.1, and 0.02 6 \u03bd\u03c4 6 0.5 is bounded by\n|\u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101)| < 0.289324 (228)\nand\nlim \u03bd\u21920 |\u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101)| = \u03bb\u00b5\u03c9. (229)\nWe can consider \u00b5\u0303 with given \u00b5\u03c9 as a function in x = \u03bd\u03c4 . We show the graph of this function at the maximal \u00b5\u03c9 = 0.01 in the interval x \u2208 [0, 1] in Figure A6.\nProof. Since \u00b5\u0303 is strictly monotonically increasing with \u00b5\u03c9\n\u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) 6 (230) \u00b5\u0303(0.1, 0.1, \u03bd, \u03c4, \u03bb, \u03b1) 6\n1 2 \u03bb\n( \u2212(\u03b1+ 0.01) erfc ( 0.01\u221a 2 \u221a \u03bd\u03c4 ) + \u03b1e0.01+ \u03bd\u03c4 2 erfc ( 0.01 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) + \u221a 2 \u03c0 \u221a \u03bd\u03c4e\u2212 0.012 2\u03bd\u03c4 + 2 \u00b7 0.01 ) 6\n1 2 \u03bb01\n( e 0.05 2 +0.01\u03b101 erfc ( 0.02 + 0.01\u221a\n2 \u221a 0.02\n) \u2212 (\u03b101 + 0.01) erfc ( 0.01\u221a 2 \u221a 0.02 ) + e\u2212 0.012 2\u00b70.5 \u221a 0.5 \u221a 2 \u03c0 + 0.01 \u00b7 2 ) < 0.21857,\nwhere we have used the monotonicity of the terms in \u03bd\u03c4 .\n0.5 1.0 1.5 2.0 2.5 3.0\n-0.2\n-0.1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nx\nFigure A6: The graph of function \u00b5\u0303 for low variances x = \u03bd\u03c4 for \u00b5\u03c9 = 0.01, where x \u2208 [0, 3], is displayed in yellow. Lower and upper bounds based on the Abramowitz bounds (Lemma 22) are displayed in green and blue, respectively.\nSimilarly, we can use the monotonicity of the terms in \u03bd\u03c4 to show that\n\u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) > \u00b5\u0303(0.1,\u22120.1, \u03bd, \u03c4, \u03bb, \u03b1) > \u22120.289324, (231)\nsuch that |\u00b5\u0303| < 0.289324 at low variances. Furthermore, when (\u03bd\u03c4)\u2192 0, the terms with the arguments of the complementary error functions erfc and the exponential function go to infinity, therefore these three terms converge to zero. Hence, the remaining terms are only 2\u00b5\u03c9 12\u03bb.\nLemma 42 (Bounds on derivatives of \u00b5\u0303 in \u2126\u2212). The derivatives of the function \u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101 (Eq. (4)) with respect to \u00b5, \u03c9, \u03bd, \u03c4 in the domain \u2126\u2212 = {\u00b5, \u03c9, \u03bd, \u03c4 | \u2212 0.1 6 \u00b5 6 0.1,\u22120.1 6 \u03c9 6 0.1, 0.05 6 \u03bd 6 0.24, 0.8 6 \u03c4 6 1.25} can be bounded as follows:\n\u2223\u2223\u2223\u2223 \u2202\u2202\u00b5\u00b5\u0303 \u2223\u2223\u2223\u2223 < 0.14 (232)\u2223\u2223\u2223\u2223 \u2202\u2202\u03c9 \u00b5\u0303 \u2223\u2223\u2223\u2223 < 0.14\u2223\u2223\u2223\u2223 \u2202\u2202\u03bd \u00b5\u0303 \u2223\u2223\u2223\u2223 < 0.52\u2223\u2223\u2223\u2223 \u2202\u2202\u03c4 \u00b5\u0303 \u2223\u2223\u2223\u2223 < 0.11.\nProof. The expression\n\u2202\n\u2202\u00b5 \u00b5\u0303 = J11 =\n1 2 \u03bb\u03c9e \u2212(\u00b5\u03c9)2 2\u03bd\u03c4\n( 2e (\u00b5\u03c9)2 2\u03bd\u03c4 \u2212 e (\u00b5\u03c9)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + \u03b1e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )) (233)\ncontains the terms e (\u00b5\u03c9)2 2\u03bd\u03c4 erfc (\n\u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4\n) and e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) which are monotonically de-\ncreasing in their arguments (Lemma 23). We can therefore obtain their minima and maximal at the minimal and maximal arguments. Since the first term has a negative sign in the expression, both terms reach their maximal value at \u00b5\u03c9 = \u22120.01, \u03bd = 0.05, and \u03c4 = 0.8.\u2223\u2223\u2223\u2223 \u2202\u2202\u00b5\u00b5\u0303\n\u2223\u2223\u2223\u2223 6 12 |\u03bb\u03c9| \u2223\u2223\u2223(2\u2212 e0.03535532 erfc (0.0353553) + \u03b1e0.1060662 erfc (0.106066))\u2223\u2223\u2223 < 0.133 (234)\nSince, \u00b5\u0303 is symmetric in \u00b5 and \u03c9, these bounds also hold for the derivate to \u03c9.\n0.0 0.2 0.4 0.6 0.8 1.0 x0.000\n0.001\n0.002\n0.003\n0.004\n0.005\nh(x)\nFigure A7: The graph of the function h(x) = \u00b5\u03032(0.1,\u22120.1, x, 1, \u03bb01, \u03b101) is displayed. It has a local maximum at x = \u03bd\u03c4 \u2248 0.187342 and h(x) \u2248 0.00451457 in the domain x \u2208 [0, 1].\nWe use the argumentation that the term with the error function is monotonically decreasing (Lemma 23) again for the expression\n\u2202\n\u2202\u03bd \u00b5\u0303 = J12 = (235)\n= 1\n4 \u03bb\u03c4e\u2212\n\u00b52\u03c92\n2\u03bd\u03c4 ( \u03b1e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 (\u03b1\u2212 1) \u221a 2\n\u03c0\u03bd\u03c4 ) 6\u2223\u2223\u2223\u222314\u03bb\u03c4\n\u2223\u2223\u2223\u2223 (|1.1072\u2212 2.68593|) < 0.52. We have used that the term 1.1072 6 \u03b101e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) 6 1.49042 and the term\n0.942286 6 (\u03b1 \u2212 1) \u221a\n2 \u03c0\u03bd\u03c4 6 2.68593. Since \u00b5\u0303 is symmetric in \u03bd and \u03c4 , we only have to chance outermost term \u2223\u2223 1 4\u03bb\u03c4 \u2223\u2223 to \u2223\u2223 14\u03bb\u03bd\u2223\u2223 to obtain the estimate \u2223\u2223 \u2202\u2202\u03c4 \u00b5\u0303\u2223\u2223 < 0.11.\nLemma 43 (Tight bound on \u00b5\u03032 in \u2126\u2212). The function \u00b5\u03032(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) (Eq. (4)) is bounded by\u2223\u2223\u00b5\u03032\u2223\u2223 < 0.005 (236) (237)\nin the domain \u2126\u2212 = {\u00b5, \u03c9, \u03bd, \u03c4 | \u2212 0.1 6 \u00b5 6 0.1,\u22120.1 6 \u03c9 6 0.1, 0.05 6 \u03bd 6 0.24, 0.8 6 \u03c4 6 1.25}.\nWe visualize the function \u00b5\u03032 at its maximal \u00b5\u03bd = \u22120.01 and for x = \u03bd\u03c4 in the form h(x) = \u00b5\u03032(0.1,\u22120.1, x, 1, \u03bb01, \u03b101) in Figure A7.\nProof. We use a similar strategy to the one we have used to show the bound on the singular value (Lemmata 10, 11, and 12), where we evaluted the function on a grid and used bounds on the derivatives together with the mean value theorem. Here we have\u2223\u2223\u00b5\u03032(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101)\u2212 \u00b5\u03032(\u00b5+ \u2206\u00b5, \u03c9 + \u2206\u03c9, \u03bd + \u2206\u03bd, \u03c4 + \u2206\u03c4, \u03bb01, \u03b101)\u2223\u2223 6 (238)\u2223\u2223\u2223\u2223 \u2202\u2202\u00b5\u00b5\u03032 \u2223\u2223\u2223\u2223 |\u2206\u00b5|+ \u2223\u2223\u2223\u2223 \u2202\u2202\u03c9 \u00b5\u03032 \u2223\u2223\u2223\u2223 |\u2206\u03c9|+ \u2223\u2223\u2223\u2223 \u2202\u2202\u03bd \u00b5\u03032 \u2223\u2223\u2223\u2223 |\u2206\u03bd|+ \u2223\u2223\u2223\u2223 \u2202\u2202\u03c4 \u00b5\u03032 \u2223\u2223\u2223\u2223 |\u2206\u03c4 |.\nWe use Lemma 42 and Lemma 41, to obtain\u2223\u2223\u2223\u2223 \u2202\u2202\u00b5\u00b5\u03032 \u2223\u2223\u2223\u2223 = 2 |\u00b5\u0303| \u2223\u2223\u2223\u2223 \u2202\u2202\u00b5\u00b5\u0303 \u2223\u2223\u2223\u2223 < 2 \u00b7 0.289324 \u00b7 0.14 = 0.08101072 (239)\u2223\u2223\u2223\u2223 \u2202\u2202\u03c9 \u00b5\u03032 \u2223\u2223\u2223\u2223 = 2 |\u00b5\u0303| \u2223\u2223\u2223\u2223 \u2202\u2202\u03c9 \u00b5\u0303 \u2223\u2223\u2223\u2223 < 2 \u00b7 0.289324 \u00b7 0.14 = 0.08101072\n\u2223\u2223\u2223\u2223 \u2202\u2202\u03bd \u00b5\u03032 \u2223\u2223\u2223\u2223 = 2 |\u00b5\u0303| \u2223\u2223\u2223\u2223 \u2202\u2202\u03bd \u00b5\u0303 \u2223\u2223\u2223\u2223 < 2 \u00b7 0.289324 \u00b7 0.52 = 0.30089696\u2223\u2223\u2223\u2223 \u2202\u2202\u03c4 \u00b5\u03032 \u2223\u2223\u2223\u2223 = 2 |\u00b5\u0303| \u2223\u2223\u2223\u2223 \u2202\u2202\u03c4 \u00b5\u0303\n\u2223\u2223\u2223\u2223 < 2 \u00b7 0.289324 \u00b7 0.11 = 0.06365128 We evaluated the function \u00b5\u03032 in a grid G of \u2126\u2212 with \u2206\u00b5 = 0.001498041, \u2206\u03c9 = 0.001498041, \u2206\u03bd = 0.0004033190, and \u2206\u03c4 = 0.0019065994 using a computer and obtained the maximal value maxG(\u00b5\u0303) 2 = 0.00451457, therefore the maximal value of \u00b5\u03032 is bounded by\nmax (\u00b5,\u03c9,\u03bd,\u03c4)\u2208\u2126\u2212\n(\u00b5\u0303)2 6 (240)\n0.00451457 + 0.001498041 \u00b7 0.08101072 + 0.001498041 \u00b7 0.08101072+ 0.0004033190 \u00b7 0.30089696 + 0.0019065994 \u00b7 0.06365128 < 0.005. (241)\nFurthermore we used error propagation to estimate the numerical error on the function evaluation. Using the error propagation rules derived in Subsection A3.4.5, we found that the numerical error is smaller than 10\u221213 in the worst case.\nLemma 44 (Main subfunction). For 1.2 6 x 6 20 and \u22120.1 6 y 6 0.1, the function\ne (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( 2x+ y\u221a\n2 \u221a x\n) (242)\nis smaller than zero, is strictly monotonically increasing in x, and strictly monotonically decreasing in y for the minimal x = 12/10 = 1.2.\nProof. We first consider the derivative of sub-function Eq. (101) with respect to x. The derivative of the function\ne (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( 2x+ y\u221a\n2 \u221a x\n) (243)\nwith respect to x is \u221a \u03c0 ( e (x+y)2 2x (x\u2212 y)(x+ y) erfc ( x+y\u221a 2 \u221a x ) \u2212 2e (2x+y)2 2x ( 4x2 \u2212 y2 ) erfc ( 2x+y\u221a 2 \u221a x )) + \u221a 2 \u221a x(3x\u2212 y)\n2 \u221a \u03c0x2\n=\n(244) \u221a \u03c0 ( e (x+y)2 2x (x\u2212 y)(x+ y) erfc ( x+y\u221a 2 \u221a x ) \u2212 2e (2x+y)2 2x (2x+ y)(2x\u2212 y) erfc ( 2x+y\u221a 2 \u221a x )) + \u221a 2 \u221a x(3x\u2212 y)\n2 \u221a \u03c0x2\n=\n\u221a \u03c0  e (x+y)22x (x\u2212y)(x+y) erfc( x+y\u221a2\u221ax)\u221a 2 \u221a x \u2212 2e (2x+y)2 2x (2x+y)(2x\u2212y) erfc ( 2x+y\u221a 2 \u221a x ) \u221a 2 \u221a x + (3x\u2212 y) 2 \u221a 2 \u221a \u03c0x2 \u221a x .\nWe consider the numerator\n\u221a \u03c0 e (x+y) 2 2x (x\u2212 y)(x+ y) erfc ( x+y\u221a 2 \u221a x ) \u221a\n2 \u221a x\n\u2212 2e\n(2x+y)2 2x (2x+ y)(2x\u2212 y) erfc (\n2x+y\u221a 2 \u221a x ) \u221a\n2 \u221a x\n+ (3x\u2212 y) . (245)\nFor bounding this value, we use the approximation\nez 2 erfc(z) \u2248 2.911\u221a \u03c0(2.911\u2212 1)z + \u221a \u03c0z2 + 2.9112 . (246)\nfrom Ren and MacKenzie [30]. We start with an error analysis of this approximation. According to Ren and MacKenzie [30] (Figure 1), the approximation error is positive in the range [0.7, 3.2]. This range contains all possible arguments of erfc that we consider. Numerically we maximized and minimized the approximation error of the whole expression\nE(x, y) = e (x+y) 2 2x (x\u2212 y)(x+ y) erfc ( x+y\u221a 2 \u221a x ) \u221a\n2 \u221a x\n\u2212 2e\n(2x+y)2 2x (2x\u2212 y)(2x+ y) erfc (\n2x+y\u221a 2 \u221a x ) \u221a\n2 \u221a x  \u2212 (247) 2.911(x\u2212 y)(x+ y)(\u221a 2 \u221a x )(\u221a\u03c0(2.911\u22121)(x+y)\u221a\n2 \u221a x\n+ \u221a \u03c0 ( x+y\u221a 2 \u221a x )2 + 2.9112 ) \u2212\n2 \u00b7 2.911(2x\u2212 y)(2x+ y)(\u221a 2 \u221a x )(\u221a\u03c0(2.911\u22121)(2x+y)\u221a\n2 \u221a x\n+ \u221a \u03c0 (\n2x+y\u221a 2 \u221a x\n)2 + 2.9112\n)  .\nWe numerically determined 0.0113556 6 E(x, y) 6 0.0169551 for 1.2 6 x 6 20 and \u22120.1 6 y 6 0.1. We used different numerical optimization techniques like gradient based constraint BFGS algorithms and non-gradient-based Nelder-Mead methods with different start points. Therefore our approximation is smaller than the function that we approximate. We subtract an additional safety gap of 0.0131259 from our approximation to ensure that the inequality via the approximation holds true. With this safety gap the inequality would hold true even for negative x, where the approximation error becomes negative and the safety gap would compensate. Of course, the safety gap of 0.0131259 is not necessary for our analysis but may help or future investigations.\nWe have the sequences of inequalities using the approximation of Ren and MacKenzie [30]:\n(3x\u2212 y) + e (x+y) 2 2x (x\u2212 y)(x+ y) erfc ( x+y\u221a 2 \u221a x ) \u221a\n2 \u221a x\n\u2212 2e\n(2x+y)2 2x (2x\u2212 y)(2x+ y) erfc (\n2x+y\u221a 2 \u221a x ) \u221a\n2 \u221a x\n\u221a\u03c0 > (248)\n(3x\u2212 y) +  2.911(x\u2212 y)(x+ y)(\u221a \u03c0 ( x+y\u221a 2 \u221a x )2 + 2.9112 + (2.911\u22121) \u221a \u03c0(x+y)\u221a 2 \u221a x )(\u221a 2 \u221a x ) \u2212\n2(2x\u2212 y)(2x+ y)2.911(\u221a 2 \u221a x )(\u221a \u03c0 (\n2x+y\u221a 2 \u221a x\n)2 + 2.9112 + (2.911\u22121) \u221a \u03c0(2x+y)\u221a\n2 \u221a x\n) \u221a\u03c0 \u2212 0.0131259 =\n(3x\u2212 y) +  (\u221a2\u221ax2.911) (x\u2212 y)(x+ y)(\u221a \u03c0(x+ y)2 + 2 \u00b7 2.9112x+ (2.911\u2212 1)(x+ y) \u221a \u03c0 ) (\u221a 2 \u221a x ) \u2212\n2(2x\u2212 y)(2x+ y) (\u221a 2 \u221a x2.911 )(\u221a 2 \u221a x ) (\u221a \u03c0(2x+ y)2 + 2 \u00b7 2.9112x+ (2.911\u2212 1)(2x+ y) \u221a \u03c0 ) \u221a\u03c0 \u2212 0.0131259 =\n(3x\u2212 y) + 2.911  (x\u2212 y)(x+ y) (2.911\u2212 1)(x+ y) + \u221a (x+ y)2 + 2\u00b72.911\n2x \u03c0\n\u2212\n2(2x\u2212 y)(2x+ y) (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 + 2\u00b72.911 2x\n\u03c0 \u2212 0.0131259 > (3x\u2212 y) + 2.911\n (x\u2212 y)(x+ y) (2.911\u2212 1)(x+ y) + \u221a( 2.9112\n\u03c0\n)2 + (x+ y)2 + 2\u00b72.911\n2x \u03c0 + 2\u00b72.9112y \u03c0\n\u2212\n2(2x\u2212 y)(2x+ y) (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 + 2\u00b72.911 2x\n\u03c0 \u2212 0.0131259 = (3x\u2212 y) + 2.911\n (x\u2212 y)(x+ y) (2.911\u2212 1)(x+ y) + \u221a( x+ y + 2.911 2\n\u03c0 )2 \u2212 2(2x\u2212 y)(2x+ y)\n(2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 + 2\u00b72.911 2x\n\u03c0 \u2212 0.0131259 = (3x\u2212 y) + 2.911  (x\u2212 y)(x+ y) 2.911(x+ y) + 2.911 2\n\u03c0\n\u2212 2(2x\u2212 y)(2x+ y) (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 + 2\u00b72.911 2x\n\u03c0 \u2212 0.0131259 = (3x\u2212 y) + (x\u2212 y)(x+ y)\n(x+ y) + 2.911\u03c0 \u2212 2(2x\u2212 y)(2x+ y)2.911 (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 + 2\u00b72.911 2x\n\u03c0\n\u2212 0.0131259 = (3x\u2212 y) + (x\u2212 y)(x+ y) (x+ y) + 2.911\u03c0 \u2212 2(2x\u2212 y)(2x+ y)2.911 (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 + 2\u00b72.911 2x\n\u03c0\n\u2212 0.0131259 =\n( \u22122(2x\u2212 y)2.911 ( (x+ y) + 2.911\n\u03c0 ) (2x+ y) +(\n(x+ y) + 2.911\n\u03c0\n) (3x\u2212 y \u2212 0.0131259) ( (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 +\n2 \u00b7 2.9112x \u03c0\n) +\n(x\u2212 y)(x+ y) ( (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 +\n2 \u00b7 2.9112x \u03c0 )) ((\n(x+ y) + 2.911\n\u03c0\n)( (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 +\n2 \u00b7 2.9112x \u03c0 ))\u22121 =(\n((x\u2212 y)(x+ y) + (3x\u2212 y \u2212 0.0131259)(x+ y + 0.9266)) (\u221a (2x+ y)2 + 5.39467x+ 3.822x+ 1.911y ) \u2212\n(249) 5.822(2x\u2212 y)(x+ y + 0.9266)(2x+ y))((\n(x+ y) + 2.911\n\u03c0\n)( (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 + 22.9112x\n\u03c0\n))\u22121 > 0 .\nWe explain this sequence of inequalities:\n\u2022 First inequality: The approximation of Ren and MacKenzie [30] and then subtracting a safety gap (which would not be necessary for the current analysis). \u2022 Equalities: The factor \u221a 2 \u221a x is factored out and canceled.\n\u2022 Second inequality: adds a positive term in the first root to obtain a binomial form. The term containing the root is positive and the root is in the denominator, therefore the whole term becomes smaller.\n\u2022 Equalities: solve for the term and factor out. \u2022 Bringing all terms to the denominator ( (x+ y) + 2.911\u03c0 )( (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 + 2\u00b72.911 2x \u03c0 ) .\n\u2022 Equalities: Multiplying out and expanding terms.\n\u2022 Last inequality > 0 is proofed in the following sequence of inequalities.\nWe look at the numerator of the last expression of Eq. (248), which we show to be positive in order to show > 0 in Eq. (248). The numerator is\n((x\u2212 y)(x+ y) + (3x\u2212 y \u2212 0.0131259)(x+ y + 0.9266)) (\u221a (2x+ y)2 + 5.39467x+ 3.822x+ 1.911y ) \u2212\n(250) 5.822(2x\u2212 y)(x+ y + 0.9266)(2x+ y) = \u2212 5.822(2x\u2212 y)(x+ y + 0.9266)(2x+ y) + (3.822x+ 1.911y)((x\u2212 y)(x+ y)+ (3x\u2212 y \u2212 0.0131259)(x+ y + 0.9266)) + ((x\u2212 y)(x+ y)+\n(3x\u2212 y \u2212 0.0131259)(x+ y + 0.9266)) \u221a (2x+ y)2 + 5.39467x =\n\u2212 8.0x3 + ( 4x2 + 2xy + 2.76667x\u2212 2y2 \u2212 0.939726y \u2212 0.0121625 )\u221a (2x+ y)2 + 5.39467x\u2212\n8.0x2y \u2212 11.0044x2 + 2.0xy2 + 1.69548xy \u2212 0.0464849x+ 2.0y3 + 3.59885y2 \u2212 0.0232425y = \u2212 8.0x3 + ( 4x2 + 2xy + 2.76667x\u2212 2y2 \u2212 0.939726y \u2212 0.0121625 )\u221a (2x+ y)2 + 5.39467x\u2212\n8.0x2y \u2212 11.0044x2 + 2.0xy2 + 1.69548xy \u2212 0.0464849x+ 2.0y3 + 3.59885y2 \u2212 0.0232425y .\nThe factor in front of the root is positive. If the term, that does not contain the root, was positive, then the whole expression would be positive and we would have proofed that the numerator is positive. Therefore we consider the case that the term, that does not contain the root, is negative. The term that contains the root must be larger than the other term in absolute values.\n\u2212 ( \u22128.0x3 \u2212 8.0x2y \u2212 11.0044x2 + 2.xy2 + 1.69548xy \u2212 0.0464849x+ 2.y3 + 3.59885y2 \u2212 0.0232425y ) < (251)( 4x2 + 2xy + 2.76667x\u2212 2y2 \u2212 0.939726y \u2212 0.0121625 )\u221a (2x+ y)2 + 5.39467x .\nTherefore the squares of the root term have to be larger than the square of the other term to show > 0 in Eq. (248). Thus, we have the inequality:( \u22128.0x3 \u2212 8.0x2y \u2212 11.0044x2 + 2.xy2 + 1.69548xy \u2212 0.0464849x+ 2.y3 + 3.59885y2 \u2212 0.0232425y )2 < (252)( 4x2 + 2xy + 2.76667x\u2212 2y2 \u2212 0.939726y \u2212 0.0121625 )2 ( (2x+ y)2 + 5.39467x ) .\nThis is equivalent to 0 < ( 4x2 + 2xy + 2.76667x\u2212 2y2 \u2212 0.939726y \u2212 0.0121625 )2 ( (2x+ y)2 + 5.39467x ) \u2212 (253)(\n\u22128.0x3 \u2212 8.0x2y \u2212 11.0044x2 + 2.0xy2 + 1.69548xy \u2212 0.0464849x+ 2.0y3 + 3.59885y2 \u2212 0.0232425y )2 =\n\u2212 1.2227x5 + 40.1006x4y + 27.7897x4 + 41.0176x3y2 + 64.5799x3y + 39.4762x3 + 10.9422x2y3\u2212 13.543x2y2 \u2212 28.8455x2y \u2212 0.364625x2 + 0.611352xy4 + 6.83183xy3 + 5.46393xy2+ 0.121746xy + 0.000798008x\u2212 10.6365y5 \u2212 11.927y4 + 0.190151y3 \u2212 0.000392287y2 .\nWe obtain the inequalities:\n\u2212 1.2227x5 + 40.1006x4y + 27.7897x4 + 41.0176x3y2 + 64.5799x3y + 39.4762x3 + 10.9422x2y3\u2212 (254)\n13.543x2y2 \u2212 28.8455x2y \u2212 0.364625x2 + 0.611352xy4 + 6.83183xy3 + 5.46393xy2+ 0.121746xy + 0.000798008x\u2212 10.6365y5 \u2212 11.927y4 + 0.190151y3 \u2212 0.000392287y2 =\n\u2212 1.2227x5 + 27.7897x4 + 41.0176x3y2 + 39.4762x3 \u2212 13.543x2y2 \u2212 0.364625x2+ y ( 40.1006x4 + 64.5799x3 + 10.9422x2y2 \u2212 28.8455x2 + 6.83183xy2 + 0.121746x \u2212\n10.6365y4 + 0.190151y2 )\n+ 0.611352xy4 + 5.46393xy2 + 0.000798008x\u2212 11.927y4 \u2212 0.000392287y2 > \u2212 1.2227x5 + 27.7897x4 + 41.0176 \u00b7 (0.0)2x3 + 39.4762x3 \u2212 13.543 \u00b7 (0.1)2x2 \u2212 0.364625x2\u2212 0.1 \u00b7 ( 40.1006x4 + 64.5799x3 + 10.9422 \u00b7 (0.1)2x2 \u2212 28.8455x2 + 6.83183 \u00b7 (0.1)2x+ 0.121746x +\n10.6365 \u00b7 (0.1)4 + 0.190151 \u00b7 (0.1)2 ) +\n0.611352 \u00b7 (0.0)4x+ 5.46393 \u00b7 (0.0)2x+ 0.000798008x\u2212 11.927 \u00b7 (0.1)4 \u2212 0.000392287 \u00b7 (0.1)2 = \u2212 1.2227x5 + 23.7796x4 + (20 + 13.0182)x3 + 2.37355x2 \u2212 0.0182084x\u2212 0.000194074 > \u2212 1.2227x5 + 24.7796x4 + 13.0182x3 + 2.37355x2 \u2212 0.0182084x\u2212 0.000194074 > 13.0182x3 + 2.37355x2 \u2212 0.0182084x\u2212 0.000194074 > 0 .\nWe used 24.7796 \u00b7 (20)4 \u2212 1.2227 \u00b7 (20)5 = 52090.9 > 0 and x 6 20. We have proofed the last inequality > 0 of Eq. (248).\nConsequently the derivative is always positive independent of y, thus\ne (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( 2x+ y\u221a\n2 \u221a x\n) (255)\nis strictly monotonically increasing in x.\nThe main subfunction is smaller than zero. Next we show that the sub-function Eq. (101) is smaller than zero. We consider the limit:\nlim x\u2192\u221e\ne (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( 2x+ y\u221a\n2 \u221a x\n) = 0 (256)\nThe limit follows from Lemma 22. Since the function is monotonic increasing in x, it has to approach 0 from below. Thus,\ne (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( 2x+ y\u221a\n2 \u221a x\n) (257)\nis smaller than zero.\nBehavior of the main subfunction with respect to y at minimal x. We now consider the derivative of sub-function Eq. (101) with respect to y. We proofed that sub-function Eq. (101) is strictly monotonically increasing independent of y. In the proof of Theorem 16, we need the minimum of sub-function Eq. (101). Therefore we are only interested in the derivative of sub-function Eq. (101) with respect to y for the minimum x = 12/10 = 1.2\nConsequently, we insert the minimum x = 12/10 = 1.2 into the sub-function Eq. (101). The main terms become\nx+ y\u221a 2 \u221a x = y + 1.2\u221a 2 \u221a 1.2 = y\u221a 2 \u221a 1.2 + \u221a 1.2\u221a 2 = 5y + 6 2 \u221a 15 (258)\nand 2x+ y\u221a\n2 \u221a x\n= y + 1.2 \u00b7 2\u221a\n2 \u221a 1.2 = y\u221a 2 \u221a 1.2 + \u221a 1.2 \u221a 2 = 5y + 12 2 \u221a 15 . (259)\nSub-function Eq. (101) becomes:\ne\n( y\n\u221a 2 \u221a\n12 10\n+ \u221a 12 10\u221a 2 )2 erfc  y\u221a 2 \u221a\n12 10\n+ \u221a 12 10\u221a 2 \u2212 2e ( y \u221a 2 \u221a 12 10 + \u221a 2 \u221a 12 10 )2 erfc  y\u221a 2 \u221a\n12 10\n+ \u221a 2\n\u221a 12\n10  . (260)\nThe derivative of this function with respect to y is \u221a 15\u03c0 ( e 1 60 (5y+6) 2 (5y + 6) erfc ( 5y+6\n2 \u221a 15\n) \u2212 2e 160 (5y+12)2(5y + 12) erfc ( 5y+12\n2 \u221a 15\n)) + 30\n6 \u221a 15\u03c0 . (261)\nWe again will use the approximation of Ren and MacKenzie [30]\nez 2 erfc(z) = 2.911\n\u221a \u03c0(2.911\u2212 1)z + \u221a \u03c0z2 + 2.9112 . (262)\nTherefore we first perform an error analysis. We estimated the maximum and minimum of\n\u221a 15\u03c0  2 \u00b7 2.911(5y + 12)\u221a \u03c0(2.911\u22121)(5y+12)\n2 \u221a 15 +\n\u221a \u03c0 ( 5y+12\n2 \u221a 15\n)2 + 2.9112 \u2212 2.911(5y + 6) \u221a \u03c0(2.911\u22121)(5y+6)\n2 \u221a 15 +\n\u221a \u03c0 ( 5y+6\n2 \u221a 15\n)2 + 2.9112 + 30 + (263)\n\u221a 15\u03c0 ( e 1 60 (5y+6) 2 (5y + 6) erfc ( 5y + 6\n2 \u221a 15\n) \u2212 2e 160 (5y+12) 2 (5y + 12) erfc ( 5y + 12\n2 \u221a 15\n)) + 30 .\nWe obtained for the maximal absolute error the value 0.163052. We added an approximation error of 0.2 to the approximation of the derivative. Since we want to show that the approximation upper bounds the true expression, the addition of the approximation error is required here. We get a sequence of inequalities:\n\u221a 15\u03c0 ( e 1 60 (5y+6) 2 (5y + 6) erfc ( 5y + 6\n2 \u221a 15\n) \u2212 2e 160 (5y+12) 2 (5y + 12) erfc ( 5y + 12\n2 \u221a 15\n)) + 30 6\n(264)\n\u221a 15\u03c0  2.911(5y + 6)\u221a \u03c0(2.911\u22121)(5y+6)\n2 \u221a 15 +\n\u221a \u03c0 ( 5y+6\n2 \u221a 15\n)2 + 2.9112 \u2212 2 \u00b7 2.911(5y + 12) \u221a \u03c0(2.911\u22121)(5y+12)\n2 \u221a 15 +\n\u221a \u03c0 ( 5y+12\n2 \u221a 15\n)2 + 2.9112 + 30 + 0.2 =\n(30 \u00b7 2.911)(5y + 6) (2.911\u2212 1)(5y + 6) + \u221a (5y + 6)2 + ( 2 \u221a\n15\u00b72.911\u221a \u03c0\n)2 \u2212 2(30 \u00b7 2.911)(5y + 12) (2.911\u2212 1)(5y + 12) + \u221a (5y + 12)2 + ( 2 \u221a\n15\u00b72.911\u221a \u03c0 )2 + 30 + 0.2 =(0.2 + 30) (2.911\u2212 1)(5y + 12) + \u221a\u221a\u221a\u221a(5y + 12)2 +(2\u221a15 \u00b7 2.911\u221a\n\u03c0 )2 (2.911\u2212 1)(5y + 6) + \u221a\u221a\u221a\u221a(5y + 6)2 +(2\u221a15 \u00b7 2.911\u221a \u03c0\n)2\u2212 2 \u00b7 30 \u00b7 2.911(5y + 12) (2.911\u2212 1)(5y + 6) + \u221a\u221a\u221a\u221a(5y + 6)2 +(2\u221a15 \u00b7 2.911\u221a\n\u03c0 )2+ 2.911 \u00b7 30(5y + 6) (2.911\u2212 1)(5y + 12) + \u221a\u221a\u221a\u221a(5y + 12)2 +(2\u221a15 \u00b7 2.911\u221a\n\u03c0\n)2 \n (2.911\u2212 1)(5y + 6) + \u221a\u221a\u221a\u221a(5y + 6)2 +(2\u221a15 \u00b7 2.911\u221a \u03c0 )2 (2.911\u2212 1)(5y + 12) + \u221a\u221a\u221a\u221a(5y + 12)2 +(2\u221a15 \u00b7 2.911\u221a \u03c0 )2  \u22121 < 0 .\nWe explain this sequence of inequalities.\n\u2022 First inequality: The approximation of Ren and MacKenzie [30] and then adding the error bound to ensure that the approximation is larger than the true value. \u2022 First equality: The factor 2 \u221a 15 and 2 \u221a \u03c0 are factored out and canceled.\n\u2022 Second equality: Bringing all terms to the denominator(2.911\u2212 1)(5y + 6) + \u221a\u221a\u221a\u221a(5y + 6)2 +(2\u221a152.911\u221a\n\u03c0 )2 (265) (2.911\u2212 1)(5y + 12) + \u221a\u221a\u221a\u221a(5y + 12)2 +(2\u221a15 \u00b7 2.911\u221a \u03c0\n)2 . \u2022 Last inequality < 0 is proofed in the following sequence of inequalities.\nWe look at the numerator of the last term in Eq. (264). We have to proof that this numerator is smaller than zero in order to proof the last inequality of Eq. (264). The numerator is\n(0.2 + 30) (2.911\u2212 1)(5y + 12) + \u221a\u221a\u221a\u221a(5y + 12)2 +(2\u221a15 \u00b7 2.911\u221a\n\u03c0 )2 (266) (2.911\u2212 1)(5y + 6) + \u221a\u221a\u221a\u221a(5y + 6)2 +(2\u221a15 \u00b7 2.911\u221a \u03c0\n)2 \u2212 2 \u00b7 30 \u00b7 2.911(5y + 12) (2.911\u2212 1)(5y + 6) + \u221a\u221a\u221a\u221a(5y + 6)2 +(2\u221a15 \u00b7 2.911\u221a\n\u03c0 )2+ 2.911 \u00b7 30(5y + 6) (2.911\u2212 1)(5y + 12) + \u221a\u221a\u221a\u221a(5y + 12)2 +(2\u221a15 .2.911\u221a\n\u03c0 )2 . We now compute upper bounds for this numerator:\n(0.2 + 30) (2.911\u2212 1)(5y + 12) + \u221a\u221a\u221a\u221a(5y + 12)2 +(2\u221a15 \u00b7 2.911\u221a\n\u03c0 )2 (267) (2.911\u2212 1)(5y + 6) + \u221a\u221a\u221a\u221a(5y + 6)2 +(2\u221a15 \u00b7 2.911\u221a \u03c0 )2\u2212\n2 \u00b7 30 \u00b7 2.911(5y + 12) (2.911\u2212 1)(5y + 6) + \u221a\u221a\u221a\u221a(5y + 6)2 +(2\u221a15 \u00b7 2.911\u221a\n\u03c0 )2+ 2.911 \u00b7 30(5y + 6) (2.911\u2212 1)(5y + 12) + \u221a\u221a\u221a\u221a(5y + 12)2 +(2\u221a15 \u00b7 2.911\u221a\n\u03c0 )2 = \u2212 1414.99y2 \u2212 584.739 \u221a (5y + 6)2 + 161.84y + 725.211 \u221a (5y + 12)2 + 161.84y\u2212\n5093.97y \u2212 1403.37 \u221a (5y + 6)2 + 161.84 + 30.2 \u221a (5y + 6)2 + 161.84 \u221a (5y + 12)2 + 161.84+\n870.253 \u221a (5y + 12)2 + 161.84\u2212 4075.17 <\n\u2212 1414.99y2 \u2212 584.739 \u221a (5y + 6)2 + 161.84y + 725.211 \u221a (5y + 12)2 + 161.84y\u2212\n5093.97y \u2212 1403.37 \u221a (6 + 5 \u00b7 (\u22120.1))2 + 161.84 + 30.2 \u221a (6 + 5 \u00b7 0.1)2 + 161.84 \u221a (12 + 5 \u00b7 0.1)2 + 161.84+\n870.253 \u221a (12 + 5 \u00b7 0.1)2 + 161.84\u2212 4075.17 =\n\u2212 1414.99y2 \u2212 584.739 \u221a (5y + 6)2 + 161.84y + 725.211 \u221a (5y + 12)2 + 161.84y \u2212 5093.97y \u2212 309.691 <\ny ( \u2212584.739 \u221a (5y + 6)2 + 161.84 + 725.211 \u221a (5y + 12)2 + 161.84\u2212 5093.97 ) \u2212 309.691 <\n\u2212 0.1 ( 725.211 \u221a (12 + 5 \u00b7 (\u22120.1))2 + 161.84\u2212 584.739 \u221a (6 + 5 \u00b7 0.1)2 + 161.84\u2212 5093.97 ) \u2212 309.691 =\n\u2212 208.604 .\nFor the first inequality we choose y in the roots, so that positive terms maximally increase and negative terms maximally decrease. The second inequality just removed the y2 term which is always negative, therefore increased the expression. For the last inequality, the term in brackets is negative for all settings of y. Therefore we make the brackets as negative as possible and make the whole term positive by multiplying with y = \u22120.1. Consequently\ne (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( 2x+ y\u221a\n2 \u221a x\n) (268)\nis strictly monotonically decreasing in y for the minimal x = 1.2.\nLemma 45 (Main subfunction below). For 0.007 6 x 6 0.875 and \u22120.01 6 y 6 0.01, the function\ne (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( 2x+ y\u221a\n2 \u221a x\n) (269)\nsmaller than zero, is strictly monotonically increasing in x and strictly monotonically increasing in y for the minimal x = 0.007 = 0.00875 \u00b7 0.8, x = 0.56 = 0.7 \u00b7 0.8, x = 0.128 = 0.16 \u00b7 0.8, and x = 0.216 = 0.24 \u00b7 0.9 (lower bound of 0.9 on \u03c4 ).\nProof. We first consider the derivative of sub-function Eq. (111) with respect to x. The derivative of the function\ne (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( 2x+ y\u221a\n2 \u221a x\n) (270)\nwith respect to x is \u221a \u03c0 ( e (x+y)2 2x (x\u2212 y)(x+ y) erfc ( x+y\u221a 2 \u221a x ) \u2212 2e (2x+y)2 2x ( 4x2 \u2212 y2 ) erfc ( 2x+y\u221a 2 \u221a x )) + \u221a 2 \u221a x(3x\u2212 y)\n2 \u221a \u03c0x2\n=\n(271) \u221a \u03c0 ( e (x+y)2 2x (x\u2212 y)(x+ y) erfc ( x+y\u221a 2 \u221a x ) \u2212 2e (2x+y)2 2x (2x+ y)(2x\u2212 y) erfc ( 2x+y\u221a 2 \u221a x )) + \u221a 2 \u221a x(3x\u2212 y)\n2 \u221a \u03c0x2\n=\n\u221a \u03c0  e (x+y)22x (x\u2212y)(x+y) erfc( x+y\u221a2\u221ax)\u221a 2 \u221a x \u2212 2e (2x+y)2 2x (2x+y)(2x\u2212y) erfc ( 2x+y\u221a 2 \u221a x ) \u221a 2 \u221a x + (3x\u2212 y) \u221a\n22 \u221a \u03c0 \u221a xx2\n.\nWe consider the numerator\n\u221a \u03c0 e (x+y) 2 2x (x\u2212 y)(x+ y) erfc ( x+y\u221a 2 \u221a x ) \u221a\n2 \u221a x\n\u2212 2e\n(2x+y)2 2x (2x+ y)(2x\u2212 y) erfc (\n2x+y\u221a 2 \u221a x ) \u221a\n2 \u221a x\n+ (3x\u2212 y) . (272)\nFor bounding this value, we use the approximation\nez 2 erfc(z) \u2248 2.911\u221a \u03c0(2.911\u2212 1)z + \u221a \u03c0z2 + 2.9112 . (273)\nfrom Ren and MacKenzie [30]. We start with an error analysis of this approximation. According to Ren and MacKenzie [30] (Figure 1), the approximation error is both positive and negative in the range [0.175, 1.33]. This range contains all possible arguments of erfc that we consider in this subsection. Numerically we maximized and minimized the approximation error of the whole expression\nE(x, y) = e (x+y) 2 2x (x\u2212 y)(x+ y) erfc ( x+y\u221a 2 \u221a x ) \u221a\n2 \u221a x\n\u2212 2e\n(2x+y)2 2x (2x\u2212 y)(2x+ y) erfc (\n2x+y\u221a 2 \u221a x ) \u221a\n2 \u221a x  \u2212 (274) 2.911(x\u2212 y)(x+ y)(\u221a 2 \u221a x )(\u221a\u03c0(2.911\u22121)(x+y)\u221a\n2 \u221a x\n+ \u221a \u03c0 ( x+y\u221a 2 \u221a x )2 + 2.9112 ) \u2212\n2 \u00b7 2.911(2x\u2212 y)(2x+ y)(\u221a 2 \u221a x )(\u221a\u03c0(2.911\u22121)(2x+y)\u221a\n2 \u221a x\n+ \u221a \u03c0 (\n2x+y\u221a 2 \u221a x\n)2 + 2.9112\n)  .\nWe numerically determined \u22120.000228141 6 E(x, y) 6 0.00495688 for 0.08 6 x 6 0.875 and \u22120.01 6 y 6 0.01. We used different numerical optimization techniques like gradient based constraint BFGS algorithms and non-gradient-based Nelder-Mead methods with different start points. Therefore our approximation is smaller than the function that we approximate.\nWe use an error gap of \u22120.0003 to countermand the error due to the approximation. We have the sequences of inequalities using the approximation of Ren and MacKenzie [30]:\n(3x\u2212 y) + e (x+y) 2 2x (x\u2212 y)(x+ y) erfc ( x+y\u221a 2 \u221a x ) \u221a\n2 \u221a x\n\u2212 2e\n(2x+y)2 2x (2x\u2212 y)(2x+ y) erfc (\n2x+y\u221a 2 \u221a x ) \u221a\n2 \u221a x\n\u221a\u03c0 > (275)\n(3x\u2212 y) +  2.911(x\u2212 y)(x+ y)(\u221a \u03c0 ( x+y\u221a 2 \u221a x )2 + 2.9112 + (2.911\u22121) \u221a \u03c0(x+y)\u221a 2 \u221a x )(\u221a 2 \u221a x ) \u2212\n2(2x\u2212 y)(2x+ y)2.911(\u221a 2 \u221a x )(\u221a \u03c0 (\n2x+y\u221a 2 \u221a x\n)2 + 2.9112 + (2.911\u22121) \u221a \u03c0(2x+y)\u221a\n2 \u221a x\n) \u221a\u03c0 \u2212 0.0003 =\n(3x\u2212 y) +  (\u221a2\u221ax2.911) (x\u2212 y)(x+ y)(\u221a \u03c0(x+ y)2 + 2 \u00b7 2.9112x+ (2.911\u2212 1)(x+ y) \u221a \u03c0 ) (\u221a 2 \u221a x ) \u2212\n2(2x\u2212 y)(2x+ y) (\u221a 2 \u221a x2.911 )(\u221a 2 \u221a x ) (\u221a \u03c0(2x+ y)2 + 2 \u00b7 2.9112x+ (2.911\u2212 1)(2x+ y) \u221a \u03c0 ) \u221a\u03c0 \u2212 0.0003 =\n(3x\u2212 y) + 2.911  (x\u2212 y)(x+ y) (2.911\u2212 1)(x+ y) + \u221a (x+ y)2 + 2\u00b72.911\n2x \u03c0\n\u2212\n2(2x\u2212 y)(2x+ y) (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 + 2\u00b72.911 2x\n\u03c0 \u2212 0.0003 > (3x\u2212 y) + 2.911\n (x\u2212 y)(x+ y) (2.911\u2212 1)(x+ y) + \u221a( 2.9112\n\u03c0\n)2 + (x+ y)2 + 2\u00b72.911\n2x \u03c0 + 2\u00b72.9112y \u03c0\n\u2212\n2(2x\u2212 y)(2x+ y) (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 + 2\u00b72.911 2x\n\u03c0 \u2212 0.0003 = (3x\u2212 y) + 2.911\n (x\u2212 y)(x+ y) (2.911\u2212 1)(x+ y) + \u221a( x+ y + 2.911 2\n\u03c0 )2 \u2212 2(2x\u2212 y)(2x+ y)\n(2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 + 2\u00b72.911 2x\n\u03c0 \u2212 0.0003 = (3x\u2212 y) + 2.911  (x\u2212 y)(x+ y) 2.911(x+ y) + 2.911 2\n\u03c0\n\u2212 2(2x\u2212 y)(2x+ y) (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 + 2\u00b72.911 2x\n\u03c0 \u2212 0.0003 = (3x\u2212 y) + (x\u2212 y)(x+ y)\n(x+ y) + 2.911\u03c0 \u2212 2(2x\u2212 y)(2x+ y)2.911 (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 + 2\u00b72.911 2x\n\u03c0\n\u2212 0.0003 =\n(3x\u2212 y) + (x\u2212 y)(x+ y) (x+ y) + 2.911\u03c0 \u2212 2(2x\u2212 y)(2x+ y)2.911 (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 + 2\u00b72.911 2x\n\u03c0\n\u2212 0.0003 =\n( \u22122(2x\u2212 y)2.911 ( (x+ y) + 2.911\n\u03c0 ) (2x+ y) +(\n(x+ y) + 2.911\n\u03c0\n) (3x\u2212 y \u2212 0.0003) ( (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 +\n2 \u00b7 2.9112x \u03c0\n) +\n(x\u2212 y)(x+ y) ( (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 +\n2 \u00b7 2.9112x \u03c0 )) ((\n(x+ y) + 2.911\n\u03c0\n)( (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 +\n2 \u00b7 2.9112x \u03c0\n))\u22121 =\n( \u22128x3 \u2212 8x2y + 4x2 \u221a (2x+ y)2 + 5.39467x\u2212 10.9554x2 + 2xy2 \u2212 2y2 \u221a (2x+ y)2 + 5.39467x +\n1.76901xy + 2xy \u221a (2x+ y)2 + 5.39467x+ 2.7795x \u221a (2x+ y)2 + 5.39467x \u2212\n0.9269y \u221a (2x+ y)2 + 5.39467x\u2212 0.00027798 \u221a\n(2x+ y)2 + 5.39467x\u2212 0.00106244x + 2y3 + 3.62336y2 \u2212 0.00053122y )(( (x+ y) + 2.911\n\u03c0\n)( (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 +\n2 \u00b7 2.9112x \u03c0 ))\u22121 =(\n\u22128x3 + ( 4x2 + 2xy + 2.7795x\u2212 2y2 \u2212 0.9269y \u2212 0.00027798 )\u221a (2x+ y)2 + 5.39467x \u2212\n8x2y \u2212 10.9554x2 + 2xy2 + 1.76901xy \u2212 0.00106244x+ 2y3 + 3.62336y2 \u2212 0.00053122y )((\n(x+ y) + 2.911\n\u03c0\n)( (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 +\n2 \u00b7 2.9112x \u03c0\n))\u22121 > 0 .\nWe explain this sequence of inequalities:\n\u2022 First inequality: The approximation of Ren and MacKenzie [30] and then subtracting an error gap of 0.0003. \u2022 Equalities: The factor \u221a 2 \u221a x is factored out and canceled.\n\u2022 Second inequality: adds a positive term in the first root to obtain a binomial form. The term containing the root is positive and the root is in the denominator, therefore the whole term becomes smaller.\n\u2022 Equalities: solve for the term and factor out. \u2022 Bringing all terms to the denominator ( (x+ y) + 2.911\u03c0 )( (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 + 2\u00b72.911 2x \u03c0 ) .\n\u2022 Equalities: Multiplying out and expanding terms.\n\u2022 Last inequality > 0 is proofed in the following sequence of inequalities.\nWe look at the numerator of the last expression of Eq. (275), which we show to be positive in order to show > 0 in Eq. (275). The numerator is\n\u2212 8x3 + ( 4x2 + 2xy + 2.7795x\u2212 2y2 \u2212 0.9269y \u2212 0.00027798 )\u221a (2x+ y)2 + 5.39467x \u2212\n(276)\n8x2y \u2212 10.9554x2 + 2xy2 + 1.76901xy \u2212 0.00106244x+ 2y3 + 3.62336y2 \u2212 0.00053122y . The factor 4x2 + 2xy + 2.7795x\u2212 2y2 \u2212 0.9269y \u2212 0.00027798 in front of the root is positive:\n4x2 + 2xy + 2.7795x\u2212 2y2 \u2212 0.9269y \u2212 0.00027798 > (277) \u22122y2 + 0.007 \u00b7 2y \u2212 0.9269y + 4 \u00b7 0.0072 + 2.7795 \u00b7 0.007\u2212 0.00027798 =\n\u22122y2 \u2212 0.9129y + 2.77942 = \u22122(y + 1.42897)(y \u2212 0.972523) > 0 . If the term that does not contain the root would be positive, then everything is positive and we have proofed the the numerator is positive. Therefore we consider the case that the term that does not contain the root is negative. The term that contains the root must be larger than the other term in absolute values. \u2212 ( \u22128x3 \u2212 8x2y \u2212 10.9554x2 + 2xy2 + 1.76901xy \u2212 0.00106244x+ 2y3 + 3.62336y2 \u2212 0.00053122y ) < (278)( 4x2 + 2xy + 2.7795x\u2212 2y2 \u2212 0.9269y \u2212 0.00027798 )\u221a (2x+ y)2 + 5.39467x .\nTherefore the squares of the root term have to be larger than the square of the other term to show > 0 in Eq. (275). Thus, we have the inequality:( \u22128x3 \u2212 8x2y \u2212 10.9554x2 + 2xy2 + 1.76901xy \u2212 0.00106244x+ 2y3 + 3.62336y2 \u2212 0.00053122y )2 <\n(279)\n( 4x2 + 2xy + 2.7795x\u2212 2y2 \u2212 0.9269y \u2212 0.00027798 )2 ( (2x+ y)2 + 5.39467x ) .\nThis is equivalent to 0 < ( 4x2 + 2xy + 2.7795x\u2212 2y2 \u2212 0.9269y \u2212 0.00027798 )2 ( (2x+ y)2 + 5.39467x ) \u2212 (280)( \u22128x3 \u2212 8x2y \u2212 10.9554x2 + 2xy2 + 1.76901xy \u2212 0.00106244x+ 2y3 + 3.62336y2 \u2212 0.00053122y )2 =\nx \u00b7 4.168614250 \u00b7 10\u22127 \u2212 y22.049216091 \u00b7 10\u22127 \u2212 0.0279456x5+ 43.0875x4y + 30.8113x4 + 43.1084x3y2 + 68.989x3y + 41.6357x3 + 10.7928x2y3 \u2212 13.1726x2y2\u2212 27.8148x2y \u2212 0.00833715x2 + 0.0139728xy4 + 5.47537xy3+ 4.65089xy2 + 0.00277916xy \u2212 10.7858y5 \u2212 12.2664y4 + 0.00436492y3 .\nWe obtain the inequalities:\nx \u00b7 4.168614250 \u00b7 10\u22127 \u2212 y22.049216091 \u00b7 10\u22127 \u2212 0.0279456x5+ (281) 43.0875x4y + 30.8113x4 + 43.1084x3y2 + 68.989x3y + 41.6357x3 + 10.7928x2y3\u2212 13.1726x2y2 \u2212 27.8148x2y \u2212 0.00833715x2+ 0.0139728xy4 + 5.47537xy3 + 4.65089xy2 + 0.00277916xy \u2212 10.7858y5 \u2212 12.2664y4 + 0.00436492y3 > x \u00b7 4.168614250 \u00b7 10\u22127 \u2212 (0.01)22.049216091 \u00b7 10\u22127 \u2212 0.0279456x5+ 0.0 \u00b7 43.0875x4 + 30.8113x4 + 43.1084(0.0)2x3 + 0.0 \u00b7 68.989x3 + 41.6357x3+ 10.7928(0.0)3x2 \u2212 13.1726(0.01)2x2 \u2212 27.8148(0.01)x2 \u2212 0.00833715x2+ 0.0139728(0.0)4x+ 5.47537(0.0)3x+ 4.65089(0.0)2x+\n0.0 \u00b7 0.00277916x\u2212 10.7858(0.01)5 \u2212 12.2664(0.01)4 + 0.00436492(0.0)3 = x \u00b7 4.168614250 \u00b7 10\u22127 \u2212 1.237626189 \u00b7 10\u22127 \u2212 0.0279456x5 + 30.8113x4 + 41.6357x3 \u2212 0.287802x2 >\n\u2212 ( x\n0.007\n)3 1.237626189 \u00b7 10\u22127 + 30.8113x4 \u2212 (0.875) \u00b7 0.0279456x4 + 41.6357x3 \u2212 (0.287802x)x 2\n0.007 =\n30.7869x4 + 0.160295x3 > 0 .\nWe used x > 0.007 and x 6 0.875 (reducing the negative x4-term to a x3-term). We have proofed the last inequality > 0 of Eq. (275).\nConsequently the derivative is always positive independent of y, thus\ne (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( 2x+ y\u221a\n2 \u221a x\n) (282)\nis strictly monotonically increasing in x.\nNext we show that the sub-function Eq. (111) is smaller than zero. We consider the limit:\nlim x\u2192\u221e\ne (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( 2x+ y\u221a\n2 \u221a x\n) = 0 (283)\nThe limit follows from Lemma 22. Since the function is monotonic increasing in x, it has to approach 0 from below. Thus,\ne (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( 2x+ y\u221a\n2 \u221a x\n) (284)\nis smaller than zero.\nWe now consider the derivative of sub-function Eq. (111) with respect to y. We proofed that subfunction Eq. (111) is strictly monotonically increasing independent of y. In the proof of Theorem 3, we need the minimum of sub-function Eq. (111). First, we are interested in the derivative of subfunction Eq. (111) with respect to y for the minimum x = 0.007 = 7/1000.\nConsequently, we insert the minimum x = 0.007 = 7/1000 into the sub-function Eq. (111):\ne\n( y\n\u221a 2 \u221a\n7 1000\n+\n\u221a 7\n1000\u221a 2 )2 erfc  y\u221a 2 \u221a\n7 1000\n+\n\u221a 7\n1000\u221a 2 \u2212 (285) 2e ( y \u221a 2 \u221a 7 1000 + \u221a 2 \u221a 7 1000 )2 erfc\n y\u221a 2 \u221a\n7 1000\n+ \u221a 2\n\u221a 7\n1000  = e 500y2 7 +y+ 7 2000 erfc ( 1000y + 7\n20 \u221a 35\n) \u2212 2e (500y+7)2 3500 erfc ( 500y + 7\n10 \u221a 35\n) .\nThe derivative of this function with respect to y is( 1000y\n7 + 1\n) e 500y2 7 +y+ 7 2000 erfc ( 1000y + 7\n20 \u221a 35\n) \u2212 (286)\n1 7 4e (500y+7)2 3500 (500y + 7) erfc\n( 500y + 7\n10 \u221a 35\n) + 20 \u221a 5\n7\u03c0 >(\n1 + 1000 \u00b7 (\u22120.01)\n7\n) e\u22120.01+ 7 2000 + 500\u00b7(\u22120.01)2 7 erfc ( 7 + 1000 + (\u22120.01)\n20 \u221a 35\n) \u2212\n1 7 4e (7+500\u00b70.01)2 3500 (7 + 500 \u00b7 0.01) erfc\n( 7 + 500 \u00b7 0.01\n10 \u221a 35\n) + 20 \u221a 5\n7\u03c0 > 3.56 .\nFor the first inequality, we use Lemma 24. Lemma 24 says that the function xex 2\nerfc(x) has the sign of x and is monotonically increasing to 1\u221a\n\u03c0 . Consequently, we inserted the maximal y = 0.01 to\nmake the negative term more negative and the minimal y = \u22120.01 to make the positive term less positive.\nConsequently\ne (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( 2x+ y\u221a\n2 \u221a x\n) (287)\nis strictly monotonically increasing in y for the minimal x = 0.007.\nNext, we consider x = 0.7 \u00b7 0.8 = 0.56, which is the maximal \u03bd = 0.7 and minimal \u03c4 = 0.8. We insert the minimum x = 0.56 = 56/100 into the sub-function Eq. (111):\ne\n( y\n\u221a 2 \u221a\n56 100\n+ \u221a 56 100\u221a 2 )2 erfc  y\u221a 2 \u221a\n56 100\n+ \u221a 56 100\u221a 2 \u2212 (288) 2e ( y \u221a 2 \u221a 56 100 + \u221a 2 \u221a 56 100 )2 erfc\n y\u221a 2 \u221a\n56 100\n+ \u221a 2\n\u221a 56\n100  . The derivative with respect to y is:\n5e\n( 5y\n2 \u221a 7 + \u221a 7 5 )2 ( 5y\n2 \u221a 7 + \u221a 7 5\n) erfc ( 5y\n2 \u221a 7 + \u221a 7 5 ) \u221a\n7 \u2212 (289)\n10e\n( 5y\n2 \u221a 7 + 2 \u221a 7 5 )2 ( 5y\n2 \u221a 7 + 2 \u221a 7 5\n) erfc ( 5y\n2 \u221a 7 + 2 \u221a 7 5 ) \u221a\n7 + 5\u221a 7\u03c0 >\n5e\n(\u221a 7\n5 \u2212 0.01\u00b75 2 \u221a 7 )2 (\u221a 7\n5 \u2212 0.01\u00b75 2 \u221a 7\n) erfc (\u221a 7\n5 \u2212 0.01\u00b75 2 \u221a 7 ) \u221a\n7 \u2212\n10e\n( 2 \u221a\n7 5 + 0.01\u00b75 2 \u221a 7 )2 ( 2 \u221a\n7 5 + 0.01\u00b75 2 \u221a 7\n) erfc ( 2 \u221a\n7 5 + 0.01\u00b75 2 \u221a 7 ) \u221a\n7 + 5\u221a 7\u03c0 > 0.00746 .\nFor the first inequality we applied Lemma 24 which states that the function xex 2\nerfc(x) is monotonically increasing. Consequently, we inserted the maximal y = 0.01 to make the negative term more negative and the minimal y = \u22120.01 to make the positive term less positive. Consequently\ne (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( 2x+ y\u221a\n2 \u221a x\n) (290)\nis strictly monotonically increasing in y for x = 0.56.\nNext, we consider x = 0.16 \u00b7 0.8 = 0.128, which is the minimal \u03c4 = 0.8. We insert the minimum x = 0.128 = 128/1000 into the sub-function Eq. (111):\ne\n( y\n\u221a 2 \u221a\n128 1000\n+ \u221a 128 1000\u221a 2 )2 erfc  y\u221a 2 \u221a\n128 1000\n+ \u221a 128 1000\u221a 2 \u2212 (291) 2e ( y \u221a 2 \u221a 128 1000 + \u221a 2 \u221a 128 1000 )2 erfc\n y\u221a 2 \u221a\n128 1000\n+ \u221a 2\n\u221a 128\n1000  = e 125y2 32 +y+ 8 125 erfc ( 125y + 16\n20 \u221a 10\n) \u2212 2e (125y+32)2 4000 erfc ( 125y + 32\n20 \u221a 10\n) .\nThe derivative with respect to y is:\n1\n16\n( e 125y2 32 +y+ 8 125 (125y + 16) erfc ( 125y + 16\n20 \u221a 10\n) \u2212 (292)\n2e (125y+32)2 4000 (125y + 32) erfc\n( 125y + 32\n20 \u221a 10\n) + 20 \u221a 10\n\u03c0\n) >\n1\n16\n( (16 + 125(\u22120.01))e\u22120.01+ 8125 + 125(\u22120.01)2 32 erfc ( 16 + 125(\u22120.01)\n20 \u221a 10\n) \u2212\n2e (32+1250.01)2 4000 (32 + 1250.01) erfc\n( 32 + 1250.01\n20 \u221a 10\n) + 20 \u221a 10\n\u03c0\n) > 0.4468 .\nFor the first inequality we applied Lemma 24 which states that the function xex 2\nerfc(x) is monotonically increasing. Consequently, we inserted the maximal y = 0.01 to make the negative term more negative and the minimal y = \u22120.01 to make the positive term less positive. Consequently\ne (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( 2x+ y\u221a\n2 \u221a x\n) (293)\nis strictly monotonically increasing in y for x = 0.128.\nNext, we consider x = 0.24 \u00b7 0.9 = 0.216, which is the minimal \u03c4 = 0.9 (here we consider 0.9 as lower bound for \u03c4 ). We insert the minimum x = 0.216 = 216/1000 into the sub-function Eq. (111):\ne\n( y\n\u221a 2 \u221a\n216 1000\n+ \u221a 216 1000\u221a 2 )2 erfc  y\u221a 2 \u221a\n216 1000\n+ \u221a 216 1000\u221a 2 \u2212 (294) 2e ( y \u221a 2 \u221a 216 1000 + \u221a 2 \u221a 216 1000 )2 erfc\n y\u221a 2 \u221a\n216 1000\n+ \u221a 2\n\u221a 216\n1000\n =\ne (125y+27)2 6750 erfc\n( 125y + 27\n15 \u221a 30\n) \u2212 2e (125y+54)2 6750 erfc ( 125y + 54\n15 \u221a 30 ) The derivative with respect to y is:\n1\n27\n( e (125y+27)2 6750 (125y + 27) erfc ( 125y + 27\n15 \u221a 30\n) \u2212 (295)\n2e (125y+54)2 6750 (125y + 54) erfc\n( 125y + 54\n15 \u221a 30\n) + 15 \u221a 30\n\u03c0\n) >\n1\n27\n( (27 + 125(\u22120.01))e (27+125(\u22120.01))2 6750 erfc ( 27 + 125(\u22120.01)\n15 \u221a 30\n) \u2212\n2e (54+1250.01)2 6750 (54 + 1250.01) erfc\n( 54 + 1250.01\n15 \u221a 30\n) + 15 \u221a 30\n\u03c0\n) ) > 0.211288 .\nFor the first inequality we applied Lemma 24 which states that the function xex 2\nerfc(x) is monotonically increasing. Consequently, we inserted the maximal y = 0.01 to make the negative term more negative and the minimal y = \u22120.01 to make the positive term less positive. Consequently\ne (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( 2x+ y\u221a\n2 \u221a x\n) (296)\nis strictly monotonically increasing in y for x = 0.216.\nLemma 46 (Monotone Derivative). For \u03bb = \u03bb01, \u03b1 = \u03b101 and the domain \u22120.1 6 \u00b5 6 0.1, \u22120.1 6 \u03c9 6 0.1, 0.00875 6 \u03bd 6 0.7, and 0.8 6 \u03c4 6 1.25. We are interested of the derivative of\n\u03c4 ( e ( \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 2e ( \u00b5\u03c9+2\u00b7\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + 2 \u00b7 \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n)) . (297)\nThe derivative of the equation above with respect to\n\u2022 \u03bd is larger than zero;\n\u2022 \u03c4 is smaller than zero for maximal \u03bd = 0.7, \u03bd = 0.16, and \u03bd = 0.24 (with 0.9 6 \u03c4 );\n\u2022 y = \u00b5\u03c9 is larger than zero for \u03bd\u03c4 = 0.00875 \u00b7 0.8 = 0.007, \u03bd\u03c4 = 0.7 \u00b7 0.8 = 0.56, \u03bd\u03c4 = 0.16 \u00b7 0.8 = 0.128, and \u03bd\u03c4 = 0.24 \u00b7 0.9 = 0.216.\nProof. We consider the domain: \u22120.1 6 \u00b5 6 0.1, \u22120.1 6 \u03c9 6 0.1, 0.00875 6 \u03bd 6 0.7, and 0.8 6 \u03c4 6 1.25.\nWe use Lemma 17 to determine the derivatives. Consequently, the derivative of\n\u03c4 ( e ( \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 2e ( \u00b5\u03c9+2\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n)) (298)\nwith respect to \u03bd is larger than zero, which follows directly from Lemma 17 using the chain rule.\nConsequently, the derivative of\n\u03c4 ( e ( \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 2e ( \u00b5\u03c9+2\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n)) (299)\nwith respect to y = \u00b5\u03c9 is larger than zero for \u03bd\u03c4 = 0.00875 \u00b7 0.8 = 0.007, \u03bd\u03c4 = 0.7 \u00b7 0.8 = 0.56, \u03bd\u03c4 = 0.16 \u00b7 0.8 = 0.128, and \u03bd\u03c4 = 0.24 \u00b7 0.9 = 0.216, which also follows directly from Lemma 17. We now consider the derivative with respect to \u03c4 , which is not trivial since \u03c4 is a factor of the whole expression. The sub-expression should be maximized as it appears with negative sign in the mapping for \u03bd.\nFirst, we consider the function for the largest \u03bd = 0.7 and the largest y = \u00b5\u03c9 = 0.01 for determining the derivative with respect to \u03c4 .\nThe expression becomes\n\u03c4 e ( 7\u03c4 10 + 1 100 \u221a 2 \u221a 7\u03c4 10 )2 erfc  7\u03c410 + 1100\u221a 2 \u221a\n7\u03c4 10\n\u2212 2e ( 2\u00b77\u03c4 10 + 1 100 \u221a 2 \u221a 7\u03c4 10 )2 erfc  2\u00b77\u03c410 + 1100\u221a 2 \u221a\n7\u03c4 10\n  . (300)\nThe derivative with respect to \u03c4 is(\u221a \u03c0 ( e (70\u03c4+1)2 14000\u03c4 (700\u03c4(7\u03c4 + 20)\u2212 1) erfc ( 70\u03c4 + 1\n20 \u221a 35 \u221a \u03c4\n) \u2212 (301)\n2e (140\u03c4+1)2 14000\u03c4 (2800\u03c4(7\u03c4 + 5)\u2212 1) erfc ( 140\u03c4 + 1\n20 \u221a 35 \u221a \u03c4\n)) + 20 \u221a 35(210\u03c4 \u2212 1) \u221a \u03c4 ) ( 14000 \u221a \u03c0\u03c4 )\u22121 .\nWe are considering only the numerator and use again the approximation of Ren and MacKenzie [30]. The error analysis on the whole numerator gives an approximation error 97 < E < 186. Therefore we add 200 to the numerator when we use the approximation Ren and MacKenzie [30]. We obtain the inequalities:\n\u221a \u03c0 ( e (70\u03c4+1)2 14000\u03c4 (700\u03c4(7\u03c4 + 20)\u2212 1) erfc ( 70\u03c4 + 1\n20 \u221a 35 \u221a \u03c4\n) \u2212 (302)\n2e (140\u03c4+1)2 14000\u03c4 (2800\u03c4(7\u03c4 + 5)\u2212 1) erfc ( 140\u03c4 + 1\n20 \u221a 35 \u221a \u03c4\n)) + 20 \u221a 35(210\u03c4 \u2212 1) \u221a \u03c4 6\n\u221a \u03c0  2.911(700\u03c4(7\u03c4 + 20)\u2212 1)\u221a \u03c0(2.911\u22121)(70\u03c4+1)\n20 \u221a 35 \u221a \u03c4\n+ \u221a \u03c0 (\n70\u03c4+1 20 \u221a 35 \u221a \u03c4\n)2 + 2.9112 \u2212\n2 \u00b7 2.911(2800\u03c4(7\u03c4 + 5)\u2212 1) \u221a \u03c0(2.911\u22121)(140\u03c4+1)\n20 \u221a 35 \u221a \u03c4\n+ \u221a \u03c0 (\n140\u03c4+1 20 \u221a 35 \u221a \u03c4\n)2 + 2.9112  + 20 \u221a 35(210\u03c4 \u2212 1) \u221a \u03c4 + 200 =\n\u221a \u03c0  (700\u03c4(7\u03c4 + 20)\u2212 1) (20 \u00b7 \u221a35 \u00b7 2.911\u221a\u03c4) \u221a \u03c0(2.911\u2212 1)(70\u03c4 + 1) + \u221a( 20 \u00b7 2.911 \u221a 35 \u221a \u03c4 )2 + \u03c0(70\u03c4 + 1)2 \u2212\n2(2800\u03c4(7\u03c4 + 5)\u2212 1) ( 20 \u00b7 \u221a 35 \u00b7 2.911 \u221a \u03c4 )\n\u221a \u03c0(2.911\u2212 1)(140\u03c4 + 1) + \u221a( 20 \u00b7 \u221a 35 \u00b7 2.911 \u221a \u03c4 )2 + \u03c0(140\u03c4 + 1)2 + (\n20 \u221a 35(210\u03c4 \u2212 1) \u221a \u03c4 + 200 ) =((\n20 \u221a 35(210\u03c4 \u2212 1) \u221a \u03c4 + 200 )(\u221a \u03c0(2.911\u2212 1)(70\u03c4 + 1) + \u221a( 20 \u00b7 \u221a 35 \u00b7 2.911 \u221a \u03c4 )2 + \u03c0(70\u03c4 + 1)2 ) ( \u221a \u03c0(2.911\u2212 1)(140\u03c4 + 1) + \u221a( 20 \u00b7 \u221a 35 \u00b7 2.911 \u221a \u03c4 )2 + \u03c0(140\u03c4 + 1)2 ) +\n2.911 \u00b7 20 \u221a 35 \u221a \u03c0(700\u03c4(7\u03c4 + 20)\u2212 1) \u221a \u03c4(\n\u221a \u03c0(2.911\u2212 1)(140\u03c4 + 1) + \u221a( 20 \u00b7 \u221a 35 \u00b7 2.911 \u221a \u03c4 )2 + \u03c0(140\u03c4 + 1)2 ) \u2212\n\u221a \u03c02 \u00b7 20 \u00b7 \u221a 35 \u00b7 2.911(2800\u03c4(7\u03c4 + 5)\u2212 1)\n\u221a \u03c4 ( \u221a \u03c0(2.911\u2212 1)(70\u03c4 + 1) + \u221a( 20 \u00b7 \u221a 35 \u00b7 2.911 \u221a \u03c4 )2 + \u03c0(70\u03c4 + 1)2 )) (( \u221a \u03c0(2.911\u2212 1)(70\u03c4 + 1) + \u221a( 20 \u221a 35 \u00b7 2.911 \u00b7 \u221a \u03c4 )2 + \u03c0(70\u03c4 + 1)2\n) ( \u221a \u03c0(2.911\u2212 1)(140\u03c4 + 1) + \u221a( 20 \u221a 35 \u00b7 2.911 \u00b7 \u221a \u03c4 )2 + \u03c0(140\u03c4 + 1)2 ))\u22121 .\nAfter applying the approximation of Ren and MacKenzie [30] and adding 200, we first factored out 20 \u221a 35 \u221a \u03c4 . Then we brought all terms to the same denominator.\nWe now consider the numerator:( 20 \u221a 35(210\u03c4 \u2212 1) \u221a \u03c4 + 200 )(\u221a \u03c0(2.911\u2212 1)(70\u03c4 + 1) + \u221a( 20 \u00b7 \u221a 35 \u00b7 2.911 \u221a \u03c4 )2 + \u03c0(70\u03c4 + 1)2 ) (303)(\n\u221a \u03c0(2.911\u2212 1)(140\u03c4 + 1) + \u221a( 20 \u00b7 \u221a 35 \u00b7 2.911 \u221a \u03c4 )2 + \u03c0(140\u03c4 + 1)2 ) +\n2.911 \u00b7 20 \u221a 35 \u221a \u03c0(700\u03c4(7\u03c4 + 20)\u2212 1) \u221a \u03c4(\n\u221a \u03c0(2.911\u2212 1)(140\u03c4 + 1) + \u221a( 20 \u00b7 \u221a 35 \u00b7 2.911 \u221a \u03c4 )2 + \u03c0(140\u03c4 + 1)2 ) \u2212\n\u221a \u03c02 \u00b7 20 \u00b7 \u221a 35 \u00b7 2.911(2800\u03c4(7\u03c4 + 5)\u2212 1) \u221a \u03c4(\n\u221a \u03c0(2.911\u2212 1)(70\u03c4 + 1) + \u221a( 20 \u00b7 \u221a 35 \u00b7 2.911 \u221a \u03c4 )2 + \u03c0(70\u03c4 + 1)2 ) =\n\u2212 1.70658\u00d7 107 \u221a \u03c0(70\u03c4 + 1)2 + 118635\u03c4\u03c43/2+\n4200 \u221a 35 \u221a \u03c0(70\u03c4 + 1)2 + 118635\u03c4 \u221a \u03c0(140\u03c4 + 1)2 + 118635\u03c4\u03c43/2 +\n8.60302\u00d7 106 \u221a \u03c0(140\u03c4 + 1)2 + 118635\u03c4\u03c43/2 \u2212 2.89498\u00d7 107\u03c43/2 \u2212\n1.21486\u00d7 107 \u221a \u03c0(70\u03c4 + 1)2 + 118635\u03c4\u03c45/2 + 8.8828\u00d7 106 \u221a \u03c0(140\u03c4 + 1)2 + 118635\u03c4\u03c45/2 \u2212\n2.43651\u00d7 107\u03c45/2 \u2212 1.46191\u00d7 109\u03c47/2 + 2.24868\u00d7 107\u03c42 + 94840.5 \u221a \u03c0(70\u03c4 + 1)2 + 118635\u03c4\u03c4 +\n47420.2 \u221a \u03c0(140\u03c4 + 1)2 + 118635\u03c4\u03c4 + 481860\u03c4 + 710.354 \u221a \u03c4 +\n820.213 \u221a \u03c4 \u221a \u03c0(70\u03c4 + 1)2 + 118635\u03c4 + 677.432 \u221a \u03c0(70\u03c4 + 1)2 + 118635\u03c4 \u2212\n1011.27 \u221a \u03c4 \u221a \u03c0(140\u03c4 + 1)2 + 118635\u03c4 \u2212 20 \u221a 35 \u221a \u03c4 \u221a \u03c0(70\u03c4 + 1)2 + 118635\u03c4 \u221a \u03c0(140\u03c4 + 1)2 + 118635\u03c4 +\n200 \u221a \u03c0(70\u03c4 + 1)2 + 118635\u03c4 \u221a \u03c0(140\u03c4 + 1)2 + 118635\u03c4 +\n677.432 \u221a \u03c0(140\u03c4 + 1)2 + 118635\u03c4 + 2294.57 =\n\u2212 2.89498\u00d7 107\u03c43/2 \u2212 2.43651\u00d7 107\u03c45/2 \u2212 1.46191\u00d7 109\u03c47/2 +( \u22121.70658\u00d7 107\u03c43/2 \u2212 1.21486\u00d7 107\u03c45/2 + 94840.5\u03c4 + 820.213 \u221a \u03c4 + 677.432 ) \u221a \u03c0(70\u03c4 + 1)2 + 118635\u03c4 +( 8.60302\u00d7 106\u03c43/2 + 8.8828\u00d7 106\u03c45/2 + 47420.2\u03c4 \u2212 1011.27 \u221a \u03c4 + 677.432\n) \u221a \u03c0(140\u03c4 + 1)2 + 118635\u03c4 +( 4200 \u221a 35\u03c43/2 \u2212 20 \u221a 35 \u221a \u03c4 + 200 )\u221a \u03c0(70\u03c4 + 1)2 + 118635\u03c4 \u221a \u03c0(140\u03c4 + 1)2 + 118635\u03c4 +\n2.24868\u00d7 107\u03c42 + 481860.\u03c4 + 710.354 \u221a \u03c4 + 2294.57 6\n\u2212 2.89498\u00d7 107\u03c43/2 \u2212 2.43651\u00d7 107\u03c45/2 \u2212 1.46191\u00d7 109\u03c47/2+( \u22121.70658\u00d7 107\u03c43/2 \u2212 1.21486\u00d7 107\u03c45/2 + 820.213 \u221a 1.25 + 1.25 \u00b7 94840.5 + 677.432 ) \u221a \u03c0(70\u03c4 + 1)2 + 118635\u03c4+( 8.60302\u00d7 106\u03c43/2 + 8.8828\u00d7 106\u03c45/2 \u2212 1011.27 \u221a 0.8 + 1.25 \u00b7 47420.2 + 677.432\n) \u221a \u03c0(140\u03c4 + 1)2 + 118635\u03c4+( 4200 \u221a 35\u03c43/2 \u2212 20 \u221a 35 \u221a \u03c4 + 200\n) \u221a \u03c0(70\u03c4 + 1)2 + 118635\u03c4 \u221a \u03c0(140\u03c4 + 1)2 + 118635\u03c4+ 2.24868\u00d7 107\u03c42 + 710.354 \u221a\n1.25 + 1.25 \u00b7 481860 + 2294.57 = \u2212 2.89498\u00d7 107\u03c43/2 \u2212 2.43651\u00d7 107\u03c45/2 \u2212 1.46191\u00d7 109\u03c47/2+( \u22121.70658\u00d7 107\u03c43/2 \u2212 1.21486\u00d7 107\u03c45/2 + 120145.\n)\u221a \u03c0(70\u03c4 + 1)2 + 118635\u03c4+(\n8.60302\u00d7 106\u03c43/2 + 8.8828\u00d7 106\u03c45/2 + 59048.2 )\u221a\n\u03c0(140\u03c4 + 1)2 + 118635\u03c4+( 4200 \u221a 35\u03c43/2 \u2212 20 \u221a 35 \u221a \u03c4 + 200 )\u221a \u03c0(70\u03c4 + 1)2 + 118635\u03c4 \u221a \u03c0(140\u03c4 + 1)2 + 118635\u03c4+\n2.24868\u00d7 107\u03c42 + 605413 = \u2212 2.89498\u00d7 107\u03c43/2 \u2212 2.43651\u00d7 107\u03c45/2 \u2212 1.46191\u00d7 109\u03c47/2+( 8.60302\u00d7 106\u03c43/2 + 8.8828\u00d7 106\u03c45/2 + 59048.2\n)\u221a 19600\u03c0(\u03c4 + 1.94093)(\u03c4 + 0.0000262866)+(\n\u22121.70658\u00d7 107\u03c43/2 \u2212 1.21486\u00d7 107\u03c45/2 + 120145. )\u221a\n4900\u03c0(\u03c4 + 7.73521)(\u03c4 + 0.0000263835)+( 4200 \u221a 35\u03c43/2 \u2212 20 \u221a 35 \u221a \u03c4 + 200 ) \u221a\n19600\u03c0(\u03c4 + 1.94093)(\u03c4 + 0.0000262866) \u221a 4900\u03c0(\u03c4 + 7.73521)(\u03c4 + 0.0000263835)+\n2.24868\u00d7 107\u03c42 + 605413 6 \u2212 2.89498\u00d7 107\u03c43/2 \u2212 2.43651\u00d7 107\u03c45/2 \u2212 1.46191\u00d7 109\u03c47/2+( 8.60302\u00d7 106\u03c43/2 + 8.8828\u00d7 106\u03c45/2 + 59048.2\n)\u221a 19600\u03c0(\u03c4 + 1.94093)\u03c4+(\n\u22121.70658\u00d7 107\u03c43/2 \u2212 1.21486\u00d7 107\u03c45/2 + 120145. )\u221a\n4900\u03c01.00003(\u03c4 + 7.73521)\u03c4+( 4200 \u221a 35\u03c43/2 \u2212 20 \u221a 35 \u221a \u03c4 + 200 )\u221a 19600\u03c01.00003(\u03c4 + 1.94093)\u03c4\u221a\n4900\u03c01.00003(\u03c4 + 7.73521)\u03c4+\n2.24868\u00d7 107\u03c42 + 605413 = \u2212 2.89498\u00d7 107\u03c43/2 \u2212 2.43651\u00d7 107\u03c45/2 \u2212 1.46191\u00d7 109\u03c47/2+( \u22123.64296\u00d7 106\u03c43/2 + 7.65021\u00d7 108\u03c45/2 + 6.15772\u00d7 106\u03c4\n) \u221a \u03c4 + 1.94093\n\u221a \u03c4 + 7.73521 + 2.24868\u00d7 107\u03c42+(\n2.20425\u00d7 109\u03c43 + 2.13482\u00d7 109\u03c42 + 1.46527\u00d7 107 \u221a \u03c4 )\u221a\n\u03c4 + 1.94093+( \u22121.5073\u00d7 109\u03c43 \u2212 2.11738\u00d7 109\u03c42 + 1.49066\u00d7 107 \u221a \u03c4 )\u221a\n\u03c4 + 7.73521 + 605413 6 \u221a\n1.25 + 1.94093 \u221a 1.25 + 7.73521 ( \u22123.64296\u00d7 106\u03c43/2 + 7.65021\u00d7 108\u03c45/2 + 6.15772\u00d7 106\u03c4 ) +\n\u221a 1.25 + 1.94093 ( 2.20425\u00d7 109\u03c43 + 2.13482\u00d7 109\u03c42 + 1.46527\u00d7 107 \u221a \u03c4 )\n+ \u221a 0.8 + 7.73521 ( \u22121.5073\u00d7 109\u03c43 \u2212 2.11738\u00d7 109\u03c42 + 1.49066\u00d7 107 \u221a \u03c4 ) \u2212\n2.89498\u00d7 107\u03c43/2 \u2212 2.43651\u00d7 107\u03c45/2 \u2212 1.46191\u00d7 109\u03c47/2 + 2.24868\u00d7 107\u03c42 + 605413 = \u2212 4.84561\u00d7 107\u03c43/2 + 4.07198\u00d7 109\u03c45/2 \u2212 1.46191\u00d7 109\u03c47/2\u2212 4.66103\u00d7 108\u03c43 \u2212 2.34999\u00d7 109\u03c42+ 3.29718\u00d7 107\u03c4 + 6.97241\u00d7 107 \u221a \u03c4 + 605413 6\n605413\u03c43/2\n0.83/2 \u2212 4.84561\u00d7 107\u03c43/2+\n4.07198\u00d7 109\u03c45/2 \u2212 1.46191\u00d7 109\u03c47/2\u2212 4.66103\u00d7 108\u03c43 \u2212 2.34999\u00d7 109\u03c42 + 3.29718\u00d7 10 7 \u221a \u03c4\u03c4\u221a\n0.8 +\n6.97241\u00d7 107\u03c4 \u221a \u03c4\n0.8 = \u03c43/2 ( \u22124.66103\u00d7 108\u03c43/2 \u2212 1.46191\u00d7 109\u03c42 \u2212 2.34999\u00d7 109 \u221a \u03c4+\n4.07198\u00d7 109\u03c4 + 7.64087\u00d7 107 ) 6\n\u03c43/2 ( \u22124.66103\u00d7 108\u03c43/2 \u2212 1.46191\u00d7 109\u03c42 + 7.64087\u00d7 10 7 \u221a \u03c4\u221a\n0.8 \u2212\n2.34999\u00d7 109 \u221a \u03c4 + 4.07198\u00d7 109\u03c4 ) =\n\u03c42 ( \u22121.46191\u00d7 109\u03c43/2 + 4.07198\u00d7 109 \u221a \u03c4 \u2212 4.66103\u00d7 108\u03c4 \u2212 2.26457\u00d7 109 ) 6(\n\u22122.26457\u00d7 109 + 4.07198\u00d7 109 \u221a 0.8\u2212 4.66103\u00d7 1080.8\u2212 1.46191\u00d7 1090.83/2 ) \u03c42 =\n\u2212 4.14199\u00d7 107\u03c42 < 0 .\nFirst we expanded the term (multiplied it out). The we put the terms multiplied by the same square root into brackets. The next inequality sign stems from inserting the maximal value of 1.25 for \u03c4 for some positive terms and value of 0.8 for negative terms. These terms are then expanded at the =-sign. The next equality factors the terms under the squared root. We decreased the negative term by setting \u03c4 = \u03c4 + 0.0000263835 under the root. We increased positive terms by setting \u03c4 + 0.000026286 = 1.00003\u03c4 and \u03c4 + 0.000026383 = 1.00003\u03c4 under the root for positive terms. The positive terms are increase, since 0.8+0.0000263830.8 = 1.00003, thus \u03c4 + 0.000026286 < \u03c4 + 0.000026383 6 1.00003\u03c4 . For the next inequality we decreased negative terms by inserting \u03c4 = 0.8 and increased positive terms by inserting \u03c4 = 1.25. The next equality expands the terms. We use upper bound of 1.25 and lower bound of 0.8 to obtain terms with corresponding exponents of \u03c4 .\nFor the last 6-sign we used the function\n\u22121.46191\u00d7 109\u03c43/2 + 4.07198\u00d7 109 \u221a \u03c4 \u2212 4.66103\u00d7 108\u03c4 \u2212 2.26457\u00d7 109 (304)\nThe derivative of this function is\n\u22122.19286\u00d7 109 \u221a \u03c4 + 2.03599\u00d7 109\u221a \u03c4 \u2212 4.66103\u00d7 108 (305)\nand the second order derivative is\n\u22121.01799\u00d7 10 9 \u03c43/2 \u2212 1.09643\u00d7 10 9 \u221a \u03c4 < 0 . (306)\nThe derivative at 0.8 is smaller than zero:\n\u2212 2.19286\u00d7 109 \u221a 0.8\u2212 4.66103\u00d7 108 + 2.03599\u00d7 10 9\n\u221a 0.8\n= (307)\n\u2212 1.51154\u00d7 108 < 0 .\nSince the second order derivative is negative, the derivative decreases with increasing \u03c4 . Therefore the derivative is negative for all values of \u03c4 that we consider, that is, the function Eq. (304) is strictly monotonically decreasing. The maximum of the function Eq. (304) is therefore at 0.8. We inserted 0.8 to obtain the maximum.\nConsequently, the derivative of\n\u03c4 ( e ( \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 2e ( \u00b5\u03c9+2\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n)) (308)\nwith respect to \u03c4 is smaller than zero for maximal \u03bd = 0.7.\nNext, we consider the function for the largest \u03bd = 0.16 and the largest y = \u00b5\u03c9 = 0.01 for determining the derivative with respect to \u03c4 .\nThe expression becomes\n\u03c4 e ( 16\u03c4 100 + 1 100 \u221a 2 \u221a 16\u03c4 100 )2 erfc  16\u03c4100 + 1100\u221a 2 \u221a\n16\u03c4 100\n\u2212 e ( 2 16\u03c4 100 + 1 100 \u221a 2 \u221a 16\u03c4 100 )2 erfc  2 16\u03c4100 + 1100\u221a 2 \u221a\n16\u03c4 100\n  . (309)\nThe derivative with respect to \u03c4 is(\u221a \u03c0 ( e (16\u03c4+1)2 3200\u03c4 (128\u03c4(2\u03c4 + 25)\u2212 1) erfc ( 16\u03c4 + 1\n40 \u221a 2 \u221a \u03c4\n) \u2212 (310)\n2e (32\u03c4+1)2 3200\u03c4 (128\u03c4(8\u03c4 + 25)\u2212 1) erfc ( 32\u03c4 + 1\n40 \u221a 2 \u221a \u03c4\n)) + 40 \u221a 2(48\u03c4 \u2212 1) \u221a \u03c4 ) ( 3200 \u221a \u03c0\u03c4 )\u22121 .\nWe are considering only the numerator and use again the approximation of Ren and MacKenzie [30]. The error analysis on the whole numerator gives an approximation error 1.1 < E < 12. Therefore we add 20 to the numerator when we use the approximation of Ren and MacKenzie [30]. We obtain the inequalities:\n\u221a \u03c0 ( e (16\u03c4+1)2 3200\u03c4 (128\u03c4(2\u03c4 + 25)\u2212 1) erfc ( 16\u03c4 + 1\n40 \u221a 2 \u221a \u03c4\n) \u2212 (311)\n2e (32\u03c4+1)2 3200\u03c4 (128\u03c4(8\u03c4 + 25)\u2212 1) erfc ( 32\u03c4 + 1\n40 \u221a 2 \u221a \u03c4\n)) + 40 \u221a 2(48\u03c4 \u2212 1) \u221a \u03c4 6\n\u221a \u03c0  2.911(128\u03c4(2\u03c4 + 25)\u2212 1)\u221a \u03c0(2.911\u22121)(16\u03c4+1)\n40 \u221a 2 \u221a \u03c4\n+ \u221a \u03c0 (\n16\u03c4+1 40 \u221a 2 \u221a \u03c4\n)2 + 2.9112 \u2212\n2 \u00b7 2.911(128\u03c4(8\u03c4 + 25)\u2212 1) \u221a \u03c0(2.911\u22121)(32\u03c4+1)\n40 \u221a 2 \u221a \u03c4\n+ \u221a \u03c0 (\n32\u03c4+1 40 \u221a 2 \u221a \u03c4\n)2 + 2.9112  + 40 \u221a 2(48\u03c4 \u2212 1) \u221a \u03c4 + 20 =\n\u221a \u03c0  (128\u03c4(2\u03c4 + 25)\u2212 1) (40\u221a22.911\u221a\u03c4) \u221a \u03c0(2.911\u2212 1)(16\u03c4 + 1) + \u221a( 40 \u221a 22.911 \u221a \u03c4 )2 + \u03c0(16\u03c4 + 1)2 \u2212\n2(128\u03c4(8\u03c4 + 25)\u2212 1) ( 40 \u221a 22.911 \u221a \u03c4 )\n\u221a \u03c0(2.911\u2212 1)(32\u03c4 + 1) + \u221a( 40 \u221a 22.911 \u221a \u03c4 )2 + \u03c0(32\u03c4 + 1)2 + 40 \u221a\n2(48\u03c4 \u2212 1) \u221a \u03c4 + 20 =((\n40 \u221a 2(48\u03c4 \u2212 1) \u221a \u03c4 + 20 )(\u221a \u03c0(2.911\u2212 1)(16\u03c4 + 1) + \u221a( 40 \u221a 22.911 \u221a \u03c4 )2 + \u03c0(16\u03c4 + 1)2 ) ( \u221a \u03c0(2.911\u2212 1)(32\u03c4 + 1) + \u221a( 40 \u221a 22.911 \u221a \u03c4 )2 + \u03c0(32\u03c4 + 1)2 ) + +\n2.911 \u00b7 40 \u221a 2 \u221a \u03c0(128\u03c4(2\u03c4 + 25)\u2212 1) \u221a \u03c4(\n\u221a \u03c0(2.911\u2212 1)(32\u03c4 + 1) + \u221a( 40 \u221a 22.911 \u221a \u03c4 )2 + \u03c0(32\u03c4 + 1)2 ) \u2212\n2 \u221a \u03c040 \u221a 22.911(128\u03c4(8\u03c4 + 25)\u2212 1)\n\u221a \u03c4 ( \u221a \u03c0(2.911\u2212 1)(16\u03c4 + 1) + \u221a( 40 \u221a 22.911 \u221a \u03c4 )2 + \u03c0(16\u03c4 + 1)2 )) (( \u221a \u03c0(2.911\u2212 1)(32\u03c4 + 1) + \u221a( 40 \u221a 22.911 \u221a \u03c4 )2 + \u03c0(32\u03c4 + 1)2\n) ( \u221a \u03c0(2.911\u2212 1)(32\u03c4 + 1) + \u221a( 40 \u221a 22.911 \u221a \u03c4 )2 + \u03c0(32\u03c4 + 1)2 ))\u22121 .\nAfter applying the approximation of Ren and MacKenzie [30] and adding 20, we first factored out 40 \u221a 2 \u221a \u03c4 . Then we brought all terms to the same denominator.\nWe now consider the numerator:( 40 \u221a 2(48\u03c4 \u2212 1) \u221a \u03c4 + 20 )(\u221a \u03c0(2.911\u2212 1)(16\u03c4 + 1) + \u221a( 40 \u221a 22.911 \u221a \u03c4 )2 + \u03c0(16\u03c4 + 1)2 ) (312)(\n\u221a \u03c0(2.911\u2212 1)(32\u03c4 + 1) + \u221a( 40 \u221a 22.911 \u221a \u03c4 )2 + \u03c0(32\u03c4 + 1)2 ) +\n2.911 \u00b7 40 \u221a 2 \u221a \u03c0(128\u03c4(2\u03c4 + 25)\u2212 1) \u221a \u03c4(\n\u221a \u03c0(2.911\u2212 1)(32\u03c4 + 1) + \u221a( 40 \u221a 22.911 \u221a \u03c4 )2 + \u03c0(32\u03c4 + 1)2 ) \u2212\n2 \u221a \u03c040 \u221a 22.911(128\u03c4(8\u03c4 + 25)\u2212 1) \u221a \u03c4(\n\u221a \u03c0(2.911\u2212 1)(16\u03c4 + 1) + \u221a( 40 \u221a 22.911 \u221a \u03c4 )2 + \u03c0(16\u03c4 + 1)2 ) =\n\u2212 1.86491\u00d7 106 \u221a \u03c0(16\u03c4 + 1)2 + 27116.5\u03c4\u03c43/2+\n1920 \u221a 2 \u221a \u03c0(16\u03c4 + 1)2 + 27116.5\u03c4 \u221a \u03c0(32\u03c4 + 1)2 + 27116.5\u03c4\u03c43/2+\n940121 \u221a \u03c0(32\u03c4 + 1)2 + 27116.5\u03c4\u03c43/2 \u2212 3.16357\u00d7 106\u03c43/2\u2212\n303446 \u221a \u03c0(16\u03c4 + 1)2 + 27116.5\u03c4\u03c45/2 + 221873 \u221a \u03c0(32\u03c4 + 1)2 + 27116.5\u03c4\u03c45/2 \u2212 608588\u03c45/2\u2212\n8.34635\u00d7 106\u03c47/2 + 117482.\u03c42 + 2167.78 \u221a \u03c0(16\u03c4 + 1)2 + 27116.5\u03c4\u03c4+\n1083.89 \u221a \u03c0(32\u03c4 + 1)2 + 27116.5\u03c4\u03c4+ 11013.9\u03c4 + 339.614 \u221a \u03c4 + 392.137 \u221a \u03c4 \u221a \u03c0(16\u03c4 + 1)2 + 27116.5\u03c4+\n67.7432 \u221a \u03c0(16\u03c4 + 1)2 + 27116.5\u03c4 \u2212 483.478 \u221a \u03c4 \u221a \u03c0(32\u03c4 + 1)2 + 27116.5\u03c4\u2212 40 \u221a 2 \u221a \u03c4 \u221a \u03c0(16\u03c4 + 1)2 + 27116.5\u03c4 \u221a \u03c0(32\u03c4 + 1)2 + 27116.5\u03c4+\n20 \u221a \u03c0(16\u03c4 + 1)2 + 27116.5\u03c4 \u221a \u03c0(32\u03c4 + 1)2 + 27116.5\u03c4+\n67.7432 \u221a \u03c0(32\u03c4 + 1)2 + 27116.5\u03c4 + 229.457 =\n\u2212 3.16357\u00d7 106\u03c43/2 \u2212 608588\u03c45/2 \u2212 8.34635\u00d7 106\u03c47/2+( \u22121.86491\u00d7 106\u03c43/2 \u2212 303446\u03c45/2 + 2167.78\u03c4 + 392.137 \u221a \u03c4 + 67.7432 ) \u221a \u03c0(16\u03c4 + 1)2 + 27116.5\u03c4+( 940121\u03c43/2 + 221873\u03c45/2 + 1083.89\u03c4 \u2212 483.478 \u221a \u03c4 + 67.7432 )\n\u221a \u03c0(32\u03c4 + 1)2 + 27116.5\u03c4+( 1920 \u221a 2\u03c43/2 \u2212 40 \u221a 2 \u221a \u03c4 + 20 )\u221a \u03c0(16\u03c4 + 1)2 + 27116.5\u03c4 \u221a \u03c0(32\u03c4 + 1)2 + 27116.5\u03c4+ 117482.\u03c42 + 11013.9\u03c4 + 339.614 \u221a \u03c4 + 229.457 6\n\u2212 3.16357\u00d7 106\u03c43/2 \u2212 608588\u03c45/2 \u2212 8.34635\u00d7 106\u03c47/2+( \u22121.86491\u00d7 106\u03c43/2 \u2212 303446\u03c45/2 + 392.137 \u221a 1.25 + 1.252167.78 + 67.7432 ) \u221a \u03c0(16\u03c4 + 1)2 + 27116.5\u03c4+( 940121\u03c43/2 + 221873\u03c45/2 \u2212 483.478 \u221a 0.8 + 1.251083.89 + 67.7432\n) \u221a \u03c0(32\u03c4 + 1)2 + 27116.5\u03c4+( 1920 \u221a 2\u03c43/2 \u2212 40 \u221a 2 \u221a \u03c4 + 20 )\u221a \u03c0(16\u03c4 + 1)2 + 27116.5\u03c4 \u221a \u03c0(32\u03c4 + 1)2 + 27116.5\u03c4+ 117482.\u03c42 + 339.614 \u221a 1.25 + 1.2511013.9 + 229.457 =\n\u2212 3.16357\u00d7 106\u03c43/2 \u2212 608588\u03c45/2 \u2212 8.34635\u00d7 106\u03c47/2+( \u22121.86491\u00d7 106\u03c43/2 \u2212 303446\u03c45/2 + 3215.89 )\u221a \u03c0(16\u03c4 + 1)2 + 27116.5\u03c4+(\n940121\u03c43/2 + 221873\u03c45/2 + 990.171 )\u221a\n\u03c0(32\u03c4 + 1)2 + 27116.5\u03c4+( 1920 \u221a 2\u03c43/2 \u2212 40 \u221a 2 \u221a \u03c4 + 20 )\u221a \u03c0(16\u03c4 + 1)2 + 27116.5\u03c4 \u221a \u03c0(32\u03c4 + 1)2 + 27116.5\u03c4+\n117482\u03c42 + 14376.6 =\n\u2212 3.16357\u00d7 106\u03c43/2 \u2212 608588\u03c45/2 \u2212 8.34635\u00d7 106\u03c47/2+( 940121\u03c43/2 + 221873\u03c45/2 + 990.171 )\u221a 1024\u03c0(\u03c4 + 8.49155)(\u03c4 + 0.000115004)+(\n\u22121.86491\u00d7 106\u03c43/2 \u2212 303446\u03c45/2 + 3215.89 )\u221a\n256\u03c0(\u03c4 + 33.8415)(\u03c4 + 0.000115428)+( 1920 \u221a 2\u03c43/2 \u2212 40 \u221a 2 \u221a \u03c4 + 20 )\u221a 1024\u03c0(\u03c4 + 8.49155)(\u03c4 + 0.000115004)\u221a\n256\u03c0(\u03c4 + 33.8415)(\u03c4 + 0.000115428)+\n117482.\u03c42 + 14376.6 6\n\u2212 3.16357\u00d7 106\u03c43/2 \u2212 608588\u03c45/2 \u2212 8.34635\u00d7 106\u03c47/2+( 940121\u03c43/2 + 221873\u03c45/2 + 990.171 )\u221a 1024\u03c01.00014(\u03c4 + 8.49155)\u03c4+(\n1920 \u221a 2\u03c43/2 \u2212 40 \u221a 2 \u221a \u03c4 + 20 )\u221a 256\u03c01.00014(\u03c4 + 33.8415)\u03c4 \u221a 1024\u03c01.00014(\u03c4 + 8.49155)\u03c4+(\n\u22121.86491\u00d7 106\u03c43/2 \u2212 303446\u03c45/2 + 3215.89 )\u221a 256\u03c0(\u03c4 + 33.8415)\u03c4+\n117482.\u03c42 + 14376.6 =\n\u2212 3.16357\u00d7 106\u03c43/2 \u2212 608588\u03c45/2 \u2212 8.34635\u00d7 106\u03c47/2+( \u221291003\u03c43/2 + 4.36814\u00d7 106\u03c45/2 + 32174.4\u03c4 )\u221a \u03c4 + 8.49155\n\u221a \u03c4 + 33.8415 + 117482.\u03c42+(\n1.25852\u00d7 107\u03c43 + 5.33261\u00d7 107\u03c42 + 56165.1 \u221a \u03c4 )\u221a\n\u03c4 + 8.49155+( \u22128.60549\u00d7 106\u03c43 \u2212 5.28876\u00d7 107\u03c42 + 91200.4 \u221a \u03c4 )\u221a\n\u03c4 + 33.8415 + 14376.6 6 \u221a\n1.25 + 8.49155 \u221a 1.25 + 33.8415 ( \u221291003\u03c43/2 + 4.36814\u00d7 106\u03c45/2 + 32174.4\u03c4 ) +\n\u221a 1.25 + 8.49155 ( 1.25852\u00d7 107\u03c43 + 5.33261\u00d7 107\u03c42 + 56165.1 \u221a \u03c4 )\n+ \u221a 0.8 + 33.8415 ( \u22128.60549\u00d7 106\u03c43 \u2212 5.28876\u00d7 107\u03c42 + 91200.4 \u221a \u03c4 ) \u2212\n3.16357\u00d7 106\u03c43/2 \u2212 608588\u03c45/2 \u2212 8.34635\u00d7 106\u03c47/2 + 117482.\u03c42 + 14376.6 =\n\u2212 4.84613\u00d7 106\u03c43/2 + 8.01543\u00d7 107\u03c45/2 \u2212 8.34635\u00d7 106\u03c47/2\u2212 1.13691\u00d7 107\u03c43 \u2212 1.44725\u00d7 108\u03c42+ 594875.\u03c4 + 712078. \u221a \u03c4 + 14376.6 6\n14376.6\u03c43/2\n0.83/2 \u2212 4.84613\u00d7 106\u03c43/2+\n8.01543\u00d7 107\u03c45/2 \u2212 8.34635\u00d7 106\u03c47/2\u2212 1.13691\u00d7 107\u03c43 \u2212 1.44725\u00d7 108\u03c42 + 594875. \u221a \u03c4\u03c4\u221a\n0.8 +\n712078.\u03c4 \u221a \u03c4\n0.8 =\n\u2212 3.1311 \u00b7 106\u03c43/2 \u2212 1.44725 \u00b7 108\u03c42 + 8.01543 \u00b7 107\u03c45/2 \u2212 1.13691 \u00b7 107\u03c43\u2212 8.34635 \u00b7 106\u03c47/2 6\n\u2212 3.1311\u00d7 106\u03c43/2 + 8.01543\u00d7 10 7 \u221a\n1.25\u03c45/2\u221a \u03c4 \u2212\n8.34635\u00d7 106\u03c47/2 \u2212 1.13691\u00d7 107\u03c43 \u2212 1.44725\u00d7 108\u03c42 = \u2212 3.1311\u00d7 106\u03c43/2 \u2212 8.34635\u00d7 106\u03c47/2 \u2212 1.13691\u00d7 107\u03c43 \u2212 5.51094\u00d7 107\u03c422 < 0 .\nFirst we expanded the term (multiplied it out). The we put the terms multiplied by the same square root into brackets. The next inequality sign stems from inserting the maximal value of 1.25 for \u03c4 for some positive terms and value of 0.8 for negative terms. These terms are then expanded at the =-sign. The next equality factors the terms under the squared root. We decreased the negative term by setting \u03c4 = \u03c4 + 0.00011542 under the root. We increased positive terms by setting \u03c4 + 0.00011542 = 1.00014\u03c4 and \u03c4 + 0.000115004 = 1.00014\u03c4 under the root for positive terms. The positive terms are increase, since 0.8+0.000115420.8 < 1.000142, thus \u03c4 + 0.000115004 < \u03c4 + 0.00011542 6 1.00014\u03c4 . For the next inequality we decreased negative terms by inserting \u03c4 = 0.8 and increased positive terms by inserting \u03c4 = 1.25. The next equality expands the terms. We use upper bound of 1.25 and lower bound of 0.8 to obtain terms with corresponding exponents of \u03c4 .\nConsequently, the derivative of\n\u03c4 ( e ( \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 2e ( \u00b5\u03c9+2\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n)) (313)\nwith respect to \u03c4 is smaller than zero for maximal \u03bd = 0.16.\nNext, we consider the function for the largest \u03bd = 0.24 and the largest y = \u00b5\u03c9 = 0.01 for determining the derivative with respect to \u03c4 . However we assume 0.9 6 \u03c4 , in order to restrict the domain of \u03c4 .\nThe expression becomes\n\u03c4 e ( 24\u03c4 100 + 1 100 \u221a 2 \u221a 24\u03c4 100 )2 erfc  24\u03c4100 + 1100\u221a 2 \u221a\n24\u03c4 100\n\u2212 e ( 2 24\u03c4 100 + 1 100 \u221a 2 \u221a 24\u03c4 100 )2 erfc  2 24\u03c4100 + 1100\u221a 2 \u221a\n24\u03c4 100\n  . (314)\nThe derivative with respect to \u03c4 is(\u221a \u03c0 ( e (24\u03c4+1)2 4800\u03c4 (192\u03c4(3\u03c4 + 25)\u2212 1) erfc ( 24\u03c4 + 1\n40 \u221a 3 \u221a \u03c4\n) \u2212 (315)\n2e (48\u03c4+1)2 4800\u03c4 (192\u03c4(12\u03c4 + 25)\u2212 1) erfc ( 48\u03c4 + 1\n40 \u221a 3 \u221a \u03c4\n)) + 40 \u221a 3(72\u03c4 \u2212 1) \u221a \u03c4 ) ( 4800 \u221a \u03c0\u03c4 )\u22121 .\nWe are considering only the numerator and use again the approximation of Ren and MacKenzie [30]. The error analysis on the whole numerator gives an approximation error 14 < E < 32. Therefore we add 32 to the numerator when we use the approximation of Ren and MacKenzie [30]. We obtain the inequalities: \u221a \u03c0 ( e (24\u03c4+1)2 4800\u03c4 (192\u03c4(3\u03c4 + 25)\u2212 1) erfc ( 24\u03c4 + 1\n40 \u221a 3 \u221a \u03c4\n) \u2212 (316)\n2e (48\u03c4+1)2 4800\u03c4 (192\u03c4(12\u03c4 + 25)\u2212 1) erfc ( 48\u03c4 + 1\n40 \u221a 3 \u221a \u03c4\n)) + 40 \u221a 3(72\u03c4 \u2212 1) \u221a \u03c4 6\n\u221a \u03c0  2.911(192\u03c4(3\u03c4 + 25)\u2212 1)\u221a \u03c0(2.911\u22121)(24\u03c4+1)\n40 \u221a 3 \u221a \u03c4\n+ \u221a \u03c0 (\n24\u03c4+1 40 \u221a 3 \u221a \u03c4\n)2 + 2.9112 \u2212\n2 \u00b7 2.911(192\u03c4(12\u03c4 + 25)\u2212 1) \u221a \u03c0(2.911\u22121)(48\u03c4+1)\n40 \u221a 3 \u221a \u03c4\n+ \u221a \u03c0 (\n48\u03c4+1 40 \u221a 3 \u221a \u03c4\n)2 + 2.9112 + 40 \u221a 3(72\u03c4 \u2212 1) \u221a \u03c4 + 32 =\n\u221a \u03c0  (192\u03c4(3\u03c4 + 25)\u2212 1) (40\u221a32.911\u221a\u03c4) \u221a \u03c0(2.911\u2212 1)(24\u03c4 + 1) + \u221a( 40 \u221a 32.911 \u221a \u03c4 )2 + \u03c0(24\u03c4 + 1)2 \u2212\n2(192\u03c4(12\u03c4 + 25)\u2212 1) ( 40 \u221a 32.911 \u221a \u03c4 )\n\u221a \u03c0(2.911\u2212 1)(48\u03c4 + 1) + \u221a( 40 \u221a 32.911 \u221a \u03c4 )2 + \u03c0(48\u03c4 + 1)2 + 40 \u221a\n3(72\u03c4 \u2212 1) \u221a \u03c4 + 32 =((\n40 \u221a 3(72\u03c4 \u2212 1) \u221a \u03c4 + 32 )(\u221a \u03c0(2.911\u2212 1)(24\u03c4 + 1) + \u221a( 40 \u221a 32.911 \u221a \u03c4 )2 + \u03c0(24\u03c4 + 1)2 ) ( \u221a \u03c0(2.911\u2212 1)(48\u03c4 + 1) + \u221a( 40 \u221a 32.911 \u221a \u03c4 )2 + \u03c0(48\u03c4 + 1)2 ) +\n2.911 \u00b7 40 \u221a 3 \u221a \u03c0(192\u03c4(3\u03c4 + 25)\u2212 1) \u221a \u03c4(\n\u221a \u03c0(2.911\u2212 1)(48\u03c4 + 1) + \u221a( 40 \u221a 32.911 \u221a \u03c4 )2 + \u03c0(48\u03c4 + 1)2 ) \u2212\n2 \u221a \u03c040 \u221a 32.911(192\u03c4(12\u03c4 + 25)\u2212 1)\n\u221a \u03c4 ( \u221a \u03c0(2.911\u2212 1)(24\u03c4 + 1) + \u221a( 40 \u221a 32.911 \u221a \u03c4 )2 + \u03c0(24\u03c4 + 1)2 )) (( \u221a \u03c0(2.911\u2212 1)(24\u03c4 + 1) + \u221a( 40 \u221a 32.911 \u221a \u03c4 )2 + \u03c0(24\u03c4 + 1)2\n) ( \u221a \u03c0(2.911\u2212 1)(48\u03c4 + 1) + \u221a( 40 \u221a 32.911 \u221a \u03c4 )2 + \u03c0(48\u03c4 + 1)2 ))\u22121 .\nAfter applying the approximation of Ren and MacKenzie [30] and adding 200, we first factored out 40 \u221a 3 \u221a \u03c4 . Then we brought all terms to the same denominator.\nWe now consider the numerator:\n( 40 \u221a 3(72\u03c4 \u2212 1) \u221a \u03c4 + 32 )(\u221a \u03c0(2.911\u2212 1)(24\u03c4 + 1) + \u221a( 40 \u221a 32.911 \u221a \u03c4 )2 + \u03c0(24\u03c4 + 1)2 ) (317)(\n\u221a \u03c0(2.911\u2212 1)(48\u03c4 + 1) + \u221a( 40 \u221a 32.911 \u221a \u03c4 )2 + \u03c0(48\u03c4 + 1)2 ) +\n2.911 \u00b7 40 \u221a 3 \u221a \u03c0(192\u03c4(3\u03c4 + 25)\u2212 1) \u221a \u03c4\n( \u221a \u03c0(2.911\u2212 1)(48\u03c4 + 1) + \u221a( 40 \u221a 32.911 \u221a \u03c4 )2 + \u03c0(48\u03c4 + 1)2 ) \u2212\n2 \u221a \u03c040 \u221a 32.911(192\u03c4(12\u03c4 + 25)\u2212 1) \u221a \u03c4(\n\u221a \u03c0(2.911\u2212 1)(24\u03c4 + 1) + \u221a( 40 \u221a 32.911 \u221a \u03c4 )2 + \u03c0(24\u03c4 + 1)2 ) =\n\u2212 3.42607\u00d7 106 \u221a \u03c0(24\u03c4 + 1)2 + 40674.8\u03c4\u03c43/2+\n2880 \u221a 3 \u221a \u03c0(24\u03c4 + 1)2 + 40674.8\u03c4 \u221a \u03c0(48\u03c4 + 1)2 + 40674.8\u03c4\u03c43/2+\n1.72711\u00d7 106 \u221a \u03c0(48\u03c4 + 1)2 + 40674.8\u03c4\u03c43/2 \u2212 5.81185\u00d7 106\u03c43/2 \u2212\n836198 \u221a \u03c0(24\u03c4 + 1)2 + 40674.8\u03c4\u03c45/2 + 611410 \u221a \u03c0(48\u03c4 + 1)2 + 40674.8\u03c4\u03c45/2\u2212\n1.67707\u00d7 106\u03c45/2 \u2212 3.44998\u00d7 107\u03c47/2 + 422935.\u03c42 + 5202.68 \u221a \u03c0(24\u03c4 + 1)2 + 40674.8\u03c4\u03c4+\n2601.34 \u221a \u03c0(48\u03c4 + 1)2 + 40674.8\u03c4\u03c4 + 26433.4\u03c4 + 415.94 \u221a \u03c4 + 480.268 \u221a \u03c4 \u221a \u03c0(24\u03c4 + 1)2 + 40674.8\u03c4 +\n108.389 \u221a \u03c0(24\u03c4 + 1)2 + 40674.8\u03c4 \u2212 592.138 \u221a \u03c4 \u221a \u03c0(48\u03c4 + 1)2 + 40674.8\u03c4\u2212 40 \u221a 3 \u221a \u03c4 \u221a \u03c0(24\u03c4 + 1)2 + 40674.8\u03c4 \u221a \u03c0(48\u03c4 + 1)2 + 40674.8\u03c4 +\n32 \u221a \u03c0(24\u03c4 + 1)2 + 40674.8\u03c4 \u221a \u03c0(48\u03c4 + 1)2 + 40674.8\u03c4 +\n108.389 \u221a \u03c0(48\u03c4 + 1)2 + 40674.8\u03c4 + 367.131 =\n\u2212 5.81185\u00d7 106\u03c43/2 \u2212 1.67707\u00d7 106\u03c45/2 \u2212 3.44998\u00d7 107\u03c47/2+( \u22123.42607\u00d7 106\u03c43/2 \u2212 836198\u03c45/2 + 5202.68\u03c4 + 480.268 \u221a \u03c4 + 108.389 ) \u221a \u03c0(24\u03c4 + 1)2 + 40674.8\u03c4+( 1.72711\u00d7 106\u03c43/2 + 611410\u03c45/2 + 2601.34\u03c4 \u2212 592.138 \u221a \u03c4 + 108.389\n) \u221a \u03c0(48\u03c4 + 1)2 + 40674.8\u03c4+( 2880 \u221a 3\u03c43/2 \u2212 40 \u221a 3 \u221a \u03c4 + 32 )\u221a \u03c0(24\u03c4 + 1)2 + 40674.8\u03c4 \u221a \u03c0(48\u03c4 + 1)2 + 40674.8\u03c4+ 422935.\u03c42 + 26433.4\u03c4 + 415.94 \u221a \u03c4 + 367.131 6\n\u2212 5.81185\u00d7 106\u03c43/2 \u2212 1.67707\u00d7 106\u03c45/2 \u2212 3.44998\u00d7 107\u03c47/2+( \u22123.42607\u00d7 106\u03c43/2 \u2212 836198\u03c45/2 + 480.268 \u221a 1.25 + 1.255202.68 + 108.389 ) \u221a \u03c0(24\u03c4 + 1)2 + 40674.8\u03c4+( 1.72711\u00d7 106\u03c43/2 + 611410\u03c45/2 \u2212 592.138 \u221a 0.9 + 1.252601.34 + 108.389\n) \u221a \u03c0(48\u03c4 + 1)2 + 40674.8\u03c4+( 2880 \u221a 3\u03c43/2 \u2212 40 \u221a 3 \u221a \u03c4 + 32 )\u221a \u03c0(24\u03c4 + 1)2 + 40674.8\u03c4 \u221a \u03c0(48\u03c4 + 1)2 + 40674.8\u03c4+ 422935\u03c42 + 415.94 \u221a 1.25 + 1.2526433.4 + 367.131 =\n\u2212 5.81185\u00d7 106\u03c43/2 \u2212 1.67707\u00d7 106\u03c45/2 \u2212 3.44998\u00d7 107\u03c47/2+( \u22123.42607\u00d7 106\u03c43/2 \u2212 836198\u03c45/2 + 7148.69 )\u221a \u03c0(24\u03c4 + 1)2 + 40674.8\u03c4+(\n1.72711\u00d7 106\u03c43/2 + 611410\u03c45/2 + 2798.31 )\u221a\n\u03c0(48\u03c4 + 1)2 + 40674.8\u03c4+( 2880 \u221a 3\u03c43/2 \u2212 40 \u221a 3 \u221a \u03c4 + 32 )\u221a \u03c0(24\u03c4 + 1)2 + 40674.8\u03c4 \u221a \u03c0(48\u03c4 + 1)2 + 40674.8\u03c4+\n422935\u03c42 + 33874 =\n\u2212 5.81185\u00d7 106\u03c43/2 \u2212 1.67707\u00d7 106\u03c45/2 \u2212 3.44998\u00d7 107\u03c47/2+( 1.72711\u00d7 106\u03c43/2 + 611410\u03c45/2 + 2798.31 )\u221a 2304\u03c0(\u03c4 + 5.66103)(\u03c4 + 0.0000766694)+(\n\u22123.42607\u00d7 106\u03c43/2 \u2212 836198\u03c45/2 + 7148.69 )\u221a\n576\u03c0(\u03c4 + 22.561)(\u03c4 + 0.0000769518)+( 2880 \u221a 3\u03c43/2 \u2212 40 \u221a 3 \u221a \u03c4 + 32 )\u221a 2304\u03c0(\u03c4 + 5.66103)(\u03c4 + 0.0000766694)\u221a\n576\u03c0(\u03c4 + 22.561)(\u03c4 + 0.0000769518)+\n422935\u03c42 + 33874 6\n\u2212 5.81185106\u03c43/2 \u2212 1.67707\u00d7 106\u03c45/2 \u2212 3.44998\u00d7 107\u03c47/2+( 1.72711\u00d7 106\u03c43/2 + 611410\u03c45/2 + 2798.31 )\u221a 2304\u03c01.0001(\u03c4 + 5.66103)\u03c4+(\n2880 \u221a 3\u03c43/2 \u2212 40 \u221a 3 \u221a \u03c4 + 32 )\u221a 2304\u03c01.0001(\u03c4 + 5.66103)\u03c4 \u221a 576\u03c01.0001(\u03c4 + 22.561)\u03c4+(\n\u22123.42607\u00d7 106\u03c43/2 \u2212 836198\u03c45/2 + 7148.69 )\n\u221a 576\u03c0(\u03c4 + 22.561)\u03c4+\n422935\u03c42 + 33874. =\n\u2212 5.81185106\u03c43/2 \u2212 1.67707\u00d7 106\u03c45/2 \u2212 3.44998\u00d7 107\u03c47/2+( \u2212250764.\u03c43/2 + 1.8055\u00d7 107\u03c45/2 + 115823.\u03c4 ) \u221a \u03c4 + 5.66103\n\u221a \u03c4 + 22.561 + 422935.\u03c42+(\n5.20199\u00d7 107\u03c43 + 1.46946\u00d7 108\u03c42 + 238086. \u221a \u03c4 )\u221a\n\u03c4 + 5.66103+( \u22123.55709\u00d7 107\u03c43 \u2212 1.45741\u00d7 108\u03c42 + 304097. \u221a \u03c4 )\u221a\n\u03c4 + 22.561 + 33874. 6 \u221a\n1.25 + 5.66103 \u221a 1.25 + 22.561 ( \u2212250764.\u03c43/2 + 1.8055\u00d7 107\u03c45/2 + 115823.\u03c4 ) +\n\u221a 1.25 + 5.66103 ( 5.20199\u00d7 107\u03c43 + 1.46946\u00d7 108\u03c42 + 238086. \u221a \u03c4 )\n+ \u221a 0.9 + 22.561 ( \u22123.55709\u00d7 107\u03c43 \u2212 1.45741\u00d7 108\u03c42 + 304097. \u221a \u03c4 ) \u2212\n5.81185106\u03c43/2 \u2212 1.67707\u00d7 106\u03c45/2 \u2212 3.44998\u00d7 107\u03c47/2 + 422935.\u03c42 + 33874. 6 33874.\u03c43/2\n0.93/2 \u2212 9.02866\u00d7 106\u03c43/2 + 2.29933\u00d7 108\u03c45/2 \u2212 3.44998\u00d7 107\u03c47/2\u2212\n3.5539\u00d7 107\u03c43 \u2212 3.19193\u00d7 108\u03c42 + 1.48578\u00d7 10 6 \u221a \u03c4\u03c4\u221a\n0.9 +\n2.09884\u00d7 106\u03c4 \u221a \u03c4\n0.9 =\n\u2212 5.09079\u00d7 106\u03c43/2 + 2.29933\u00d7 108\u03c45/2\u2212 3.44998\u00d7 107\u03c47/2 \u2212 3.5539\u00d7 107\u03c43 \u2212 3.19193\u00d7 108\u03c42 6\n\u2212 5.09079\u00d7 106\u03c43/2 + 2.29933\u00d7 10 8 \u221a\n1.25\u03c45/2\u221a \u03c4 \u2212 3.44998\u00d7 107\u03c47/2\u2212\n3.5539\u00d7 107\u03c43 \u2212 3.19193\u00d7 108\u03c42 = \u2212 5.09079\u00d7 106\u03c43/2 \u2212 3.44998\u00d7 107\u03c47/2 \u2212 3.5539\u00d7 107\u03c43 \u2212 6.21197\u00d7 107\u03c42 < 0 .\nFirst we expanded the term (multiplied it out). The we put the terms multiplied by the same square root into brackets. The next inequality sign stems from inserting the maximal value of 1.25 for \u03c4 for some positive terms and value of 0.9 for negative terms. These terms are then expanded at the =-sign. The next equality factors the terms under the squared root. We decreased the negative term by setting \u03c4 = \u03c4 + 0.0000769518 under the root. We increased positive terms by setting \u03c4 + 0.0000769518 = 1.0000962\u03c4 and \u03c4 + 0.0000766694 = 1.0000962\u03c4 under the root for positive terms. The positive terms are increase, since 0.8+0.00007695180.8 < 1.0000962, thus \u03c4 + 0.0000766694 < \u03c4 + 0.0000769518 6 1.0000962\u03c4 . For the next inequality we decreased negative terms by inserting \u03c4 = 0.9 and increased positive terms by inserting \u03c4 = 1.25. The next\nequality expands the terms. We use upper bound of 1.25 and lower bound of 0.9 to obtain terms with corresponding exponents of \u03c4 .\nConsequently, the derivative of\n\u03c4 ( e ( \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 2e ( \u00b5\u03c9+2\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n)) (318)\nwith respect to \u03c4 is smaller than zero for maximal \u03bd = 0.24 and the domain 0.9 6 \u03c4 6 1.25.\nLemma 47. In the domain \u22120.01 6 y 6 0.01 and 0.64 6 x 6 1.875, the function f(x, y) = e 1 2 (2y+x) erfc ( x+y\u221a\n2x\n) has a global maximum at y = 0.64 and x = \u22120.01 and a global minimum at\ny = 1.875 and x = 0.01.\nProof. f(x, y) = e 1 2 (2y+x) erfc ( x+y\u221a\n2x\n) is strictly monotonically decreasing in x, since its derivative\nwith respect to x is negative:\ne\u2212 y2 2x (\u221a \u03c0x3/2e (x+y)2 2x erfc ( x+y\u221a 2 \u221a x ) + \u221a 2(y \u2212 x) )\n2 \u221a \u03c0x3/2\n< 0\n\u21d0\u21d2 \u221a \u03c0x3/2e (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) + \u221a 2(y \u2212 x) < 0\n\u221a \u03c0x3/2e (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) + \u221a 2(y \u2212 x) 6\n2x3/2\nx+y\u221a 2 \u221a x\n+ \u221a (x+y)2\n2x + 4 \u03c0\n+ y \u221a 2\u2212 x \u221a 2 6\n2 \u00b7 0.643/2\n0.01+0.64\u221a 2 \u221a 0.64 + \u221a (0.01+0.64)2 2\u00b70.64 + 4 \u03c0 + 0.01 \u221a 2\u2212 0.64 \u221a 2 = \u22120.334658 < 0. (319)\nThe two last inqualities come from applying Abramowitz bounds 22 and from the fact that the expression 2x 3/2\nx+y\u221a 2 \u221a x +\n\u221a (x+y)2\n2x + 4 \u03c0\n+ y \u221a 2\u2212 x \u221a 2 does not change monotonicity in the domain and hence\nthe maximum must be found at the border. For x = 0.64 that maximizes the function f(x, y) is monotonically in y, because its derivative w.r.t. y at x = 0.64 is\ney ( 1.37713 erfc(0.883883y + 0.565685)\u2212 1.37349e\u22120.78125(y+0.64) 2 ) < 0\n\u21d0\u21d2 ( 1.37713 erfc(0.883883y + 0.565685)\u2212 1.37349e\u22120.78125(y+0.64) 2 ) < 0(\n1.37713 erfc(0.883883y + 0.565685)\u2212 1.37349e\u22120.78125(y+0.64) 2 ) 6(\n1.37713 erfc(0.883883 \u00b7 \u22120.01 + 0.565685)\u2212 1.37349e\u22120.78125(0.01+0.64) 2 ) =\n0.5935272325870631\u2212 0.987354705867739 < 0. (320)\nTherefore, the values y = 0.64 and x = \u22120.01 give a global maximum of the function f(x, y) in the domain \u22120.01 6 y 6 0.01 and 0.64 6 x 6 1.875 and the values y = 1.875 and x = 0.01 give the global minimum.\nA4 Additional information on experiments\nIn this section, we report the hyperparameters that were considered for each method and data set and give details on the processing of the data sets.\nA4.1 121 UCI Machine Learning Repository data sets: Hyperparameters\nFor the UCI data sets, the best hyperparameter setting was determined by a grid-search over all hyperparameter combinations using 15% of the training data as validation set. The early stopping parameter was determined on the smoothed learning curves of 100 epochs of the validation set. Smoothing was done using moving averages of 10 consecutive values. We tested \u201crectangular\u201d and \u201cconic\u201d layers \u2013 rectangular layers have constant number of hidden units in each layer, conic layers start with the given number of hidden units in the first layer and then decrease the number of hidden units to the size of the output layer according to the geometric progession. If multiple hyperparameters provided identical performance on the validation set, we preferred settings with a higher number of layers, lower learning rates and higher dropout rates. All methods had the chance to adjust their hyperparameters to the data set at hand.\nTable A4: Hyperparameters considered for self-normalizing networks in the UCI data sets.\nHyperparameter Considered values\nNumber of hidden units {1024, 512, 256} Number of hidden layers {2, 3, 4, 8, 16, 32} Learning rate {0.01, 0.1, 1} Dropout rate {0.05, 0} Layer form {rectangular, conic}\nTable A5: Hyperparameters considered for ReLU networks with MS initialization in the UCI data sets.\nHyperparameter Considered values\nNumber of hidden units {1024, 512, 256} Number of hidden layers {2,3,4,8,16,32} Learning rate {0.01, 0.1, 1} Dropout rate {0.5, 0} Layer form {rectangular, conic}\nTable A6: Hyperparameters considered for batch normalized networks in the UCI data sets.\nHyperparameter Considered values\nNumber of hidden units {1024, 512, 256} Number of hidden layers {2, 3, 4, 8, 16, 32} Learning rate {0.01, 0.1, 1} Normalization {Batchnorm} Layer form {rectangular, conic}\nTable A7: Hyperparameters considered for weight normalized networks in the UCI data sets.\nHyperparameter Considered values\nNumber of hidden units {1024, 512, 256} Number of hidden layers {2, 3, 4, 8, 16, 32} Learning rate {0.01, 0.1, 1} Normalization {Weightnorm} Layer form {rectangular, conic}\nTable A8: Hyperparameters considered for layer normalized networks in the UCI data sets.\nHyperparameter Considered values\nNumber of hidden units {1024, 512, 256} Number of hidden layers {2, 3, 4, 8, 16, 32} Learning rate {0.01, 0.1, 1} Normalization {Layernorm} Layer form {rectangular, conic}\nTable A9: Hyperparameters considered for Highway networks in the UCI data sets.\nHyperparameter Considered values\nNumber of hidden layers {2, 3, 4, 8, 16, 32} Learning rate {0.01, 0.1, 1} Dropout rate {0, 0.5}\nTable A10: Hyperparameters considered for Residual networks in the UCI data sets.\nHyperparameter Considered values\nNumber of blocks {2, 3, 4, 8, 16} Number of neurons per blocks {1024, 512, 256} Block form {rectangular, diavolo} Bottleneck {25%, 50%} Learning rate {0.01, 0.1, 1}\nA4.2 121 UCI Machine Learning Repository data sets: detailed results\nMethods compared. We used data sets and preprocessing scripts by Fern\u00e1ndez-Delgado et al. [10] for data preparation and defining training and test sets. With several flaws in the method comparison[37] that we avoided, the authors compared 179 machine learning methods of 17 groups in their experiments. The method groups were defined by Fern\u00e1ndez-Delgado et al. [10] as follows: Support Vector Machines, RandomForest, Multivariate adaptive regression splines (MARS), Boosting, Rule-based, logistic and multinomial regression, Discriminant Analysis (DA), Bagging, Nearest Neighbour, DecisionTree, other Ensembles, Neural Networks, Bayesian, Other Methods, generalized linear models (GLM), Partial least squares and principal component regression (PLSR), and Stacking. However, many of methods assigned to those groups were merely different implementations of the same method. Therefore, we selected one representative of each of the 17 groups for method comparison. The representative method was chosen as the group\u2019s method with the median performance across all tasks. Finally, we included 17 other machine learning methods of Fern\u00e1ndez-Delgado et al. [10], and 6 FNNs, BatchNorm, WeightNorm, LayerNorm, Highway, Residual and MSRAinit networks, and self-normalizing neural networks (SNNs) giving a total of 24 compared methods.\nResults of FNN methods for all 121 data sets. The results of the compared FNN methods can be found in Table A11.\nSmall and large data sets. We assigned each of the 121 UCI data sets into the group \u201clarge datasets\u201d or \u201csmall datasets\u201d if the had more than 1,000 data points or less, respectively. We expected that Deep Learning methods require large data sets to competitive to other machine learning methods. This resulted in 75 small and 46 large data sets.\nResults. The results of the method comparison are given in Tables A12 and A13 for small and large data sets, respectively. On small data sets, SVMs performed best followed by RandomForest and SNNs. On large data sets, SNNs are the best method followed by SVMs and Random Forest.\nimage-segmentation 2310 19 0.9114 0.9090 0.9024 0.8919 0.8481 0.8938 0.8838 ionosphere 351 34 0.8864 0.9091 0.9432 0.9545 0.9432 0.9318 0.9432 iris 150 5 0.9730 0.9189 0.8378 0.9730 0.9189 1.0000 0.9730 led-display 1000 8 0.7640 0.7200 0.7040 0.7160 0.6280 0.6920 0.6480 lenses 24 5 0.6667 1.0000 1.0000 0.6667 0.8333 0.8333 0.6667 letter 20000 17 0.9726 0.9712 0.8984 0.9762 0.9796 0.9580 0.9742 libras 360 91 0.7889 0.8667 0.8222 0.7111 0.7444 0.8000 0.8333 low-res-spect 531 101 0.8571 0.8496 0.9023 0.8647 0.8571 0.8872 0.8947 lung-cancer 32 57 0.6250 0.3750 0.1250 0.2500 0.5000 0.5000 0.2500 lymphography 148 19 0.9189 0.7297 0.7297 0.6757 0.7568 0.7568 0.7838 magic 19020 11 0.8692 0.8629 0.8673 0.8723 0.8713 0.8690 0.8620 mammographic 961 6 0.8250 0.8083 0.7917 0.7833 0.8167 0.8292 0.8208 miniboone 130064 51 0.9307 0.9250 0.9270 0.9254 0.9262 0.9272 0.9313 molec-biol-promoter 106 58 0.8462 0.7692 0.6923 0.7692 0.7692 0.6923 0.4615 molec-biol-splice 3190 61 0.9009 0.8482 0.8833 0.8557 0.8519 0.8494 0.8607 monks-1 556 7 0.7523 0.6551 0.5833 0.7546 0.9074 0.5000 0.7014 monks-2 601 7 0.5926 0.6343 0.6389 0.6273 0.3287 0.6644 0.5162 monks-3 554 7 0.6042 0.7454 0.5880 0.5833 0.5278 0.5231 0.6991 mushroom 8124 22 1.0000 1.0000 1.0000 1.0000 0.9990 0.9995 0.9995 musk-1 476 167 0.8739 0.8655 0.8992 0.8739 0.8235 0.8992 0.8992 musk-2 6598 167 0.9891 0.9945 0.9915 0.9964 0.9982 0.9927 0.9951 nursery 12960 9 0.9978 0.9988 1.0000 0.9994 0.9994 0.9966 0.9966 oocytes_merluccius_nucleus_4d 1022 42 0.8235 0.8196 0.7176 0.8000 0.8078 0.8078 0.7686 oocytes_merluccius_states_2f 1022 26 0.9529 0.9490 0.9490 0.9373 0.9333 0.9020 0.9412 oocytes_trisopterus_nucleus_2f 912 26 0.7982 0.8728 0.8289 0.7719 0.7456 0.7939 0.8202 oocytes_trisopterus_states_5b 912 33 0.9342 0.9430 0.9342 0.8947 0.8947 0.9254 0.8991 optical 5620 63 0.9711 0.9666 0.9644 0.9627 0.9716 0.9638 0.9755 ozone 2536 73 0.9700 0.9732 0.9716 0.9669 0.9669 0.9748 0.9716 page-blocks 5473 11 0.9583 0.9708 0.9656 0.9605 0.9613 0.9730 0.9708 parkinsons 195 23 0.8980 0.9184 0.8367 0.9184 0.8571 0.8163 0.8571 pendigits 10992 17 0.9706 0.9714 0.9671 0.9708 0.9734 0.9620 0.9657 pima 768 9 0.7552 0.7656 0.7188 0.7135 0.7188 0.6979 0.6927 pittsburg-bridges-MATERIAL 106 8 0.8846 0.8462 0.9231 0.9231 0.8846 0.8077 0.9231 pittsburg-bridges-REL-L 103 8 0.6923 0.7692 0.6923 0.8462 0.7692 0.6538 0.7308 pittsburg-bridges-SPAN 92 8 0.6957 0.5217 0.5652 0.5652 0.5652 0.6522 0.6087 pittsburg-bridges-T-OR-D 102 8 0.8400 0.8800 0.8800 0.8800 0.8800 0.8800 0.8800 pittsburg-bridges-TYPE 105 8 0.6538 0.6538 0.5385 0.6538 0.1154 0.4615 0.6538 planning 182 13 0.6889 0.6667 0.6000 0.7111 0.6222 0.6444 0.6889 plant-margin 1600 65 0.8125 0.8125 0.8375 0.7975 0.7600 0.8175 0.8425 plant-shape 1600 65 0.7275 0.6350 0.6325 0.5150 0.2850 0.6575 0.6775 plant-texture 1599 65 0.8125 0.7900 0.7900 0.8000 0.8200 0.8175 0.8350 post-operative 90 9 0.7273 0.7273 0.5909 0.7273 0.5909 0.5455 0.7727 primary-tumor 330 18 0.5244 0.5000 0.4512 0.3902 0.5122 0.5000 0.4512 ringnorm 7400 21 0.9751 0.9843 0.9692 0.9811 0.9843 0.9719 0.9827 seeds 210 8 0.8846 0.8654 0.9423 0.8654 0.8654 0.8846 0.8846 semeion 1593 257 0.9196 0.9296 0.9447 0.9146 0.9372 0.9322 0.9447 soybean 683 36 0.8511 0.8723 0.8617 0.8670 0.8883 0.8537 0.8484 spambase 4601 58 0.9409 0.9461 0.9435 0.9461 0.9426 0.9504 0.9513 spect 265 23 0.6398 0.6183 0.6022 0.6667 0.6344 0.6398 0.6720 spectf 267 45 0.4973 0.6043 0.8930 0.7005 0.2299 0.4545 0.5561 statlog-australian-credit 690 15 0.5988 0.6802 0.6802 0.6395 0.6802 0.6860 0.6279 statlog-german-credit 1000 25 0.7560 0.7280 0.7760 0.7720 0.7520 0.7400 0.7400\nmethodGroup method avg. rank p-value\nSVM LibSVM_weka 9.3 RandomForest RRFglobal_caret 9.6 2.5e-01 SNN SNN 9.6 3.8e-01 LMR SimpleLogistic_weka 9.9 1.5e-01 NeuralNetworks lvq_caret 10.1 1.0e-01 MARS gcvEarth_caret 10.7 3.6e-02 MSRAinit MSRAinit 11.0 4.0e-02 LayerNorm LayerNorm 11.3 7.2e-02 Highway Highway 11.5 8.9e-03 DiscriminantAnalysis mda_R 11.8 2.6e-03 Boosting LogitBoost_weka 11.9 2.4e-02 Bagging ctreeBag_R 12.1 1.8e-03 ResNet ResNet 12.3 3.5e-03 BatchNorm BatchNorm 12.6 4.9e-04 Rule-based JRip_caret 12.9 1.7e-04 WeightNorm WeightNorm 13.0 8.3e-05 DecisionTree rpart2_caret 13.6 7.0e-04 OtherEnsembles Dagging_weka 13.9 3.0e-05 Nearest Neighbour NNge_weka 14.0 7.7e-04 OtherMethods pam_caret 14.2 1.5e-04 PLSR simpls_R 14.3 4.6e-05 Bayesian NaiveBayes_weka 14.6 1.2e-04 GLM bayesglm_caret 15.0 1.6e-06 Stacking Stacking_weka 20.9 2.2e-12\nA4.3 Tox21 challenge data set: Hyperparameters\nFor the Tox21 data set, the best hyperparameter setting was determined by a grid-search over all hyperparameter combinations using the validation set defined by the challenge winners [28]. The hyperparameter space was chosen to be similar to the hyperparameters that were tested by Mayr et al. [28]. The early stopping parameter was determined on the smoothed learning curves of 100 epochs of the validation set. Smoothing was done using moving averages of 10 consecutive values. We tested \u201crectangular\u201d and \u201cconic\u201d layers \u2013 rectangular layers have constant number of hidden units in each layer, conic layers start with the given number of hidden units in the first layer and then decrease the number of hidden units to the size of the output layer according to the geometric progession. All methods had the chance to adjust their hyperparameters to the data set at hand.\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n\u22126 \u22123 0 3 6\nd e\nn s it y\nnetwork inputs\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n\u22126 \u22123 0 3 6\nd e\nn s it y\nnetwork inputs\nFigure A8: Distribution of network inputs of an SNN for the Tox21 data set. The plots show the distribution of network inputs z of the second layer of a typical Tox21 network. The red curves display a kernel density estimator of the network inputs and the black curve is the density of a standard normal distribution. Left panel: At initialization time before learning. The distribution of network inputs is close to a standard normal distribution. Right panel: After 40 epochs of learning. The distributions of network inputs is close to a normal distribution.\nDistribution of network inputs. We empirically checked the assumption that the distribution of network inputs can well be approximated by a normal distribution. To this end, we investigated the density of the network inputs before and during learning and found that these density are close to normal distributions (see Figure A8).\nA4.4 HTRU2 data set: Hyperparameters\nFor the HTRU2 data set, the best hyperparameter setting was determined by a grid-search over all hyperparameter combinations using one of the 9 non-testing folds as validation fold in a nested cross-validation procedure. Concretely, if M was the testing fold, we used M \u2212 1 as validation fold, and for M = 1 we used fold 10 for validation. The early stopping parameter was determined on the smoothed learning curves of 100 epochs of the validation set. Smoothing was done using moving averages of 10 consecutive values. We tested \u201crectangular\u201d and \u201cconic\u201d layers \u2013 rectangular layers have constant number of hidden units in each layer, conic layers start with the given number of hidden units in the first layer and then decrease the number of hidden units to the size of the output layer according to the geometric progession. All methods had the chance to adjust their hyperparameters to the data set at hand.\nA5 Other fixed points\nA similar analysis with corresponding function domains can be performed for other fixed points, for example for \u00b5 = \u00b5\u0303 = 0 and \u03bd = \u03bd\u0303 = 2, which leads to a SELU activation function with parameters \u03b102 = 1.97126 and \u03bb02 = 1.06071.\nA6 Bounds determined by numerical methods\nIn this section we report bounds on previously discussed expressions as determined by numerical methods (min and max have been computed).\n0(\u00b5=0.06,\u03c9=0,\u03bd=1.35,\u03c4=1.12) < \u2202J11 \u2202\u00b5 < .00182415(\u00b5=\u22120.1,\u03c9=0.1,\u03bd=1.47845,\u03c4=0.883374)\n(321)\n0.905413(\u00b5=0.1,\u03c9=\u22120.1,\u03bd=1.5,\u03c4=1.25) < \u2202J11 \u2202\u03c9 < 1.04143(\u00b5=0.1,\u03c9=0.1,\u03bd=0.8,\u03c4=0.8)\n\u22120.0151177(\u00b5=\u22120.1,\u03c9=0.1,\u03bd=0.8,\u03c4=1.25) < \u2202J11 \u2202\u03bd < 0.0151177(\u00b5=0.1,\u03c9=\u22120.1,\u03bd=0.8,\u03c4=1.25)\n\u22120.015194(\u00b5=\u22120.1,\u03c9=0.1,\u03bd=0.8,\u03c4=1.25) < \u2202J11 \u2202\u03c4 < 0.015194(\u00b5=0.1,\u03c9=\u22120.1,\u03bd=0.8,\u03c4=1.25)\n\u22120.0151177(\u00b5=\u22120.1,\u03c9=0.1,\u03bd=0.8,\u03c4=1.25) < \u2202J12 \u2202\u00b5 < 0.0151177(\u00b5=0.1,\u03c9=\u22120.1,\u03bd=0.8,\u03c4=1.25)\n\u22120.0151177(\u00b5=0.1,\u03c9=\u22120.1,\u03bd=0.8,\u03c4=1.25) < \u2202J12 \u2202\u03c9 < 0.0151177(\u00b5=0.1,\u03c9=\u22120.1,\u03bd=0.8,\u03c4=1.25)\n\u22120.00785613(\u00b5=0.1,\u03c9=\u22120.1,\u03bd=1.5,\u03c4=1.25) < \u2202J12 \u2202\u03bd < 0.0315805(\u00b5=0.1,\u03c9=0.1,\u03bd=0.8,\u03c4=0.8)\n0.0799824(\u00b5=0.1,\u03c9=\u22120.1,\u03bd=1.5,\u03c4=1.25) < \u2202J12 \u2202\u03c4 < 0.110267(\u00b5=\u22120.1,\u03c9=0.1,\u03bd=0.8,\u03c4=0.8)\n0(\u00b5=0.06,\u03c9=0,\u03bd=1.35,\u03c4=1.12) < \u2202J21 \u2202\u00b5 < 0.0174802(\u00b5=0.1,\u03c9=0.1,\u03bd=0.8,\u03c4=0.8)\n0.0849308(\u00b5=0.1,\u03c9=\u22120.1,\u03bd=0.8,\u03c4=0.8) < \u2202J21 \u2202\u03c9 < 0.695766(\u00b5=0.1,\u03c9=0.1,\u03bd=1.5,\u03c4=1.25)\n\u22120.0600823(\u00b5=0.1,\u03c9=\u22120.1,\u03bd=0.8,\u03c4=1.25) < \u2202J21 \u2202\u03bd < 0.0600823(\u00b5=\u22120.1,\u03c9=0.1,\u03bd=0.8,\u03c4=1.25)\n\u22120.0673083(\u00b5=0.1,\u03c9=\u22120.1,\u03bd=1.5,\u03c4=0.8) < \u2202J21 \u2202\u03c4 < 0.0673083(\u00b5=\u22120.1,\u03c9=0.1,\u03bd=1.5,\u03c4=0.8)\n\u22120.0600823(\u00b5=0.1,\u03c9=\u22120.1,\u03bd=0.8,\u03c4=1.25) < \u2202J22 \u2202\u00b5 < 0.0600823(\u00b5=\u22120.1,\u03c9=0.1,\u03bd=0.8,\u03c4=1.25)\n\u22120.0600823(\u00b5=0.1,\u03c9=\u22120.1,\u03bd=0.8,\u03c4=1.25) < \u2202J22 \u2202\u03c9 < 0.0600823(\u00b5=\u22120.1,\u03c9=0.1,\u03bd=0.8,\u03c4=1.25)\n\u22120.276862(\u00b5=\u22120.01,\u03c9=\u22120.01,\u03bd=0.8,\u03c4=1.25) < \u2202J22 \u2202\u03bd < \u22120.084813(\u00b5=\u22120.1,\u03c9=0.1,\u03bd=1.5,\u03c4=0.8)\n0.562302(\u00b5=0.1,\u03c9=\u22120.1,\u03bd=1.5,\u03c4=1.25) < \u2202J22 \u2202\u03c4 < 0.664051(\u00b5=0.1,\u03c9=0.1,\u03bd=0.8,\u03c4=0.8)\n\u2223\u2223\u2223\u2223\u2202J11\u2202\u00b5 \u2223\u2223\u2223\u2223 < 0.00182415(0.0031049101995398316) (322)\u2223\u2223\u2223\u2223\u2202J11\u2202\u03c9 \u2223\u2223\u2223\u2223 < 1.04143(1.055872374194189)\u2223\u2223\u2223\u2223\u2202J11\u2202\u03bd \u2223\u2223\u2223\u2223 < 0.0151177(0.031242911235461816)\n\u2223\u2223\u2223\u2223\u2202J11\u2202\u03c4 \u2223\u2223\u2223\u2223 < 0.015194(0.03749149348255419)\u2223\u2223\u2223\u2223\u2202J12\u2202\u00b5 \u2223\u2223\u2223\u2223 < 0.0151177(0.031242911235461816)\u2223\u2223\u2223\u2223\u2202J12\u2202\u03c9 \u2223\u2223\u2223\u2223 < 0.0151177(0.031242911235461816)\u2223\u2223\u2223\u2223\u2202J12\u2202\u03bd \u2223\u2223\u2223\u2223 < 0.0315805(0.21232788238624354)\u2223\u2223\u2223\u2223\u2202J12\u2202\u03c4 \u2223\u2223\u2223\u2223 < 0.110267(0.2124377655377270)\u2223\u2223\u2223\u2223\u2202J21\u2202\u00b5 \u2223\u2223\u2223\u2223 < 0.0174802(0.02220441024325437)\u2223\u2223\u2223\u2223\u2202J21\u2202\u03c9 \u2223\u2223\u2223\u2223 < 0.695766(1.146955401845684)\u2223\u2223\u2223\u2223\u2202J21\u2202\u03bd \u2223\u2223\u2223\u2223 < 0.0600823(0.14983446469110305)\u2223\u2223\u2223\u2223\u2202J21\u2202\u03c4 \u2223\u2223\u2223\u2223 < 0.0673083(0.17980135762932363)\u2223\u2223\u2223\u2223\u2202J22\u2202\u00b5 \u2223\u2223\u2223\u2223 < 0.0600823(0.14983446469110305)\u2223\u2223\u2223\u2223\u2202J22\u2202\u03c9 \u2223\u2223\u2223\u2223 < 0.0600823(0.14983446469110305)\u2223\u2223\u2223\u2223\u2202J22\u2202\u03bd \u2223\u2223\u2223\u2223 < 0.562302(1.805740052651535)\u2223\u2223\u2223\u2223\u2202J22\u2202\u03c4 \u2223\u2223\u2223\u2223 < 0.664051(2.396685907216327)\nA7 References\n[1] Abramowitz, M. and Stegun, I. (1964). Handbook of Mathematical Functions, volume 55 of Applied Mathematics Series. National Bureau of Standards, 10th edition.\n[2] Ba, J. L., Kiros, J. R., and Hinton, G. (2016). Layer normalization. arXiv preprint arXiv:1607.06450.\n[3] Bengio, Y. (2013). Deep learning of representations: Looking forward. In Proceedings of the First International Conference on Statistical Language and Speech Processing, pages 1\u201337, Berlin, Heidelberg.\n[4] Blinn, J. (1996). Consider the lowly 2\u00d72 matrix. IEEE Computer Graphics and Applications, pages 82\u201388.\n[5] Bradley, R. C. (1981). Central limit theorems under weak dependence. Journal of Multivariate Analysis, 11(1):1\u201316.\n[6] Cires\u0327an, D. and Meier, U. (2015). Multi-column deep neural networks for offline handwritten chinese character classification. In 2015 International Joint Conference on Neural Networks (IJCNN), pages 1\u20136. IEEE.\n[7] Clevert, D.-A., Unterthiner, T., and Hochreiter, S. (2015). Fast and accurate deep network learning by exponential linear units (ELUs). 5th International Conference on Learning Representations, arXiv:1511.07289.\n[8] Dugan, P., Clark, C., LeCun, Y., and Van Parijs, S. (2016). Phase 4: Dcl system using deep learning approaches for land-based or ship-based real-time recognition and localization of marine mammals-distributed processing and big data applications. arXiv preprint arXiv:1605.00982.\n[9] Esteva, A., Kuprel, B., Novoa, R., Ko, J., Swetter, S., Blau, H., and Thrun, S. (2017). Dermatologist-level classification of skin cancer with deep neural networks. Nature, 542(7639):115\u2013118.\n[10] Fern\u00e1ndez-Delgado, M., Cernadas, E., Barro, S., and Amorim, D. (2014). Do we need hundreds of classifiers to solve real world classification problems. Journal of Machine Learning Research, 15(1):3133\u20133181.\n[11] Goldberg, D. (1991). What every computer scientist should know about floating-point arithmetic. ACM Comput. Surv., 223(1):5\u201348.\n[12] Graves, A., Mohamed, A., and Hinton, G. (2013). Speech recognition with deep recurrent neural networks. In IEEE International conference on acoustics, speech and signal processing (ICASSP), pages 6645\u20136649.\n[13] Graves, A. and Schmidhuber, J. (2009). Offline handwriting recognition with multidimensional recurrent neural networks. In Advances in neural information processing systems, pages 545\u2013552.\n[14] Gulshan, V., Peng, L., Coram, M., Stumpe, M. C., Wu, D., Narayanaswamy, A., Venugopalan, S., Widner, K., Madams, T., Cuadros, J., et al. (2016). Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. JAMA, 316(22):2402\u20132410.\n[15] Harrison, J. (1999). A machine-checked theory of floating point arithmetic. In Bertot, Y., Dowek, G., Hirschowitz, A., Paulin, C., and Th\u00e9ry, L., editors, Theorem Proving in Higher Order Logics: 12th International Conference, TPHOLs\u201999, volume 1690 of Lecture Notes in Computer Science, pages 113\u2013130. Springer-Verlag.\n[16] He, K., Zhang, X., Ren, S., and Sun, J. (2015a). Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n[17] He, K., Zhang, X., Ren, S., and Sun, J. (2015b). Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 1026\u20131034.\n[18] Hochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural Computation, 9(8):1735\u20131780.\n[19] Huval, B., Wang, T., Tandon, S., et al. (2015). An empirical evaluation of deep learning on highway driving. arXiv preprint arXiv:1504.01716.\n[20] Ioffe, S. and Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of The 32nd International Conference on Machine Learning, pages 448\u2013456.\n[21] Kahan, W. (2004). A logarithm too clever by half. Technical report, University of California, Berkeley.\n[22] Korolev, V. and Shevtsova, I. (2012). An improvement of the Berry\u2013Esseen inequality with applications to Poisson and mixed Poisson random sums. Scandinavian Actuarial Journal, 2012(2):81\u2013105.\n[23] Krizhevsky, A., Sutskever, I., and Hinton, G. (2012). Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pages 1097\u20131105.\n[24] LeCun, Y. and Bengio, Y. (1995). Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks, 3361(10):1995.\n[25] LeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning. Nature, 521(7553):436\u2013444.\n[26] Loosemore, S., Stallman, R. M., McGrath, R., Oram, A., and Drepper, U. (2016). The GNU C Library: Application Fundamentals. GNU Press, Free Software Foundation, 51 Franklin St, Fifth Floor, Boston, MA 02110-1301, USA, 2.24 edition.\n[27] Lyon, R., Stappers, B., Cooper, S., Brooke, J., and Knowles, J. (2016). Fifty years of pulsar candidate selection: From simple filters to a new principled real-time classification approach. Monthly Notices of the Royal Astronomical Society, 459(1):1104\u20131123.\n[28] Mayr, A., Klambauer, G., Unterthiner, T., and Hochreiter, S. (2016). DeepTox: Toxicity prediction using deep learning. Frontiers in Environmental Science, 3:80.\n[29] Muller, J.-M. (2005). On the definition of ulp(x). Technical Report Research report RR2005-09, Laboratoire de l\u2019Informatique du Parall\u00e9lisme.\n[30] Ren, C. and MacKenzie, A. R. (2007). Closed-form approximations to the error and complementary error functions and their applications in atmospheric science. Atmos. Sci. Let., pages 70\u201373.\n[31] Sak, H., Senior, A., Rao, K., and Beaufays, F. (2015). Fast and accurate recurrent neural network acoustic models for speech recognition. arXiv preprint arXiv:1507.06947.\n[32] Salimans, T. and Kingma, D. P. (2016). Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Advances in Neural Information Processing Systems, pages 901\u2013909.\n[33] Schmidhuber, J. (2015). Deep learning in neural networks: An overview. Neural Networks, 61:85\u2013117.\n[34] Silver, D., Huang, A., Maddison, C., et al. (2016). Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484\u2013489.\n[35] Srivastava, R. K., Greff, K., and Schmidhuber, J. (2015). Training very deep networks. In Advances in Neural Information Processing Systems, pages 2377\u20132385.\n[36] Sutskever, I., Vinyals, O., and Le, Q. V. (2014). Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104\u20133112.\n[37] Wainberg, M., Alipanahi, B., and Frey, B. J. (2016). Are random forests truly the best classifiers? Journal of Machine Learning Research, 17(110):1\u20135.\nList of Figures\n1 FNN and SNN trainin error curves . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n2 Visualization of the mapping g . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\nA3 Graph of the main subfunction of the derivative of the second moment . . . . . . . 30\nA4 Graph of the Abramowitz bound for the complementary error function. . . . . . . . 37\nA5 Graphs of the functions ex 2 erfc(x) and xex 2 erfc(x). . . . . . . . . . . . . . . . . 38\nA6 The graph of function \u00b5\u0303 for low variances . . . . . . . . . . . . . . . . . . . . . . 56\nA7 Graph of the function h(x) = \u00b5\u03032(0.1,\u22120.1, x, 1, \u03bb01, \u03b101) . . . . . . . . . . . . . 57 A8 Distribution of network inputs in Tox21 SNNs. . . . . . . . . . . . . . . . . . . . 94\nList of Tables\n1 Comparison of seven FNNs on 121 UCI tasks . . . . . . . . . . . . . . . . . . . . 8\n2 Comparison of FNNs at the Tox21 challenge dataset . . . . . . . . . . . . . . . . . 8\n3 Comparison of FNNs and reference methods at HTRU2 . . . . . . . . . . . . . . . 9\nA4 Hyperparameters considered for self-normalizing networks in the UCI data sets. . . 85\nA5 Hyperparameters considered for ReLU networks in the UCI data sets. . . . . . . . 85\nA6 Hyperparameters considered for batch normalized networks in the UCI data sets. . 85\nA7 Hyperparameters considered for weight normalized networks in the UCI data sets. . 86\nA8 Hyperparameters considered for layer normalized networks in the UCI data sets. . . 86\nA9 Hyperparameters considered for Highway networks in the UCI data sets. . . . . . . 86\nA10 Hyperparameters considered for Residual networks in the UCI data sets. . . . . . . 86\nA11 Comparison of FNN methods on all 121 UCI data sets. . . . . . . . . . . . . . . . 88\nA12 Method comparison on small UCI data sets . . . . . . . . . . . . . . . . . . . . . 90\nA13 Method comparison on large UCI data sets . . . . . . . . . . . . . . . . . . . . . . . 91\nA14 Hyperparameters considered for self-normalizing networks in the Tox21 data set. . 92\nA15 Hyperparameters considered for ReLU networks in the Tox21 data set. . . . . . . . 92\nA16 Hyperparameters considered for batch normalized networks in the Tox21 data set. . 92\nA17 Hyperparameters considered for weight normalized networks in the Tox21 data set. 93\nA18 Hyperparameters considered for layer normalized networks in the Tox21 data set. . 93\nA19 Hyperparameters considered for Highway networks in the Tox21 data set. . . . . . 93\nA20 Hyperparameters considered for Residual networks in the Tox21 data set. . . . . . 93\nA21 Hyperparameters considered for self-normalizing networks on the HTRU2 data set. 95\nA22 Hyperparameters considered for ReLU networks on the HTRU2 data set. . . . . . . 95\nA23 Hyperparameters considered for BatchNorm networks on the HTRU2 data set. . . . 95\nA24 Hyperparameters considered for WeightNorm networks on the HTRU2 data set. . . 96\nA25 Hyperparameters considered for LayerNorm networks on the HTRU2 data set. . . . 96\nA26 Hyperparameters considered for Highway networks on the HTRU2 data set. . . . . 96\nA27 Hyperparameters considered for Residual networks on the HTRU2 data set. . . . . 96\nBrief index\nAbramowitz bounds, 37\nBanach Fixed Point Theorem, 13 bounds\nderivatives of Jacobian entries, 21 Jacobian entries, 23 mean and variance, 24 singular value, 25, 27\ncentral limit theorem, 6 complementary error function\nbounds, 37 definition, 37\ncomputer-assisted proof, 33 contracting variance, 29\ndefinitions, 2 domain\nsingular value, 19 Theorem 1, 12 Theorem 2, 12 Theorem 3, 13\ndropout, 6\nerf, 37 erfc, 37 error function\nbounds, 37 definition, 37 properties, 39\nexpanding variance, 32 experiments, 7, 85\nastronomy, 8 HTRU2, 8, 95\nhyperparameters, 95 methods compared, 7 Tox21, 7, 92\nhyperparameters, 8, 92 UCI, 7, 85\ndetails, 85 hyperparameters, 85 results, 86\ninitialization, 6\nJacobian, 20 bounds, 23 definition, 20 derivatives, 21 entries, 20, 23 singular value, 21 singular value bound, 25\nlemmata, 19 Jacobian bound, 19\nmapping g, 2, 4\ndefinition, 11 mapping in domain, 29\nself-normalizing neural networks, 2 SELU\ndefinition, 3 parameters, 4, 11\nTheorem 1, 5, 12 proof, 13 proof sketch, 5 Theorem 2, 6, 12 proof, 14 Theorem 3, 6, 12 proof, 18"
        }
    ],
    "title": "Self-Normalizing Neural Networks",
    "year": 2021
}