{
    "abstractText": "For decades, applications deployed on a world-wide scale have been forced to give up at least one of (1) strict serializability (2) low latency writes (3) high transactional throughput. In this paper we discuss SLOG: a system that avoids this tradeoff for workloads which contain physical region locality in data access. SLOG achieves high-throughput, strictly serializable ACID transactions at geo-replicated distance and scale for all transactions submitted across the world, all the while achieving low latency for transactions that initiate from a location close to the home region for data they access. Experiments find that SLOG can reduce latency by more than an order of magnitude relative to state-of-the-art strictly serializable geo-replicated database systems such as Spanner and Calvin, while maintaining high throughput under contention. PVLDB Reference Format: Kun Ren, Dennis Li, and Daniel J. Abadi. SLOG: Serializable, Low-latency, Geo-replicated Transactions. PVLDB, 12(11): 1747-1761, 2019. DOI: https://doi.org/10.14778/3342263.3342647",
    "authors": [
        {
            "affiliations": [],
            "name": "Kun Ren"
        },
        {
            "affiliations": [],
            "name": "Dennis Li"
        },
        {
            "affiliations": [],
            "name": "Daniel J. Abadi"
        }
    ],
    "id": "SP:f12398a807ddc99c9d992be12fdb09060864f539",
    "references": [
        {
            "authors": [
                "D. Abadi"
            ],
            "title": "Correctness Anomalies Under Serializable Isolation",
            "year": 2019
        },
        {
            "authors": [
                "D.J. Abadi"
            ],
            "title": "Consistency Tradeoffs in Modern Distributed Database System Design: CAP is Only Part of the Story",
            "venue": "IEEE Computer, 45(2),",
            "year": 2012
        },
        {
            "authors": [
                "D.J. Abadi",
                "J.M. Faleiro"
            ],
            "title": "An Overview of Deterministic Database Systems",
            "venue": "Communications of the ACM (CACM), 61(9):78\u201388, September",
            "year": 2018
        },
        {
            "authors": [
                "M.S. Ardekani",
                "P. Sutra",
                "M. Shapiro"
            ],
            "title": "Non-monotonic Snapshot Isolation: Scalable and Strong Consistency for Geo-replicated Transactional Systems",
            "venue": "Proceedings of the 2013 IEEE 32nd International Symposium on Reliable Distributed Systems, SRDS \u201913, pages 163\u2013172,",
            "year": 2013
        },
        {
            "authors": [
                "M.S. Ardekani",
                "D.B. Terry"
            ],
            "title": "A Self-Configurable Geo-Replicated Cloud Storage System",
            "venue": "Symposium on Operating Systems Design and Implementation (OSDI), pages 367\u2013381,",
            "year": 2014
        },
        {
            "authors": [
                "R. Attar",
                "P.A. Bernstein",
                "N. Goodman"
            ],
            "title": "Site Initialization, Recovery, and Backup in a Distributed Database System",
            "venue": "IEEE Transactions on Software Engineering, 10(6):645\u2013650, Nov.",
            "year": 1984
        },
        {
            "authors": [
                "P. Bailis",
                "A. Fekete",
                "M.J. Franklin",
                "A. Ghodsi",
                "J.M. Hellerstein",
                "I. Stoica"
            ],
            "title": "Coordination Avoidance in Database Systems",
            "venue": "PVLDB, 8(3):185\u2013196,",
            "year": 2014
        },
        {
            "authors": [
                "J. Baker",
                "C. Bond",
                "J.C. Corbett",
                "J. Furman",
                "A. Khorlin",
                "J. Larson",
                "J.-M. Leon",
                "Y. Li",
                "A. Lloyd",
                "V. Yushprakh"
            ],
            "title": "Megastore: Providing Scalable, Highly Available Storage for Interactive Services",
            "venue": "Proceedings of the Conference on Innovative Data system Research (CIDR), pages 223\u2013234,",
            "year": 2011
        },
        {
            "authors": [
                "A.J. Bernstein",
                "D.S. Gerstl",
                "P.M. Lewis"
            ],
            "title": "Concurrency Control for Step-decomposed Transactions",
            "venue": "Information Systems, 24(9):673\u2013698, December",
            "year": 1999
        },
        {
            "authors": [
                "P. Bernstein",
                "N. Goodman"
            ],
            "title": "A Proof Technique for Concurrency Control and Recovery Algorithms for Replicated Databases",
            "venue": "Distributed Computing, 2(1):32\u201344, Mar",
            "year": 1987
        },
        {
            "authors": [
                "P.A. Bernstein",
                "S. Das"
            ],
            "title": "Scaling Optimistic Concurrency Control by Approximately Partitioning the Certifier and Log",
            "venue": "IEEE Data Engineering Bulletin, Jan-38(1):32\u201349, March",
            "year": 2015
        },
        {
            "authors": [
                "P.A. Bernstein",
                "D.W. Shipman",
                "W.S. Wong"
            ],
            "title": "Formal Aspects of Serializability in Database Concurrency Control",
            "venue": "IEEE Transactions on Software Engineering, 5(3):203\u2013216, May",
            "year": 1979
        },
        {
            "authors": [
                "Y. Breitbart",
                "H. Garcia-Molina",
                "A. Silberschatz"
            ],
            "title": "Overview of Multidatabase Transaction Management",
            "venue": "The VLDB Journal, 1(2):181\u2013240, Oct.",
            "year": 1992
        },
        {
            "authors": [
                "P. Chairunnanda",
                "K. Daudjee",
                "M.T. Ozsu"
            ],
            "title": "ConfluxDB: Multi-Master Replication for Partitioned Snapshot Isolation Databases",
            "venue": "PVLDB, 7(11):947\u2013958,",
            "year": 2014
        },
        {
            "authors": [
                "J. Chen",
                "Y. Chen",
                "Z. Chen",
                "A. Ghazal",
                "G. Li",
                "S. Li",
                "W. Ou",
                "Y. Sun",
                "M. Zhang",
                "M. Zhou"
            ],
            "title": "Data Management at Huawei: Recent Accomplishments and Future Challenges",
            "venue": "IEEE International Conference on Data Engineering (ICDE), pages 13\u201324,",
            "year": 2019
        },
        {
            "authors": [
                "B.F. Cooper",
                "R. Ramakrishnan",
                "U. Srivastava",
                "A. Silberstein",
                "P. Bohannon",
                "H.-A. Jacobsen",
                "N. Puz",
                "D. Weaver",
                "R. Yerneni"
            ],
            "title": "PNUTS: Yahoo!\u2019s Hosted Data Serving Platform",
            "venue": "PVLDB, 1(2):1277\u20131288,",
            "year": 2008
        },
        {
            "authors": [
                "B.F. Cooper",
                "A. Silberstein",
                "E. Tam",
                "R. Ramakrishnan",
                "R. Sears"
            ],
            "title": "Benchmarking Cloud Serving Systems with YCSB",
            "venue": "Proceedings of the 1st ACM Symposium on Cloud Computing, SoCC \u201910, pages 143\u2013154,",
            "year": 2010
        },
        {
            "authors": [
                "J.C. Corbett",
                "J. Dean",
                "M. Epstein",
                "A. Fikes",
                "C. Frost",
                "J.J. Furman",
                "S. Ghemawat",
                "A. Gubarev",
                "C. Heiser",
                "P. Hochschild",
                "W. Hsieh",
                "S. Kanthak",
                "E. Kogan",
                "H. Li",
                "A. Lloyd",
                "S. Melnik",
                "D. Mwaura",
                "D. Nagle",
                "S. Quinlan",
                "R. Rao",
                "L. Rolig",
                "Y. Saito",
                "M. Szymaniak",
                "C. Taylor",
                "R. Wang",
                "D. Woodford"
            ],
            "title": "Spanner: Google\u2019s Globally Distributed Database",
            "venue": "ACM Transactions on Computer Systems (TOCS), 31(3):8:1\u20138:22, Aug.",
            "year": 2013
        },
        {
            "authors": [
                "S. Das",
                "D. Agrawal",
                "A. El Abbadi"
            ],
            "title": "G-Store: A Scalable Data Store for Transactional Multi Key Access in the Cloud",
            "venue": "Proceedings of the 1st ACM Symposium on Cloud Computing, SoCC \u201910, pages 163\u2013174,",
            "year": 2010
        },
        {
            "authors": [
                "K. Daudjee",
                "K. Salem"
            ],
            "title": "Inferring a Serialization Order for Distributed Transactions",
            "venue": "IEEE International Conference on Data Engineering (ICDE),",
            "year": 2006
        },
        {
            "authors": [
                "K. Daudjee",
                "K. Salem"
            ],
            "title": "Lazy Database Replication with Snapshot Isolation",
            "venue": "Proceedings of the International Conference on Very Large Data Bases (VLDB),",
            "year": 2006
        },
        {
            "authors": [
                "G. DeCandia",
                "D. Hastorun",
                "M. Jampani",
                "G. Kakulapati",
                "A. Lakshman",
                "A. Pilchin",
                "S. Sivasubramanian",
                "P. Vosshall",
                "W. Vogels"
            ],
            "title": "Dynamo: Amazon\u2019s Highly Available Key-value Store",
            "venue": "Proceedings of Twenty-first ACM SIGOPS Symposium on Operating Systems Principles, SOSP \u201907, pages 205\u2013220,",
            "year": 2007
        },
        {
            "authors": [
                "D. Didona",
                "R. Guerraoui",
                "J. Wang",
                "W. Zwaenepoel"
            ],
            "title": "Causal Consistency and Latency Optimality",
            "venue": "Friend or Foe? PVLDB,",
            "year": 2018
        },
        {
            "authors": [
                "A.J. Elmore",
                "V. Arora",
                "R. Taft",
                "A. Pavlo",
                "D. Agrawal",
                "A. El Abbadi"
            ],
            "title": "Squall: Fine-grained live reconfiguration for partitioned main memory databases",
            "venue": "Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data, SIGMOD \u201915, pages 299\u2013313,",
            "year": 2015
        },
        {
            "authors": [
                "K.P. Eswaran",
                "J.N. Gray",
                "R.A. Lorie",
                "I.L. Traiger"
            ],
            "title": "The Notions of Consistency and Predicate Locks in a Database System",
            "venue": "Communications of the ACM (CACM), 19(11):624\u2013633, Nov.",
            "year": 1976
        },
        {
            "authors": [
                "J. Faleiro",
                "A. Thomson",
                "D.J. Abadi"
            ],
            "title": "Lazy Evaluation of Transactions in Database Systems",
            "venue": "Proceedings of the 2014 International Conference on Management of Data, SIGMOD \u201914,",
            "year": 2014
        },
        {
            "authors": [
                "J.M. Faleiro",
                "D. Abadi",
                "J.M. Hellerstein"
            ],
            "title": "High Performance Transactions via Early Write Visibility",
            "venue": "PVLDB, 10(5):613\u2013624,",
            "year": 2017
        },
        {
            "authors": [
                "J.M. Faleiro",
                "D.J. Abadi"
            ],
            "title": "FIT: A Distributed Database Performance Tradeoff",
            "venue": "IEEE Data Engineering Bulletin, 38(1): 10-17,",
            "year": 2015
        },
        {
            "authors": [
                "J.M. Faleiro",
                "D.J. Abadi"
            ],
            "title": "Rethinking Serializable Multiversion Concurrency Control",
            "venue": "PVLDB, 8(11):1190\u20131201,",
            "year": 2015
        },
        {
            "authors": [
                "J.M. Faleiro",
                "D.J. Abadi"
            ],
            "title": "Latch-free Synchronization in Database Systems: Silver Bullet or Fool\u2019s Gold",
            "venue": "In Proceedings of the Conference on Innovative Data system Research (CIDR),",
            "year": 2017
        },
        {
            "authors": [
                "H. Garcia-Molina"
            ],
            "title": "Using Semantic Knowledge for Transaction Processing in a Distributed Database",
            "venue": "ACM Transactions on Database Systems (TODS), 8(2):186\u2013213, June",
            "year": 1983
        },
        {
            "authors": [
                "H. Garcia-Molina",
                "K. Salem"
            ],
            "title": "Sagas",
            "venue": "Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data, SIGMOD \u201987, pages 249\u2013259,",
            "year": 1987
        },
        {
            "authors": [
                "D.K. Gifford"
            ],
            "title": "Information Storage in a Decentralized Computer System",
            "venue": "PhD thesis, Stanford, CA, USA,",
            "year": 1981
        },
        {
            "authors": [
                "J. Gray",
                "P. Helland",
                "P. O\u2019Neil",
                "D. Shasha"
            ],
            "title": "The Dangers of Replication and a Solution",
            "venue": "In Proceedings of the 1996 International Conference on Management of Data,",
            "year": 1996
        },
        {
            "authors": [
                "J. Gray",
                "A. Reuter"
            ],
            "title": "Transaction Processing: Concepts and Techniques",
            "venue": "Morgan Kaufmann Publishers Inc.,",
            "year": 1993
        },
        {
            "authors": [
                "P. Helland"
            ],
            "title": "Life Beyond Distributed Transactions: An Apostate\u2019s Opinion",
            "venue": "Proceedings of the Conference on Innovative Data system Research (CIDR),",
            "year": 2007
        },
        {
            "authors": [
                "M.P. Herlihy",
                "J.M. Wing"
            ],
            "title": "Linearizability: A Correctness Condition for Concurrent Objects",
            "venue": "ACM Transactions on Programming Languages and Systems (TOPLAS), 12(3):463\u2013492, July",
            "year": 1990
        },
        {
            "authors": [
                "C. Humble",
                "F. Marinescu"
            ],
            "title": "Trading consistency for scalability in distributed architectures",
            "venue": "http://www.infoq.com/news/2008/03/ebaybase,",
            "year": 2008
        },
        {
            "authors": [
                "R. Jimenez-Peris",
                "M. Patino-Martinez",
                "S. Arevalo"
            ],
            "title": "Deterministic scheduling for transactional multithreaded replicas",
            "venue": "IEEE SRDS,",
            "year": 2000
        },
        {
            "authors": [
                "E.P.C. Jones",
                "D.J. Abadi",
                "S.R. Madden"
            ],
            "title": "Concurrency control for partitioned databases",
            "venue": "Proceedings of the 2010 International Conference on Management of Data, SIGMOD \u201910,",
            "year": 2010
        },
        {
            "authors": [
                "B. Kemme",
                "G. Alonso"
            ],
            "title": "Don\u2019t Be Lazy, Be Consistent: Postgres-R, A New Way to Implement Database Replication",
            "venue": "Proceedings of the International Conference on Very Large Data Bases (VLDB),",
            "year": 2000
        },
        {
            "authors": [
                "T. Kraska",
                "G. Pang",
                "M. Franklin",
                "S. Madden",
                "A. Fekete"
            ],
            "title": "MDCC: Multi-data center consistency",
            "venue": "European Conference on Computer Systems,",
            "year": 2013
        },
        {
            "authors": [
                "A. Lakshman",
                "P. Malik"
            ],
            "title": "Cassandra: A Decentralized Structured Storage System",
            "venue": "SIGOPS Operating Systems Review, 44(2):35\u201340, April",
            "year": 2010
        },
        {
            "authors": [
                "C. Li",
                "D. Porto",
                "A. Clement",
                "J. Gehrke",
                "N. Preguica",
                "R. Rodrigues"
            ],
            "title": "Making Geo-Replicated Systems Fast as Possible, Consistent when Necessary",
            "venue": "Operating Systems Design and Implementation (OSDI),",
            "year": 2012
        },
        {
            "authors": [
                "Q. Lin",
                "P. Chang",
                "G. Chen",
                "B.C. Ooi",
                "K.-L. Tan",
                "Z. Wang"
            ],
            "title": "Towards a Non-2PC Transaction Management in Distributed Database Systems",
            "venue": "Proceedings of the 2016 International Conference on Management of Data, SIGMOD \u201916, pages 1659\u20131674,",
            "year": 2016
        },
        {
            "authors": [
                "C. Liu",
                "B.G. Lindsay",
                "S. Bourbonnais",
                "E. Hamel",
                "T.C. Truong",
                "J. Stankiewitz"
            ],
            "title": "Capturing Global Transactions from Multiple Recovery Log Files in a Partitioned Database System",
            "venue": "Proceedings of the International Conference on Very Large Data Bases (VLDB),",
            "year": 2003
        },
        {
            "authors": [
                "W. Lloyd",
                "M.J. Freedman",
                "M. Kaminsky",
                "D.G. Andersen"
            ],
            "title": "Don\u2019t Settle for Eventual: Scalable Causal Consistency for Wide-Area Storage with COPS",
            "venue": "Proceedings of the Symposium on Operating Systems Principles, SOSP \u201911,",
            "year": 2011
        },
        {
            "authors": [
                "W. Lloyd",
                "M.J. Freedman",
                "M. Kaminsky",
                "D.G. Andersen"
            ],
            "title": "Stronger Semantics for Low-Latency Geo-Replicated Storage",
            "venue": "Proceedings of the 10th USENIX Conference on Networked Systems Design and Implementation, NSDI\u201913, pages 313\u2013328,",
            "year": 2013
        },
        {
            "authors": [
                "J. Lockerman",
                "J.M. Faleiro",
                "J. Kim",
                "S. Sankaram",
                "D.J. Abadi",
                "J. Aspnes",
                "S. Siddhartha",
                "M. Balakrishnan"
            ],
            "title": "The FuzzyLog: A Partially Ordered Shared Log",
            "venue": "13th USENIX Symposium on Operating Systems Design and Implementation, pages 357\u2013372, Oct.",
            "year": 2018
        },
        {
            "authors": [
                "H. Mahmoud",
                "F. Nawab",
                "A. Pucher",
                "D. Agrawal",
                "A.E. Abbadi"
            ],
            "title": "Low-latency Multi-datacenter Databases using Replicated Commit",
            "venue": "PVLDB, 6(9):661\u2013672,",
            "year": 2013
        },
        {
            "authors": [
                "U.F. Minhas",
                "R. Liu",
                "A. Aboulnaga",
                "K. Salem",
                "J. Ng",
                "S. Robertson"
            ],
            "title": "Elastic Scale-Out for Partition-Based Database Systems",
            "venue": "IEEE International Conference on Data Engineering Workshops, ICDEW \u201912, pages 281\u2013288,",
            "year": 2012
        },
        {
            "authors": [
                "H. Moniz",
                "J. a. Leit\u00e3o",
                "R.J. Dias",
                "J. Gehrke",
                "N. Pregui\u00e7a",
                "R. Rodrigues"
            ],
            "title": "Blotter: Low Latency Transactions for Geo-Replicated Storage",
            "venue": "In Proceedings of the 26th International Conference on World Wide Web, WWW \u201917,",
            "year": 2017
        },
        {
            "authors": [
                "S. Mu",
                "Y. Cui",
                "Y. Zhang",
                "W. Lloyd",
                "J. Li"
            ],
            "title": "Extracting More Concurrency from Distributed Transactions",
            "venue": "Symposium on Operating Systems Design and Implementation (OSDI),",
            "year": 2014
        },
        {
            "authors": [
                "F. Nawab",
                "D. Agrawal",
                "A. El Abbadi"
            ],
            "title": "DPaxos: Managing Data Closer to Users for Low-Latency and Mobile Applications",
            "venue": "Proceedings of the 2018 International Conference on Management of Data, SIGMOD \u201918, pages 1221\u20131236,",
            "year": 2018
        },
        {
            "authors": [
                "F. Nawab",
                "V. Arora",
                "D. Agrawal",
                "A.E. Abbadi"
            ],
            "title": "Minimizing Commit Latency of Transactions in Geo-Replicated Data Stores",
            "venue": "Proceedings of the 2015 International Conference on Management of Data, SIGMOD \u201915,",
            "year": 2015
        },
        {
            "authors": [
                "E. Pacitti",
                "P. Minet",
                "E. Simon"
            ],
            "title": "Fast Algorithms for Maintaining Replica Consistency in Lazy Master Replicated Databases",
            "venue": "In Proceedings of the International Conference on Very Large Data Bases (VLDB),",
            "year": 1999
        },
        {
            "authors": [
                "E. Pacitti",
                "P. Minet",
                "E. Simon"
            ],
            "title": "Replica Consistency in Lazy Master Replicated Databases",
            "venue": "Kluwer Academic, 9(3), May",
            "year": 2001
        },
        {
            "authors": [
                "E. Pacitti",
                "M.T. zsu",
                "C. Coulon"
            ],
            "title": "Preventive Multi-Master Replication in a Cluster of Autonomous Databases",
            "venue": "In Proceedings of Euro-Par,",
            "year": 2003
        },
        {
            "authors": [
                "C. Papadimitriou",
                "P. Bernstein",
                "J. Ronthie"
            ],
            "title": "Some Computational Problems Related to Database Concurrency Control",
            "venue": "Proceedings of the Conference on Theoretical Computer Science, pages 275\u2013282,",
            "year": 1977
        },
        {
            "authors": [
                "C.H. Papadimitriou"
            ],
            "title": "The Serializability of Concurrent Database Updates",
            "venue": "Journal of the ACM, 26(4):631\u2013653, Oct.",
            "year": 1979
        },
        {
            "authors": [
                "K. Petersen",
                "M.J. Spreitzer",
                "D.B. Terry",
                "M.M. Theimer",
                "A.J. Demers"
            ],
            "title": "Flexible Update Propagation for Weakly Consistent Replication",
            "venue": "Proceedings of the Symposium on Operating Systems Principles, SOSP \u201997,",
            "year": 1997
        },
        {
            "authors": [
                "K. Ren",
                "J. Faleiro",
                "D.J. Abadi"
            ],
            "title": "Design Principles for Scaling Multi-core OLTP Under High Contention",
            "venue": "Proceedings of the 2016 International Conference on Management of Data, SIGMOD \u201916, pages 1583\u20131598,",
            "year": 2016
        },
        {
            "authors": [
                "K. Ren",
                "A. Thomson",
                "D.J. Abadi"
            ],
            "title": "Lightweight Locking for Main Memory Database Systems",
            "venue": "PVLDB, 6(2):145\u2013156,",
            "year": 2012
        },
        {
            "authors": [
                "K. Ren",
                "A. Thomson",
                "D.J. Abadi"
            ],
            "title": "An Evaluation of the Advantages and Disadvantages of Deterministic Database Systems",
            "venue": "PVLDB, 7(10):821\u2013832,",
            "year": 2014
        },
        {
            "authors": [
                "K. Ren",
                "A. Thomson",
                "D.J. Abadi"
            ],
            "title": "VLL: A Lock Manager Redesign for Main Memory Database Systems",
            "venue": "VLDB Journal 24(5): 681-705, October",
            "year": 2015
        },
        {
            "authors": [
                "M. Serafini",
                "E. Mansour",
                "A. Aboulnaga",
                "K. Salem",
                "T. Rafiq",
                "U.F. Minhas"
            ],
            "title": "Accordion: Elastic Scalability for Database Systems Supporting Distributed Transactions",
            "venue": "PVLDB, 7(12):1035\u20131046,",
            "year": 2014
        },
        {
            "authors": [
                "D. Shasha",
                "F. Llirbat",
                "E. Simon",
                "P. Valduriez"
            ],
            "title": "Transaction Chopping: Algorithms and Performance Studies",
            "venue": "ACM Transactions on Database Systems (TODS), 20(3):325\u2013363, September",
            "year": 1995
        },
        {
            "authors": [
                "D. Shasha",
                "E. Simon",
                "P. Valduriez"
            ],
            "title": "Simple Rational Guidance for Chopping up Transactions",
            "venue": "Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data, pages 298\u2013307,",
            "year": 1992
        },
        {
            "authors": [
                "J. Shute",
                "R. Vingralek",
                "B. Samwel",
                "B. Handy",
                "C. Whipkey",
                "E. Rollins",
                "M. Oancea",
                "K. Littlefield",
                "D. Menestrina",
                "S. Ellner",
                "J. Cieslewicz",
                "I. Rae",
                "T. Stancescu",
                "H. Apte"
            ],
            "title": "F1: A Distributed SQL Database That Scales",
            "venue": "PVLDB, 6(11):1068\u20131079,",
            "year": 2013
        },
        {
            "authors": [
                "Y. Sovran",
                "R. Power",
                "M.K. Aguilera",
                "J. Li"
            ],
            "title": "Transactional Storage for Geo-replicated Systems",
            "venue": "Proceedings of the Symposium on Operating Systems Principles, SOSP \u201911, pages 385\u2013400,",
            "year": 2011
        },
        {
            "authors": [
                "M. Stonebraker",
                "S.R. Madden",
                "D.J. Abadi",
                "S. Harizopoulos",
                "N. Hachem",
                "P. Helland"
            ],
            "title": "The End of an Architectural Era (It\u2019s Time for a Complete Rewrite)",
            "venue": "Proceedings of the International Conference on Very Large Data Bases (VLDB),",
            "year": 2007
        },
        {
            "authors": [
                "M. Stonebraker",
                "A. Weisberg"
            ],
            "title": "The VoltDB Main Memory DBMS",
            "venue": "IEEE Data Engineering Bulletin, 36(2):21\u201327, June",
            "year": 2013
        },
        {
            "authors": [
                "R. Taft",
                "E. Mansour",
                "M. Serafini",
                "J. Duggan",
                "A.J. Elmore",
                "A. Aboulnaga",
                "A. Pavlo",
                "M. Stonebraker"
            ],
            "title": "E-store: Fine-grained Elastic Partitioning for Distributed Transaction Processing Systems",
            "venue": "PVLDB, 8(3):245\u2013256,",
            "year": 2014
        },
        {
            "authors": [
                "D.B. Terry",
                "A.J. Demers",
                "K. Petersen",
                "M. Spreitzer",
                "M. Theimer",
                "B.W. Welch"
            ],
            "title": "Session Guarantees for Weakly Consistent Replicated Data",
            "venue": "Proceedings of the Third International Conference on Parallel and Distributed Information Systems, PDIS \u201994, pages 140\u2013149,",
            "year": 1994
        },
        {
            "authors": [
                "D.B. Terry",
                "M.M. Theimer",
                "K. Petersen",
                "A.J. Demers",
                "M.J. Spreitzer",
                "C.H. Hauser"
            ],
            "title": "Managing update conflicts in Bayou, a weakly connected replicated storage system",
            "venue": "Proceedings of the Symposium on Operating Systems Principles, SOSP \u201995,",
            "year": 1995
        },
        {
            "authors": [
                "A. Thomson",
                "D.J. Abadi"
            ],
            "title": "The case for determinism in database systems",
            "venue": "PVLDB, 3(1):70\u201380,",
            "year": 2010
        },
        {
            "authors": [
                "A. Thomson",
                "D.J. Abadi"
            ],
            "title": "Modularity and Scalability in Calvin",
            "venue": "IEEE Data Engineering Bulletin, 36(2): 48-55,",
            "year": 2013
        },
        {
            "authors": [
                "A. Thomson",
                "D.J. Abadi"
            ],
            "title": "Deterministic database systems",
            "venue": "US Patent 8700563,",
            "year": 2014
        },
        {
            "authors": [
                "A. Thomson",
                "D.J. Abadi"
            ],
            "title": "CalvinFS: Consistent WAN Replication and Scalable Metadata Management for Distributed File Systems",
            "venue": "FAST, pages 1\u201314,",
            "year": 2015
        },
        {
            "authors": [
                "A. Thomson",
                "T. Diamond",
                "P. Shao",
                "K. Ren",
                "S.-C. Weng",
                "D.J. Abadi"
            ],
            "title": "Calvin: Fast distributed transactions for partitioned database systems",
            "venue": "Proceedings of the 2012 International Conference on Management of Data, SIGMOD \u201912,",
            "year": 2012
        },
        {
            "authors": [
                "A. Thomson",
                "T. Diamond",
                "S.-C. Weng",
                "K. Ren",
                "P. Shao",
                "D.J. Abadi"
            ],
            "title": "Fast Distributed Transactions and Strongly Consistent Replication for OLTP Database Systems",
            "venue": "ACM Transactions on Database Systems (TODS), 39(2):11:1\u201311:39,",
            "year": 2014
        },
        {
            "authors": [
                "A. Verbitski",
                "A. Gupta",
                "D. Saha",
                "M. Brahmadesam",
                "K. Gupta",
                "R. Mittal",
                "S. Krishnamurthy",
                "S. Maurice",
                "T. Kharatishvili",
                "X. Bao"
            ],
            "title": "Amazon aurora: Design considerations for high throughput cloud-native relational databases",
            "venue": "Proceedings of the 2017 ACM International Conference on Management of Data, SIGMOD \u201917, pages 1041\u20131052,",
            "year": 2017
        },
        {
            "authors": [
                "A. Verbitski",
                "A. Gupta",
                "D. Saha",
                "J. Corey",
                "K. Gupta",
                "M. Brahmadesam",
                "R. Mittal",
                "S. Krishnamurthy",
                "S. Maurice",
                "T. Kharatishvilli",
                "X. Bao"
            ],
            "title": "Amazon aurora: On avoiding distributed consensus for i/os, commits, and membership changes",
            "venue": "Proceedings of the 2018 International Conference on Management of Data, SIGMOD \u201918, pages 789\u2013796,",
            "year": 2018
        },
        {
            "authors": [
                "W. Vogels"
            ],
            "title": "Eventually Consistent",
            "venue": "Queue, 6(6):14\u201319, Oct.",
            "year": 2008
        },
        {
            "authors": [
                "S.-H. Wu",
                "T.-Y. Feng",
                "M.-K. Liao",
                "S.-K. Pi",
                "Y.-S. Lin"
            ],
            "title": "T-Part: Partitioning of Transactions for Forward-Pushing in Deterministic Database Systems",
            "venue": "Proceedings of the 2016 International Conference on Management of Data, SIGMOD \u201916, pages 1553\u20131565,",
            "year": 2016
        },
        {
            "authors": [
                "X. Yan",
                "L. Yang",
                "H. Zhang",
                "X.C. Lin",
                "B. Wong",
                "K. Salem",
                "T. Brecht"
            ],
            "title": "Carousel: Low-latency transaction processing for globally-distributed data",
            "venue": "Proceedings of the 2018 International Conference on Management of Data, SIGMOD \u201918, pages 231\u2013243,",
            "year": 2018
        },
        {
            "authors": [
                "I. Zhang",
                "N.K. Sharma",
                "A. Szekeres",
                "A. Krishnamurthy",
                "D.R.K. Ports"
            ],
            "title": "Building consistent transactions with inconsistent replication",
            "venue": "ACM Transactions on Computer Systems (TOCS), 35(4), Dec.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Zhang",
                "R. Power",
                "S. Zhou",
                "Y. Sovran",
                "M.K. Aguilera",
                "J. Li"
            ],
            "title": "Transaction chains: achieving serializability with low latency in geo-distributed storage systems",
            "venue": "Proceedings of the Symposium on Operating Systems Principles, SOSP \u201913,",
            "year": 2013
        }
    ],
    "sections": [
        {
            "text": "PVLDB Reference Format: Kun Ren, Dennis Li, and Daniel J. Abadi. SLOG: Serializable, Low-latency, Geo-replicated Transactions. PVLDB, 12(11): 1747-1761, 2019. DOI: https://doi.org/10.14778/3342263.3342647"
        },
        {
            "heading": "1. INTRODUCTION",
            "text": "Many modern applications replicate data across geographic regions in order to (a) achieve high availability in the event of region failure and (b) serve low-latency reads to clients spread across the world. Existing database systems that support geographic replication force the user to give up one at least one of the following essential features: (1) strict serializability (2) low-latency writes (3) high throughput multi-region transactions \u2014 even under contention.\n(1) Strict serializability [16, 17, 41, 64], in the context of distributed database systems, implies both strong isolation (one-copy serializable [10]) and real-time ordering guarantees. More precisely, concurrent transaction processing must be equivalent to executing transactions in a one-copy serial order, S, such that for every pair of transactions X and Y, if X starts after Y completes, then X follows Y in S. This implies that all reads within a transaction must see the value of any writes that committed before the transaction began, no matter where that write was performed world-wide. Furthermore, if a transaction, A, begins after (in real time) transaction B completes, no client can see the effect of A without the effect\nThis work is licensed under the Creative Commons AttributionNonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. For any use beyond those covered by this license, obtain permission by emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights licensed to the VLDB Endowment. Proceedings of the VLDB Endowment, Vol. 12, No. 11 ISSN 2150-8097. DOI: https://doi.org/10.14778/3342263.3342647\nof B. Strict serializability reduces application code complexity and bugs, since it behaves like a system that is running on a single machine processing transactions sequentially [4, 5].\nBy giving up strict serializability, it is straightforward to achieve the other two properties mentioned above (low-latency writes and high multi-region transactional throughput). If reads are allowed to access stale data, then reads can be served by any replica (which improves read latency) and replication can be entirely asynchronous (which improves write latency) . Furthermore, avoiding the multiregion coordination necessary to enforce strict serializability facilitates throughput scalability of distributed database systems [11].\nMuch research effort has been spent in designing systems that reduce the consistency level below strict serializability, while still providing useful guarantees to the application. For example, Dynamo [26], Cassandra [47], and Riak [3] use eventual consistency; PNUTs [20] supports timeline consistency; COPS [51], Eiger [52], and Contrarian [27] support a variation of causal consistency; Lynx (Transaction Chains) [95] supports non-strict serializability with read-your-writes consistency [81]; Walter [77], Jessy [8], and Blotter [56] support variations of snapshot consistency.\n(2) Low latency writes. Despite the suitability of these weaker consistency models for numerous classes of applications, they often expose applications to potential race condition bugs, and typically require skilled application programmers. Thus, the demand for systems that support strict serializability has only increased. Many recent geo-replicated data stores, including from two of the three major cloud vendors (Google with several systems [12, 22, 76] and Microsoft with Cosmos DB) support strong consistency models at least under some configurations. Spanner [22] is widely used throughout Google, and is now available in the Google Cloud for anybody to use. Other examples in industry include TiDB, comdb2, FaunaDB, and (with a few caveats) CockroachDB and YugaByte. Examples from the research community include Helios [59], MDCC [46], Calvin [87], Carousel [93], and TAPIR [94].\nEvery single one of these above cited strictly serializable systems pays at least one cross-region round trip (coordination) message to commit a write transaction. This type of coordination enables strict serializability, but increases the latency of every write. The farther apart the regions, the longer it takes to complete a write.\n(3) High transaction throughput Cross-region coordination on every write is not necessary to guarantee strict serializability. If every read of a data item is served from the location of the most recent write to that data item, then there is no need to synchronously replicate writes across regions. For example, if one region is declared as the master region for a data item ([20, 23, 38, 61, 66, 67]), and all writes and consistent reads of that data item are directed to this region, such as done by NuoDB [66, 67] and G-Store [23], then it is possible to achieve strict serializability along with low latency\nreads and writes \u2014 as long as those reads and writes initiate from a region near the master region for the data being accessed.\nMany workloads have a locality in their access patterns. For example, for applications that revolve around users, the data associated with a user is heavily skewed to being accessed from the physical location of that user. Thus, for these workloads, it is possible to achieve strict serializability and low latency writes. However, arbitrary transactions may access multiple data items, where each data item is mastered at a different region. For such transactions, achieving strict serializability requires coordination across regions. Existing approaches prevent conflicting transactions from running during this coordination. Cross-region coordination is time consuming since messages must be sent over a WAN. Thus, the time window that conflicting transactions cannot run is large, which reduces system throughput. For example, we implemented a version of NuoDB1, and ran some benchmarks (see Section 4) where we varied the contention level and percentage of multi-region transactions. As shown in Figure 1, under high contention, the throughput of the system dropped dramatically at even low percentages of multi-region transactions, and also fell (although less dramatically) at low contention. For this reason, many systems that allow different data to be controlled by different regions do not support multi-region transactions, such as PNUTS [20] and DPaxos [58].\nIn this paper, we present the design of a system, SLOG, that is the first (to the best of our knowledge) to achieve all three: (1) strict serializability (2) low-latency reads and writes (on at least some transactions) and (3) high throughput. SLOG uses locality in access patterns to assign a home region to each data granule. Reads and writes to nearby data occur rapidly, without cross-region communication. However, reads and writes to remote data, along with transactions that access data from multiple regions, must pay crossregion communication costs. Nonetheless, SLOG uses a deterministic architecture to move most of this communication outside of conflict boundaries, thereby enabling these transactions to be processed at high throughput, even for high contention workloads.\nSLOG supports two availability levels: one in which the only synchronous replication is internally within a home region (which is susceptible to unavailability in the event of an entire region failure), and one in which data is synchronously replicated to one or more nearby regions (or availability zones) to achieve an availabil-\n1NuoDB declined to give us their software, so we implemented our own version. NuoDB uses a MVCC protocol that is susceptible to write skew anomalies. Our version used locking instead in order to guarantee strict serializability.\nity similar to Amazon Aurora [89, 90] where the system remains available even in the event of a failure of an entire region. Unlike Spanner, Cosmos DB, Aurora, or the other previously cited systems that support synchronous cross-region replication, SLOG\u2019s deterministic architecture ensures that its throughput is unaffected by the presence of synchronous cross-region replication."
        },
        {
            "heading": "2. BACKGROUND",
            "text": "SLOG supports strictly serializable transactions that access data mastered in multiple regions. Unlike systems such as L-Store [49] and G-Store [23], which remaster data on the fly so that all data accessed by a transaction becomes mastered at the same physical location, SLOG does not remaster data as part of executing a transaction. Instead, it utilizes a coordination protocol across regions to avoid data remastership. One important technique used by SLOG to overcome the scalability and throughput limitations caused by coordination across partitions is to leverage a deterministic execution framework. Our technique is inspired by the work on Lazy Transactions [30], Calvin [87, 88], T-Part [92], PWV [31], and FaunaDB [1] which use determinism to move coordination outside of transactional boundaries, thereby enabling conflicting transactions to run during this coordination process.\nIn the above-cited deterministic systems, all nodes involved in processing a transaction \u2014 all replicas and all partitions within a replica \u2014 run an agreement protocol prior to processing a batch of transactions that plans out how to process the batch. This plan is deterministic in the sense that all replicas that see the same plan must have only one possible final state after processing transactions according to this plan. Once all parties agree to the plan, processing occurs (mostly) independently on each node, with the system relying on the plan\u2019s determinism in order to avoid replica divergence. This approach prevents the dramatic reductions in throughput under data contention that are observed in systems such as NuoDB [66, 67] and the strictly serializable transaction implementation on top of FuzzyLog [53] that perform coordination inside transactional boundaries, and G-Store and L-Store which prevent contended data access during the remastering operations.\nDeterminism also reduces latency and improves throughput by eliminating any possibility of distributed and local deadlock [7, 70, 78, 79, 87], and reducing (or eliminating) distributed commit protocols such as two-phase commit [7, 83, 84, 86, 87].\nUnfortunately, in order to create a deterministic plan of execution, more knowledge about the transaction is needed prior to processing it relative to traditional nondeterministic systems. Most importantly, the entire transaction must be present during this planning process. This makes deterministic database systems a poor fit for ORM tools and other applications that submit transactions to the database in pieces.\nSecond, advanced planning usually requires knowledge regarding which data will be accessed by a transaction [33, 34, 83, 87]. Most deterministic systems do not require the client to specify this information when the transaction is submitted. Instead they attempt to statically derive this knowledge from inspecting the transaction code [87], make conservative estimates [31], and/or speculatively execute parts of the transaction, such as the OLLP protocol in Calvin or the multi-stage execution process in FaunaDB.\nSLOG\u2019s use of determinism causes it to inherit both of these requirements. As we will describe in Section 4, we implement SLOG inside the open source version of Calvin\u2019s codebase. Since Calvin uses a combination of static analysis and OLLP to determine read and write sets prior to transaction execution, our implementation also uses these techniques.\nOne important challenge in the design of SLOG is the lack of information about all transactions running in the system during the planning process. In Calvin, every transaction, no matter where it originates from, is sequenced by a global Paxos process. This enables Calvin to have complete knowledge of the input to the system while planning how a batch of transactions will be executed. However, this comes at the cost of requiring every transaction to pay the cross-region latency to run Paxos across regions. SLOG removes the global Paxos process in order to reduce latency, but this causes unawareness of transactions submitted to replicas located in different regions during the planning of transaction processing."
        },
        {
            "heading": "3. SLOG",
            "text": "In order to achieve low latency writes, replication across geographic regions must be performed either entirely asynchronously, or synchronously only to nearby regions; otherwise every write must pay a round trip network cost across the geographic diameter of the system [6]. In order to maintain strong consistency in the face of asynchronous replication, SLOG uses a master-oriented architecture, where every data item is mastered at a single \u201chome\u201d replica. Writes and linearizable reads of a data item must be directed to its home replica.\nA region in SLOG is a set of servers that are connected via a low-latency network, usually within the same data center. Transactions are classified into two categories: if all data that will be accessed have their master replica in the same region, it is singlehome. Otherwise, it is multi-home. Single-home transactions are sent to its home and are confirmed as completed either (a) after the region completes the transaction (SLOG-B) or (b) after the region completes the transaction AND it has been replicated to a configurable number of nearby regions (SLOG-HA). Either way, transactions that initiate near the home region of the data being accessed will complete without global coordination. In contrast, multi-home transactions, and even single-home transactions that initiate physically far from that home, will experience larger latency. SLOG does not require any remastering of data to process multi-home transactions, but does perform dynamic data remastering as access frequency from particular locations changes over time (Section 3.4).\nThe most technically challenging problem in the design of SLOG is the execution of multi-home transactions. The goal is to maintain strict serializability guarantees and high throughput in the presence of potentially large numbers of multi-home transactions, even though they may access contended data, and even though they require coordination across physically distant regions."
        },
        {
            "heading": "3.1 High-level overview",
            "text": "Each SLOG region contains a number of servers over which data stored at that region is partitioned (and replicated). Some of this data is mastered by that region (it is the home region for that data) and the rest is a replica of data from a different home region. The partitioning of the data across servers within a region is independent of the home status of data \u2014 each partition will generally have a mix of locally-mastered and remotely-mastered data.\nEach region maintains a local input log which is implemented via Paxos across its servers. This local log only contains transactions that are expected to modify data mastered at that region. This input log is sent to all other regions in batches. Each batch is labeled with a sequence number, and the other regions use this sequence number to ensure that they have not missed any batches from that region. Thus, each region is eventually able to reconstruct the complete local log from every other region, potentially with some delay in the event of a network partition. Regions use deterministic transaction processing (see Section 2) to replay the local log from other regions\nin parallel (at the same speed as the original processing), while ensuring that their copy of the data ends up in the same final state as the original region that sent this log.\nData replication across regions is not strictly necessary in SLOG since the master region for a data item must oversee all writes and linearizable reads to it. However, by continuously replaying local logs from other regions, the system is able to support local snapshot reads of data mastered at other regions at any desired point in the versioned history. Furthermore, by keeping the replicas close to up to date, the process of dynamically remastering a data item is cheaper (see Section 3.4).\nIf there are n regions in the system, each region must process its own input log, in addition to the (n \u2212 1) input logs of the other regions2. If all transactions are single-home, then it is impossible for transactions in different logs to conflict with each other, and the processing of these input logs can be interleaved arbitrarily without risking violations of strict serializability or replica non-divergence guarantees. However, achieving strict serializability in the presence of multi-home transactions is not straightforward, since coordination is required across regions.\nIn Sections 3.2 and 3.3, we explain how SLOG processes transactions in more detail. We start by discussing single-home transactions, and then discuss multi-home transactions."
        },
        {
            "heading": "3.2 (Assumed) Single-home transactions",
            "text": "Data is assigned home regions in granules which can either be individual records, or a sorted range of records. SLOG associates two pieces of metadata with each granule: the identifier of its master region at the time it was written, and a count of the total number of times its master has (recently) changed. This metadata is stored in the granule header and replicated with the data granule. The counter is used to ensure correctness of remastering (Section 3.4) and never needs to exceed the number of regions in the deployment. The counter cycles back to 0 after it reaches a max count, and an extra bit is used to detect wrap-around, so the number of bits needed for the counter is always one more than the number of bits needed for the region identifier. Thus, these two pieces of metadata can be combined into a single 8 bit integer for deployments across 8 regions or fewer. Each region contains a distributed index called the \u201cLookup Master\u201d which maintains a mapping of granule ids to this 8-bit value in the granule header. This index is asynchronously updated after a metadata change and thus may return stale metadata.\nClients can send transactions to the nearest region whether or not it is the home for the data accessed by the transaction. As described in Section 2, we assume that a region can determine the precise set of granules that will be accessed by a transaction, before processing it, either directly or through the advanced techniques we cited.\nWhen a region receives a transaction to process, it sends a message to its Lookup Master to obtain its cached value for the home of each granule accessed by the transaction. The returned locations are stored inside the transaction code. If every granule is currently mapped to the same home region, the transaction becomes initially assumed to be a single-home transaction, and is shipped to that region. Otherwise, it is assumed to be multi-home, and handled according to the algorithm described in Section 3.3. This functionality is shown in the ReceiveNewTxn pseudocode in Figure 2.\nOnce the (assumed) single-home transaction reaches its home region, it is appended into an in-memory batch of transactional input on the server at which it arrives, and this server appends this batch to that region\u2019s input log via Paxos. This corresponds to the\n2For ease of exposition, we will assume that each region contains a complete copy of all data stored across the system. SLOG can also work correctly with incomplete replication.\nbeginning and end of InsertIntoLocalLog in Figure 2. A separate Paxos process interleaves batches from the local log with batches from the local logs that are received from other regions in order to create that region\u2019s view of the global log. This is shown in the ReceiveBatch function. The pseudocode for InsertIntoLocalLog and ReceiveBatch in Figure 2 are written as if they are being called on the region as a whole. The details of how these functions are mapped to individual machines at a region are not shown, to make the pseudocode easier to read.\nProcessing of transactions by the different servers within that region works similarly to distributed deterministic database systems\nsuch as Calvin: data is partitioned across servers, and each server reads transactions from the same global log and requests locks on any data present in its local partition in the order that the transactions making those requests appear in the log [69, 71, 87] (see the lock manager code in Figure 2). Locks are granted in the order they are requested, which, when also ordering lock requests by the order in which transactions appear in the global log, makes deadlock impossible [68]. The only difference relative to traditional deterministic database systems is that prior to reading or writing a data granule, the metadata associated with that granule must be checked for the current home and counter values. If the counter that is found is different than the counter that was returned from the Lookup Master when the transaction was originally submitted to the system, then the transaction is aborted and restarted.\nDeterministic systems ensure that all data progress through the same sequence of updates at every replica, without runtime coordination. Since home metadata is part of the data granule, the metadata is updated at the same point within the sequence of updates of that granule at every region. Therefore, any assumed single-home transaction that is not actually single-home will be exposed as nonsingle-home independently at each region (i.e., each region will independently, without coordination, observe that it is not singlehome, and will all agree to abort the transaction without any crossregion communication). The transaction is then restarted as a multihome transaction. Similarly, if an assumed single-home transaction is indeed single-home, but the assumed home is incorrect, all regions will see the incorrect home and counter annotation and independently agree to abort the transaction.\nIn SLOG-B, the client is notified that a transaction has committed as soon as the first region commits it. For SLOG-HA, the client cannot be notified of the commit until the batch of the input log that contains this transaction has been replicated to a configurable number of nearby regions. Since only the input is replicated, replication latency can be overlapped with transaction processing.\nFigure 3 shows an example of this process. Each region contains a complete copy of the database, with its \u201chome\u201d granule shown\nin red and underlined. T1 and T2 update granule A and therefore are sent to region 0 (batch 0-1 is created that contains T1 and T2); T3 and T4 update granule B and are sent to region 1 (which places them in separate batches \u2014 T3 is in batch 1-1, and T4 in batch 1- 2). Region 0 sends batch 0-1 to the other two regions, and region 1 sends batch 1-1 and batch 1-2 to the other two regions. Thus all three batches appear in the global log of all three regions. However, the order in which these batches appear in the three respective global logs is different. The only guarantee is that 1-2 will appear after 1-1, since they originated from the same region.\nIf all transactions are single-home, then it is guaranteed that each region\u2019s local log will access a disjoint set of database system granules (i.e., transactions across local logs do not conflict with each other). Therefore, the limited degree to which the global logs are ordered differently across different regions will not cause replica divergence. However, if there exist multi-home transactions in the workload, these differences may result in replica divergence. For example, if T2 accessed both A and B (instead of just A in our example above), the order of processing T2 and T3 now makes a difference since they both access B. Since T2 and T3 may appear in different orders in the global logs, there is a risk of region divergence. We will now discuss how SLOG deals with this problem."
        },
        {
            "heading": "3.3 Multi-home transactions",
            "text": "All multi-home transactions, no matter where they originate, must be ordered with respect to each other. A variety of global ordering algorithms could be used, such as (1) Using a single ordering server [19], (2) Running Paxos across all regions [87], or (3) Sending all multi-home transactions to the same region to be ordered by the local log there. Our current implementation uses the third option for simplicity; however, the second option is more robust to failure or unavailability of an entire region. The last line of ReceiveNewTxn in Figure 2 is vague enough to allow for either option. Either way, all regions receive the complete set of multi-home transactions in their proper order via calls to InsertIntoLocalLog.\nRecall that the assumed home information for each granule is stored inside the transaction by ReceiveNewTxn. InsertIntoLocal-\nLog checks to see if the region at which the code is running is the assumed home for any of the granules that will be accessed. If so, it generates a special kind of transaction, called a LockOnlyTxn, that consists only3 of locking the local reads and local writes of the multi-home transaction.\nA LockOnlyTxn is similar to a single-home transaction \u2013 it only involves local granules, and it is placed in the local log of its home region (alongside regular single-home transactions) which is replicated (eventually) into the global log of every other region. The only difference is that they do not have to include executable code. The code for the multi-home transaction can arrive separately \u2014 as part of the local log from the multi-home transaction ordering module, which eventually gets integrated into the global log at every region. LockOnlyTxns exist to specify how the multi-home transaction should be ordered relative to single-home transactions at that region. Depending on where the LockOnlyTxn gets placed in the local log of that region, it will ensure that the multi-home transaction will observe the writes of all transactions earlier than it in the local log, and none of the writes from transactions after it.\nThis leads to a difference in execution of single-home vs. multihome transactions. Single-home transactions appear once in a region\u2019s global log. The lock manager thread reads the single-home transaction from the global log and acquires all necessary locks before handing over the transaction to an execution thread for processing. In contrast, a multi-home transaction exists in several different locations in a region\u2019s global log. There could be an entry containing the code for that transaction, and then separately (usually later in the log), several LockOnlyTxns \u2014 one from each region that houses data expected to be accessed by that transaction. The code can start to be processed as soon as it is reached in the global log. However, the code will block whenever it tries to access data for which the corresponding LockOnlyTxn has yet to complete. The bottom half of Figure 2 shows the pseudocode for this process.\nFigure 4 shows an example of multi-home transaction processing. The home information and set of transactions submitted to the system are the same as in Figure 3. The only difference is that T2 now accesses A and B and is thus multi-home. Whichever region receives T2 from the client annotates it with the assumed home information (which, in this example, is correct) and sends it to the multi-home ordering module. At Region 0, InsertIntoLocalLog(T2) is called after it has placed single-home T1 into its local log. It therefore places its generated LockOnlyTxn for T2 after T1. LockOnlyTxns for a given multi-home transaction are shown in the figure with the subscript of the region that generated it4. InsertIntoLocalLog(T2) is called at Region 1 between placing single home transactions T3 and T4 into its local log and thus places the LockOnlyTxn for T2 it generates there. InsertIntoLocalLog(T2) is also called at Region 2 but no LockOnlyTxn is generated since it was not listed as the home for any data accessed by that transaction. The local logs are replicated to every other region and are interleaved differently, which results in T2\u2019s LockOnlyTxns being ordered differently at each region. This is not problematic since LockOnlyTxns always access disjoint data and thus commute.\nRegions do not diverge since (1) the global log at each region maintains the original order of the local logs that it interleaves, (2) local logs from different regions access disjoint data (unless the\n3In some cases, a LockOnlyTxn may include a limited amount of code, such as non-dependent reads/writes of the locked data. 4The global log entries that contain T2\u2019s code are not shown in the figure, since the code blocks anyway until the LockOnlyTxns are reached. Furthermore, in some cases, code can be included in LockOnlyTxns, which makes the separate shipping of transaction code unnecessary.\nhome for a data granule has moved \u2013 see Section 3.4), and (3) the deterministic processing engine ensures that transactions are processed such that the final result is equivalent to the result of processing them in the serial order present in that region\u2019s global log.\n3.3.1 Proof of strict serializability Serializability theory analyzes the order in which reads and writes\noccur to data in a database. This order is usually called a schedule [29], history [63], or log [16]. Serializability proofs take the form of proving that a given schedule, in which read and writes from different transactions may be interleaved with each other, is equivalent to a serial schedule. In replicated systems, the equivalence must be to a 1-copy serial schedule [10].\nDefine the relationship Ti reads-x-from Tj as done by Bernstein and Goodman [14]: Ti reads-x-from Tj holds if transaction Tj writes to x, and no other transaction writes to it before Ti reads from it. Two schedules are equivalent if they have the same set of reads-from relationships \u2014 for all i, j, and x, Ti reads-x-from Tj in one schedule iff this relationship holds in the other [14].\nA serial schedule is one-copy equivalent (1-serial) if for all i, j, and x, if Ti reads-x-from Tj then Tj is the last transaction prior to Ti that writes to any copy of x [10, 14].\nSLOG has one global log per region. Even though each global log contains the same set of transactions, each region may process its global log according to a different schedule.\nLEMMA 3.1. All region schedules are equivalent A SLOG region requests and acquires locks in the order that transactions appear in its global log. Therefore, when a transaction Ti reads x, the Tj for which Ti reads-x-from Tj will hold is the closest previous transaction to Ti in the global log that writes x. Since there is only one master region for a given x, and all reads and writes to x are found in the local log for its master region, this Tj will come from the same local log as Ti. Thus, the Tj for which Ti reads-x-from Tj will hold will be the same in all global logs, since each global log orders batches from any given local log identically.\nLEMMA 3.2. All serializable schedules in SLOG are 1-copy serializable. We already explained in Lemma 3.1 that if Ti readsx-from Tj holds, then Ti and Tj are in the same local log and Tj is the closest previous transaction to Ti in that local log that writes x. It is impossible for any other local log to contain a transaction that writes to x since only the master region for a data item can include transactions that write to x in its local log.\nLess formally: a 1-serial schedule represents a serial execution of transactions in which the replicated copies of each data item behave like a single logical copy of that data item [10]. In SLOG, this is enabled by ensuring that all writes and reads (excluding read-only snapshot transactions running at a reduced consistency level) to a data item (granule) are managed by its master.\nSince all region schedules are equivalent and all serializable schedules in SLOG are 1-copy serializable, all we have to prove is that the schedule from any region is strictly serializable. This is straightforward since any schedule produced by a 2PL implementation, if locks are held until after the transaction commits, and all reads read the value of the most recent write to that data item, then the resulting schedule is strictly serializable [16, 64]. SLOG\u2019s deterministic locking scheme acquires all locks prior to processing a transaction and releases all locks after commit. Thus it is a form of 2PL.\nSLOG actually guarantees external consistency [37] which is slightly stronger than strict serializability. External consistency enforces an ordering of all transactions \u2014 even concurrent ones \u2014 such that the execution is equivalent to executing the transactions\nserially in commit time order. SLOG commits conflicting transactions in the order they appear in each local log (but does not order commit times across local logs). Since all conflicting transactions appear in the same local log, the order in the log specifies their equivalent serial order."
        },
        {
            "heading": "3.4 Dynamic remastering",
            "text": "As access patterns change, it may become desirable to move the home of a data granule, which we refer to as a \u201cremaster\u201d operation. Previous work has shown that simple algorithms are sufficient for deciding when to remaster data. For example, PNUTS changes a data item\u2019s master region if the last 3 accesses to it were from a region other than its master [20]. Tuba similarly uses a simple cost model based on recent accesses to decide when to remaster [9]. The challenge in SLOG is not when to remaster (SLOG uses the PNUTS heuristic). Rather the challenge is how to remaster data without bringing the system offline, while maintaining strict serializability and region non-divergence guarantees and high throughput.\nAs specified in Section 3.2, SLOG stores two metadata values with each data granule: the identifier of its master region at the time it was written, and a count of the number of times its master has changed. A remaster request modifies both parts of this metadata, and can, for the most part, be treated similarly to any other write request. The request is sent to the home region for that granule, which will insert the request in its local log, which will eventually cause the request to appear in the global logs of all regions. When each region processes this request in its global log, it updates the granule metadata. The Lookup Master of that region is also asynchronously updated to reflect the new mastership information.\nA remaster request writes to a single data granule and is thus a single-home transaction. It is sent to its current home region potentially concurrently with other requests to access data within the granule. The order that these requests are placed in the local log of this home region determines which ones will be successful. Transactions that are placed in the local log after the remaster request will see the new granule metadata when they access it, and will abort and resubmit to the new master.\nRemastering causes a particular danger to the non-divergence guarantees of SLOG. As mentioned in Section 3.2, the first SLOG region to receive a transaction annotates it with the (home,counter) metadata that was returned from its LookupMaster cache for every granule that it accesses, and stores this cached metadata inside the transaction code. Regions use this cached information to decide whether they are responsible for handling it entirely (if it is singlehome) or creating a LockOnlyTxn for it (multi-home). Once the LookupMaster at a region is updated after the region has processed the remaster request, it will start annotating transactions with the new master information, even though other regions may not have processed the remaster request yet. These (up-to-date) annotations will cause the transaction to be placed into the local log of the new home region for that granule. However, the remaster request itself is processed by the local log of the old home region. This leads to a potential race condition across the local logs. For example, assume a transaction, T, accesses the remastered granule and is sent to the new master region. Some regions may place the log batch that contains T before the log batch that contains the remaster request, while other regions may place it afterwards. This is possible because the local logs from different regions can be interleaved differently in the global log at different regions. As long as T and the remaster request are in different local logs, no assumptions can be made about their relative order in each region\u2019s global log. It is true that the remaster request was submitted to SLOG first, and must be completed by at least one region before T can possibly come into\nexistence, so most of the time T will appear after the remaster request in every global log. However, this cannot be guaranteed. If the commit or abort decision of T is dependent on how a region interleaves local logs, replica divergence may occur.\nAn example of this divergence danger is shown in Figure 5. In this example, replica 0 receives transaction T1, and annotates the transaction with the current home for the accessed granule B \u2014 region 1 \u2014 and sends it there. After replica 1 executes T1, the threshold necessary to remaster granule B to replica 0 is reached, and a remaster transaction Tremaster is generated and sent to replica 1. Tremaster is thus inserted into replica 1\u2019s local log. Meanwhile, replica 1 receives two other transactions that also access B: T2 and T3. The relative location of the Tremaster transaction in replica 1\u2019s local log is after T2 but before T3. After execution of Tremaster , the home for B will have changed, and T3 is now in the wrong local log. Since all regions see transactions from the same local log in the same order, all regions will see T3 after Tremaster and independently decide to abort it. The end result is that T3 gets aborted and resubmitted to SLOG as T3new, which gets placed into the local log of B\u2019s new home: region 0. The problem is: local logs from different regions may be interleaved differently. Regions 0 and 1 place the local log from region 0 that contains T3new after the local log entry from region 1 that contains T2. Thus, they see the serial order: T1, T2, and then T3. However, region 2 places the local log from region 0 that contains T3new before the local log entry from region 1 that contains T2. Thus, it sees a different serial order: T1, T3, T2. This leads to potential replica divergence.\nThe counter part of the metadata is used to circumvent this danger. Prior to requesting a lock on a granule, SLOG compares the counter that was written into the transaction metadata by the LookupMaster at the region the transaction was submitted to with the current counter in storage at that region. If the counter is too low, it can be certain that the LookupMaster that annotated the transaction had out of date mastership information, and the transaction is immediately aborted (every region will independently come to this same conclusion). If the counter is too high, the transaction blocks until the region has processed a number of remaster requests for that data granule equal to the difference between the counters.\nPerforming counter comparisons prior to lock acquisition instead of afterwards ensures that the lock is available for the remaster request, which needs the lock to update the header. However, the\nchecks must be repeated after lock acquisition. The pseudocode presented in Figure 6 outlines how counter checks are performed prior to requesting locks, and the end of the pseudocode in Figure 2 shows how these checks are performed again during execution."
        },
        {
            "heading": "4. EXPERIMENTAL RESULTS",
            "text": "Since SLOG relies on deterministic processing to avoid two phase commit and achieve high throughput even when there are many multi-home transactions in a workload, we implemented SLOG via starting with the open source Calvin codebase [2], adding the two Paxos logs per region and the Lookup Master index, and processing transactions according to the specification outlined in Section 3.\nAs described in Section 1, SLOG is the first system to achieve (1) strict serializability (2) low latency writes (on at least some transactions and (3) high throughput transactions even under high conflict workloads. Throughput and latency are performance measures, which we investigate in this section. We only compare SLOG to other systems that achieve strict serializability. Our two primary comparison points are architectures that achieve low latency writes or high throughput, but not both. More specifically, we built a nondeterministic system that is based on the design of NuoDB: different data is mastered in different regions (NuoDB calls this \u201cchairmanship\u201d) and all writes and reads to data are controlled by the chairman. Transactions that access data chaired by different machines require coordination across these machines. NuoDB uses an MVCC algorithm that is susceptible to write skew anomalies (and thus is not serializable). In order to guarantee strict serializability, we use instead a traditional distributed 2PL algorithm where the lock manager for a granule is located in its home partition. We will call this implementation 2PL-coord. We expect 2PL-coord to achieve low-latency writes (reads and writes to data that are mastered nearby can complete without coordination), but poor throughput (as explained in Section 1).\nSince we started with the original Calvin codebase, we use Calvin as our second comparison point. By moving almost all commit and replication coordination outside of transactional boundaries, Calvin achieves high throughput even for high conflict workloads [70, 87]. However, the latency of every transaction is at least equal to the latency of multi-region Paxos that is run prior to every transaction processed by the system. We do not expect SLOG to achieve the\nsame throughput as Calvin for two reasons: (1) Multi-home transactions cause the conflicting intermediate transactions between two of its LockOnlyTxns to potentially block. (2) The ability of the system to remaster granules dynamically causes more checks and potential aborts for each transaction. On the other hand, SLOG is expected to yield much better latency than Calvin for workloads containing many single-home transactions. We thus ran some experiments to more precisely explore these differences in throughput and latency between Calvin and SLOG.\nAs a third comparison point, we use Cloud Spanner. Cloud Spanner is an entirely separate code base than Calvin, and is far more production-ready and feature-complete. Thus, this is not an applesto-apples comparison, and raw magnitude performance differences mean little. Nonetheless, information can be gleaned from the performance trends of each system relative to itself as experimental variables are varied, which shed light on the performance consequences of the architectural differences between the systems.\nFor these experiments, we used system deployments that replicated all data three ways across data-centers. All experiments (except for the Cloud Spanner experiments which we will discuss later) were run on EC2 x1e.2xlarge instances (each instance contains 244 GB of memory, and 8 CPU cores) across six geographical regions: US-East (Virginia), US-East (Ohio), US-West (Oregon), US-West (California), EU (Ireland) and EU (London). The database system was partitioned across 4 machines within each region. For SLOGHA (the HA mode from Section 3 that is tolerant to failure of an entire region), the US-East(Virginia) region is synchronously replicated to US-East (Ohio); US-West (Oregon) to US-West (California); and EU (Ireland) to EU (London).\nWe use a version of the Yahoo Cloud Serving Benchmark (YCSB) [21] adapted for transactions, and also the TPC-C New Order transaction to benchmark these systems. In both cases, each record (tuple) is stored in a separate SLOG granule, and we therefore use the terms \u201crecord\u201d and \u201cgranule\u201d interchangeably in this section. We vary the important parameters that can affect system performance relative to each other: likelihood of transactions to lock the same records (i.e., conflict ratio between transactions), percentage of transactions that are multi-partition within a region, and the ratio of single-home to multi-home transactions.\nFor YCSB, we use a table of 8.8 billion records, each 100 bytes. Since we have 4 EC2 instances per region, each instance contains\na partition consisting of 2.2 billion records. Each region contains a complete replica of the data. Each transaction reads and updates 10 records. In order to carefully vary the contention of the workload, we divide the data set into \u201chot records\u201d and \u201ccold records\u201d. Each transaction reads and writes at (uniformly) random two hot records, and all remaining reads and writes are to cold records. Cold record accesses have negligible contention, so the contention of an experiment can be finely tuned by varying the size of the hot record set.\nFor the TPC-C dataset, each of the 4 EC2 instances per region contain 240 warehouses (960 warehouses in total), and each warehouse contains 10 districts. In addition to serving as a natural partitioning attribute, warehouses also make for a natural \u201chousing\u201d attribute \u2014 the home for a record associated with a particular warehouse should be at the region closest to the physical location of that warehouse (we spread the TPC-C warehouses evenly across the regions in our deployment). Thus, unlike our YCSB experiments, for TPC-C, only multi-partition transactions can be multi-home."
        },
        {
            "heading": "4.1 Throughput experiments",
            "text": "Throughout this section we use the abbreviations sp and mp to refer to single- and multi-partition transactions, and sh and mh for single-home and multi-home. For YCSB data, it is possible for a single partition to contain records that are mastered at different regions. Therefore, it is possible for a transaction to be both singlepartition (sp) and multi-home (mh).\nWe compare SLOG to our primary comparison points: 2PLcoord and Calvin in this section, and to Spanner in Section 4.3. Our first experiment, shown in Figure 7, varies the % mp and % mh parameters. Calvin is unaffected by % mh since it has no concept of a \u201chome\u201d for data. The performance of 2PL-coord is only presented at 0% mp (which is its best case scenario).\nFigure 7(a) shows the results of our YCSB experiment under low contention (each partition has 10,000 hot records). When running SLOG with 0% mh, the overhead of multi-home transactions is not present in the workload, and, as expected, all systems perform the same. As more multi-home transactions are added to the workload, the throughput of SLOG and 2PL-coord deteriorates relative to Calvin. In SLOG, the reason is two-fold: first, extra computational resources are consumed by generating the LockOnlyTxns and processing their associated log entries. Second, whenever a LockOnlyTxn appears in the global log of a region prior to a differ-\nent LockOnlyTxn from the same original multi-home transaction, that LockOnlyTxn and all transactions that conflict with it (including single-home transactions) must block until the later LockOnlyTxn is processed. Under low contention, the percentage of transactions that are blocked is low; however, under high contention (Figure 7(b)), the contention and reduction of throughput caused by multi-home transactions is higher.\nThe throughput drop for multi-home transactions is much larger for 2PL-coord than for SLOG. This is because 2PL-coord has a longer window during which contending transactions cannot run. Once a transaction acquires a lock on a remote master, this lock is held for the entire duration of transaction processing, including the time to acquire other locks, execute transaction logic, and perform the commit protocol. In contrast, SLOG only prevents conflicting transactions from running during the window between placing the first and last LockOnlyTxn into its global log. The difference between 2PL-coord and SLOG is more extreme at high contention (Figure 7(b)) where 2PL-coord\u2019s throughput is so low at 50% and 100% multi-home that the bars are not visible on the graph. [See Figure 1 for a line graph version of 2PL-coord for this experiment.]\nMulti-partition transactions require coordination across the partitions in any ACID-compliant distributed database system. Traditional database systems use 2PC to ensure that all partitions commit. In contrast, deterministic database systems like Calvin reduce this to a single phase that can be overlapped with transaction processing. Nonetheless, this coordination is not zero-cost in Calvin, and throughput decreases with more multi-partition transactions. In SLOG, part of the blocking that is imposed by the multi-home transaction processing code of SLOG is overlapped with the waiting that is necessary anyway because of multi-partition coordination. Therefore, the relative cost of multi-home transactions in SLOG is reduced as the % mp parameter increases.\nThis also explains why the throughput drop from Calvin to SLOG is more significant when the entire workload is single-partition. However, when there are multi-partition transactions in the workload, the difference between Calvin and SLOG is much smaller, since Calvin also must pay the extra coordination cost for multipartition transactions. In general, we have found that in practice, when every transaction is multi-partition, then no matter the contention level, and no matter the % multi-home level, the performance of Calvin and SLOG is similar.\nUnder high contention (Figure 7(b)), SLOG increases in throughput from 50% mh to 100% mh as a result of how the high contention\nworkload is generated. We have 6 regions in this experiment, and each region is the master of 1/6 of all database records, which includes 1/6 of the hot records, and 1/6 of the cold records. For a single-home transaction, both hot records in the transaction come from the same pool of one sixth of all the hot records in the table. However, for a multi-home transaction, the two hot records from the transaction come from different pools \u2014 5/6 of all the hot records. By expanding the pool of hot records that the transaction can draw from, this actually reduces the contention.\nThe throughput of SLOG-HA is similar to SLOG since deterministic database systems do not need to hold locks during replication, so the synchronous replication to a different region does not cause additional contention, and there is no reduction in concurrency. Furthermore, the computing overhead of managing the replication itself on a per batch basis is negligible.\nFigure 8 shows the results of the same experiment as Figure 7(a) at the worst case scenario of SLOG relative to Calvin: 100% multihome and 0% multi-partition transactions. Each client continuously sends 200 transactions per second to the system. 2PL-coord saturates much earlier than the other systems.\nThe results of the experiment on New Order transactions on the TPC-C dataset is shown in Figure 9. In TPC-C, warehouse and district records are \u201chot records\u201d, and each New Order transaction needs to read one or two warehouse records, and update one district record. TPC-C specification requires that 10% of New Order transactions need to access two separate warehouses, which may become multi-partition and/or multi-home transactions if those two warehouses are located in separate partitions (which is greater than 75% probability in our 4-partition set-up) or have different home regions. We ran this experiment using both this default TPC-C requirement, and also with 100% of transactions touching multiple warehouses (in order to increase the number of multi-partition and multi-home transactions in the workload). Since only transactions that touch multiple warehouses can be multi-home for this dataset, we have less flexibility in our ability to experiment with multi-home transactions. Thus, for the default TPC-C specification of 10% multi-warehouse transactions, the lines in the figure labeled as \u201c0%/50%/100% of mw are mh\u201d correspond to 0%, 5%, and 10% of all total transactions that are multi-home. However, when we make 100% of transactions multi-warehouse, 0%, 50%, and 100% mh correspond to 0%, 50%, and 100% of all total transactions.\nFor the 10% multi-warehouse experiment (TPC-C\u2019s required configuration), the performance of Calvin and SLOG are similar. These\nresults resemble the YCSB experiment: when there are small numbers of multi-home transactions (a maximum of 10% in this case), the extra overhead in SLOG to process multi-home transactions is less noticeable. At 100% multi-warehouse transactions, Calvin performs slightly better than SLOG when there were 50% or 100% multi-home transactions. These results again resemble the YCSB experiment when there are large numbers of multi-partition and multi-home transactions. Once again, the reason why the difference between Calvin and SLOG is not larger is because SLOG is able to overlap the coordination required for multi-partition transactions with the coordination required for multi-home transactions.\n4.1.1 Dynamic Remastering We now investigate the overhead of dynamic remastering of data\nin SLOG. We first ran an experiment where there are guaranteed to be no actual remastering operations in the workload, so SLOG does not have to perform the checks to see if mastership annotations at transaction submit time remain correct at runtime. As shown in Figure 10, we found that the savings achieved by avoiding these checks is negligible. This is because the home and counter information are stored inside the granule header. Since the granule has to be brought into cache anyway in order to be accessed, the extra CPU overhead of the two extra if-statements is not noticeable.\nWe also ran an experiment to investigate the actual overhead of record remastering. In this experiment, we measured the temporary throughput reduction during the remastering process of a single record. In order to isolate the throughput effects of just the remastering operation itself, and ignore the throughput benefits that arise from the reduction of multi-home transactions as a result of this remastering, the access-set of each transaction was reduced from 10 to 1. In this experiment, each region has one machine, and we vary the HOT set size as 10, 50 and 1000 records. The remaster operation is initiated approximately five seconds into the experiment.\nThe experimental results are in Figure 11. Once the remaster transaction is reached and processed in the local log of the region at which it was appended, throughput begins to drop because all subsequent transactions that access the same data are annotated with the old master information and will be aborted and resubmitted. At lower contention, this drop is small because there are fewer transactions that update the same record and must be aborted. Only at very high levels of contention is the drop noticeable. We magnified the size of the drop in the figure by having the y-axis labels not start at 0, but the actual percentage drop is no more than 3% even when\nthe HOT set size is 10. The reason why the overhead is small is because transactions are aborted and resubmitted prior to their execution, so the amount of resources wasted on aborted transactions is small. Throughput returns to normal once the Lookup Master cache at each region has been updated to reflect the correct new master, and no new transactions submitted to the system have incorrect annotations and have to be aborted.\n4.1.2 Throughput Experiments Conclusions Of the two expected sources of throughput reduction of SLOG\nrelative to Calvin, we found that only one is significant: the presence of multi-home transactions in the workload. However, the throughput reduction caused by multi-home transactions is softened by the presence of multi-partition transactions in a workload.\nThere are two categories of workloads: those that are easy to partition by data access and those that are hard to partition. Workloads that are easy to partition also tend to have location locality using the same partitioning function (as we found in our TPC-C experiment). For example, a partitioning scheme based on a group of users, group of products, or group of accounts usually have associated locations with each of these groups. Thus, different partitions will be mastered at different regions based on the access affinity of that partition to a location. Therefore, we expect most workloads to be in one of two categories: either it is mostly single-partition and mostly-single-home, or otherwise both multi-partition and multihome transactions are common. For both these categories, our experiments resulted in Calvin and SLOG achieving similar throughput even though Calvin has access to all transactions submitted to the system during pre-planning, while SLOG does not. Meanwhile, under high contention, SLOG\u2019s throughput is an order of magnitude higher than nondeterministic systems such as 2PL-coord (and, as will be discussed in Section 4.3, also Spanner)."
        },
        {
            "heading": "4.2 Latency experiments",
            "text": "Figure 12 shows the cumulative distribution function (CDF) of transaction latency for SLOG-B, SLOG-HA, and Calvin with 0%, 1%, 10% and 100% multi-home transactions for the same experimental setup as Figure 7(a). Figure 13 looks deeper at the 10% multi-home case, showing 50%, 90%, and 99% latency as the number of clients sending 200 transactions per second to the system is varied. In these experiments, single-home transactions originate from a location near their home.\nAs described above, Calvin places every strictly serializable transaction into a multi-region Paxos log, and thus its latency is dominated by the WAN round-trip times necessary for insertion into this log [32, 87]. To reduce the size of this log, Calvin only stores the batch identifiers in the log. The contents of each batch are replicated separately. Thus, Calvin generally incurs more than 200 ms latency for each transaction.\nAs expected, SLOG\u2019s ability to commit single-home transactions as soon as they are processed at their home region allows SLOG to dramatically reduce its latency relative to Calvin. When the entire workload consists of such transactions (0% mh), almost every transaction achieves less than 10 ms latency \u2014 more than an order of magnitude faster than Calvin. The latency of SLOG is dominated by the batch delay prior to transaction processing, as each SLOG machine batches received transactions for 5ms prior to inserting the batch into the Paxos-maintained intra-region local log. In addition, communication with the distributed Lookup Master of a region usually requires network communication, and adds several milliseconds to the transaction latency.\nA multi-home transaction being processed at a region cannot complete until all the component LockOnlyTxns arrive from the relevant regions. The farther away the relevant regions, the longer the latency. This delay is potentially longer than the global Paxos delay of Calvin, since Paxos only requires a majority of messages to be received, whereas multi-home transactions require messages from specific (potentially remote) regions. If any region runs behind in its log processing module, this delay gets magnified in the latency of other regions that wait for specific LockOnlyTxn log records from it. This effect is best observed in the 99% latency lines in Figure 13. In addition to the message delay, multi-home transactions also experience log queuing delay at both the remote and local regions, which further increases their latency.\nSLOG-HA has a higher latency than SLOG-B since a region\u2019s input log is synchronously replicated to another region prior to commit. However, this additional latency is less noticeable for higher % mh since replication of log records in deterministic systems can begin prior to transaction execution (since only the input is replicated) and thus the replication latency is overlapped with the other latency caused by multi-home transactions discussed above.\nSection 3.1 mentioned that SLOG also supports snapshot readonly transactions (which run at serializable isolation instead of strict serializable). The latency of such queries are similar to the 0% mh line in Figure 12 since they are processed at a single region."
        },
        {
            "heading": "4.3 Comparison to Spanner",
            "text": "Spanner is similar to SLOG in that it also supports strictly serializable transactions on top of geo-replicated storage. However, the performance profile of Spanner is substantially different. To investigate these differences, we ran the same YCSB benchmark on Cloud Spanner. As mentioned above, Spanner is a different (and more production-ready) codebase relative to SLOG, and runs on a different cloud platform than our SLOG codebase, so comparing the absolute performance numbers has little meaning. However, the differences in performance trends can yield insight into the consequences of the architectural differences between the systems.\nSince Google\u2019s cloud does not have equivalent regions as AWS, we experimented with two different region-sets for Spanner: (1) nam3 where replicas are in Northern Virginia and South Carolina (and thus closer together than our Amazon regions) and (2) nameur-asia1 where replicas are more globally distributed (US, Belgium, and Taiwan). However, Cloud Spanner currently refuses to allow remote regions to be able to accept writes and participate in the Paxos protocol (due to the latency increase and throughput reduction this entails). Thus, the regions in Europe and Asia in nam-eur-asia1 are read-only and stale, and writes require communication only within a 1000 mile radius in the US. Therefore, we found nam3 and nam-eur-asia1 to have similar performance, and we report only nam3 in the figures (they are slightly more favorable to Spanner). We generated the workload for Spanner from 6 n1-standard-8 machines in Northern Virginia, each machine with 8 vCPUs and 30GB of RAM. We deployed SLOG on the most similar type of instance available on EC2: c4.2xlarge instances with 15GB RAM and 8 CPU cores. Spanner performed poorly on the large YCSB dataset from the previous experiments in a 4-node configuration, so we reduced the size of the YCSB dataset for both Spanner and SLOG to 40,000,000 records.\nCloud Spanner did not allow us to control how data was partitioned across the four nodes in each region. However, since our workload accessed 10 random tuples from the dataset, it must be assumed that virtually all transactions are multi-partition (since the chance that all 10 tuples come from the same 1 out of 4 partitions is very low). Thus, we compare Spanner against the 100% mp versions of SLOG. Figure 14 presents normalized results where the throughput of SLOG and Spanner at different contention levels is displayed as a fraction of their throughput at the lowest contention.\nFigure 14 shows that Spanner\u2019s throughput decreases rapidly as contention increases. At the highest contention, Spanner\u2019s through-\nput decreased by a factor of 37 relative to its throughput at low contention. This is because Spanner does not allow conflicting transactions to run during two phase commit and Paxos-implemented georeplication. As contention increases, the number of transactions that can run concurrently decreases. If Cloud Spanner allowed synchronous replication over a larger diameter than their current 1000 mile limit, Paxos would take much longer, and Spanner\u2019s throughput reduction would be even more severe. In contrast, SLOG maintains strict serializability despite doing replication asynchronously, and does not require two phase commit. Thus, SLOG blocks contending transactions for a much shorter period relative to Spanner5. It still experiences decreasing throughput as contention increases, but this decrease is less than a factor of 5 at the highest contention, as opposed to Spanner\u2019s factor of 37.\nSpanner\u2019s latency was similar to Calvin\u2019s for the same experiment from Figure 12. This is because write transactions in both Calvin and Spanner run Paxos over the geographic diameter of the deployment. Thus, SLOG has a significant latency advantage over Spanner for write transactions. However, read-only transactions do not require Paxos in Spanner and, if they originate near the Paxos leader, can complete in the same order of magnitude as single-home transactions in SLOG."
        },
        {
            "heading": "5. RELATED WORK",
            "text": "Asynchronous replication is widely used in geo-replicated distributed systems [3, 20, 26, 47, 48, 51, 52, 65, 82], and singlemaster replication [25, 60] is a popular implementation of asynchronous replication. Most of these systems use weak consistency models which are able to achieve low latency and high throughput, but do not support strictly serializable transactions. In order to support strict serializability, reads and writes must only be processed by the master copy in such \u201cactive-passive\u201d architectures. We discussed examples of systems that do this in Section 1 and Section 4. Existing work (prior to SLOG) either do not support transactions that access data at multiple masters [18, 20, 38, 58, 61, 72], rely on physical clock synchronization [62], or otherwise yield poor latency for such transactions (see Section 4).\nThe challenges of supporting general serializable ACID transactions over a globally-replicated and distributed system has been discussed in detail in prior work [38, 40, 42, 91]. The MDCC [46],\n5Spanner\u2019s absolute throughput numbers are also significantly lower than SLOG\u2019s.\nTAPIR [94], Replicated Commit [54], and Carousel [93] protocols reduce WAN round-trips; however, they still all incur cross-region latency to commit transactions.\nSLOG supports online data remastering which is related to a large body of work in this area [9, 28, 55, 73, 80]. Unlike GStore [23] and L-Store [49], SLOG\u2019s core protocol does not require remastering. Rather, it is an optimization that handles situations where data locality access patterns change. Nonetheless, it is an important feature that is inspired by previous work. SLOG applies these ideas within the particular deterministic system environment in which SLOG is built.\nSLOG decomposes transactions into multiple LockOnlyTxns in order to handle multi-region transactions. This approach is related to other research on transaction decomposition [13, 17, 31, 35, 36, 39, 57, 74, 75]. However, LockOnlyTxns in SLOG do not necessarily include transaction code. Rather, their primary purpose is to order deterministic processing of a particular transaction relative to other transactions that may access overlapping datasets. LockOnlyTxns in SLOG only include code when it is straightforward to automatically generate this code from the original transaction. Unlike other transaction decomposition approaches, the LockOnlyTxn approach does not result in SLOG placing any kind of restrictions on the transactions that are being decomposed, and SLOG fully supports ad-hoc transactions.\nTransaction Chains [95] achieves low latency geo-replication by chopping transactions into pieces, and using static analysis to determine if each hop can execute separately. In contrast, SLOG does not require any static analysis or independence requirements of transactions. Furthermore, SLOG supports strict serializability, whereas Transaction Chains supports only serializability with readyour-writes consistency.\nPrevious work explores merging or synchronizing multiple logs to generate a global serialization order for partitioned systems [15, 24, 50]; however, this work does not support geo-replicated configurations. ConfluxDB [18] uses log merging to implement a multireplica scheme, but does not support multi-master transactions, and only supports snapshot isolation.\nThere have been several proposals for deterministic database system designs [43, 44, 45, 69, 78, 79, 83, 84, 85, 87]. These papers also make the observation that deterministic database systems facilitate replication since the same input can be independently sent to two different replicas without concern for replica divergence. However, none of those deterministic database systems are able to achieve low-latency geo-replicated transactions."
        },
        {
            "heading": "6. CONCLUSION",
            "text": "Current state-of-the-art geo-replicated systems force their users to give up one of: (1) strict serializability (2) low latency writes (3) high transactional throughput. Some widely-used systems force their users to give up two of them. For example, Spanner experiences poor throughput under data contention, while also paying high-latency cross-region Paxos for each write. SLOG leverages physical region locality in an application workload in order to achieve all three, while also supporting online consistent dynamic \u201cre-mastering\u201d of data as application patterns change over time."
        },
        {
            "heading": "7. ACKNOWLEDGMENTS",
            "text": "We are grateful to Cuong Nguyen, Tasnim Kabir, Jianjun Chen, Sanket Purandare, Jose Faleiro, Tarikul Papon, and the anonymous reviewers for their feedback on this paper. We also thank the paper\u2019s shepherd: Phil Bernstein. This work was sponsored by the NSF under grants IIS-1718581 and IIS-1763797.\n8. REFERENCES [1] https://fauna.com/. [2] https://github.com/kunrenyale/calvindb. [3] Riak. http://wiki.basho.com/riak.html. [4] D. Abadi. Correctness Anomalies Under Serializable\nIsolation. http://dbmsmusings.blogspot.com/2019/06/correctnessanomalies-under.html.\n[5] D. Abadi and M. Freels. Serializability vs Strict Serializability: The Dirty Secret of Database Isolation Levels. https://fauna.com/blog/serializability-vs-strictserializability-the-dirty-secret-of-database-isolation-levels.\n[6] D. J. Abadi. Consistency Tradeoffs in Modern Distributed Database System Design: CAP is Only Part of the Story. IEEE Computer, 45(2), 2012.\n[7] D. J. Abadi and J. M. Faleiro. An Overview of Deterministic Database Systems. Communications of the ACM (CACM), 61(9):78\u201388, September 2018.\n[8] M. S. Ardekani, P. Sutra, and M. Shapiro. Non-monotonic Snapshot Isolation: Scalable and Strong Consistency for Geo-replicated Transactional Systems. In Proceedings of the 2013 IEEE 32nd International Symposium on Reliable Distributed Systems, SRDS \u201913, pages 163\u2013172, 2013.\n[9] M. S. Ardekani and D. B. Terry. A Self-Configurable Geo-Replicated Cloud Storage System. In Symposium on Operating Systems Design and Implementation (OSDI), pages 367\u2013381, 2014.\n[10] R. Attar, P. A. Bernstein, and N. Goodman. Site Initialization, Recovery, and Backup in a Distributed Database System. IEEE Transactions on Software Engineering, 10(6):645\u2013650, Nov. 1984.\n[11] P. Bailis, A. Fekete, M. J. Franklin, A. Ghodsi, J. M. Hellerstein, and I. Stoica. Coordination Avoidance in Database Systems. PVLDB, 8(3):185\u2013196, 2014.\n[12] J. Baker, C. Bond, J. C. Corbett, J. Furman, A. Khorlin, J. Larson, J.-M. Leon, Y. Li, A. Lloyd, and V. Yushprakh. Megastore: Providing Scalable, Highly Available Storage for Interactive Services. In Proceedings of the Conference on Innovative Data system Research (CIDR), pages 223\u2013234, 2011.\n[13] A. J. Bernstein, D. S. Gerstl, and P. M. Lewis. Concurrency Control for Step-decomposed Transactions. Information Systems, 24(9):673\u2013698, December 1999.\n[14] P. Bernstein and N. Goodman. A Proof Technique for Concurrency Control and Recovery Algorithms for Replicated Databases. Distributed Computing, 2(1):32\u201344, Mar 1987.\n[15] P. A. Bernstein and S. Das. Scaling Optimistic Concurrency Control by Approximately Partitioning the Certifier and Log. IEEE Data Engineering Bulletin, Jan-38(1):32\u201349, March 2015.\n[16] P. A. Bernstein, D. W. Shipman, and W. S. Wong. Formal Aspects of Serializability in Database Concurrency Control. IEEE Transactions on Software Engineering, 5(3):203\u2013216, May 1979.\n[17] Y. Breitbart, H. Garcia-Molina, and A. Silberschatz. Overview of Multidatabase Transaction Management. The VLDB Journal, 1(2):181\u2013240, Oct. 1992.\n[18] P. Chairunnanda, K. Daudjee, and M. T. Ozsu. ConfluxDB: Multi-Master Replication for Partitioned Snapshot Isolation Databases. PVLDB, 7(11):947\u2013958, 2014.\n[19] J. Chen, Y. Chen, Z. Chen, A. Ghazal, G. Li, S. Li, W. Ou, Y. Sun, M. Zhang, and M. Zhou. Data Management at Huawei: Recent Accomplishments and Future Challenges. In IEEE International Conference on Data Engineering (ICDE), pages 13\u201324, 2019.\n[20] B. F. Cooper, R. Ramakrishnan, U. Srivastava, A. Silberstein, P. Bohannon, H.-A. Jacobsen, N. Puz, D. Weaver, and R. Yerneni. PNUTS: Yahoo!\u2019s Hosted Data Serving Platform. PVLDB, 1(2):1277\u20131288, 2008.\n[21] B. F. Cooper, A. Silberstein, E. Tam, R. Ramakrishnan, and R. Sears. Benchmarking Cloud Serving Systems with YCSB. In Proceedings of the 1st ACM Symposium on Cloud Computing, SoCC \u201910, pages 143\u2013154, 2010.\n[22] J. C. Corbett, J. Dean, M. Epstein, A. Fikes, C. Frost, J. J. Furman, S. Ghemawat, A. Gubarev, C. Heiser, P. Hochschild, W. Hsieh, S. Kanthak, E. Kogan, H. Li, A. Lloyd, S. Melnik, D. Mwaura, D. Nagle, S. Quinlan, R. Rao, L. Rolig, Y. Saito, M. Szymaniak, C. Taylor, R. Wang, and D. Woodford. Spanner: Google\u2019s Globally Distributed Database. ACM Transactions on Computer Systems (TOCS), 31(3):8:1\u20138:22, Aug. 2013.\n[23] S. Das, D. Agrawal, and A. El Abbadi. G-Store: A Scalable Data Store for Transactional Multi Key Access in the Cloud. In Proceedings of the 1st ACM Symposium on Cloud Computing, SoCC \u201910, pages 163\u2013174, 2010.\n[24] K. Daudjee and K. Salem. Inferring a Serialization Order for Distributed Transactions. In IEEE International Conference on Data Engineering (ICDE), 2006.\n[25] K. Daudjee and K. Salem. Lazy Database Replication with Snapshot Isolation. In Proceedings of the International Conference on Very Large Data Bases (VLDB), 2006.\n[26] G. DeCandia, D. Hastorun, M. Jampani, G. Kakulapati, A. Lakshman, A. Pilchin, S. Sivasubramanian, P. Vosshall, and W. Vogels. Dynamo: Amazon\u2019s Highly Available Key-value Store. In Proceedings of Twenty-first ACM SIGOPS Symposium on Operating Systems Principles, SOSP \u201907, pages 205\u2013220, 2007.\n[27] D. Didona, R. Guerraoui, J. Wang, and W. Zwaenepoel. Causal Consistency and Latency Optimality: Friend or Foe? PVLDB, 11(11):1618\u20131632, 2018.\n[28] A. J. Elmore, V. Arora, R. Taft, A. Pavlo, D. Agrawal, and A. El Abbadi. Squall: Fine-grained live reconfiguration for partitioned main memory databases. In Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data, SIGMOD \u201915, pages 299\u2013313, 2015.\n[29] K. P. Eswaran, J. N. Gray, R. A. Lorie, and I. L. Traiger. The Notions of Consistency and Predicate Locks in a Database System. Communications of the ACM (CACM), 19(11):624\u2013633, Nov. 1976.\n[30] J. Faleiro, A. Thomson, and D. J. Abadi. Lazy Evaluation of Transactions in Database Systems. In Proceedings of the 2014 International Conference on Management of Data, SIGMOD \u201914, 2014.\n[31] J. M. Faleiro, D. Abadi, and J. M. Hellerstein. High Performance Transactions via Early Write Visibility. PVLDB, 10(5):613\u2013624, 2017.\n[32] J. M. Faleiro and D. J. Abadi. FIT: A Distributed Database Performance Tradeoff. IEEE Data Engineering Bulletin, 38(1): 10-17, 2015.\n[33] J. M. Faleiro and D. J. Abadi. Rethinking Serializable Multiversion Concurrency Control. PVLDB, 8(11):1190\u20131201, 2015.\n[34] J. M. Faleiro and D. J. Abadi. Latch-free Synchronization in Database Systems: Silver Bullet or Fool\u2019s Gold? In Proceedings of the Conference on Innovative Data system Research (CIDR), 2017.\n[35] H. Garcia-Molina. Using Semantic Knowledge for Transaction Processing in a Distributed Database. ACM Transactions on Database Systems (TODS), 8(2):186\u2013213, June 1983.\n[36] H. Garcia-Molina and K. Salem. Sagas. In Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data, SIGMOD \u201987, pages 249\u2013259, 1987.\n[37] D. K. Gifford. Information Storage in a Decentralized Computer System. PhD thesis, Stanford, CA, USA, 1981. AAI8124072.\n[38] J. Gray, P. Helland, P. O\u2019Neil, and D. Shasha. The Dangers of Replication and a Solution. In Proceedings of the 1996 International Conference on Management of Data, SIGMOD \u201996, 1996.\n[39] J. Gray and A. Reuter. Transaction Processing: Concepts and Techniques. In Morgan Kaufmann Publishers Inc., 1993.\n[40] P. Helland. Life Beyond Distributed Transactions: An Apostate\u2019s Opinion. In Proceedings of the Conference on Innovative Data system Research (CIDR), 2007.\n[41] M. P. Herlihy and J. M. Wing. Linearizability: A Correctness Condition for Concurrent Objects. ACM Transactions on Programming Languages and Systems (TOPLAS), 12(3):463\u2013492, July 1990.\n[42] C. Humble and F. Marinescu. Trading consistency for scalability in distributed architectures. http://www.infoq.com/news/2008/03/ebaybase, 2008.\n[43] R. Jimenez-Peris, M. Patino-Martinez, and S. Arevalo. Deterministic scheduling for transactional multithreaded replicas. In IEEE SRDS, 2000.\n[44] E. P. C. Jones, D. J. Abadi, and S. R. Madden. Concurrency control for partitioned databases. In Proceedings of the 2010 International Conference on Management of Data, SIGMOD \u201910, 2010.\n[45] B. Kemme and G. Alonso. Don\u2019t Be Lazy, Be Consistent: Postgres-R, A New Way to Implement Database Replication. In Proceedings of the International Conference on Very Large Data Bases (VLDB), 2000.\n[46] T. Kraska, G. Pang, M. Franklin, S. Madden, and A. Fekete. MDCC: Multi-data center consistency. In European Conference on Computer Systems, 2013.\n[47] A. Lakshman and P. Malik. Cassandra: A Decentralized Structured Storage System. SIGOPS Operating Systems Review, 44(2):35\u201340, April 2010.\n[48] C. Li, D. Porto, A. Clement, J. Gehrke, N. Preguica, and R. Rodrigues. Making Geo-Replicated Systems Fast as Possible, Consistent when Necessary. In Operating Systems Design and Implementation (OSDI), 2012.\n[49] Q. Lin, P. Chang, G. Chen, B. C. Ooi, K.-L. Tan, and Z. Wang. Towards a Non-2PC Transaction Management in Distributed Database Systems. In Proceedings of the 2016 International Conference on Management of Data, SIGMOD \u201916, pages 1659\u20131674, 2016.\n[50] C. Liu, B. G. Lindsay, S. Bourbonnais, E. Hamel, T. C. Truong, and J. Stankiewitz. Capturing Global Transactions from Multiple Recovery Log Files in a Partitioned Database System. In Proceedings of the International Conference on Very Large Data Bases (VLDB), 2003.\n[51] W. Lloyd, M. J. Freedman, M. Kaminsky, and D. G. Andersen. Don\u2019t Settle for Eventual: Scalable Causal Consistency for Wide-Area Storage with COPS. In Proceedings of the Symposium on Operating Systems Principles, SOSP \u201911, 2011.\n[52] W. Lloyd, M. J. Freedman, M. Kaminsky, and D. G. Andersen. Stronger Semantics for Low-Latency Geo-Replicated Storage. In Proceedings of the 10th USENIX Conference on Networked Systems Design and Implementation, NSDI\u201913, pages 313\u2013328, 2013.\n[53] J. Lockerman, J. M. Faleiro, J. Kim, S. Sankaram, D. J. Abadi, J. Aspnes, S. Siddhartha, and M. Balakrishnan. The FuzzyLog: A Partially Ordered Shared Log. In 13th USENIX Symposium on Operating Systems Design and Implementation, pages 357\u2013372, Oct. 2018.\n[54] H. Mahmoud, F. Nawab, A. Pucher, D. Agrawal, and A. E. Abbadi. Low-latency Multi-datacenter Databases using Replicated Commit. PVLDB, 6(9):661\u2013672, 2013.\n[55] U. F. Minhas, R. Liu, A. Aboulnaga, K. Salem, J. Ng, and S. Robertson. Elastic Scale-Out for Partition-Based Database Systems. In IEEE International Conference on Data Engineering Workshops, ICDEW \u201912, pages 281\u2013288, 2012.\n[56] H. Moniz, J. a. Leita\u0303o, R. J. Dias, J. Gehrke, N. Preguic\u0327a, and R. Rodrigues. Blotter: Low Latency Transactions for Geo-Replicated Storage. In Proceedings of the 26th International Conference on World Wide Web, WWW \u201917, pages 263\u2013272, 2017.\n[57] S. Mu, Y. Cui, Y. Zhang, W. Lloyd, and J. Li. Extracting More Concurrency from Distributed Transactions. In Symposium on Operating Systems Design and Implementation (OSDI), 2014.\n[58] F. Nawab, D. Agrawal, and A. El Abbadi. DPaxos: Managing Data Closer to Users for Low-Latency and Mobile Applications. In Proceedings of the 2018 International Conference on Management of Data, SIGMOD \u201918, pages 1221\u20131236, 2018.\n[59] F. Nawab, V. Arora, D. Agrawal, and A. E. Abbadi. Minimizing Commit Latency of Transactions in Geo-Replicated Data Stores. In Proceedings of the 2015 International Conference on Management of Data, SIGMOD \u201915, 2015.\n[60] E. Pacitti, P. Minet, , and E. Simon. Fast Algorithms for Maintaining Replica Consistency in Lazy Master Replicated Databases. In Proceedings of the International Conference on Very Large Data Bases (VLDB), 1999.\n[61] E. Pacitti, P. Minet, and E. Simon. Replica Consistency in Lazy Master Replicated Databases. In Kluwer Academic, 9(3), May 2001.\n[62] E. Pacitti, M. T. zsu, and C. Coulon. Preventive Multi-Master Replication in a Cluster of Autonomous Databases. In Proceedings of Euro-Par, 2003.\n[63] C. Papadimitriou, P. Bernstein, and J. Ronthie. Some Computational Problems Related to Database Concurrency Control. In Proceedings of the Conference on Theoretical Computer Science, pages 275\u2013282, 1977.\n[64] C. H. Papadimitriou. The Serializability of Concurrent Database Updates. Journal of the ACM, 26(4):631\u2013653, Oct. 1979.\n[65] K. Petersen, M. J. Spreitzer, D. B. Terry, M. M. Theimer, and A. J. Demers. Flexible Update Propagation for Weakly Consistent Replication. In Proceedings of the Symposium on Operating Systems Principles, SOSP \u201997, 1997.\n[66] S. Proctor. Exploring the Architecture of the NuoDB Database, Part 1. Blog Post. https://www.infoq.com/articles/nuodb-architecture-1.\n[67] S. Proctor. Exploring the Architecture of the NuoDB Database, Part 2. Blog Post. https://www.infoq.com/articles/nuodb-architecture-2.\n[68] K. Ren, J. Faleiro, and D. J. Abadi. Design Principles for Scaling Multi-core OLTP Under High Contention. In Proceedings of the 2016 International Conference on Management of Data, SIGMOD \u201916, pages 1583\u20131598, 2016.\n[69] K. Ren, A. Thomson, and D. J. Abadi. Lightweight Locking for Main Memory Database Systems. PVLDB, 6(2):145\u2013156, 2012.\n[70] K. Ren, A. Thomson, and D. J. Abadi. An Evaluation of the Advantages and Disadvantages of Deterministic Database Systems. PVLDB, 7(10):821\u2013832, 2014.\n[71] K. Ren, A. Thomson, and D. J. Abadi. VLL: A Lock Manager Redesign for Main Memory Database Systems. VLDB Journal 24(5): 681-705, October 2015.\n[72] J. Runkel. Active-Active Application Architectures with MongoDB. https://www.mongodb.com/blog/post/activeactive-application-architectures-with-mongodb.\n[73] M. Serafini, E. Mansour, A. Aboulnaga, K. Salem, T. Rafiq, and U. F. Minhas. Accordion: Elastic Scalability for Database Systems Supporting Distributed Transactions. PVLDB, 7(12):1035\u20131046, 2014.\n[74] D. Shasha, F. Llirbat, E. Simon, and P. Valduriez. Transaction Chopping: Algorithms and Performance Studies. ACM Transactions on Database Systems (TODS), 20(3):325\u2013363, September 1995.\n[75] D. Shasha, E. Simon, and P. Valduriez. Simple Rational Guidance for Chopping up Transactions. In Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data, pages 298\u2013307, 1992.\n[76] J. Shute, R. Vingralek, B. Samwel, B. Handy, C. Whipkey, E. Rollins, M. Oancea, K. Littlefield, D. Menestrina, S. Ellner, J. Cieslewicz, I. Rae, T. Stancescu, and H. Apte. F1: A Distributed SQL Database That Scales. PVLDB, 6(11):1068\u20131079, 2013.\n[77] Y. Sovran, R. Power, M. K. Aguilera, and J. Li. Transactional Storage for Geo-replicated Systems. In Proceedings of the Symposium on Operating Systems Principles, SOSP \u201911, pages 385\u2013400, 2011.\n[78] M. Stonebraker, S. R. Madden, D. J. Abadi, S. Harizopoulos, N. Hachem, and P. Helland. The End of an Architectural Era (It\u2019s Time for a Complete Rewrite). In Proceedings of the International Conference on Very Large Data Bases (VLDB), 2007.\n[79] M. Stonebraker and A. Weisberg. The VoltDB Main Memory DBMS. IEEE Data Engineering Bulletin, 36(2):21\u201327, June 2013.\n[80] R. Taft, E. Mansour, M. Serafini, J. Duggan, A. J. Elmore, A. Aboulnaga, A. Pavlo, and M. Stonebraker. E-store: Fine-grained Elastic Partitioning for Distributed Transaction Processing Systems. PVLDB, 8(3):245\u2013256, 2014.\n[81] D. B. Terry, A. J. Demers, K. Petersen, M. Spreitzer, M. Theimer, and B. W. Welch. Session Guarantees for Weakly Consistent Replicated Data. In Proceedings of the Third International Conference on Parallel and Distributed Information Systems, PDIS \u201994, pages 140\u2013149, 1994.\n[82] D. B. Terry, M. M. Theimer, K. Petersen, A. J. Demers, M. J. Spreitzer, and C. H. Hauser. Managing update conflicts in Bayou, a weakly connected replicated storage system. In Proceedings of the Symposium on Operating Systems Principles, SOSP \u201995, 1995.\n[83] A. Thomson and D. J. Abadi. The case for determinism in database systems. PVLDB, 3(1):70\u201380, 2010.\n[84] A. Thomson and D. J. Abadi. Modularity and Scalability in Calvin. In IEEE Data Engineering Bulletin, 36(2): 48-55, 2013.\n[85] A. Thomson and D. J. Abadi. Deterministic database systems. US Patent 8700563, 2014.\n[86] A. Thomson and D. J. Abadi. CalvinFS: Consistent WAN Replication and Scalable Metadata Management for Distributed File Systems. In FAST, pages 1\u201314, 2015.\n[87] A. Thomson, T. Diamond, P. Shao, K. Ren, S.-C. Weng, and D. J. Abadi. Calvin: Fast distributed transactions for partitioned database systems. In Proceedings of the 2012 International Conference on Management of Data, SIGMOD \u201912, 2012.\n[88] A. Thomson, T. Diamond, S.-C. Weng, K. Ren, P. Shao, and D. J. Abadi. Fast Distributed Transactions and Strongly Consistent Replication for OLTP Database Systems. ACM Transactions on Database Systems (TODS), 39(2):11:1\u201311:39, 2014.\n[89] A. Verbitski, A. Gupta, D. Saha, M. Brahmadesam, K. Gupta, R. Mittal, S. Krishnamurthy, S. Maurice, T. Kharatishvili, and X. Bao. Amazon aurora: Design considerations for high throughput cloud-native relational databases. In Proceedings of the 2017 ACM International Conference on Management of Data, SIGMOD \u201917, pages 1041\u20131052, 2017.\n[90] A. Verbitski, A. Gupta, D. Saha, J. Corey, K. Gupta, M. Brahmadesam, R. Mittal, S. Krishnamurthy, S. Maurice, T. Kharatishvilli, and X. Bao. Amazon aurora: On avoiding distributed consensus for i/os, commits, and membership changes. In Proceedings of the 2018 International Conference on Management of Data, SIGMOD \u201918, pages 789\u2013796, 2018.\n[91] W. Vogels. Eventually Consistent. Queue, 6(6):14\u201319, Oct. 2008.\n[92] S.-H. Wu, T.-Y. Feng, M.-K. Liao, S.-K. Pi, and Y.-S. Lin. T-Part: Partitioning of Transactions for Forward-Pushing in Deterministic Database Systems. In Proceedings of the 2016 International Conference on Management of Data, SIGMOD \u201916, pages 1553\u20131565, 2016.\n[93] X. Yan, L. Yang, H. Zhang, X. C. Lin, B. Wong, K. Salem, and T. Brecht. Carousel: Low-latency transaction processing for globally-distributed data. In Proceedings of the 2018 International Conference on Management of Data, SIGMOD \u201918, pages 231\u2013243, 2018.\n[94] I. Zhang, N. K. Sharma, A. Szekeres, A. Krishnamurthy, and D. R. K. Ports. Building consistent transactions with inconsistent replication. ACM Transactions on Computer Systems (TOCS), 35(4), Dec. 2018.\n[95] Y. Zhang, R. Power, S. Zhou, Y. Sovran, M. K. Aguilera, and J. Li. Transaction chains: achieving serializability with low latency in geo-distributed storage systems. In Proceedings of the Symposium on Operating Systems Principles, SOSP \u201913, 2013."
        }
    ],
    "title": "SLOG: Serializable, Low-latency, Geo-replicated Transactions",
    "year": 2019
}