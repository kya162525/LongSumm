{
    "abstractText": "Image captioning models have achieved impressive results on datasets containing limited visual concepts and large amounts of paired image-caption training data. However, if these models are to ever function in the wild, a much larger variety of visual concepts must be learned, ideally from less supervision. To encourage the development of image captioning models that can learn visual concepts from alternative data sources, such as object detection datasets, we present the first large-scale benchmark for this task. Dubbed \u2018nocaps\u2019, for novel object captioning at scale, our benchmark consists of 166,100 human-generated captions describing 15,100 images from the Open Images validation and test sets. The associated training data consists of COCO image-caption pairs, plus Open Images imagelevel labels and object bounding boxes. Since Open Images contains many more classes than COCO, nearly 400 object classes seen in test images have no or very few associated training captions (hence, nocaps). We extend existing novel object captioning models to establish strong baselines for this benchmark and provide analysis to guide future work.",
    "authors": [
        {
            "affiliations": [],
            "name": "Harsh Agrawal"
        },
        {
            "affiliations": [],
            "name": "Mark Johnson"
        },
        {
            "affiliations": [],
            "name": "Karan Desai"
        },
        {
            "affiliations": [],
            "name": "Dhruv Batra"
        },
        {
            "affiliations": [],
            "name": "Yufei Wang"
        },
        {
            "affiliations": [],
            "name": "Devi Parikh"
        },
        {
            "affiliations": [],
            "name": "Xinlei Chen"
        },
        {
            "affiliations": [],
            "name": "Stefan Lee"
        },
        {
            "affiliations": [],
            "name": "Rishabh Jain"
        },
        {
            "affiliations": [],
            "name": "Peter Anderson"
        }
    ],
    "id": "SP:d9dd542c63304a8dd7825987d852cd9df43730ea",
    "references": [
        {
            "authors": [
                "P. Anderson",
                "B. Fernando",
                "M. Johnson",
                "S. Gould"
            ],
            "title": "SPICE: Semantic Propositional Image Caption Evaluation",
            "venue": "ECCV,",
            "year": 2016
        },
        {
            "authors": [
                "P. Anderson",
                "B. Fernando",
                "M. Johnson",
                "S. Gould"
            ],
            "title": "Guided open vocabulary image captioning with constrained beam search",
            "venue": "EMNLP,",
            "year": 2017
        },
        {
            "authors": [
                "P. Anderson",
                "S. Gould",
                "M. Johnson"
            ],
            "title": "Partially-supervised image captioning",
            "venue": "NIPS,",
            "year": 2018
        },
        {
            "authors": [
                "P. Anderson",
                "X. He",
                "C. Buehler",
                "D. Teney",
                "M. Johnson",
                "S. Gould",
                "L. Zhang"
            ],
            "title": "Bottom-up and top-down attention for image captioning and visual question answering",
            "venue": "CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "L. Bottou"
            ],
            "title": "Large-scale machine learning with stochastic gradient descent",
            "venue": "Proceedings of COMPSTAT\u20192010, pages 177\u2013186. Springer,",
            "year": 2010
        },
        {
            "authors": [
                "X. Chen",
                "H. Fang",
                "T.-Y. Lin",
                "R. Vedantam",
                "S. Gupta",
                "P. Doll\u00e1r",
                "C.L. Zitnick"
            ],
            "title": "Microsoft COCO captions: Data collection and evaluation server",
            "venue": "arXiv:1504.00325,",
            "year": 2015
        },
        {
            "authors": [
                "X. Chen",
                "T.-Y.L. Hao Fang",
                "R. Vedantam",
                "S. Gupta",
                "P. Dollar",
                "C.L. Zitnick"
            ],
            "title": "Microsoft COCO Captions: Data Collection and Evaluation Server",
            "venue": "arXiv preprint arXiv:1504.00325,",
            "year": 2015
        },
        {
            "authors": [
                "G. Csurka"
            ],
            "title": "Domain adaptation for visual applications: A comprehensive survey",
            "venue": "Advances in Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "A.P. Dempster",
                "N.M. Laird",
                "D.B. Rubin"
            ],
            "title": "Maximum likelihood from incomplete data via the EM algorithm",
            "venue": "Journal of the royal statistical society. Series B (methodological),",
            "year": 1977
        },
        {
            "authors": [
                "J. Deng",
                "W. Dong",
                "R. Socher",
                "L.-J. Li",
                "K. Li",
                "L. Fei- Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "CVPR,",
            "year": 2009
        },
        {
            "authors": [
                "J. Donahue",
                "L.A. Hendricks",
                "S. Guadarrama",
                "M. Rohrbach",
                "S. Venugopalan",
                "K. Saenko",
                "T. Darrell"
            ],
            "title": "Long-term recurrent convolutional networks for visual recognition and description",
            "venue": "CVPR,",
            "year": 2015
        },
        {
            "authors": [
                "H. Fang",
                "S. Gupta",
                "F.N. Iandola",
                "R. Srivastava",
                "L. Deng",
                "P. Dollar",
                "J. Gao",
                "X. He",
                "M. Mitchell",
                "J.C. Platt",
                "C.L. Zitnick",
                "G. Zweig"
            ],
            "title": "From captions to visual concepts and back",
            "venue": "CVPR,",
            "year": 2015
        },
        {
            "authors": [
                "L. He",
                "K. Lee",
                "O. Levy",
                "L. Zettlemoyer"
            ],
            "title": "Jointly predicting predicates and arguments in neural semantic role labeling",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 364\u2013369. Association for Computational Linguistics,",
            "year": 2018
        },
        {
            "authors": [
                "L.A. Hendricks",
                "S. Venugopalan",
                "M. Rohrbach",
                "R. Mooney",
                "K. Saenko",
                "T. Darrell"
            ],
            "title": "Deep Compositional Captioning: Describing Novel Object Categories without Paired Training Data",
            "venue": "CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "L.A. Hendricks",
                "S. Venugopalan",
                "M. Rohrbach",
                "R.J. Mooney",
                "K. Saenko",
                "T. Darrell"
            ],
            "title": "Deep compositional captioning: Describing novel object categories without paired training data",
            "venue": "CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "M. Hodosh",
                "P. Young",
                "J. Hockenmaier"
            ],
            "title": "Framing image description as a ranking task: Data, models and evaluation metrics",
            "venue": "Journal of Artificial Intelligence Research, 47:853\u2013 899,",
            "year": 2013
        },
        {
            "authors": [
                "J. Huang",
                "V. Rathod",
                "C. Sun",
                "M. Zhu",
                "A. Korattikara",
                "A. Fathi",
                "I. Fischer",
                "Z. Wojna",
                "Y. Song",
                "S. Guadarrama",
                "K. Murphy"
            ],
            "title": "Speed/accuracy trade-offs for modern convolutional object detectors",
            "venue": "CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "A. Karpathy",
                "L. Fei-Fei"
            ],
            "title": "Deep visual-semantic alignments for generating image descriptions",
            "venue": "CVPR,",
            "year": 2015
        },
        {
            "authors": [
                "R. Kiros",
                "R. Salakhutdinov",
                "R.S. Zemel"
            ],
            "title": "Unifying visual-semantic embeddings with multimodal neural language models",
            "venue": "arXiv:1411.2539,",
            "year": 2015
        },
        {
            "authors": [
                "I. Krasin",
                "T. Duerig",
                "N. Alldrin",
                "V. Ferrari",
                "S. Abu-El-Haija",
                "A. Kuznetsova",
                "H. Rom",
                "J. Uijlings",
                "S. Popov",
                "A. Veit",
                "S. Belongie",
                "V. Gomes",
                "A. Gupta",
                "C. Sun",
                "G. Chechik",
                "D. Cai",
                "Z. Feng",
                "D. Narayanan",
                "K. Murphy"
            ],
            "title": "Openimages: A public dataset for large-scale multi-label and multi-class image classification",
            "venue": "Dataset available from https://github.com/openimages,",
            "year": 2017
        },
        {
            "authors": [
                "G. Kulkarni",
                "V. Premraj",
                "V. Ordonez",
                "S. Dhar",
                "S. Li",
                "Y. Choi",
                "A.C. Berg",
                "T.L. Berg"
            ],
            "title": "Babytalk: Understanding and generating simple image descriptions",
            "venue": "PAMI, 35(12):2891\u2013 2903,",
            "year": 2013
        },
        {
            "authors": [
                "A. Lavie",
                "A. Agarwal"
            ],
            "title": "Meteor: An automatic metric for MT evaluation with high levels of correlation with human judgments",
            "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL): Second Workshop on Statistical Machine Translation,",
            "year": 2007
        },
        {
            "authors": [
                "O. Levy",
                "Y. Goldberg"
            ],
            "title": "Dependency-based word embeddings",
            "venue": "ACL,",
            "year": 2014
        },
        {
            "authors": [
                "C. Lin"
            ],
            "title": "Rouge: a package for automatic evaluation of summaries",
            "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL) Workshop: Text Summarization Branches Out,",
            "year": 2004
        },
        {
            "authors": [
                "S. Liu",
                "Z. Zhu",
                "N. Ye",
                "S. Guadarrama",
                "K. Murphy"
            ],
            "title": "Improved image captioning via policy gradient optimization of SPIDEr",
            "venue": "ICCV,",
            "year": 2017
        },
        {
            "authors": [
                "J. Lu",
                "C. Xiong",
                "D. Parikh",
                "R. Socher"
            ],
            "title": "Knowing when to look: Adaptive attention via a visual sentinel for image captioning",
            "venue": "CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "J. Lu",
                "J. Yang",
                "D. Batra",
                "D. Parikh"
            ],
            "title": "Neural baby talk",
            "venue": "CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "J. Mao",
                "J. Xu",
                "K. Jing",
                "A.L. Yuille"
            ],
            "title": "Training and evaluating multimodal word embeddings with large-scale web annotated images",
            "venue": "NIPS,",
            "year": 2016
        },
        {
            "authors": [
                "V. Ordonez",
                "G. Kulkarni",
                "T.L. Berg"
            ],
            "title": "Im2text: Describing images using 1 million captioned photographs",
            "venue": "NIPS,",
            "year": 2011
        },
        {
            "authors": [
                "D.P. Papadopoulos",
                "J.R. Uijlings",
                "F. Keller",
                "V. Ferrari"
            ],
            "title": "We don\u2019t need no bounding-boxes: Training object class detectors using only human verification",
            "venue": "CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "D.P. Papadopoulos",
                "J.R. Uijlings",
                "F. Keller",
                "V. Ferrari"
            ],
            "title": "Extreme clicking for efficient object annotation",
            "venue": "ICCV,",
            "year": 2017
        },
        {
            "authors": [
                "K. Papineni",
                "S. Roukos",
                "T. Ward",
                "W. Zhu"
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "ACL,",
            "year": 2002
        },
        {
            "authors": [
                "J. Pennington",
                "R. Socher",
                "C.D. Manning"
            ],
            "title": "GloVe: Global Vectors for Word Representation",
            "venue": "EMNLP,",
            "year": 2014
        },
        {
            "authors": [
                "M. Peters",
                "M. Neumann",
                "M. Iyyer",
                "M. Gardner",
                "C. Clark",
                "K. Lee",
                "L. Zettlemoyer"
            ],
            "title": "Deep contextualized word representations",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 2227\u20132237. Association for Computational Linguistics,",
            "year": 2018
        },
        {
            "authors": [
                "B. Recht",
                "R. Roelofs",
                "L. Schmidt",
                "V. Shankar"
            ],
            "title": "Do CIFAR-10 classifiers generalize to CIFAR-10? arXiv:1806.00451, 2018",
            "year": 2018
        },
        {
            "authors": [
                "S. Ren",
                "K. He",
                "R. Girshick",
                "J. Sun"
            ],
            "title": "Faster R-CNN: Towards real-time object detection with region proposal networks",
            "venue": "NIPS,",
            "year": 2015
        },
        {
            "authors": [
                "S.J. Rennie",
                "E. Marcheret",
                "Y. Mroueh",
                "J. Ross",
                "V. Goel"
            ],
            "title": "Self-critical sequence training for image captioning",
            "venue": "CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "P. Sharma",
                "N. Ding",
                "S. Goodman",
                "R. Soricut"
            ],
            "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
            "venue": "Proceedings of ACL,",
            "year": 2018
        },
        {
            "authors": [
                "K. Tran",
                "X. He",
                "L. Zhang",
                "J. Sun",
                "C. Carapcea",
                "C. Thrasher",
                "C. Buehler",
                "C. Sienkiewicz"
            ],
            "title": "Rich Image Captioning in the Wild",
            "venue": "CVPR Workshops,",
            "year": 2016
        },
        {
            "authors": [
                "L. van der Maaten",
                "G. Hinton"
            ],
            "title": "Visualizing data using t- SNE",
            "venue": "Journal of Machine Learning Research,",
            "year": 2008
        },
        {
            "authors": [
                "R. Vedantam",
                "C.L. Zitnick",
                "D. Parikh"
            ],
            "title": "CIDEr: Consensus-based image description evaluation",
            "venue": "CVPR,",
            "year": 2015
        },
        {
            "authors": [
                "S. Venugopalan",
                "L.A. Hendricks",
                "M. Rohrbach",
                "R.J. Mooney",
                "T. Darrell",
                "K. Saenko"
            ],
            "title": "Captioning Images with Diverse Objects",
            "venue": "CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "O. Vinyals",
                "A. Toshev",
                "S. Bengio",
                "D. Erhan"
            ],
            "title": "Show and tell: A neural image caption generator",
            "venue": "CVPR,",
            "year": 2015
        },
        {
            "authors": [
                "A. Wang",
                "A. Singh",
                "J. Michael",
                "F. Hill",
                "O. Levy",
                "S.R. Bowman"
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "ICLR,",
            "year": 2019
        },
        {
            "authors": [
                "Y. Wu",
                "L. Zhu",
                "L. Jiang",
                "Y. Yang"
            ],
            "title": "Decoupled novel object captioner",
            "venue": "CoRR, abs/1804.03803,",
            "year": 2018
        },
        {
            "authors": [
                "K. Xu",
                "J. Ba",
                "R. Kiros",
                "K. Cho",
                "A.C. Courville",
                "R. Salakhutdinov",
                "R.S. Zemel",
                "Y. Bengio"
            ],
            "title": "Show, attend and tell: Neural image caption generation with visual attention",
            "venue": "ICML,",
            "year": 2015
        },
        {
            "authors": [
                "D. Yadav",
                "R. Jain",
                "H. Agrawal",
                "P. Chattopadhyay",
                "T. Singh",
                "A. Jain",
                "S.B. Singh",
                "S. Lee",
                "D. Batra"
            ],
            "title": "Evalai: Towards better evaluation systems for ai agents",
            "venue": "arXiv:1902.03570,",
            "year": 2019
        },
        {
            "authors": [
                "Z. Yang",
                "Y. Yuan",
                "Y. Wu",
                "R. Salakhutdinov",
                "W.W. Cohen"
            ],
            "title": "Review networks for caption generation",
            "venue": "NIPS,",
            "year": 2016
        },
        {
            "authors": [
                "T. Yao",
                "Y. Pan",
                "Y. Li",
                "T. Mei"
            ],
            "title": "Incorporating copying mechanism in image captioning for learning novel objects",
            "venue": "CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "P. Young",
                "A. Lai",
                "M. Hodosh",
                "J. Hockenmaier"
            ],
            "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
            "venue": "Transactions of the Association for Computational Linguistics, 2:67\u2013 78,",
            "year": 2014
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Recent progress in image captioning, the task of generating natural language descriptions of visual content [11, 12, 18,19,43,46], can be largely attributed to the publicly available large-scale datasets of image-caption pairs [6, 16, 50] as well as steady modeling improvements [4, 26, 37, 48]. However, these models generalize poorly to images in the wild [39] despite impressive benchmark performance, because they are trained on datasets which cover a tiny fraction of the long-tailed distribution of visual concepts in the real world. For example, models trained on COCO Captions [6] can typically describe images containing dogs, people and umbrellas, but not accordions or dolphins. This\n\u2039First two authors contributed equally, listed in alphabetical order.\nlimits the usefulness of these models in real-world applications, such as providing assistance for people with impaired vision, or for improving natural language query-based image retrieval.\nTo generalize better \u2018in the wild\u2019, we argue that captioning models should be able to leverage alternative data sources \u2013 such as object detection datasets \u2013 in order to describe objects not present in the caption corpora on which they are trained. Such objects which have detection annotations but are not present in caption corpora are referred to as novel objects and the task of describing images containing novel objects is termed novel object captioning [2,3,15,27,42,45,49]. Until now, novel object captioning approaches have been evaluated using a proof-ofconcept dataset introduced in [14]. This dataset has restric-\n1\nar X\niv :1\n81 2.\n08 65\n8v 3\n[ cs\n.C V\n] 3\n0 Se\ntive assumptions \u2013 it contains only 8 novel object classes held out from the COCO dataset [15], deliberately selected to be highly similar to existing ones (e.g. horse is seen, zebra is novel). This has left the large-scale performance of these methods open to question. Given the emerging interest and practical necessity of this task, we introduce nocaps, the first large-scale benchmark for novel object captioning, containing nearly 400 novel object classes.\nIn detail, the nocaps benchmark consists of a validation and test set comprised of 4,500 and 10,600 images, respectively, sourced from the Open Images object detection dataset [20] and annotated with 11 human-generated captions per image (10 reference captions for automatic evaluation plus a human baseline). Crucially, we provide no additional paired image-caption data for training. Instead, as illustrated in Figure 1, training data for the nocaps benchmark is image-caption pairs from the COCO Captions 2017 [6] training set (118K images containing 80 object classes), plus the Open Images V4 object detection training set (1.7M images annotated with bounding boxes for 600 object classes and image labels for 20K categories).\nTo be successful, image captioning models may utilize COCO paired image-caption data to learn to generate syntactically correct captions, while leveraging the massive Open Images detection dataset to learn many more visual concepts. Our key scientific goal is to disentangle \u2018how to recognize an object\u2019 from \u2018how to talk about it\u2019. After learning the name of a novel object, a human can immediately talk about its attributes and relationships. It is therefore intellectually dissatisfying that existing models, having already internalized a huge number of caption examples, can\u2019t also be taught new objects. As with previous work, this task setting is also motivated by the observation that collecting human-annotated captions is resource intensive and scales poorly as object diversity grows, while on the other hand, large-scale object classification and detection datasets already exist [10, 20] and their collection can be massively scaled, often semi-automatically [30, 31].\nTo establish the state-of-the-art on our challenging benchmark, we evaluate two of the best performing existing approaches [2, 27] and report their performance based on well-established evaluation metrics \u2013 CIDEr [41] and SPICE [1]. To provide finer-grained analysis, we further break performance down over three subsets \u2013 in-domain, near-domain and out-of-domain\u2013 corresponding to the similarity of depicted objects to COCO classes. While these models do improve over a baseline model trained only on COCO Captions, they still fall well short of human performance on this task \u2013 indicating there is still work to be done to scale to \u2018in-the-wild\u2019 image captioning. In summary, we make three main contributions: - We collect nocaps \u2013 the first large-scale benchmark for\nnovel object captioning, containing \u201e400 novel objects.\n- We undertake a detailed investigation of the performance and limitations of two existing state-of-the-art models on this task and contrast them against human performance. - We make improvements and suggest simple heuristics that improve the performance of constrained beam search significantly on our benchmark. We believe that improvements on nocaps will accelerate progress towards image captioning in the wild. We are hosting a public evaluation server on EvalAI [47] to benchmark progress on nocaps. For reproducibility and to spur innovation, we have also released code to replicate our experiments at: https://github.com/nocaps-org."
        },
        {
            "heading": "2. Related Work",
            "text": "Novel Object Captioning Novel object captioning includes aspects of both transfer learning and domain adaptation [8]. Test images contain previously unseen, or \u2018novel\u2019 objects that are drawn from a target distribution (in this case, Open Images [20]) that differs from the source/training distribution (COCO [6]). To obtain a captioning model that performs well in the target domain, the Deep Compositional Captioner [15] and its extension, the Novel Object Captioner [42], both attempt to transfer knowledge by leveraging object detection datasets and external text corpora by decomposing the captioning model into visual and textual components that can be trained with separate loss functions as well as jointly using the available image-caption data.\nSeveral alternative approaches elect to use the output of object detectors more explicitly. Two concurrent works, Neural Baby Talk [27] and the Decoupled Novel Object Captioner [45], take inspiration from Baby Talk [21] and propose neural approaches to generate slotted caption templates, which are then filled using visual concepts identified by modern state-of-the-art object detectors. Related to Neural Baby Talk, the LSTM-C [49] model augments a standard recurrent neural network sentence decoder with a copying mechanism which may select words corresponding to object detector predictions to appear in the output sentence.\nIn contrast to these works, several approaches to novel object captioning are architecture agnostic. Constrained Beam Search [2] is a decoding algorithm that can be used to enforce the inclusion of selected words in captions during inference, such as novel object classes predicted by an object detector. Building on this approach, partiallyspecified sequence supervision (PS3) [3] uses Constrained Beam Search as a subroutine to estimate complete captions for images containing novel objects. These complete captions are then used as training targets in an iterative algorithm inspired by expectation maximization (EM) [9].\nIn this work, we investigate two different approaches: Neural Baby Talk (NBT) [27] and Constrained Beam Search (CBS) [2] on our challenging benchmark \u2013 both of which recently claimed state-of-the-art on the proof-of-\nconcept novel object captioning dataset [15].\nImage Caption Datasets In the past, two paradigms for collecting image-caption datasets have emerged: direct annotation and filtering. Direct-annotated datasets, such as Flickr 8K [16], Flickr 30K [50] and COCO Captions [6] are collected using crowd workers who are given instructions to control the quality and style of the resulting captions. To improve the reliability of automatic evaluation metrics, these datasets typically contain five or more captions per image. However, even the largest of these, COCO Captions, is based only on a relatively small set of 80 object classes. In contrast, filtered datasets, such as Im2Text [29], Pinterest40M [28] and Conceptual Captions [38], contain large numbers of image-caption pairs harvested from the web. These datasets contain many diverse visual concepts, but are also more likely to contain non-visual content in the description due to the automated nature of the collection pipelines. Furthermore, these datasets lack human baselines, and may not include enough captions per image for good correlation between automatic evaluation metrics and human judgments [1, 41].\nOur benchmark, nocaps, aims to fill the gap between these datasets, by providing a high-quality benchmark with 10 reference captions per image and many more visual concepts than COCO. To the best of our knowledge, nocaps is the only image captioning benchmark in which humans outperform state-of-the-art models in automatic evaluation."
        },
        {
            "heading": "3. nocaps",
            "text": "In this section, we detail the nocaps collection process, constrast it with COCO Captions [6], and introduce the evaluation protocol and benchmark guidelines."
        },
        {
            "heading": "3.1. Caption Collection",
            "text": "The images in nocaps are sourced from the Open Images V4 [20] validation and test sets. 1 Open Images is currently the largest available human-annotated object detection dataset, containing 1.9M images of complex scenes annotated with object bounding boxes for 600 classes (with an average of 8.4 object instances per image in the training\n1The images used in nocaps come from the Open Images V4 dataset and are provided under their original license (CC BY 2.0)\nset). Moreover, out of the 500 classes that are not overly broad (e.g. \u2018clothing\u2019) or infrequent (e.g. \u2018paper cutter\u2019), nearly 400 are never or rarely mentioned in COCO Captions [6] (which we select as image-caption training data), making these images an ideal basis for our benchmark.\nImage Subset Selection Since Open Images is primarily an object detection dataset, a large fraction of images contain well-framed iconic perspectives of single objects. Furthermore, the distribution of object classes is highly unbalanced, with a long-tail of object classes that appear relatively infrequently. However, for image captioning, images containing multiple objects and rare object co-occurrences are more interesting and challenging. Therefore, we select subsets of images from the Open Images validation and test splits by applying the following sampling procedure.\nFirst, we exclude all images for which the correct image rotation is non-zero or unknown. Next, based on the ground-truth object detection annotations, we exclude all images that contain only instances from a single object category. Then, to capture as many visually complex images as possible, we include all images containing more than 6 unique object classes. Finally, we iteratively select from the remaining images using a sampling procedure that encourages even representation both in terms of object classes and image complexity (based on the number of unique classes per image). Concretely, we divide the remaining images into 5 pools based on the number of unique classes present in the image (from 2\u20136 inclusive). Then, taking each pool in turn, we randomly sample n images and among these, we select the image that when added to our benchmark results in the highest entropy over object classes. This prevents nocaps from being overly dominated by frequently occurring object classes such as person, car or plant. In total, we select 4,500 validation images (from a total of 41,620 images in Open Images validation set) and 10,600 test images (from a total of 125,436 images in Open Images test set). On average, the selected images contain 4.0 object classes and 8.0 object instances each (see Figure 2). Collecting Image Captions from Humans To evaluate model-generated image captions, we collected 11 English captions for each image from a large pool of crowd-workers\non Amazon Mechanical Turk (AMT). Out of 11 captions, we randomly sample one caption per image to establish human performance on nocaps and use the remaining 10 captions as reference captions for automatic evaluation. Prior work suggests that automatic caption evaluation metrics correlate better with human judgment when more reference captions are provided [1, 41], motivating us to collect more reference captions than COCO (only 5 per image).\nOur image caption collection interface closely resembles the interface used for collection of the COCO Captions dataset, albeit with one important difference. Since the nocaps dataset contains more rare and fine-grained classes than COCO, in initial pilot studies we found that human annotators could not always correctly identify the objects in the image. For example, as illustrated in Figure 3, a red panda was incorrectly described as a brown rodent. We therefore experimented with priming workers by displaying the list of ground-truth object classes present in the image. To minimize the potential for this priming to reduce the language diversity of the resulting captions, the object classes were presented as \u2018keywords\u2019, and workers were explicitly instructed that it was not necessary to mention all the displayed keywords. To reduce clutter, we did not display object classes which are classified in Open Images as parts, e.g. human hand, tire, door handle. Pilot studies comparing captions collected with and without priming demonstrated that primed workers produced more qualitative accurate and descriptive captions (see Figure 3). Therefore, all nocaps captions, including our human baselines, were collected using this priming-modified COCO collection interface.\nTo help maintain the quality of the collected captions, we used only US-based workers who had completed at least 5K previous tasks on AMT with more than 95% approval rate. We also spot-checked the captions written by each worker and blocked workers providing low-quality captions. Captions written by these workers were then discarded and replaced with captions written by high-quality workers. Over-\nDataset 1-grams 2-grams 3-grams 4-grams\nCOCO 6,913 46,664 92,946 119,582 nocaps 8,291 59,714 116,765 144,577"
        },
        {
            "heading": "3.3. Evaluation",
            "text": ""
        },
        {
            "heading": "3.2. Dataset Analysis",
            "text": "To facilitate evaluation and avoid exposing the novel object captions, we host an evaluation server for nocaps on EvalAI [47] \u2013 as such, we put forth these guidelines for using nocaps: \u2013 Do not use additional paired image-caption data col-\nlected from humans. Improving evaluation scores by leveraging additional human-generated paired imagecaption data is antithetical to this benchmark \u2013 the only paired image-caption dataset that should be used is the COCO Captions 2017 training split. However, external text corpora, knowledge bases, and object detection datasets may be used during training or inference. \u2013 Do not leverage ground truth object annotations. We note that ground-truth object detection annotations are available for Open Images validation and test splits (and hence, for nocaps). While ground-truth annotations may be used to establish performance upper bounds on the validation set, they should never be used in a submission to the evaluation server unless this is clearly disclosed. We anticipate that researchers may wish to investigate the limits of performance on nocaps without any restraints on the training datasets. We therefore maintain a separate leaderboard for this purpose \"nocaps (XD)\" 2 leaderboard. Metrics As with existing captioning benchmarks, we rely on automatic metrics to evaluate the quality of modelgenerated captions. We focus primarily on CIDEr [41] and SPICE [1], which have been shown to have the strongest correlation with human judgments [25] and have been used in prior novel object captioning work [3,14,27], but we also report Bleu [32], Meteor [22] and ROUGE [24]. These metrics test whether models mention novel objects accurately [43] as well as describe them fluently [22].It is worth\n2XD stands for \"extra data\"\nnoting that the absolute scale of these metrics is not comparable across datasets due to the differing number of reference captions and corpus-wide statistics. Evaluation Subsets We further break down performance on nocaps over three subsets of the validation and test splits corresponding to varied \u2018nearness\u2019 to COCO.\nTo determine these subsets, we manually map the 80 COCO classes to Open Images classes. We then select an additional 39 Open Images classes that are not COCO classes, but are nonetheless mentioned more than 1,000 times in the COCO captions training set (e.g. \u2018table\u2019, \u2018plate\u2019 and \u2018tree\u2019). We classify these 119 classes as in-domain relative to COCO. There are 87 Open Images classes that are not present in nocaps3. The remaining 394 classes are outof-domain. Image subsets are then determined as follows: \u2013 in-domain images contain only objects belonging to in-\ndomain classes. Since these objects have been described in the paired image-caption training data, we expect caption models trained only on COCO to perform reasonably well on this subset, albeit with some negative impact due\n3These classes are not included either because they are not present in the underlying Open Images val and test splits, or because they got filtered out by our image subset selection strategy favoring more complex images.\nto image domain shift. This subset contains 1,311 test images (13K captions). \u2013 near-domain images contain both in-domain and out-ofdomain object classes. These images are more challenging for COCO trained models, especially when the most salient objects in the image are novel. This is the largest subset containing 7,406 test images (74K captions). \u2013 out-of-domain images do not contain any in-domain classes, and are visually very distinct from COCO images. We expect this subset to be the most challenging and models trained only on COCO data are likely to make \u2018embarrassing errors\u2019 [25] on this subset, reflecting the current performance of COCO trained models in the wild. This subset contains 1,883 test images (19K captions)."
        },
        {
            "heading": "4. Experiments",
            "text": "To provide an initial measure of the state-of-the-art on nocaps, we extend and present results for two contemporary approaches to novel object captioning \u2013 Neural Baby Talk (NBT) [27] and Constrained Beam Search (CBS) [2] inference method which we apply both to NBT and to the popular UpDown captioner [4]. We briefly recap these approaches for completeness but encourage readers to seek the original works for further details. Bottom-Up Top-Down Captioner (UpDown) [4] reasons over visual features extracted using object detectors trained on a large numbers of object and attribute classes and produces near state-of-the-art for single model captioning performance on COCO. For visual features, we use the publicly available Faster R-CNN [36] detector trained on Visual Genome by [4] to establish a strong baseline trained exclusively on paired image-caption data. Neural Baby Talk (NBT) [27] first generates a hybrid textual template with slots explicitly tied to specific image regions, and then fill these slots with words associated with visual concepts identified by an object detector. This gives NBT the capability to caption novel objects when combined with an appropriate pretrained object detector. To adapt NBT to the nocaps setting, we incorporate the Open Images detector and train the language model using Visual Genome image features. We use fixed GloVe embeddings [33] in the visual feature representation for an object region for better contextualization of words corresponding to novel objects. Open Images Object Detection. Both CBS and NBT make use of object detections; we use the same pretrained Faster R-CNN model trained on Open Images for both. Specifically, we use a model4 from the Tensorflow model zoo [17] which achieves a detection mean average precision at 0.5 IoU (mAP@0.5) of 54%. Constrained Beam Search (CBS) [2] CBS is an inferencetime procedure that can force language models to include specific words referred to as constraints \u2013 achieving this by\n4tf_faster_rcnn_inception_resnet_v2_atrous_oidv4\ncasting the decoding problem as a finite state machine with transitions corresponding to constraint satisfaction. We apply CBS to both the baseline UpDown model and NBT based on detected objects. Following [2], we use a Finite State Machine (FSM) with 24 states to incorporate up to three selected objects as constraints, including two and three word phrases. After decoding, we select the highest logprobability caption that satisfies at least two constraints. Constraint Filtering Although the original work [2] selected constraints from detections randomly, in preliminary experiments in the nocaps setting we find that a simple heuristic significantly improves the performance of CBS.To generate caption constraints from object detections, we refine the raw object detection labels by removing 39 Open Images classes that are \u2018parts\u2019 (e.g. human eyes) or rarely mentioned (e.g. mammal). Specifically, we resolve overlapping detections (IoU \u011b 0.85) by removing the higher-order of the two objects (e.g. , a \u2018dog\u2019 would suppress a \u2018mammal\u2019) based on the Open Images class hierarchy (keeping both if equal). Finally, we take the top-3 objects based on detection confidence as constraints. Language Embeddings To handle novel vocabulary, CBS requires word embeddings or a language model to estimate the likelihood of word transitions. We extend the original model \u2013 which incorporated GloVe [33] and dependency embeddings [23] \u2013 to incorporate the recently proposed ELMo [34] model, which increased performance in our preliminary experiments. As captions are decoded left-to-right, we can only use the forward representation of ELMo as input encodings rather than the full bidirectional model as in [13, 44]. We also initialize the softmax layer of our caption decoder with that of ELMo and fix it during training to improve the model\u2019s generalization to unseen or rare words. Training and Implementation Details. We train all models on the COCO training set and tune parameters on the nocaps validation set. All models are trained with crossentropy loss, i.e. we do not use RL fine-tuning to optimize for evaluation metrics [37]."
        },
        {
            "heading": "5. Results and Analysis",
            "text": "We report results on the nocaps test set in Table 2. While our best approach (UpDown + ELMo + CBS, which is explained further below) outperforms the COCO-trained UpDown baseline captioner significantly (\u201e19 CIDEr), it still under-performs humans by a large margin (\u201e12 CIDEr). As expected the most sizable gap occurs for out-of-domain instances (\u201e25 CIDEr). This shows that while existing novel object captioning techniques do improve over standard models, captioning in-the-wild still presents a considerable open challenge.\nIn the remainder of this section, we discuss detailed results on the nocaps and COCO validation sets (Table 3) to help guide future work. Overall, the evidence suggests that\nfurther progress can be made through stronger object detectors and stronger language models, but open questions remain \u2013 such as the best way to combine these elements, and the extent to which that solution should involve learning vs. inference techniques like CBS. We align these discussions in the context of a series of specific questions below. \u2013 Do models optimized for nocaps maintain their per-\nformance on COCO? We find significant gains in nocaps performance correspond to large losses on COCO (rows 2-3 vs 1 \u2013 dropping \u201e20 CIDEr and \u201e3 SPICE). Given the similarity of the collection methodology, we do not expect to see significant differences in linguistic structure between COCO and nocaps. However, recent work has observed significant performance degradation when transferring models across datasets even when the new target dataset is an exact recreation of the old dataset [35]. Limiting this degradation in the captioning setting is a potential focus for future work.\n\u2013 How important is constraint filtering? Applying CBS greatly improves performance for both UpDown and NBT (particularly on the out-of-domain captions), but success depends heavily on the quality of the constraints. Without our 39-class blacklist and overlap filtering, we find overall nocaps validation performance falls \u201e8 CIDEr and \u201e3 SPICE for our UpDown + ELMo + CBS model \u2013 with most of the losses coming from the blacklisted classes. It seems likely that more sophisticated constraint selection techniques that consider image context could improve performance further. \u2013 Do better language models help in CBS? To handle novel vocabulary, CBS requires representations for the novel words. We compare using ELMo encoding (row 3) as described in Section 4 with the setting in which word embeddings are only learned during COCO training (row 2). Note that in this setting the embedding for any word not found in COCO is randomly initialized. Surprisingly, the trained embeddings perform on par with the\nELMo embeddings for the in-domain and near-domain subsets, although the model with ELMo performs much better on the out-of-domain subset. It appears that even relatively rare occurrences of nocaps object names in COCO are sufficient to learn useful linguistic models, but not visual grounding as shown by the COCO-only model\u2019s poor scores (row 1). \u2013 Do better object detectors help? To evaluate reliance on object detections, we supply ground truth detections sorted by decreasing area to our full models (rows 4 and 7). These ground truth detections undergo the same constraint filtering as predicted ones. Comparing to prediction-reliant models (rows 3 and 6), we see large gains on all splits (rows 4 vs 3 \u2013 \u201e9 CIDEr and \u201e0.6 SPICE gain for UpDown). As detectors improve, we expect to see commensurate gains on nocaps benchmark.\nTo qualitatively assess some of the differences between the various approaches, in Figure 6 we illustrate some examples of the captions generated using various model configurations. As expected, all our baseline models are able to generate accurate captions for in-domain images. For near-domain and out-of-domain, our UpDown model trained only on COCO fails to identify novel objects such as rifle and dolphin, and confuses them with known objects such as baseball bat or bird. The remaining models leverage the Open Images training data, enabling them to potentially describe these novel object classes. While they do produce\nmore reasonable descriptions, there remains much room for improvement in both grounding and grammar."
        },
        {
            "heading": "6. Conclusion",
            "text": "In this work, we motivate the need for a stronger and more rigorous benchmark to assess progress on the task of novel object captioning. We introduce nocaps, a largescale benchmark consisting of 166,100 human-generated captions describing 15,100 images containing more than 500 unique object classes and many more visual concepts. Compared to the existing proof-of-concept dataset for novel object captioning [14], our benchmark contains a fifty-fold increase in the number of novel object classes that are rare or absent in training captions (394 vs 8). Further, we collected twice the number of evaluation captions per image to improve the fidelity of automatic evaluation metrics.\nWe extend two recent approaches for novel object captioning to provide strong baselines for the nocaps benchmark. While our final models improve significantly over a direct transfer from COCO, they still perform well below the human baseline \u2013 indicating there is significant room for improvement on this task. We provide further analysis to help guide future efforts, showing that it helps to leverage large language corpora via pretrained word embeddings and language models, that better object detectors help (and can be a source of further improvements), and that simple heuristics for determining which object detections to men-\ntion in a caption have a significant impact."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank Jiasen Lu for helpful discussions about Neural Baby Talk. This work was supported in part by NSF, AFRL, DARPA, Siemens, Samsung, Google, Amazon, ONR YIPs and ONR Grants N00014-16-1-{2713,2793}. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the U.S. Government, or any sponsor."
        },
        {
            "heading": "1. Data Collection Interface",
            "text": "3. Additional Details about nocaps Benchmark"
        },
        {
            "heading": "3.1. Evaluation Subsets",
            "text": "As outlined in Section 3.3 of the main paper, to determine the in-domain, near-domain and out-of-domain subsets of nocaps, we first classify Open Images classes as either in-domain or out-of-domain with respect to COCO. To identify the in-domain Open Images classes, we manually map the 80 COCO classes to Open Images classes. We then select an additional 39 Open Images classes that are not COCO classes, but are nonetheless mentioned more than 1,000 times in the COCO captions training set (e.g. \u2018table\u2019, \u2018plate\u2019 and \u2018tree\u2019), and we classify all 119 of these classes as in-domain. The remaining classes are considered to be out-of-domain.\nTo put this in perspective, in Figure 10 we plot the number of mentions of both the in-domain classes (in orange) and the out-of-domain classes (in blue) in the COCO Captions training set using a log scale. As intended, the in-domain object classes occur much more frequently in COCO Captions compared to out-of-domain object classes. However, it is worth noting that the out-of-domain are not necessarily absent from COCO Captions, but they are relatively infrequent which makes these concepts hard to learn from COCO.\nOpen Images classes ignored during image subset selection: We also note that 87 Open Images classes were not considered during the image subset selection procedure to create nocaps, for one of the following reasons:\n\u2022 Parts: In our image subset selection strategy (refer Section 3.1 of the main paper), we ignored \u2018part\u2019 categories such as \u2018vehicle registration plate\u2019, \u2018wheel\u2019, \u2018human-eye\u2019, which always occur with parent categories such car, person; \u2022 Super-categories: Our image subset selection strategy also ignored super-categories such as \u2018sports equipment\u2019, \u2018home appliance\u2019, \u2018auto part\u2019 which are often too broad and subsumes both COCO and Open Images categories; \u2022 Solo categories: Certain categories such as \u2018chime\u2019 and \u2018stapler\u2019 did not appear in images alongside any other classes, and so were filtered out by our image subset selection strategy; and \u2022 Rare categories: Some rare categories such as \u2018armadillo\u2019, \u2018pencil sharpener\u2019 and \u2018pizza cutter\u2019 do not actually occur in the underlying Open Images val and test splits."
        },
        {
            "heading": "3.2. T-SNE Visualization in Visual Feature Embedding Space",
            "text": ""
        },
        {
            "heading": "3.3. T-SNE Visualization in Linguistic Feature Embedding Space",
            "text": ""
        },
        {
            "heading": "3.4. Linguistic Similarity to COCO",
            "text": "Overall, our collection methodology closely follows COCO. However, we do introduce keyword priming to the collection interface (refer Figure 7) which has the potential to introduce some linguistic differences between nocaps and COCO. To quantitatively assess linguistic differences between the two datasets, we review the performance of COCO-trained models on the nocaps validation set while controlling for visual similarity to COCO. As a proxy for visual similarity to COCO, we use the average cosine distance in FC7 CNN feature space between each nocaps image and the 10 closest COCO images.\nAs illustrated in Table 4, the baseline UpDown model (trained using COCO) exceeds human performance on the decile of nocaps images which are most similar to COCO images (decile=1, avg. cosine distance=0.15), consistent with the trends seen in the COCO dataset. This suggests that the linguistic structure of COCO and nocaps captions is extremely similar. As the nocaps images become visually more distinct from COCO images, the performance of UpDown drops consistently. This suggests that no linguistic variations have been introduced between COCO and nocaps due to priming and the degradation in the performance is due to visual differences. Similar trends are observed for our best model (UpDown + ELMo + CBS) although the performance degradation with increasing visual dissimilarity to COCO is much less."
        },
        {
            "heading": "4. Additional Implementation Details for Baseline Models",
            "text": ""
        },
        {
            "heading": "4.1. Neural Baby Talk (NBT)",
            "text": "In this section, we describe our modifications to the original authors\u2019 implementation of Neural Baby Talk (NBT) [27] to enable the model to produce captions for images containing novel objects present in nocaps.\nGrounding Regions for Visual Words Given an image, NBT leverages an object detector to obtain a set of candidate image region proposals, and further produces a caption template, with slots explicitly tied to specific image regions. In order to accurately caption nocaps images, the object detector providing candidate region proposals must be able to detect the object classes present in nocaps (and broadly, Open Images). Hence, we use a FasterRCNN [36] model pre-trained using Open Images V4 [20] (referred as OI detector henceforth), to obtain candidate region proposals as described in Section 4 of the main paper. This model can detect 601 object classes of Open Images, which includes the novel object classes of nocaps. In contrast, the authors\u2019 implementation uses a Faster-RCNN trained using COCO.\nFor every image in COCO train 2017 split, we extract image region proposals after the second stage of detection, with an IoU threshold of 0.5 to avoid highly overlapping region proposals, and a class detection confidence threshold of 0.5 to reduce false positive detections. This results in number of region proposals per image varies up to a maximum of 18.\nBottom-Up Visual Features The language model in NBT (Refer Figure 4 in [27]) has two separate attention layers, and takes visual features as input in three different manners:\n- The first attention layer learns an attention distribution over region features, extracted using ResNet-101 + RoI Align layer. - The second attention layer learns an attention distribution over spatial CNN features from the last convolutional layer of ResNet-101 (7\nx 7 grid, 2048 channels). - The word embedding input is concatenated with FC7 features from ResNet-101 at every time-step.\nAll the three listed visual features are extracted using ResNet-101, with the first being specific to visual words, while the second and third provide the holistic context of the image. We replace the ResNet-101 feature extractor with the publicly available Faster-RCNN model pre-trained using Visual Genome (referred as VG detector henceforth), same as [4]. Given a set of candidate region proposals obtained from OI detector, we extract 2048-dimensional bottom-up features using the VG detector and use them as input to first attention layer (and also for input to the Pointer Network). For input to the second attention layer, we extract top-36 bottom-up features (class agnostic) using the VG detector. Similarly, we perform mean-pooling of these 36 features for input to the language model at every time-step.\nFine-grained Class Mapping NBT fills the slots in each caption template using words corresponding to the object classes detected in the corresponding image regions. However, object classes are coarse labels (e.g. \u2018cake\u2019), whereas captions typically refer entities in a fine-grained fashion (e.g. \u2018cheesecake\u2019, \u2018cupcake\u2019, \u2018coffeecake\u2019 etc.). To account for these linguistic variations, NBT predicts a fine-grained class for each object class using a separate MLP classifier. To determine the output vocabulary for this fine-grained classifier we extend the fine-grained class mapping used for COCO (Refer Table 5 in [27]), adding Open Images object classes. Several fine-grained classes in original mapping are already present in Open Images (e.g. \u2018man\u2019, \u2018woman\u2019 \u2013 fine-grained classes of \u2018person\u2019), we drop them as fine-grained classes from original mapping and retain them as Open Images object classes.\nVisual Word Prediction Criterion In order to ensure correctness in visual grounding, the authors\u2019 implementation uses three criteria to decide whether a particular region proposal should be tied with a \"slot\" in the caption template. At any time during decoding, when the Pointer Network attends to a visual feature (instead of the visual sentinel), the corresponding region proposal is tied with the \"slot\" if:\n- The class prediction threshold of this region proposal is higher than 0.5. - The IoU of this region proposal with at least one of the ground truth bounding boxes is greater than 0.5. - The predicted class is same as the object class of ground truth bounding box having highest IoU with this region proposal.\nWe drop the third criterion, as the OI detector can predict several fine-grained classes in context of COCO, such as \u2018man\u2019 and \u2018woman\u2019 (while the ground truth object class would be \u2018person\u2019). Keeping the third criterion intact in nocaps setting would suppress such region proposals, and result in lesser visual grounding, which is not desirable for NBT. Relaxation of this criterion might introduce false positives from detection in the caption but prevents reduction in visual grounding.\nWe encourage the reader to refer the authors\u2019 implementation for further details. We will release code for our modifications."
        },
        {
            "heading": "4.2. Constrained Beam Search (CBS)",
            "text": "Determining Constraints When using constrained beam search (CBS) [2], we decoded the model in question while forcing the generated caption to include words corresponding to object classes detected in the image. For object detection, we use the same Faster-RCNN [36] model pre-trained using Open Images V4 [20] (OI detector) that is used in conjunction with NBT. However, not all detected object classes are used as constraints. We perform constraint filtering by removing the 39 object classes listed in Table 5 from the constraint set, as these classes are either object\nparts, or classes that we consider to be either too rare or too broad. We also suppress highly overlapping objects as described in Section 4 of the main paper.\nTo quantify the impact of this simple constraint filtering heuristic, in Table 6 we report the results of the following ablation studies:\n- Using all the object classes for constraints (w/o class), - Using overlapping objects for constraints (w/o overlap), and - Using no filtering heuristic at all (w/o both).\nNote that in all cases we rank objects based on confident score for detected objects and pick the top-3 as the constraints. We report results for three models, the baseline model (UpDown), the baseline model using Glove [33] and dependency-based [23] word embeddings (UpDown + GD) and our ELMo-based model (UpDown + ELMo +CBS). Table 6 shows that removing the above 39 classes significantly improves the performance of constrained beam search and removing overlapping objects can also slightly improve the performance. This conclusion is consistent across the three models.\nFinite State Machine Constrained Beam Search implements constraints in the decoding process using a Finite State Machine (FSM). In all experiments we use a 24 state FSM. We use 8 states for standard three single word constraints D1, D2 and D3. As shown in Figure 15, the outputs of this FSM are the captions that mention at least two constraints out of three. Each Di (i = 1,2,3) represents a set of alternative constraint words (e.g., bike, bikes). Di can also be multi-word expressions. Our FSM can dynamically support two-word or three-word phrases in Di by extending additional one states (see Figure 13) or two states (see Figure 14) for two-word or three-word phrases respectively. Since D1, D2 and D3 are all used 4 times in the base eight-state FSM, we need to allocate 4 states for a single two-word expression and 8 states for a single three-word expression.\nIntegrating UpDown Model with ELMo When using ELMo [34], we use a dynamic representation ofwc, h\u03041t and h\u03042t as the input word embeddingwtELMo for our caption model. wc is the character embedding of input words and h\u0304it (i \u201c 1, 2) is the hidden output of ith LSTM layer of ELMo. We combine them via:\nwtELMo \u201c \u03b30 \u00a8 wc ` \u03b31 \u00a8 h\u03041t ` \u03b32 \u00a8 h\u03042t (1)\nwhere \u03b3i (i=0, 1, 2) are three trainable scalars. When using wtELMo as the external word representation of other models, we fixed all the parameters of ELMo but \u03b3i (i=0, 1, 2).\nIn addition, to handle unseen objects in training data, following [2], we initialize the softmax layer matrix (Wp, bp) using word embedding and keep this layer fixed during training. This allow our caption model to produce similar logits score for the words that share similar vectors and values in Wp and bp. We have:\nWp \u201cWELMo (2) bp \u201c bELMo (3)\nwhere WELMo and bELMo is the softmax layer in original ELMo language model. To align the different dimension in softmax layer and LSTM hidden state, we add an additional fully connected layer with a non-linearity function tanh. We have:\nvt \u201c tanhpWth2t ` btq (4) P pyt|y1:t\u00b41, Iq \u201c softmaxpWpvt ` bpq (5)\nwhere Wt P RH\u02c6E , bt P RE , H is LSTM hidden dimension, E is the word embedding dimension, Wp P RE\u02c6D , bp P RD and D is the vocabulary size.\nOther details of using ELMo In our experiment, we use the full tensorflow checkpoint trained on 1 Billion Word Language Model Benchmark5 from official ELMo tensorflow implementation project6.\nWhen selecting vocabularies for our model, we first extract all words from COCO captions and open image object labels. We then extend the open image object labels to both singular and plural word forms. Finally, we remove all the words that are not in ELMo output vocabularies. This allow us to use ELMo LM prediction for each decoding step.\nOur UpDown + ELMo model is optimized by SGD [5]. We conduct hyper-parameter tuning the model and choose the model based on its performance on nocaps val.\n5http://www.statmt.org/lm-benchmark/ 6https://github.com/allenai/bilm-tf/"
        }
    ],
    "title": "nocaps: novel object captioning at scale",
    "year": 2019
}