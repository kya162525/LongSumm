{
    "abstractText": "Managed languages such as Java and Scala are prevalently used in development of large-scale distributed systems. Under the managed runtime, when performing data transfer across machines, a task frequently conducted in a Big Data system, the system needs to serialize a sea of objects into a byte sequence before sending them over the network. The remote node receiving the bytes then deserializes them back into objects. This process is both performance-inefficient and laborintensive: (1) object serialization/deserialization makes heavy use of reflection, an expensive runtime operation and/or (2) serialization/deserialization functions need to be hand-written and are error-prone. This paper presents Skyway, a JVMbased technique that can directly connect managed heaps of different (local or remote) JVM processes. Under Skyway, objects in the source heap can be directly written into a remote heap without changing their formats. Skyway provides performance benefits to any JVM-based system by completely eliminating the need (1) of invoking serialization/deserialization functions, thus saving CPU time, and (2) of requiring developers to hand-write serialization functions. CCS Concepts \u2022 Information systems \u2192 Data management systems; \u2022 Software and its engineering\u2192Memory management;",
    "authors": [
        {
            "affiliations": [],
            "name": "Khanh Nguyen"
        },
        {
            "affiliations": [],
            "name": "Lu Fang"
        },
        {
            "affiliations": [],
            "name": "Christian Navasca"
        },
        {
            "affiliations": [],
            "name": "Guoqing Xu"
        },
        {
            "affiliations": [],
            "name": "Brian Demsky"
        },
        {
            "affiliations": [],
            "name": "Shan Lu"
        }
    ],
    "id": "SP:69e667494b277a44ae6484c811777711527d6b12",
    "references": [
        {
            "authors": [
                "S. ALSUBAIEE",
                "Y. ALTOWIM",
                "H. ALTWAIJRY",
                "A. BEHM",
                "V.R. BORKAR",
                "Y. BU",
                "M.J. CAREY",
                "I. CETINDIL",
                "M. CHEELANGI",
                "K. FARAAZ",
                "E. GABRIELOVA",
                "R. GROVER",
                "Z. HEILBRON",
                "Y. KIM",
                "C. LI",
                "G. LI",
                "J.M. OK",
                "N. ONOSE",
                "P. PIRZADEH",
                "V.J. TSOTRAS",
                "R. VERNICA",
                "J. WEN",
                "T. WESTMANN"
            ],
            "title": "AsterixDB: A scalable, open source BDMS",
            "venue": "Proc. VLDB Endow. 7,",
            "year": 2014
        },
        {
            "authors": [
                "L. BACKSTROM",
                "D. HUTTENLOCHER",
                "J. KLEINBERG",
                "LAN"
            ],
            "title": "Group formation in large social networks: Membership, growth, and evolution",
            "venue": "KDD",
            "year": 2006
        },
        {
            "authors": [
                "A. BELAY",
                "G. PREKAS",
                "A. KLIMOVIC",
                "S. GROSSMAN",
                "C. KOZYRAKIS",
                "E. BUGNION"
            ],
            "title": "IX: A protected dataplane operating system for high throughput and low latency",
            "year": 2014
        },
        {
            "authors": [
                "V.R. BORKAR",
                "M.J. CAREY",
                "R. GROVER",
                "N. ONOSE",
                "R. VERNICA"
            ],
            "title": "Hyracks: A flexible and extensible foundation for dataintensive computing",
            "venue": "ICDE",
            "year": 2011
        },
        {
            "authors": [
                "Y. BU",
                "V. BORKAR",
                "G. XU",
                "M.J. CAREY"
            ],
            "title": "A bloat-aware design for big data applications",
            "venue": "ISMM",
            "year": 2013
        },
        {
            "authors": [
                "R. CHAIKEN",
                "B. JENKINS",
                "LARSON",
                "P.-A",
                "B. RAMSEY",
                "D. SHAKIB",
                "S. WEAVER",
                "J. ZHOU"
            ],
            "title": "SCOPE: easy and efficient parallel processing of massive data sets",
            "venue": "Proc. VLDB Endow. 1,",
            "year": 2008
        },
        {
            "authors": [
                "J. CHASE",
                "M. BAKER-HARVEY",
                "H. LEVY",
                "E. LAZOWSKA"
            ],
            "title": "Opal: A single address space system for 64-bit architectures",
            "venue": "SIGOPS Oper. Syst. Rev. 26,",
            "year": 1992
        },
        {
            "authors": [
                "J. DEAN",
                "S. GHEMAWAT"
            ],
            "title": "MapReduce: Simplified data processing on large clusters",
            "venue": "OSDI",
            "year": 2004
        },
        {
            "authors": [
                "I. EL HAJJ",
                "A. MERRITT",
                "G. ZELLWEGER",
                "D. MILOJICIC",
                "R. ACHER- MANN",
                "P. FARABOSCHI",
                "HWU",
                "W.-M",
                "T. ROSCOE",
                "K. SCHWAN"
            ],
            "title": "SpaceJMP: Programming with multiple virtual address spaces",
            "year": 2016
        },
        {
            "authors": [
                "L. FANG",
                "K. NGUYEN",
                "G. XU",
                "B. DEMSKY",
                "LU"
            ],
            "title": "Interruptible tasks: Treating memory pressure as interrupts for highly scalable data-parallel programs",
            "year": 2015
        },
        {
            "authors": [
                "I. GOG",
                "J. GICEVA",
                "M. SCHWARZKOPF",
                "K. VASWANI",
                "D. VYTINIO- TIS",
                "G. RAMALINGAM",
                "M. COSTA",
                "D.G. MURRAY",
                "S. HAND",
                "ISARD",
                "M. Broom"
            ],
            "title": "Sweeping out garbage collection from big data systems",
            "venue": "HotOS",
            "year": 2015
        },
        {
            "authors": [
                "S.M. HAND"
            ],
            "title": "Self-paging in the nemesis operating system",
            "venue": "OSDI",
            "year": 1999
        },
        {
            "authors": [
                "M. ISARD",
                "M. BUDIU",
                "Y. YU",
                "A. BIRRELL",
                "D. FETTERLY"
            ],
            "title": "Dryad: distributed data-parallel programs from sequential building blocks",
            "venue": "EuroSys",
            "year": 2007
        },
        {
            "authors": [
                "A. LINDSTROM",
                "J. ROSENBERG",
                "A. DEARLE"
            ],
            "title": "The grand unified theory of address spaces",
            "venue": "HotOS",
            "year": 1995
        },
        {
            "authors": [
                "X. LU",
                "M.W.U. RAHMAN",
                "N. ISLAM",
                "D. SHANKAR",
                "D.K. PANDA"
            ],
            "title": "Accelerating Spark with RDMA for big data processing: Early experiences",
            "venue": "HOTI",
            "year": 2014
        },
        {
            "authors": [
                "M. MAAS",
                "T. HARRIS",
                "K. ASANOVI\u0106",
                "J. KUBIATOWICZ"
            ],
            "title": "Trash Day: Coordinating garbage collection in distributed systems",
            "venue": "HotOS",
            "year": 2015
        },
        {
            "authors": [
                "M. MAAS",
                "T. HARRIS",
                "K. ASANOVI\u0106",
                "J. KUBIATOWICZ"
            ],
            "title": "Taurus: A holistic language runtime system for coordinating distributed managed-language applications",
            "venue": "ASPLOS",
            "year": 2016
        },
        {
            "authors": [
                "J. NELSON",
                "B. HOLT",
                "B. MYERS",
                "P. BRIGGS",
                "L. CEZE",
                "S. KAHAN",
                "M. OSKIN"
            ],
            "title": "Latency-tolerant software distributed shared memory",
            "venue": "In USENIX ATC",
            "year": 2015
        },
        {
            "authors": [
                "K. NGUYEN",
                "L. FANG",
                "G. XU",
                "B. DEMSKY",
                "S. LU",
                "S. ALAMIAN",
                "O. MUTLU"
            ],
            "title": "Yak: A high-performance big-data-friendly garbage collector",
            "venue": "OSDI",
            "year": 2016
        },
        {
            "authors": [
                "NGUYEN K",
                "WANG K",
                "BU Y",
                "FANG L",
                "HU J",
                "XU"
            ],
            "title": "FACADE: A compiler and runtime for (almost) object-bounded big data applications",
            "year": 2015
        },
        {
            "authors": [
                "C. OLSTON",
                "B. REED",
                "U. SRIVASTAVA",
                "R. KUMAR",
                "A. TOMKINS"
            ],
            "title": "Pig Latin: a not-so-foreign language for data processing",
            "year": 2008
        },
        {
            "authors": [
                "S. PETER",
                "J. LI",
                "I. ZHANG",
                "D.R.K. PORTS",
                "D. WOOS",
                "A. KR- ISHNAMURTHY",
                "T. ANDERSON",
                "T. ROSCOE"
            ],
            "title": "Arrakis: The operating system is the control plane",
            "year": 2014
        },
        {
            "authors": [
                "R. PIKE",
                "S. DORWARD",
                "R. GRIESEMER",
                "S. QUINLAN"
            ],
            "title": "Interpreting the data: Parallel analysis with Sawzall",
            "venue": "Sci. Program",
            "year": 2005
        },
        {
            "authors": [
                "R. RASHID",
                "A. TEVANIAN",
                "M. YOUNG",
                "D. GOLUB",
                "R. BARON",
                "D. BLACK",
                "W. BOLOSKY",
                "J. CHEW"
            ],
            "title": "Machine-independent virtual memory management for paged uniprocessor and multiprocessor architectures",
            "year": 1987
        },
        {
            "authors": [
                "P. STUEDI",
                "B. METZLER",
                "A. TRIVEDI"
            ],
            "title": "jVerbs: Ultra-low latency for data center applications",
            "venue": "SOCC",
            "year": 2013
        },
        {
            "authors": [
                "M. TAKAHASHI",
                "K. KONO",
                "T. MASUDA"
            ],
            "title": "Efficient kernel support of fine-grained protection domains for mobile code",
            "venue": "ICDCS",
            "year": 1999
        },
        {
            "authors": [
                "A. THUSOO",
                "J.S. SARMA",
                "N. JAIN",
                "Z. SHAO",
                "P. CHAKKA",
                "S. ANTHONY",
                "H. LIU",
                "P. WYCKOFF",
                "R. MURTHY"
            ],
            "title": "Hive: a warehousing solution over a map-reduce framework",
            "venue": "Proc. VLDB Endow. 2,",
            "year": 2009
        },
        {
            "authors": [
                "D.J. WATTS",
                "S.H. STROGATZ"
            ],
            "title": "Collective dynamics of \u2018smallworld",
            "venue": "networks. Nature",
            "year": 1998
        },
        {
            "authors": [
                "M. WEGIEL",
                "C. KRINTZ"
            ],
            "title": "XMem: Type-safe, transparent, shared memory for cross-runtime communication and coordination",
            "venue": "PLDI",
            "year": 2008
        },
        {
            "authors": [
                "YANG",
                "H.-C",
                "A. DASDAN",
                "HSIAO",
                "R.-L",
                "D.S. PARKER"
            ],
            "title": "Mapreduce-merge: simplified relational data processing on large clusters",
            "year": 2007
        },
        {
            "authors": [
                "Y. YU",
                "P.K. GUNDA",
                "M. ISARD"
            ],
            "title": "Distributed aggregation for data-parallel computing: Interfaces and implementations",
            "venue": "SOSP",
            "year": 2009
        },
        {
            "authors": [
                "Y. YU",
                "M. ISARD",
                "D. FETTERLY",
                "M. BUDIU",
                "U. ERLINGSSON",
                "P.K. GUNDA"
            ],
            "title": "AND CURREY, J. DryadLINQ: a system for generalpurpose distributed data-parallel computing using a high-level language",
            "year": 2008
        },
        {
            "authors": [
                "M. ZAHARIA",
                "M. CHOWDHURY",
                "T. DAS",
                "A. DAVE",
                "J. MA",
                "M. MC- CAULEY",
                "M.J. FRANKLIN",
                "S. SHENKER",
                "I. STOICA"
            ],
            "title": "Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing",
            "year": 2012
        }
    ],
    "sections": [
        {
            "text": "CCS Concepts \u2022 Information systems \u2192 Data management systems; \u2022 Software and its engineering\u2192Memory management;\nKeywords Big data, distributed systems, data transfer, serialization and deserialization\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org."
        },
        {
            "heading": "ASPLOS \u201918, March 24\u201328, 2018, Williamsburg, VA, USA",
            "text": "\u00a9 2018 Copyright held by the owner/author(s). Publication rights licensed to the Association for Computing Machinery. ACM ISBN 978-1-4503-4911-6/18/03. . . $15.00 https://doi.org/10.1145/3173162.3173200"
        },
        {
            "heading": "ACM Reference Format:",
            "text": "Khanh Nguyen, Lu Fang, Christian Navasca, Guoqing Xu, Brian Demsky, and Shan Lu. 2018. Skyway: Connecting Managed Heaps in Distributed Big Data Systems. In Proceedings of ASPLOS \u201918. ACM, New York, NY, USA, ?? pages. https://doi.org/10.1145/ 3173162.3173200"
        },
        {
            "heading": "1 Introduction",
            "text": "Modern Big Data systems need to frequently shuffle data in the cluster \u2013 a map/reduce framework such as Hadoop shuffles the results of each map worker before performing reduction on them; a dataflow system such as Spark supports many RDD transformations that need to shuffle data across nodes. As most of these systems are written in managed languages such as Java and Scala, data is represented as objects in a managed heap. Transferring an object o across nodes is complicated, involving three procedures shown in Figure 1. (1) A serialization procedure turns the whole object graph reachable from o into a binary sequence. This procedure reformats each object \u2014 among other things, it extracts the object data, strips the object header, removes all references stored in an object, and changes the representation of certain meta data. (2) This byte sequence is transferred to a receiver machine. (3) A deserialization procedure reads out the byte sequence, creates objects accordingly, and eventually rebuilds the object graph in the managed heap of the receiver machine.\nProblems While many serialization/deserialization (S/D) libraries [5, 19, 30] have been developed, large inefficiencies exist in their implementations. Both our own experience (\u00a72) and evidence from previous work [24] show that S/D accounts for 30% of the execution time in Spark. To explain why S/D is so costly, we discuss the handling of three key pieces of information these procedures have to extract, transfer, and reconstruct for every object reachable from o: (1) object data (i.e., primitive-type fields), (2) object references (i.e., reference-type fields), and (3) object type.\n(1) Object-data access: An S/D library needs to invoke reflective functions such as Reflection.getField and Reflection.setField to enumerate and access every field to extract, on the sender side, and then write-back, on the receiver side, each primitive object field individually. In a Big"
        },
        {
            "heading": "ASPLOS \u201918, March 24\u201328, 2018, Williamsburg, VA, USA K. Nguyen et al.",
            "text": "Data system, each data transfer involves many millions of objects, which would invoke these functions for millions of times or more. Reflection is a very expensive runtime operation. It allows the program to dynamically inspect or invoke classes, methods, fields, or properties without type information available statically at the cost of time-consuming string lookups, and is undesirable in performance-critical tasks.\n(2) Type representation: Each type is represented by a special (meta) object in a managed runtime, and is referenced by the headers of the objects of the type. However, type references cannot be used to represent types in a byte sequence, because the meta objects representing the same type may have different addresses in different runtimes. The Java serializer represents every type by a string that contains the name of a class and all its super classes. This design causes meta data (i.e., type strings) to consume a huge portion of the byte sequence transferred across the network. Furthermore, reflection must be used to resolve the type from each string during object re-creation on the receiver node.\n(3) Reference adjustment: References contained in reference-type fields of transferred objects need to be adjusted, since those objects will be placed in different addresses on the receiver node. The Java serializer uses reflection to obtain and inline the contents of referenced objects into the binary representation of the referencing object. It constructs all objects reachable from o on the receiver machine using reflection, and then sets reference fields with the addresses of the just created referenced objects through reflection.\nRecent Progresses Many third-party libraries have been developed. In particular, Kryo [19] is the library recommended in Spark. Kryo asks developers (1) to manually define S/D functions for types involved in data transfer, which speeds up object-data access, and (2) to manually register these types in a consistent order across all nodes, which makes it possible to use integers to represent types. Other libraries [5, 30? ] follow similar principles.\nHowever, the fundamental inefficiencies in data transfer still remain in Kryo \u2013 the user-defined functions need to be invoked for every transferred object at both the sender side and the receiver side. Due to the extremely large number of invocations of these S/D functions during sending and receiving, serialization and deserialization still takes a large portion of a data processing task\u2019s run time.\nFurthermore, tremendous burden is put on developers who use Kryo. It is difficult for developers to understand how many and what types are involved, let alone consistently registering these types and developing correct and efficient S/D functions for each type. For instance, consider a HashMap object. Its serialization involves its key-value array, all the key/value pairs, and every key/value object. Its deserialization needs to recreate key and value objects, pair them, and additionally reshuffle key/value pairs to correctly recreate the key-value array because the hash values of keys may have changed.\nOur Solution \u2013 Skyway The key problem with existing S/D libraries is that, with an existing JVM, there are no alternative routes to transfer objects other than first disassembling and pushing them down to a (different) binary format, and then reassembling and pulling them back up into a remote heap. In this paper, we advocate to build a \u201cskyway\u201d between managed heaps (shown in Figure 1) so that data objects no longer need to be pushed down to a lower level for transfer.\nSkyway enhances the JVM, and enables object graphs to be moved as is from heap to heap and used on a remote node right after the move. Specifically, given a root object o specified by the application (e.g., the RDD object in Spark), the Skyway-enhanced JVM performs a GC-like heap traversal starting from o, copies every reachable object into an output buffer, and conducts lightweight adjustment to machinedependent meta data stored in an object without changing the object format. This output buffer can then be copied as a whole directly into the remote heap and used almost immediately after the transfer. This provides the following benefits to existing and future Big Data systems: (1) Skyway completely eliminates the cost of accessing fields and types, saving computation costs; and (2) the developer does not need to hand-write any S/D functions.\nTo achieve these goals, Skyway addresses the aforementioned three issues much more efficiently than all the existing S/D libraries, as discussed below.\nFirst, Skyway, by changing the JVM, transfers every object as a whole, which completely eliminates the need of accessing individual data fields. Furthermore, since the hashcode of an object is cached in the header of the object, transferring the entirety of each object preserves the original hashcode of the object, so that hash-based data structures can be used on the receiver node without rehashing \u2014 a process that takes a great amount of time in traditional S/D.\nSecond, Skyway represents types by employing an automated global type-numbering procedure \u2013 the master node"
        },
        {
            "heading": "Skyway: Connecting Managed Heaps in Distributed Big Data Systems ASPLOS \u201918, March 24\u201328, 2018, Williamsburg, VA, USA",
            "text": "maintains a registry of all types and their IDs, and each worker node communicates with the master to obtain IDs for its classes upon class loading. This process enables all classes across the cluster to be globally numbered without any developer intervention and thus each ID can be used to uniquely identify the same class on different nodes.\nThird, Skyway employs an efficient \u201crelativization\u201d technique to adjust references. As objects are copied into the output buffer, pointers stored in them are relativized in linear time \u2014 they are changed from absolute addresses to relative addresses. Upon receiving the buffer, the Skyway client on the receiver node performs another linear scan of the input buffer to absolutize the relative information in the buffer.\nSkyway may push more bytes over the network than S/D libraries, because it transfers the entirety of each object yet S/D libraries do not transfer object headers. However, much evidence [? ] shows that bottlenecks in real systems are shifting from I/O to computing, and hence, we believe this design strikes the right design tradeoff \u2014 the savings on the computation cost significantly outweigh the extra network I/O cost incurred by the extra bytes transferred on a modern network. Our empirical results show that, even on a 1000Mb/s Ethernet (e.g., most data centers use networks with higher bandwidth), transferring 50% of more data (about 100GB in total) in Spark for a real graph dataset increases the execution by only 4% (on network and read I/O) whereas the savings achieved by eliminating the S/D invocations are beyond 20%.\nWhy Does It Work? It is important to note that Skyway is not a general-purpose serializer. Our insight why Skyway would work well for Big Data processing is two-fold. First, data processing applications frequently shuffle many millions of objects and do so in strongly delimited phases. Hence, sending objects in batch without changing their formats provides significant execution efficiency. Second, the use of modern network technology enables extra bytes to be quickly transferred without incurring much overhead.\nWe have implemented Skyway in OpenJDK 8. Our evaluation on a Java serializer benchmark set JSBS [? ], Spark [41], and Flink [? ] shows that (1) Skyway outperforms all the 90 existing S/D libraries on JSBS, which uses a media-content based dataset \u2013 for example, it is 2.2\u00d7 faster than Kryo and 67.3\u00d7 faster than the Java serializer; (2) compared with Kryo and the Java serializer, Skyway improves the overall Spark performance by 16% and 36% for four representative analytical tasks over four real-world datasets; (3) for another real-world system Flink, Skyway improves its overall performance by 19% compared against Flink\u2019s highly-optimized built-in serializers."
        },
        {
            "heading": "2 Background and Motivation",
            "text": "This section gives a closer examination of S/D and its cost using Spark as an example."
        },
        {
            "heading": "2.1 Background",
            "text": "When Does S/D Happen? Spark conducts S/D throughout the execution. There are two categories of S/D tasks: closure serialization and data serialization. Closure S/D occurs between the driver and a worker. Since a Spark program is launched by the driver, the driver needs to execute portions of it on remote workers.\nFigure 3 shows a Spark program that reads a sequence of strings, each of which represents a date, from a text file (Line 27). It next parses these strings by invoking a map function on the RDD (Line 28). The map transformation takes a lambda expression (i.e., a closure) as input, which parses each string by invoking the parse function that turns a string into a Date object. Finally, the RDD action collect is invoked to bring all Date objects to the driver.\nWhile this program is executed by the driver, Spark schedules the execution of the closure (i.e., the lambda expression passed to map) on the worker nodes. Closure serialization is thus needed to transfer the closure and everything it needs from the driver to each worker node. In this example, the closure refers to the object parser created outside its scope. Hence, parser also needs to be serialized during closure"
        },
        {
            "heading": "ASPLOS \u201918, March 24\u201328, 2018, Williamsburg, VA, USA K. Nguyen et al.",
            "text": "serialization. This explains why the DateParser class needs to implement the Java Serializable interface.\nThe second type of S/D is data serialization that occurs between different workers or a worker and the driver. For example, action collect would cause all Date objects on the worker nodes to be transferred back to the driver. When each Date object is serialized, all the (Year4D, Month2D, and Day2D) objects directly or transitively reachable from it are serialized as well. To shuffle data across nodes, Spark serializes data objects on each node (e.g., the result of a map operation) into disk files with a shuffling algorithm (e.g., sortbased or hash-based). These files are then sent to different remote nodes where data objects are deserialized.\nHow Does S/D Work? The Kryo serializer requires the developer to register classes using the following code snippet:\n1 SparkConf conf = new SparkConf (); 2 conf.set(\"spark.kryo.registrator\", \"org.apache. spark.examples.MyRegistrator\"); 3 ... 4 public class MyRegistrator implements KryoRegistrator { 5 public void registerClasses(Kryo kryo) { 6 kryo.register(Date.class); 7 kryo.register(Year4D.class); 8 kryo.register(Month2D.class); 9 kryo.register(Day2D.class);\n10 } 11 }\nThe order in which these classes are registered defines an integer ID for each class. Using these integer class identifiers, the bytes generated by Kryo do not contain strings, leading to significant space savings during data transfer. Furthermore, Kryo deserializer can now resolve types without using reflection \u2014 Kryo automatically generates code like\n1 switch(id) { 2 case 0: return new Date(); 3 case 1: return new Year4D (); 4 ... 5 }\nthat uses regular new instructions to create objects on the receiving node.\nHowever, in any real-world application, there can be a large number of user classes defined (including many classes from different libraries). Fully understanding what classes are referenced (directly or transitively) is a very labor-intensive process. Moreover, the developer has to manually develop S/D functions for each of these types; without these functions, the standard Java serializer would be used instead.\nIn both Kryo and the standard Java serializer, the number of times S/D functions are invoked is proportional to the dataset cardinality; every data transfer can easily require several millions of S/D invocations, taking a significant fraction of the execution time."
        },
        {
            "heading": "2.2 Motivation",
            "text": "To understand the S/D costs in the real world, we have performed a set of experiments on Spark. We execute Spark on a small cluster of 3 worker nodes, each with 2 Xeon(R) CPU E5-2640 v3 processors, 32GB memory, 1 SSD, running CentOS 6.8. These three nodes are part of a large cluster connected via InfiniBand. We ran a TriangleCounting algorithm over the LiveJournal graph [6] that counts the number of triangles induced by graph edges. It is widely used in social network analysis for analyzing the graph connectivity properties [35]. We used Oracle JDK 8 (build 25.71) and let each slave run one single executor \u2013 the single-thread execution on each slave made it easy for us to measure the breakdown of performance. The size of the input graph was around 1.2GB and we gave each JVM a 20GB heap \u2013 a large enough heap to perform in-memory computation \u2013 as is the recommended practice in Spark. Tungsten sort was used to shuffle data.\nFigure 4(a) shows Spark\u2019s performance under the Kryo and Java serializers. Before transferring data over the network, Spark shuffles and sorts records, and saves the sorted records as disk files. The cost is thus broken down into five components: computation time, serialization time (measured as time spent on turning RDD records into byte sequences), write I/O (measured as the time writing bytes onto disk), deserialization time (measured as time spent on reconstructing RDD record objects from bytes), and read I/O (measured as time reading bytes). Since each JVM has a large heap compared to the amount of data processed, the garbage collection cost is less than 2% and thus not shown on the figure. The network cost is negligible and included in the read I/O.\nOne observation is that the invocation of S/D functions takes a huge portion (more than 30%) of the total execution time under both Kryo and the Java serializer. Under Kryo, the invocations of the serialization and deserialization take 18.2% and 14.1% of the total execution time, respectively; under the Java serializer, these two take 16.3% and 17.8%. The actual write and read I/O time is much shorter in comparison, taking"
        },
        {
            "heading": "Skyway: Connecting Managed Heaps in Distributed Big Data Systems ASPLOS \u201918, March 24\u201328, 2018, Williamsburg, VA, USA",
            "text": ""
        },
        {
            "heading": "Cluster",
            "text": "1.4% and 1.1% under Kryo, and 2.3% and 9.9% under the Java serializer. The read I/O is significantly increased under the Java serializer primarily because the Java serializer needs to read many type strings. For example, serializing an object containing a 1-byte data field can generate a 50-byte sequence [1] \u2013 in addition to its own field and the fields in its superclasses, the serializer needs to (1) write out the class name and (2) recursively write out the description of the superclasses of the object\u2019s class until it reaches java.lang.Object (i.e., the root of all classes). This is validated by the \u201cRemote Bytes\u201d results in Figure 4(b).\nAnother observation is that the S/D process is a bottleneck that cannot be easily removed by upgrading hardware. Unlike other bottlenecks such as GC (that can be eliminated almost entirely by using a large heap) or I/O (that can be significantly reduced by using fast SSDs and InfiniBand networks), S/D is a memory- and compute-intensive process that turns heap objects into bytes and vice versa. The inefficiencies inherent in the process strongly call for system-level optimizations."
        },
        {
            "heading": "3 Design Overview",
            "text": "This section provides an overview of Skyway, explaining how Skyway is designed towards three goals \u2014 correctness, efficiency, and ease of integration.\nFigure 5 shows the system architecture of Skyway, including three major parts. First, to achieve correct data transfer, Skyway modifies the JVM to conduct object traversal, object cloning, and adjustment within each cloned object. Second, to achieve efficient data transfer, Skyway carefully maintains input and output buffers, and streams buffer content across machines. Third, to make Skyway easy to use, Skyway library provides a set of easy-to-use and backward-compatible APIs for application developers."
        },
        {
            "heading": "3.1 Correctness",
            "text": "Skyway adjusts machine-specific parts of each transferred object to guarantee execution correctness. First, Skyway fills the type field of an object header with an automatically maintained global type-ID during sending, and later replaces it\nwith the correct type representation on the receiving node. The details are presented in \u00a74.1. Second, Skyway replaces the references stored in all non-primitive fields of an object with relativized references during sending, and turns them back to the correct absolute references during receiving. The details are presented in \u00a74.2. Finally, certain meta data such as GC bits and lock bits need to be reset when objects are moved to another machine. Skyway resets these flags at sending, and does not need to access them at receiving.\nSkyway also provides support for heterogeneous clusters where JVMs on different machines may support different object formats. If the sender and receiver nodes have different JVM specifications, Skyway adjusts the format of each object (e.g., header size, pointer size, or header format) when copying it into the output buffer. This incurs an extra cost only on the sender node while the receiver node pays no extra cost for using the transferred objects. For homogeneous clusters, such platform-adjustment cost is not incurred on any nodes. The only assumption Skyway uses is that the sender and the receiver use the same version of each transfer-related class \u2013 if two versions of the same class have different fields, object reading would fail. However, this assumption is not unique for Skyway; it needs to hold for all other serializers as well."
        },
        {
            "heading": "3.2 Efficiency",
            "text": "Skyway uses a GC-like traversal to discover the object graph reachable from a set of root objects. To improve efficiency, Skyway uses buffering \u2014 Skyway copies every object encountered during the traversal into a buffer on the sending node (i.e., output buffer) and streams the buffer content to the corresponding buffer(s) on the receiving node (i.e., input buffer). Both output and input buffers are carefully designed for efficiency concerns. Multi-threaded data transfer is also supported (cf. \u00a74).\nSkyway output buffers are segregated by receivers \u2014 objects with the same destination are put into the same output buffer. Only one such output buffer exists for each destination. The output buffer can be safely cleared after its objects are sent. Skyway input buffers are segregated by senders, so that data objects coming from different senders can be written simultaneously without synchronization. Note that the heap of a receiver node may actually contain multiple input buffers for each sender, each holding objects sent in a different round of shuffling from the sender. Skyway does not reuse an old input buffer unless the developer explicitly frees the buffer using an API \u2013 frameworks such as Spark cache all RDDs in memory and thus Skyway keeps all input buffers.\nOutput buffers are located in off-the-heap native memory \u2013 they will not interfere with the GC, which could reclaim data objects before they are sent if these buffers were in the managed heap. Input buffers are allocated from the managed heap so that data coming from a remote node is directly written into the heap and can be used right away. Furthermore, while each input buffer is shown as consuming contiguous"
        },
        {
            "heading": "ASPLOS \u201918, March 24\u201328, 2018, Williamsburg, VA, USA K. Nguyen et al.",
            "text": "heap space in Figure 5, we allow it to span multiple small memory chunks for two reasons. First, due to streaming, the receiver may not have the knowledge of the number of sent bytes, and hence, determining the input-buffer size is difficult. Second, allocating large contiguous space can quickly lead to memory fragmentation, which can be effectively mitigated by using smaller memory chunks. Details can be found in \u00a74.3.\nStreaming is an important feature Skyway provides for these buffers: for an output buffer, it is both time-inefficient and space-consuming if we do not send data until all objects are in; for an input buffer, streaming would allow the computation to be performed in parallel with data transfer. Supporting streaming creates many challenges, e.g., how to adapt pointers without multiple scans and how to manage memory on the receiver node. Details can be found in \u00a74.2."
        },
        {
            "heading": "3.3 Ease of Integration",
            "text": "Skyway aims to provide a simple interface for application developers. Skyway should support not only the development of brand new systems but also easy S/D library integration for existing systems such as Spark. To this end, Skyway provides a set of high-level Java APIs that are directly compatible with the standard Java serializer.\nSkyway provides SkywayObjectOutputStream and SkywayObjectInputStream classes that are subclasses of the standard ObjectOutputStream and ObjectInputStream. These two classes create an interface for Skyway\u2019s (native) implementation of the readObject and writeObject methods. A SkywayObjectOutputStream/SkywayObjectInputStream object is associated with an output/input buffer. We have also created our SkywayFileOutputStream/SkywayFileInputStream and SkywaySocketOutputStream/SkywaySocketInputStream classes \u2013 one can easily program with Skyway in the same way as programming with the Java serializer.\nSwitching a program from using its original library to using Skyway requires light code modifications. For example, we do not need to change object-writing/reading calls such as stream.writeObject(o) at all. The only modification is to (1) instantiate stream to be a SkywayFileOutputStream object instead of any other type of ObjectOutputStream objects and (2) identify a shuffling phase with an API function shuffleStart. Since all of our output buffers need to be cleared before the next shuffling phase starts (\u00a74), Skyway needs a mark from the developer to know when to clear the buffers. Identifying shuffling phases is often simple \u2013 in many systems, a shuffling phase is implemented by a shuffle function and the developer can simply place a call to shuffleStart in the beginning of the function. Also note that, user programs written to run on Big Data systems, such as the one in Figure 3, mostly do not directly use S/D libraries and hence can benefit from Skyway without changes.\nFinally, Skyway provides an interface that allows developers to easily update some object fields after the transfer,\nsuch as re-initializing some fields for semantic reasons. For example, the code snippet below updates field timestamp in the class Record with the value returned by the user-defined function updateTimeStamp when a Record object is transferred. Of course, we expect this interface to be used rarely \u2014 the need to update object data content after a transfer never occurs in our experiments.\n1 /* Register the update function */ 2 registerUpdate(Record.class , Record.class.getField(\n\"timeStamp\"), SkywayFieldUpdateFunctions. getFunction(SkywayUpdate.class , \" updateTimeStamp\", \"()[B\");\n3 ... 4 class SkywayUpdate{ 5 /*The actual update function */ 6 public byte[] updateTimeStamp (){ 7 return new byte []{0}; 8 } 9 }"
        },
        {
            "heading": "4 Implementation",
            "text": "We implemented Skyway in Oracle\u2019s production JVM OpenJDK 1.8.0 (build 25.71). In addition to implementing our object transfer technique, we have modified the classloader subsystem, the object/heap layout, and the Parallel Scavenge garbage collector, which is the default GC in OpenJDK 8. We have also provided a Skyway library for developers."
        },
        {
            "heading": "4.1 Global Class Numbering",
            "text": "Skyway develops a distributed type-registration system that automatically allows different representations of the same class on different JVM instances to share the same integer ID. This system completely eliminates the need of using strings to represent types during data transfer (as in the standard Java serializer) or the involvement of human developers to understand and register classes (as in Kryo).\nSkyway type registration runs inside every JVM and maintains a type registry, which maps every type string to its unique integer ID. The driver JVM assigns IDs to all classes; it maintains a complete type registry covering all the classes that have been loaded in the cluster and made known to the driver since the computation starts. Every worker JVM has a registry view, which is a subset of the type registry on the driver; it checks with the driver to obtain the ID for every class that it loads and does not yet exist in the local registry view. An example of these registries is shown in Figure 6.\nAlgorithm 1 describes the algorithms running on the driver and worker JVMs. The selection of the driver is done by the user through an API call inserted in the client code. For example, for Spark, one can naturally specify the JVM running the Spark driver as the Skyway driver, and all the Spark worker nodes run Skyway workers. Fault tolerance is provided by the application \u2013 e.g., upon a crash, Spark restarts the system on the Skyway-equipped JVMs; Skyway\u2019s driver JVM will be launched on the node that hosts Spark\u2019s driver."
        },
        {
            "heading": "Skyway: Connecting Managed Heaps in Distributed Big Data Systems ASPLOS \u201918, March 24\u201328, 2018, Williamsburg, VA, USA",
            "text": "At the beginning, the driver populates the registry by scanning its own loaded classes after the JVM finishes its startup logic (Lines 4 \u2013 8). Next, the driver switches to background by running a daemon thread that listens on a port to process lookup requests from the workers (Lines 10 \u2013 19).\nSkyway uses a pull-based communication between the driver and workers. Upon launching a worker JVM, it first requests (Line 22) and obtains (Line 12) the current complete type registry from the driver through a \u201cREQUEST_VIEW\u201d message. This provides each worker JVM with a view of all classes loaded so far in the cluster at its startup. The rationale behind this design is that most classes that will be needed by this worker JVM are likely already registered by the driver or other workers. Hence, getting their IDs in a batch is much more efficient than making individual remote-fetch requests.\nWe modify the class loader on each worker JVM so that during the loading of a class, the loader obtains the ID for the class. The loader first consults the registry view in its own JVM. If it cannot find the class, it goes on to communicate with the driver (Lines 29 \u2013 34) by a \u201cLOOKUP\u201d message with the class name string. The driver returns the ID if the string exists in its own registry or creates a new ID and registers it with the class name (Line 18). Once the worker receives this ID, it updates its registry view (Line 34). Finally, the worker JVM writes this ID into the meta object of the class (Line 35). In the JVM terminology, a meta object is called a \u201cklass\u201d (as shown in Figure 6). We add an extra field in each klass to accommodate its ID.\nDuring deserialization, if we encounter an unloaded class on the worker JVM, Skyway instructs the class loader to load the missing class since the type registry knows the full class name. While other options (e.g., low-collision hash functions such as the MD and SHA families) can achieve the same goal of assigning each class a unique ID, Skyway cannot use them as they cannot be used to recover class names.\nComparing with the standard Java serializer that sends a type string over the network together with every object, Skyway sends a type string at most once for every class on each machine during the whole computation. Naturally, the number of strings communicated under Skyway is several orders-of-magnitude smaller. Comparing with Kryo, Skyway\nAlgorithm 1: Driver and worker algorithms for global class numbering.\n1 /* Driver Program */ 2 /*Part 1: right after the JVM starts up*/ 3 JVMSTARTUP() /*Normal JVM startup logic*/ 4 /*Initialize the type registry*/ 5 globalID \u2190 0 6 registry \u2190 EMPTY_MAP 7 foreach class k loaded in the driver JVM do 8 registry \u2190 registry \u222a {(NAME(k ), globalID++) }\n9 /*Part 2: a daemon thread that constantly listens*/ 10 while Message m = ListenToWorkers() do 11 if m .type == \u201cREQUEST_VIEW\u201d then 12 SENDMSG(m .workerAddr , registry)\n13 else if m .type == \u201cLOOKUP\u201d then 14 /*The content of a \u201cLOOKUP\u201d message from worker to driver is a class string*/ 15 id \u2190 LOOKUP(registry, m .content) 16 if id == Null then 17 id \u2190 globalID++ 18 registry \u2190 registry \u222a {(m .content, id) } 19 SENDMSG(m .workerAddr , id)\n20 /* Worker Program*/ 21 /* Part 1: inside the JVM startup logic*/ 22 SENDMSG(driverAddr , COMPOSEMSG(\u201cREQUEST_VIEW\u201d, Null, myAddr))\n23 Message m = LISTENTODRIVER() 24 /*The content of a \u201cLOOKUP\u201d message is the registry map*/\nregistryView \u2190m .content 25 /* Part 2: after the class loading routine*/ 26 clsName \u2190 GETCLASSNAME() 27 metaObj \u2190 LOADCLASS(clsName) 28 id \u2190 LOOKUP(registryView, clsName) 29 if id == Null then 30 SENDMSG(driverAddr , COMPOSEMSG(\u201cLOOKUP\u201d, clsName,myAddr)) 31 Message m = LISTENTODRIVER() 32 /*The content of a message from driver to worker is an ID*/ 33 id \u2190 m.content 34 registryView \u2190 registryView \u222a {(clsName, id) } 35 WRITETID(metaObj, id)\nautomatically registers all classes, and eliminates the need for developers to understand what classes will be involved in data transfer, leading to significantly reduced human effort."
        },
        {
            "heading": "4.2 Sending Object Graph",
            "text": "Overview When writeObject(root) is invoked on a SkywayObjectOutputStream object, Skyway starts to traverse and send the object graph reachable from root. Algorithm 2 describes the single-threaded logic of copying the object graph reachable from a user-specified root, and we discuss the multi-threaded extension later in this section.\nAt a high level, Skyway mimics a BFS-based GC traversal. It maintains a queue gray holding records of every object that has been visited but not yet processed, as well as the location addr at which this object will be placed in the output buffer ob. Every iteration of the main loop (Line 8) processes the top record in gray and conducts three tasks.\nFirst, based on the object-address pair (s, addr) retrieved from gray, an object s is cloned into buffer ob at a location"
        },
        {
            "heading": "ASPLOS \u201918, March 24\u201328, 2018, Williamsburg, VA, USA K. Nguyen et al.",
            "text": "Algorithm 2: Copying the object graph reachable from object root and relativizing pointers for a single thread.\nInput: Shuffling phase ID sID, a top object root, output buffer ob 1 ob.allocableAddr \u2190 0 2 Word w \u2190 READ(root, OFFSET_BADDR) 3 pID \u2190 HIGHESTBYTE(w ) 4 /*root has not been visited in the current phase*/ 5 if pID < sID then 6 /*gray is a list of pairs of objects and their buffer addresses*/ 7 gray \u2190 {(root, ob.allocableAddr ) } 8 while gray , \u2205 do 9 Object-Address pair (s, addr ) \u2190 REMOVETOP(gray)\n10 CLONEINBUFFER(s , ob, addr \u2212 ob.flushedBytes) 11 /*Update the clone of s in the buffer*/ 12 WRITE(addr , OFFSET_BADDR, 0) 13 RESETMARKBITS(addr) 14 WRITE(addr , OFFSET_KLASS, s .klass.tID) 15 foreach Reference-typed field f of s do 16 Object o \u2190 s .f 17 if o ,Null then 18 Word v \u2190 READ(o, OFFSET_BADDR) 19 phaseID \u2190 HIGHESTBYTE(v ) 20 if phaseID < sID then 21 /* o has not been copied yet*/ newAddr \u2190 ob.allocableAddr 22 WRITE(o, OFFSET_BADDR, COMPOSE(sID, newAddr)) 23 PUSHTOQUEUE(gray, {(o, newAddr)}) 24 ob.allocableAddr += GETSIZE(o)"
        },
        {
            "heading": "25 else",
            "text": "26 newAddr \u2190 LOWEST7BYTES(v ) 27 WRITE(addr , OFFSET(f ), newAddr)"
        },
        {
            "heading": "28 else",
            "text": "29 oldAddr \u2190 LOWEST7BYTES(w ) 30 WRITEBACKWARDREFERENCE(oldAddr)"
        },
        {
            "heading": "31 SETTOPMARK()",
            "text": "calculated from addr (Line 10). CLONEINBUFFER would also adjust the format of the clone if Skyway detects that the receiver JVM has a different specification from the sender JVM, following a user-provided configuration file that specifies the object formats in different JVMs. Second, the header of the clone is updated (Lines 12 \u2013 22). Third, for every reference-typed field f of s, Skyway pushes the referenced object o into the working queue gray if o has not been visited yet and then updates f with a relativized address (i.e., o\u2019s position in output buffer), which will enable a fast reference adjustment on the receiver machine (Lines 15 \u2013 27).\nAs objects are copied into the buffer, which is in native memory, the buffer may be flushed (i.e., the streaming process). A flushing is triggered by an allocation at Line 10 \u2014 the allocation first checks whether the buffer still has space for the object s; if not, the buffer ob is flushed and the value of ob.flushedBytes is increased by the size of the buffer.\nReference Relativization Imagine that a reference field f of an object s points to an object o. Skyway needs to adjust f in the output buffer, as o may be put at a different address on the receiver node. Skyway replaces the cloned field f with the relative address in ob where o will be cloned to. This will\nallow the receiver node to easily calculate the correct absolute value for every reference in an input buffer, once the input buffer\u2019s starting address is determined.\nWe first describe the overall relativization algorithm, and then discuss how Skyway addresses the three challenges caused by streaming and multi-phase data shuffling.\nAs shown on Lines 15 \u2013 27 of Algorithm 2, for each reference-type field s . f , Skyway follows the reference to find the object (o). Skyway determines whether o has been visited in the current data-shuffling phase; details are discussed shortly. If not (Line 20), we know o will be cloned to the end of the output buffer at location ob.allocableAddr. We use this location to fill the baddr field of o (Line 22), and bump up ob.allocableAddr by the size of o to keep tracking the starting address of the next cloned object in ob. If o has been visited (Line 26), we retrieve its location in the output buffer from the lowest seven bytes of the baddr field in its object header, which we will explain more later. We then update the clone of f with this buffer location newAddr at which the clone of o will be or has already been placed (Line 27).\nThe first challenge is related to streaming. When Skyway tries to update f with the output-buffer location of o\u2019s clone (f points to o), this clone may have been streamed out and no longer exists in the physical output buffer. Therefore, Skyway has to carefully store such buffer-location information, making it available throughout a data-shuffling phase. Skyway saves the buffer location in the header of the original object, not the clone, using an extra field baddr. The modified object layout is shown in Figure 7(a). When o is reached again via a reference from another object o\u2032, the baddr in o will be used to update the reference in the clone of o\u2032.\nThe second challenge is also related to streaming. The buffer location stored in baddr of an object s and in its record in gray-queue both represent the accumulative bytes that have been committed to other objects in output buffer before s. However, when Skyway clones o into the buffer, it needs to account for the streaming effect that the physical buffer may have been flushed multiple times. Therefore, Skyway subtracts the number of bytes previously flushed ob.flushedBytes from addr when computing the actual address in the buffer to which s should be copied (Line 10).\nThe third challenge is due to multi-phase data shuffling. Since one object may be involved in multiple phases of shuffling, we need to separate the use of its baddr field for different shuffling phases. Skyway employs an sID to uniquely identify a shuffling phase. Whenever Skyway updates the baddr field, the current sID is written to as a prefix to the highest byte of baddr. Thus, Skyway can easily check whether the content in a baddr field is computed during the same phase of data shuffling (i.e., valid) or an earlier phase (i.e., invalid). Examples are on Lines 2 \u2013 5 and Lines 19 \u2013 20 of Algorithm 2. In the former case, if root has already been copied in the same shuffling phase (due to a copy procedure initiated by"
        },
        {
            "heading": "Skyway: Connecting Managed Heaps in Distributed Big Data Systems ASPLOS \u201918, March 24\u201328, 2018, Williamsburg, VA, USA",
            "text": "another root object), Skyway simply creates a backward reference pointing to its location in the buffer (Line 30). Skyway provides an API function shuffleStart that can be used by developers to mark a shuffling phase. sID is incremented when shuffleStart is invoked.\nHeader Update Lines 12 \u2013 14 update the header of the cloned object in buffer. Following Figure 7, Skyway first clears the baddr field of the cloned object; this field will be used later to restore the object on the receiver side. Second, Skyway processes the mark word in the header, resetting the GC and lock bits while preserving the object hashcode. Since hashcodes are used to determine the layout of a hash-based data structure (e.g., HashMap or HashSet), reusing them on the receiver side enables the immediate reuse of the data structure layout without rehashing. Third, Skyway replaces the klass pointer with the type ID stored in the klass meta object (Line 14).\nRoot Object Recognition After copying all objects reachable from root into the buffer, we set a top mark, which is a special byte indicating the starting point of the next top-level object. The reason for setting this mark is the following. For the original implementation of writeObject, an invocation of the function on a top object would in turn invoke the function itself recursively on the fields of the object to serialize the referenced objects. The deserialization process is exactly a reverse process \u2013 each invocation of readObject in the deserialization processes the bytes written in by its corresponding invocation of writeObject in serialization. However, Skyway\u2019s implementation of writeObject works in a different way \u2013 one invocation of the function on a top object triggers a system-level graph traversal that finds and copies all of its reachable objects. Similarly, Skyway\u2019s readObject also reads one object from the byte sequence instead of recursively reading out all reachable objects.\nAlthough on the receiver side we can still compute all reachable objects for a root, this computation also needs a graph traversal and is time-consuming. As an optimization, we let the sender explicitly mark the root objects so that the receiver-side computation can be avoided. This is achieved by top marks. With these top marks, Skyway can easily skip the lower-level objects in the middle and find the next top object. Note that this treatment does not affect the semantics of the program \u2013 all the data structures reachable from top objects are recovered by the system, not by the application APIs.\nSupport for Threads Algorithm 2 does not work in cases that multiple threads on one node try to transfer the same object concurrently (i.e., shared objects). Since each datatransfer thread has its own output buffer and the baddr field of a shared object can only store the relative buffer address for one thread t at a time, when other threads visit the object later, they would mistakenly use this address that is specific to t . To solve the problem, we let the lower seven bytes of baddr store both stream/thread ID (with the two highest bytes) and relative address (with the five lowest bytes).\nWhen an object is first visited by t , t\u2019s thread ID is written into baddr together with the address specific to t\u2019s buffer. When the object is visited again, Skyway first checks whether the ID of the visiting thread matches the thread ID stored in its baddr. If it does, baddr of the object is used; otherwise, Skyway switches to a hash table-based approach \u2013 each thread maintains a thread-local hash table; the object and its buffer address for the thread are added into the hash table as a key and a value. Compare-and-swap (CAS) is used to provide thread safety when updating each baddr.\nThis approach prevents a thread from mistakenly using the object\u2019s buffer address for another thread. An object will have distinct copies in multiple output buffers when visited by different threads; these copies will become separate objects after delivered to a remote node. This semantics is consistent with that of the existing serializers."
        },
        {
            "heading": "4.3 Receiving Object Graph",
            "text": "With the careful design on sending, the receiving logic is much simpler. To receive objects from a sender, the receiver JVM first prepares an input buffer, whose size is user-tunable, for the sender in its managed heap to store the transferred objects. A subtle issue here is that a sender node may use multiple streams (in multiple threads) to send data to the same receiver node simultaneously. To avoid race conditions, the receiver node creates an input buffer for each stream of each sender so that different streams/threads can transfer data without synchronizations. We create oversized buffers to fit large objects whose size exceeds the size of a regular buffer.\nAfter the input buffer is filled, Skyway performs a linear scan of the buffer to absolutize types and pointers. For the klass field of each object, Skyway queries the local registry view to get the correct klass pointer based on the type ID"
        },
        {
            "heading": "ASPLOS \u201918, March 24\u201328, 2018, Williamsburg, VA, USA K. Nguyen et al.",
            "text": "and writes the pointer into the field. For a relative address a stored in a reference field, Skyway replaces it with a + s where s is the starting address of this input buffer.\nThere is one challenge related to streaming. Since Skyway may not know the total size of the incoming data while allocating the buffer, one buffer of a fixed length may not be large enough. Skyway solves this by supporting linked chunks \u2013 a new chunk can be created and linked to the old chunk when the old one runs out of space. Skyway does not allow an object to span multiple chunks for efficiency. Furthermore, when a buffer contains multiple chunks, the address translation discussed above needs to be changed. We first need to calculate which chunk i a relative address a would fall in. Then, because previous chunks might not be fully filled, we need to calculate the offset of a in the i-th chunk. Suppose si is the starting address of chunk i and hence, si + offset is the final absolute address for a. This address will be used to replace a in each pointer field.\nAs each input buffer corresponds to a distinct sender, we can safely start the computation to process objects in each buffer for which streaming is finished. This would not create safety issues because objects that come from different nodes cannot reference each other. However, we do need to block the computation on buffers into which data is being streamed until the absolutization pass is done.\nInteraction with GC After receiving the objects, it is important for the Skyway client on the receiver JVM to make these objects reachable in the garbage collection. Skyway allocates all input buffers in the old generation (tenured) of the managed heap. In Skyway, we use the Parallel Scavenge GC (i.e., the default GC in OpenJDK 8), which employs a card table that groups objects into fixed-sized buckets and tracks which buckets contain objects with young pointers. Therefore, we add support in Skyway that updates the card table appropriately to represent new pointers generated from each data transfer."
        },
        {
            "heading": "5 Evaluation",
            "text": "To thoroughly evaluate Skyway, we have conducted three sets of experiments, one on a widely-used suite of benchmarks and the other two on widely-deployed systems Spark and Flink. The first set of experiments focuses on comparing Skyway with all existing S/D libraries \u2013 since most of these libraries cannot be directly plugged into a real system, we used the Java serializer benchmark set (JSBS) [? ], which was designed specifically to evaluate Java/Scala serializers, to understand where Skyway stands among existing S/D libraries. JSBS was initially designed to assess single-machine S/D. We modified this program to make it work in a distributed setting; details are discussed shortly.\nIn the second and third set of experiments, we modified the Spark and Flink code to replace the use of Kryo and the Java serializer (in Spark) and built-in serializers (in Flink) with\nSkyway in order to assess the benefit of Skyway to real-world distributed systems. All of our experiments were run on a cluster with 11 nodes, each with 2 Xeon(R) CPU E5-2640 v3 processors, 32GB memory, 1 100GB SSD, running CentOS 6.8 and connected by a 1000Mb/s Ethernet. Each node ran 8 job instances. The JVM on each node was configured to have a 30GB heap."
        },
        {
            "heading": "5.1 Java Serializer Benchmark Set",
            "text": "The JSBS contains several workloads under which each serializer and deserializer is repeatedly executed. Each workload contains several media content objects which consist of primitive int and long fields as well as reference-type fields. The driver program creates millions of such objects, each of which is around 1KB in JSON format. These objects are serialized into in-memory byte arrays, which are then deserialized back to heap objects. To understand the cost of transferring the byte sequences generated by different serializers, we modified the benchmark, turning it into a distributed program \u2013 each node serializes these objects, broadcasts the generated bytes to all the other nodes, and deserializes the received bytes back into objects. To execute this program, we involved five nodes and executed this process 1000 times repeatedly. The average S/D time for each object and the network cost are reported.\nWe have compared Skyway exhaustively with 90 existing S/D libraries. Due to space constraints, we excluded from the paper 63 slower libraries whose total S/D time exceeds 10"
        },
        {
            "heading": "Skyway: Connecting Managed Heaps in Distributed Big Data Systems ASPLOS \u201918, March 24\u201328, 2018, Williamsburg, VA, USA",
            "text": "seconds. The performance of the fastest 28 libraries is shown in Figure 8. Skyway, without needing any user-defined S/D functions, is the fastest of all of them. For example, it is 2.2\u00d7 faster than Kryo-manual, which requires manual development of S/D functions. It is more than 67\u00d7 faster than the Java serializer, which is not shown in the figure.\nColfer [? ] is the only serializer whose performance is close to (but still 1.5\u00d7 slower than) that of Skyway. It employs a compiler colf(1) to generate serialization source code from schema definitions to marshal and unmarshal data structures. Hence, the use of colf(1) requires user-defined schema of data formats, which, again, creates a practicality obstacle if data structures are complicated and understanding their layouts is a daunting task.\nSkyway\u2019s faster S/D speed is achieved at the cost of greater numbers of bytes serialized. For example, Skyway generates, on average, 50% more bytes than the existing serializers. The details of the numbers of bytes are omitted from the paper due to space constraints. Note that the increased data amount does not cause the network cost to change much, whereas the computation cost in S/D is significantly reduced."
        },
        {
            "heading": "5.2 Improving Spark with Skyway",
            "text": "Experience We have modified Spark version 2.1.0 (released December 2016) to replace the use of Kryo-manual with the Skyway library. Spark was executed under Hadoop version 2.6.5 and Scala version 2.11. Our experience shows that the library replacement was rather straightforward \u2013 to use Skyway, we created a Skyway serializer that wraps the existing Input/OutputStream with our SkywayInput/OuputStream objects. We modified the Spark configuration (spark.serializer) to invoke the Skyway serializer instead of Kryo. Since data serialization in Spark shuffles orders of magnitude more data than closure serialization, we only used Skyway for data serialization. The Java serializer was still used for closure serialization. The entire SkywaySerializer class contains less than 100 lines of code, most of which was adapted directly from the existing JavaSerializer class. The number of lines of new code we wrote ourselves was only 10: 2 lines to wrap the I/O stream parameters, 3 lines to modify calls to readObject, and 5 lines to specify tuning parameters (e.g., buffer size).\nPrograms and Datasets We ran Spark with four representative programs: WordCount (WC), PageRank (PR), ConnectedComponents (CC), and TriangleCounting (TC). WordCount is a simple MapReduce application that needs only one round of\ndata shuffling. The other three programs are iterative graph applications that need to shuffle data in each iteration. We used four real-world graphs as input \u2013 LiveJournal (LJ) [6], Orkut (OR) [? ], UK-2005 (UK) [? ], and Twitter-2010 (TW) [? ]; Table ?? lists their details.\nFor PageRank over Twitter-2010, Spark could not reach convergence in a reasonable amount of time (i.e., 10 hours) for all configurations. We had to terminate Spark at the end of the 10th iteration and thus the performance we report is w.r.t. the first 10 iterations. All the other iterative applications ran to complete convergence. We have experimented with three serializers: the Java serializer, Kryo, and Skyway.\nSpark Performance Figure ??(a) reports the running time comparisons among three serializers over the four input graphs. Since different programs have very different performance numbers, we plot them separately on different scales. For each dataset, WordCount and ConnectedComponents finished much more quickly than PageRank and TriangleCounting. This is primarily due to the nature of the application \u2013 WordCount has one single iteration and one single round of shuffling; it is much easier for ConnectedComponents (i.e., a label propagation application, which finishes in 3-5 iterations) to reach convergence than the other two applications that often need many more iterations.\nIt is the same reason that explains why Skyway performs better for PageRank and TriangleCounting \u2013 since they perform many rounds of data shuffling, a large portion of their execution time is taken by S/D and thus the savings in data transfer achieved by Skyway are much more significant for these two applications than the other two.\nA detailed summary of each run-time component is provided in Table 1. Network time is included in Read. On average, Skyway makes Spark run 36% and 16% faster than the Java serializer and Kryo. Compared to the Java serializer, Kryo achieves most of its savings from avoiding reading/writing type strings since Kryo relies on developers to register classes. As a result, the I/O in network and local reads has been significantly reduced. Skyway, on the contrary, benefits most from the reduced deserialization cost. Since the transferred objects can be immediately used, the process of recreating millions of objects and calling their constructors is"
        },
        {
            "heading": "ASPLOS \u201918, March 24\u201328, 2018, Williamsburg, VA, USA K. Nguyen et al.",
            "text": "completely eliminated. Furthermore, it is worth noting, again, that Kryo achieves its benefit via heavyweight manual development \u2013 there is a package of more than 20 classes (several thousands of lines of code) in Spark developed to use Kryo, while Skyway completely eliminates this manual burden and simultaneously achieves even higher performance gains.\nThe number of bytes transferred under Skyway is about the same as the Java serializer, but 77% more than Kryo due to the transferring of the entirety of each object. The increased data size is also reflected in the increased write I/O. Skyway\u2019s read I/O time is shorter than that of the Java serializer. This is primarily due to the elimination of object creation \u2013 we only need one single scan of each buffer instead of reading in individual bytes to create objects as done in Kryo. Skyway\u2019s read I/O is longer than that of Kryo because Kryo transfers much less bytes.\nTo understand what constitutes the extra bytes produced by Skyway, we analyzed these bytes for our Spark applications. Our results show that, on average, object headers take 51%, object paddings take 34%, and the remaining 15% are taken by pointers. Since headers and paddings dominate these extra bytes, future work could focus on compressing headers and paddings during sending.\nMemory Overhead To understand the overhead of the extra word field baddr in each object header, we ran the Spark\nprograms with the unmodified HotSpot and compared peak heap consumption with that of Skyway (by periodically running pmap). We found that the difference (i.e.,, the overhead) is relatively small. Across our four programs, this overhead varies from 2.1% to 21.8%, with an average of 15.4%."
        },
        {
            "heading": "5.3 Improving Flink with Skyway",
            "text": "We evaluated Skyway with the latest version of Flink (1.3.2, released August 2017) executing under Hadoop version 2.6.5. Flink has both streaming and batch processing models. Here we focus on the batch-processing model, and particularly, query answering applications.\nFlink reads input data into a set of tuples (e.g., rows in relational database); the type of each field in a tuple must be known at compile time. Flink can thus select a built-in serializer for each field to use when creating tuples from the input. Flink falls back to the Kryo serializer when encountering a type with neither a Flink-customized nor a user-defined serializer available. Since the read/write interface is clearly defined, we could easily integrate Skyway into Flink.\nWe used the TPC-H [? ] data generator to generate a 100GB dataset as our input. Next, we transformed 5 representative SQL queries generated by TPC-H into Flink applications. The description of these queries can be found in Table ??. They were selected due to the diverse operations they perform and database tables they access."
        },
        {
            "heading": "Skyway: Connecting Managed Heaps in Distributed Big Data Systems ASPLOS \u201918, March 24\u201328, 2018, Williamsburg, VA, USA",
            "text": "Figure ??(b) shows Flink\u2019s performance improvement using Skyway. Performance summary is also shown in Table ??.\nIn Flink, the amount of time in deserialization (8.7%) is much less than that in serialization (23.5% on average). This is because Flink does not deserialize all fields of a row upon receiving it \u2013 only those involved in the transformation are deserialized. Despite this lazy mechanism, Skyway could improve Flink\u2019s performance by, an overall of 19%, compared to Flink\u2019s built-in serializer. The total number of bytes written by Skyway is also higher than the baseline \u2013 on average, Skyway emits 68% more bytes. It is worth noting that Skyway is compared with Flink\u2019s highly optimized built-in serializer; it is statically chosen and optimized specifically for the data types involved in the queries, and has been shown to outperform generic serializers such as Kryo."
        },
        {
            "heading": "6 Related Work",
            "text": "Object Sharing in the OS The idea of sharing memory segments across processes has been studied in the OS design [11, 14, 17, 20, 31]. An object can exist in different address spaces, allowing the system to share memory across simultaneously executing processes. Mach [31] introduces the concept of a memory object mappable by various processes. The idea was later adopted in the Opal [11] and Nemesis [17] operating systems to describe memory segments characterized by fixed virtual offsets. Lindstrom [20] expands these notions to shareable containers that contain code segments and private memory, leveraging a capability model to enforce protection. Although most contemporary OSes allow one process to be associated with a single virtual address space (SVAS), there exist systems that support multiple virtual address space (MVAS) abstractions.\nThe idea of multiple address spaces has mainly been applied to achieve protection in a shared environment [11, 14, 33]. More recently, to support the vast physical memory whose capacity may soon exceed the virtual address space size supported by today\u2019s CPUs, SpaceJMP [14] provides a new operating system design that promotes virtual address spaces to first-class citizens, which enables process threads to attach to, detach from, and switch between multiple virtual address spaces. Although this line of work is not directly related\nto Skyway, they share a similar goal of achieving memory efficiency when objects are needed by multiple processes. XMem [36] is a JVM-based technique that shares heap space across JVM instances. None of these techniques target object transfer in distributed systems.\nMemory Management in Big Data Systems A variety of data computation models and processing systems have been developed in the past decade [4, 8, 10, 12, 13, 18, 27, 29, 34, 37\u201339, 41]. MapReduce [? ] has inspired much research on distributed data-parallel computation, including Hyracks [? ], Hadoop [4], Spark [41], and Dryad [18]. It has been extended [37] with Merge to support joins and adapted [12] to support pipelining. Yu et al. propose a programming model [38] for distributed aggregation for dataparallel systems. A number of high-level declarative languages for data-parallel computation have been proposed, including Sawzall [29], Pig Latin [27], SCOPE [10], Hive [34], and DryadLINQ [39]. These frameworks are all developed in managed languages and perform their computations on top of the managed runtime. Hence, data shuffling in these systems can benefit immediately from Skyway, as demonstrated in our evaluation (\u00a75).\nRecently, there has been much interest in optimizing memory management in language runtimes for efficient data processing [9, 15, 16, 22, 23, 25, 26]. These works are largely orthogonal to Skyway, although Skyway also fits in the category of language runtime optimizations. ITask [15] provides a library-based programming model for developing interruptible tasks in data-parallel systems. ITask solves the memory management problem using an orthogonal approach that interrupts tasks and dumps live data to disk. In addition, it is designed specifically for data-parallel programs and does not work for general (managed) systems."
        },
        {
            "heading": "7 Conclusion",
            "text": "This paper presents Skyway, the first JVM-based system that provides efficient data transfer among managed heaps. Our evaluation shows that Skyway outperforms all existing S/D libraries and improves the widely-deployed systems Spark and Flink."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank the anonymous reviewers for their valuable and thorough comments. We are also grateful to Kathryn McKinley who pointed us to important related works. This material is based upon work supported by the National Science Foundation under grants CCF-1319786, CNS-1321179, CCF1409829, IIS-1546543, CNS-1514256, CNS-1613023, CNS1703598, and by the Office of Naval Research under grants N00014-14-1-0549 and N00014-16-1-2913."
        },
        {
            "heading": "ASPLOS \u201918, March 24\u201328, 2018, Williamsburg, VA, USA K. Nguyen et al.",
            "text": ""
        }
    ],
    "title": "Skyway: Connecting Managed Heaps in Distributed Big Data Systems",
    "year": 2018
}