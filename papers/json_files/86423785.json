{
    "abstractText": "We explore whether we can observe Time\u2019s Arrow in a temporal sequence\u2013is it possible to tell whether a video is running forwards or backwards? We investigate this somewhat philosophical question using computer vision and machine learning techniques. We explore three methods by which we might detect Time\u2019s Arrow in video sequences, based on distinct ways in which motion in video sequences might be asymmetric in time. We demonstrate good video forwards/backwards classification results on a selection of YouTube video clips, and on natively-captured sequences (with no temporallydependent video compression), and examine what motions the models have learned that help discriminate forwards from backwards time.",
    "authors": [
        {
            "affiliations": [],
            "name": "Lyndsey C. Pickup"
        },
        {
            "affiliations": [],
            "name": "Zheng Pan"
        },
        {
            "affiliations": [],
            "name": "Donglai Wei"
        },
        {
            "affiliations": [],
            "name": "YiChang Shih"
        },
        {
            "affiliations": [],
            "name": "Changshui Zhang"
        },
        {
            "affiliations": [],
            "name": "Andrew Zisserman"
        },
        {
            "affiliations": [],
            "name": "Bernhard Sch\u00f6lkopf"
        },
        {
            "affiliations": [],
            "name": "William T. Freeman"
        }
    ],
    "id": "SP:13b12e927978a3b572998ec8423423707e487ff2",
    "references": [
        {
            "authors": [
                "C. Ballester",
                "V. Caselles",
                "J. Verdera",
                "M. Bertalmo",
                "G. Sapiro"
            ],
            "title": "A variational model for filling-in gray level and color images",
            "venue": "ICCV",
            "year": 2001
        },
        {
            "authors": [
                "T. Brox",
                "A. Bruhn",
                "N. Papenberg",
                "J. Weickert"
            ],
            "title": "High accuracy optical flow estimation based on a theory for warping",
            "venue": "ECCV. Springer-Verlag",
            "year": 2004
        },
        {
            "authors": [
                "P.J. Burt",
                "E.H. Adelson"
            ],
            "title": "The Laplacian pyramid as a compact image code",
            "venue": "IEEE Trans. Comput., 31(4)",
            "year": 1983
        },
        {
            "authors": [
                "T. Dekel Basha",
                "Y. Moses",
                "S. Avidan"
            ],
            "title": "Photo sequencing",
            "venue": "ECCV",
            "year": 2012
        },
        {
            "authors": [
                "K.G. Derpanis",
                "M. Lecce",
                "K. Daniilidis",
                "R.P. Wildes"
            ],
            "title": "Dynamic scene understanding: The role of orientation features in space and time in scene classification",
            "venue": "CVPR",
            "year": 2012
        },
        {
            "authors": [
                "D.J. Field"
            ],
            "title": "Wavelets",
            "venue": "vision and the statistics of natural scenes. Phil. Trans. R. Soc. Lond. A, 357",
            "year": 1999
        },
        {
            "authors": [
                "J. Fiser",
                "R.N. Aslin"
            ],
            "title": "Statistical learning of higher-order temporal structure from visual shape sequences",
            "venue": "Journal of Experimental Psychology: Learning, Memory, and Cognition, 28(3)",
            "year": 2002
        },
        {
            "authors": [
                "A. Gaidon",
                "Z. Harchaoui",
                "C. Schmid"
            ],
            "title": "Temporal Localization of Actions with Actoms",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2013
        },
        {
            "authors": [
                "R. Kemp",
                "G. Pike",
                "P. White",
                "A. Musselman"
            ],
            "title": "Perception and recognition of normal and negative faces: the role of shape from shading and pigmentation cues",
            "venue": "Perception, 25",
            "year": 1996
        },
        {
            "authors": [
                "I. Laptev",
                "M. Marsza\u0142ek",
                "C. Schmid",
                "B. Rozenfeld"
            ],
            "title": "Learning realistic human actions from movies",
            "venue": "CVPR",
            "year": 2008
        },
        {
            "authors": [
                "B.D. Lucas",
                "T. Kanade"
            ],
            "title": "An iterative image registration technique with an application to stereo vision",
            "venue": "IJCAI, volume 81",
            "year": 1981
        },
        {
            "authors": [
                "J. Peters",
                "D. Janzing",
                "A. Gretton",
                "B. Sch\u00f6lkopf"
            ],
            "title": "Detecting the direction of causal time series",
            "venue": "ICML. ACM Press",
            "year": 2009
        },
        {
            "authors": [
                "J. Portilla",
                "E.P. Simoncelli"
            ],
            "title": "A parametric texture model based on joint statistics of complex wavelet coefficients",
            "venue": "IJCV, 40",
            "year": 2000
        },
        {
            "authors": [
                "H. Price"
            ],
            "title": "Time\u2019s Arrow and Archimedes\u2019 Point: New Directions for the Physics of Time",
            "venue": "Oxford University Press",
            "year": 2007
        },
        {
            "authors": [
                "V.S. Ramachandran"
            ],
            "title": "The Perceptual World",
            "venue": "chapter Perceiving shape from shading. W. H. Freeman",
            "year": 1990
        },
        {
            "authors": [
                "H. Reichenbach"
            ],
            "title": "The Direction of Time",
            "venue": "Dover Books on Physics",
            "year": 1971
        },
        {
            "authors": [
                "S. Roth",
                "M.J. Black"
            ],
            "title": "Fields of experts: A framework for learning image priors",
            "venue": "CVPR",
            "year": 2005
        },
        {
            "authors": [
                "B. Sch\u00f6lkopf",
                "D. Janzing",
                "J. Peters",
                "E. Sgouritsa",
                "K. Zhang",
                "J. Mooij"
            ],
            "title": "On causal and anticausal learning",
            "venue": "ICML, New York, NY, USA",
            "year": 2012
        },
        {
            "authors": [
                "J.H. Searcy",
                "J.C. Bartlett"
            ],
            "title": "Inversion and processing of component and spatial-relational information of faces",
            "venue": "Journal of Experimental Psychology: Human Perception and Performance, 22",
            "year": 1996
        },
        {
            "authors": [
                "C. Tomasi",
                "T. Kanade"
            ],
            "title": "Detection and tracking of point features",
            "venue": "Technical Report CMU-CS-91-132, School of Computer Science, Carnegie Mellon University",
            "year": 1991
        },
        {
            "authors": [
                "P. Tuisku",
                "T.K. Pernu",
                "A. Annila"
            ],
            "title": "In the light of time",
            "venue": "Proc. R. Soc. A, (465)",
            "year": 2009
        },
        {
            "authors": [
                "A. Vedaldi",
                "B. Fulkerson"
            ],
            "title": "VLFeat - an open and portable library of computer vision algorithms",
            "venue": "ACM International Conference on Multimedia",
            "year": 2010
        },
        {
            "authors": [
                "H. Wang",
                "A. Kl\u00e4ser",
                "C. Schmid",
                "C. Liu"
            ],
            "title": "Action Recognition by Dense Trajectories",
            "venue": "CVPR",
            "year": 2011
        },
        {
            "authors": [
                "Y. Wexler",
                "E. Shechtman",
                "M. Irani"
            ],
            "title": "Space-time video completion",
            "venue": "CVPR",
            "year": 2004
        },
        {
            "authors": [
                "D. Zoran",
                "Y. Weiss"
            ],
            "title": "From learning models of natural image patches to whole image restoration",
            "venue": "ICCV",
            "year": 2011
        }
    ],
    "sections": [
        {
            "text": "We explore whether we can observe Time\u2019s Arrow in a temporal sequence\u2013is it possible to tell whether a video is running forwards or backwards? We investigate this somewhat philosophical question using computer vision and machine learning techniques.\nWe explore three methods by which we might detect Time\u2019s Arrow in video sequences, based on distinct ways in which motion in video sequences might be asymmetric in time. We demonstrate good video forwards/backwards classification results on a selection of YouTube video clips, and on natively-captured sequences (with no temporallydependent video compression), and examine what motions the models have learned that help discriminate forwards from backwards time."
        },
        {
            "heading": "1. Introduction",
            "text": "How much of what we see on a daily basis could be time-reversed without us noticing that something is amiss? Videos of certain motions can be time-reversed without looking wrong, for instance the opening and closing of automatic doors, flexing of the digits of a human hand, and elastic bouncing or harmonic motions for which the damping is small enough as to be undetectable in a short video. The same is not true, however, of a cow eating grass, or a dog shaking water from its coat. In these two scenarios, we don\u2019t automatically accept that a complicated system (chewed food, or the spatter pattern of water drops) will re-compose itself into a simpler one.\nThere has been much study of how the underlying physics, and the second law of thermodynamics, create a direction of time [14, 16]. While physics addresses whether the world is the same forwards and backwards, our goal will be to assay whether and how the direction of time manifests\nitself visually. This is a fundamental property of the visual world that we should understand. Thus we ask the question: can we see the Arrow of Time? \u2013 can we distinguish a video playing forward from one playing backward?\nHere, we seek to use low-level visual information \u2013 closer to the underlying physics \u2013 to see Time\u2019s Arrow, not object-level visual cues. We are not interested in memorizing that cars tend to drive forward, but rather in studying the common temporal structure of images. We expect that such regularities will make the problem amenable to a learningbased approach. Some sequences will be difficult or impossible; others may be straightforward. We want to know: how strong is the signal indicating Time\u2019s Arrow? Where do we see it, and when is it reliable? We study these questions using several different learning-based approaches.\nAsking such fundamental questions about the statistical structure of images has helped computer vision researchers to formulate priors and to develop algorithms exploiting such priors. For example, asking, \u201care images invariant over scale?\u201d led researchers to develop the now widelyused multi-resolution spatial pyramid architectures [3]. Human vision researchers have learned properties of human visual processing by asking \u201ccan we recognize faces upside down?\u201d [19], or about tonescale processing by asking,\u201ccan we recognize faces if the tonescale is inverted?\u201d [9]. Both symmetries and asymmetries of priors provide useful information for visual inference. The prior assumption that light comes from above is asymmetric and helps us disambiguate convex and concave shapes [15]. Spatial translation invariance, on the other hand, allows us to train object detection or recognition systems without requiring training data at all possible spatial locations.\nLearning and representing such statistical structure has provided the prior probabilities needed for a variety of computer vision tasks for processing or synthesizing images and image sequences, including, for example: noise removal\n1\n[17], inpainting over spatial or temporal gaps [1, 24], image enhancement [25], and texture synthesis [13].\nIn contrast to quite extensive research on the statistical properties of the spatial structure in images [6], there has been far less investigation of temporal structure (one of the exceptions to this rule includes [7]). However, knowledge of the temporal structure of images/videos can be used in a variety of tasks, including video enhancement or analysis, filling-in missing data, and the prediction of what will be seen next, an important feedback signal for robotics applications. Thus, we can similarly ask: are video temporal statistics symmetric in time?\nWe expect that the answer to this question can guide us as we develop priors and subsequent algorithms. For example, an understanding of the precise asymmetries of temporal visual priors may have implications for methods for video denoising or video decompression or optical flow estimation or ordering of images [4]. In addition, causal inference, of which seeing Time\u2019s Arrow is a special case, has been connected to machine learning topics relevant to computer vision, such as semi-supervised learning, transfer learning and covariate shift adaptation [18]. Our question relates to this issue of causality, but seen through the lens of vision.\nIn the sequel, we probe how Time\u2019s Arrow can be determined from video sequences in three distinct ways: first, section 3, by using features that represent local motion patterns and learning from these whether there is an asymmetry in temporal behaviour that can be used to determine the video direction; second, section 4, by explicitly representing evidence of causality; and third, section 5, by using a simple auto-regressive model to distinguish whether a cause at a time point influences future events or not (since it can\u2019t influence the past)."
        },
        {
            "heading": "2. Dataset and baseline method",
            "text": "In this section we describe the two video datasets and a\nbaseline method for measuring Time\u2019s Arrow."
        },
        {
            "heading": "2.1. YouTube dataset",
            "text": "We have assembled a set of videos that act as a common test bed for the methods investigated in this paper. Our criteria when selecting videos is that the information on Time\u2019s Arrow should not be \u2018semantic\u2019 but should be available principally from the \u2018physics\u2019. Thus we do not include strong semantic cues for example from the coupling of the asymmetries of animals and vehicles with their directions of motion (e.g. eyes at the front of the head tending to move forwards); or from the order of sequences of actions (such as traffic lights or clock hands). What remains is the \u2018physics\u2019 \u2013 gravity, friction, entropy, etc [21].\nThis dataset consists of 180 video clips from YouTube. This was obtained manually using more than 50 keywords, aimed at retrieving a diverse set of videos from which we\nmight learn low-level motion-type cues that indicate the direction of time. Keywords included \u201cdance\u201d, \u201csteam train\u201d, and \u201cdemolition\u201d, amongst other terms. The dataset is available at http://www.robots.ox.ac.uk/data/arrow/.\nSelection criteria: Video clips were selected to be 6\u201310 seconds long, giving at least 100 frames on which to run the computations. This meant having to discard clips from many professionally-produced videos where each shot\u2019s duration was too short, since each clip was required to be a single shot. We restricted the selections to HD videos, allowing us to subsample extensively to avoid block artifacts and minimize the interaction with any given compression method.\nVideos with poor focus or lighting were discarded, as were videos with excessive camera motion or motion blur due to hand shake. Videos with special effects and computer-generated graphics were avoided, since the underlying physics describing these may not match exactly with the real-world physics underpinning our exploration of Time\u2019s Arrow. Similarly, split-screen views, cartoons and computer games were all discarded. Finally, videos were required to be in landscape format with correct aspect ratios and minimal interlacing artefacts. In a few cases, the dataset still contains frames in which small amounts of text or television channel badges have been overlaid, though we tried to minimize this effect as well.\nFinally, the dataset contains a few clips from \u201cbackwards\u201d videos on YouTube; these are videos where the authors or uploaders have performed the time-flip before submitting the video to YouTube, so any time-related artefacts that might have been introduced by YouTube\u2019s subsequent compression will also be reversed in these cases relative to the forward portions of our data. In these cases, because relatively few clips meeting all our our criteria exist, multiple shots of different scenes were taken from the same source video in a few cases. In all other cases, only one short clip from each YouTube video was used.\nIn total, there are 155 forward videos and 25 reverse videos in the dataset. Two frames from each of six example videos are shown in Figure 1. The first five are forwards-time examples (horse in water, car crash, mine blasting, baseball game, water spray), and the right-most pair of frames is from a backwards-time example where pillows are \u201cun-thrown\u201d.\nTrain/val/test split and evaluation procedure. For evaluating the methods described below, the dataset was divided into 60 testing videos and 120 training videos, in three different ways such that each video appeared as a testing video exactly once and training exactly twice. Where methods required it, the 120 videos of the training set were further subdivided into 70 training and 50 validation videos. The three\ntrain/test splits are labeled A, B and C, and are the same for each of the methods we report. The backwards-to-forwards video ratios for the three test sets were 9:51, 8:52 and 8:52 respectively.\nThe evaluation measure for each method is the proportion of the testing videos on which the method could correctly predict the time-direction."
        },
        {
            "heading": "2.2. Tennis-ball dataset",
            "text": "We also filmed a small number of video clips using a camera which could record in a video codec which used only intra-frame, rather than inter-frame coding, meaning that there was no possibility of compression artefacts holding any time-direction information. This dataset comprises 13 HD videos of tennis balls being rolled along a floor and colliding with other rolling or static balls. An example of one of these sequences is used later in Figure 6."
        },
        {
            "heading": "2.3. Baseline method",
            "text": "We consider Spatial-temporal Orientated Energy (SOE) [5] as an off-the-shelf method which can be employed to describe video motion. SOE is a filterbased feature for video classification, and is sensitive to different spacetime textures. Our implementation is faithful to that laid out in [5], and comprises third-derivativeof-Gaussian filters in eight uniformly-distributed spatial directions, with three different temporal scales. As in the prior work, we split each video into 2 \u00d7 2 spatial sub-regions, and concatenated the SOE responses for each sub-region for form a final feature.\nUsing these SOE features in a linear SVM classifier, the baseline performance on the three splits of the YouTube dataset described above are 57%, 48% and 60% respectively. The relatively poor performance can be attributed to the difficulty in generalizing motion over different subregions, and the fact that these features were designed to characterize regular motion textures, rather than particular one-off dynamic events."
        },
        {
            "heading": "3. Statistical flow method",
            "text": "Without semantic information, the most telling cues for the Arrow of Time in video data may well come from pat-\nterns of motion. So can simply looking at the occurrences of particular motion patterns across a time-sequence give us clues to Time\u2019s Arrow?\nThere are many other subtle physical reasons why the forward and reverse time directions may look different on average. Friction and damping will tend to have characteristic effects on dynamic systems, and the reverse of these sorts of processes \u2013 objects taking in small amounts of heat energy from their surroundings in order to accelerate themselves \u2013 are far less likely to happen, so the way in which objects move is very relevant.\nWe propose a method we call Flow-Words in order to capture local regions of motion in a video so that we can examine which types of motion exhibit temporal asymmetries that might be useful for detecting the Arrow of Time.\nFlow-words are based on a SIFT-like descriptor of mo-\ntion occurring in small patches of a video. We first register the frames of a video in order to compensate for hand-shake and intentional camera motion (panning and zooming etc.), and then we assume that any residual motion is due to objects moving in the scene. Rather than computing a SIFT based on image edge gradients, motion gradients from an optical flow computation as substituted, giving a descriptor which represents local histograms of image motion. An example of the descriptors used to build these flow-words is shown in Figure 2.\nThese object-motion descriptors are then vectorquantized to form a discrete set of flow words, and a bag-offlow-word descriptor representing the entire video sequence is thus computed. With sufficient forward and reverse-time examples, a classifier can be trained to discriminate between the two classes.\nImplementation details. In a similar manner to [23], computation starts from dense trajectories. However, instead of only representing the histogram of optical flow over a region, as in the HOF features of [10], we also represent its spatial layout in a local patch in a manner similar to a local SIFT descriptor.\nIn detail, frames were downsized to 983 pixels wide in a bid to remove block-level artefacts and to provide a consistent image size for the rest of the pipeline. Images at t \u2212 1 and t+ 1 were registered to the image at time t, and an optical flow computation carried out [2]. This was repeated over the video sequence in a temporal sliding window, i.e. giving T \u2212 2 optical flow outputs for a sequence of length T . These outputs take the form of motion images in the horizontal and vertical directions. A normal VLFeat dense SIFT descriptor [22] uses intensity gradient images in the x- and y-directions internally in its computations, so to describe the flows instead of the image structure, we simply operated on the vertical and horizontal motion images instead of the intensity gradients. This yielded motion-patch descriptors whose elements represented the magnitudes of motion in various directions passing through the tth frame, on a small 4\u00d7 4 grid (bin size 6 pixels) sampled once every 3 pixels in the horizontal and vertical directions.\nWe suppressed static trajectories, because we were only interested in dynamic behaviour for this work. This was achieved by setting the motion-gradient images to zero where the image difference was below a threshold, and not including motion-patch descriptors from such areas in further processing. This excised large areas, as can be seen in the constant background color in Figure 2.\nDivergence operator for flow-words: In analyzing the results, it will be useful to quantify the divergence or other motion-field properties of a given flow-word descriptor. Using the divergence theorem, we make an approximation to the integral of the divergence over a descriptor by consid-\nering the net outward and inward motion fluxes across a square that joins the centers of the four corner cells on the descriptor grid, assuming that motion at every point within a grid cell is drawn independently from the histogram which is represented at the centre of that cell. The resulting divergence operator for flow-words is shown in the central and right parts of figure 3, with the negative and positive flux contributions separated for visualization."
        },
        {
            "heading": "3.1. Learning",
            "text": "A dictionary of 4000 words was learnt from a random subset of the training data (O(107) examples) using Kmeans clustering, and each descriptor was assigned to its closest word. A video sequence was then described by a normalized histogram of visual flow-words across the entire time-span of the video. Performance was improved if the square roots of the descriptor values were taken prior to clustering, rather than the raw values themselves.\nFor each video in the training/validation set, we extracted four descriptor histograms: (A): the native direction of the video; (B): this video mirrored in the left-right direction; (C): the original video time-flipped; and (D): the timeflipped left-right-mirrored version. An SVM was trained using four histograms A\u2013D extracted from each video of a training set, 280 (4 \u00d7 70) videos in total. Similarly, the 50 videos of the validation set generate 200 histograms A\u2013D in total, and each is classified with the trained SVM. For a valid classification, A andB must have one sign, and C and D another. We combine the SVM scores asA+B\u2212C\u2212D, and this should give a positive score for forwards clips, a negative score for backwards clips, and some chance to selfcorrect if not all of the component scores are correct.\nTheC parameter for the SVMwas chosen to be the value that maximized the classification accuracy on the validation set over all three train/test splits. Once this C parameter was fixed, the whole set of 120 training videos, including the previously-withheld validation set, was used to learn a final SVM for each train/test split.\nTesting proceeds in a similar manner to that used for the validation set: the SVM scores for the four bag-of-flow-\nword representations for each testing video are combined as A + B \u2212 C \u2212 D, and the sign of the score gives the video\u2019s overall classification."
        },
        {
            "heading": "3.2. Experimental results",
            "text": "Training and testing were carried out according to the procedure set out in 3.1. The three trained models were used to classify the 60 videos in the three testing partitions, and achieved accuracies of 90%, 77% and 75% for splits A to C respectively.\nOf the videos that were mis-classified, a few have lots of individuals moving separately in the shot: groups of people or animals, or a large number of small balls, which might not be well-suited for the scale of feature we are using. A further three were stop-motion films of seeds growing, which tend to have jerkier motions than most normal videos.\nTests on the Tennis-ball dataset were run in a leave-oneout manner, because of the small size of the dataset. The model was trained on 12 videos (and their various flips), and then tested on the withheld video, and all 13 possible arrangements were run. The correct time direction was found in 12/13 cases, i.e. over 90% accuracy."
        },
        {
            "heading": "3.3. Further analysis of Flow-words results",
            "text": "We can use the trained flow-words model to look at which flow-words were particularly characteristic of forwards or backwards motion. This was achieved by taking the SVM weights fixed using the training and set, and applying them to the descriptors for the test videos in their native playing direction. The 10 words (0.25% of the total vocabulary size) contributing most positively to the scores for forwards-direction clips, or most negatively for the backwards-direction clips, were recorded.\nFigures 4 and 5 show flow-word results on two of the test cases. Each of these figures is split into four parts: a synopsis of the video motion, a plot showing where in the video clip the 10 most informative flow-words for classification originated (out of 4000 possible flow-words from this vocabulary), the most-informative frame marked with all the instances of any of these 10 highly-weighted words, and then finally the top four most informative flow-words for the given video clip.\nIn Figure 4, the example comes from one of the videos which was uploaded to YouTube after being time-reversed, and the synopsis images show a boy emerging from a pool with an inverse-splash. The motion of the water in this splash is the strongest indicator in this clip that time is running in the backwards direction. In Figure 5, time runs in the forwards direction as usual, and the synopsis shows a steam train gradually approaching the camera. The gentle upwards divergence, arising from the plume of steam from the train, is a good indicator of Time\u2019s Arrow, and interestingly, the most-useful words for capturing this appear peri-\nodically in the video clip as the rhythm of the train\u2019s motion causes the fumes to billow in a regular way.\nOverall, there is a very slight tendency for flow-words describing forwards-time videos to have a higher divergence score than those describing backwards videos. Correlations scores between model weight and flow-word divergence are small but consistent, with models learnt on the three splits having correlation coefficients of 0.329, 0.033 and 0.038 respectively."
        },
        {
            "heading": "4. Motion-causation method",
            "text": "The flow-words motion above considered how we could examine simple motion patterns, but not how those patterns related to one another. To explore motion relations, we now sketch a method that considers motions causing other motions. Our starting hypothesis is that it is far more common for one motion to cause multiple motions than for multiple motions to collapse into one consistent motion. For instance, the cue ball during a snooker break-off hits the pack of red balls and scatters them, but in a reverse-time direction, it is statistically highly unlikely that the initial conditions of the red balls (position, velocity, spin etc) can be set up exactly right, so that they come together, stop perfectly, and shoot the cue ball back towards the cue.\nTo exploit this observation, we simply look at regions in\nthe video from frame to frame where differences are appearing, which we assume are due to object motion. Smoothed regions of motion in one frame are compared to similar regions in the previous frame. These motion regions are illustrated in Figure 6. Broadly speaking, we expect more occurrences of one region splitting in two than of two regions joining to become one, in the forwards-time direction.\nImplementation outline: We warp the image at t + 1 It+1 into the frame of It, using pixels from It to fill any regions not present in It+1. This yields a warped image W t+1. The difference |(It \u2212W t+1)| now highlights mov-\ning areas. A smooth-edged binary mask of image motion is made by summing this difference over color channels, resizing down to 400 pixels, convolving with a disk function and then thresholding. The different regions in the motion mask are enumerated. Where one or more regions at time t intersect more than one region each at time t \u2212 1, a violation is counted for that frame pair, since this implies motion merging. We count the violations for each time direction of a sequence separately, with a variety of parameter settings. We used three different threshold-radius pairs: radii of 5, 6.6 and 8.7 pixels, and corresponding thresholds of 0.1, 0.01 and 0.056, where image intensities lie between 0 and 1. Finally, we trained a standard linear SVM using the violation counts as 6d features (two time-directions; three parameterizations).\nExperimental results: Results on splits A to C were 70%, 73% and 73% respectively. This is weaker than the flow-words method, and performance drops to just below 60% overall if only the first radius-threshold pair is used (i.e. a 2d decision space instead of 6d).\nWhile the accuracy of the motion-causation approach is weaker, it is at the same time computationally much simpler and quicker to compute than the flow-words method. There is also the chance that it may complement the capabilities of the flow-words method well, because the motion-causation considers spatial location of motion, and comparison of motion between neighboring frames, whereas the flow-words method at present considers only snapshots of motion for each frame separately. Of the 50 videos that this system mis-classified, only 10 overlapped with the mis-classified set from the flow-words method.\nOn the Tennis-ball dataset, the motion-causation method also achieved 12/13 correct classifications; the misclassified sequence was different to that mis-classified by the flow-words method."
        },
        {
            "heading": "5. AR method",
            "text": "Researchers have recently studied the question of measuring the direction of time as a special case of the problem of inferring causal direction in cause-effect models. Peters et al. [12] showed that, for non-Gaussian additive noise and dynamics obeying a linear ARMA (auto-regressive moving average) model, the noise added at some point of time is independent of the past values of the time series, but not of the future values. This allows us to determine the direction of time by independence testing, and Peters et al. [12] used it to successfully analyze the direction of time for EEG data. Intuitively, this insight formalizes our intuition that changes (noise) added to a process at some point of time influences the future, but not the past.\nHere we consider the special case of AR models (i.e. no moving average part); however, we deal with vector-valued\ntime series, which strictly speaking goes beyond the validity of the theoretical analysis of Peters et al. [12]. We found that an order of two worked well in our setting. In a nutshell, we model the time series\u2019 next value as a linear function of the past two values plus additive independent noise.\nThe assumption that some image motions will be modeled as AR models with additive non-Gaussian noise leads to a simple algorithm for measuring the direction of time in a video: track the velocities of moving points, fit those velocities with an ARmodel and perform an independence test between the velocities and model residuals (errors). This process is illustrated in Figure 7.\nThe independence testing follows the work of [12], and is based on an estimate of the Hilbert-Schmidt norm of the cross-covariance operator between two reproducing kernel Hilbert spaces associated with the two variables whose independence we are testing. The norm provides us with a test statistic, and this in turn allows us to estimate a p-value for the null hypothesis of independence. If the p-value is small, the observed value of the norm is very unlikely under the null hypothesis and the latter should be rejected. Ideally, we would hope that the p-value should be small in the backward direction, and large (i.e. significantly bigger than 0) in the forward direction.\nImplementation outline: We analyze the motion of a set of feature points extracted by KLT trackers [11, 20], running tracking in both forward and backward directions. For each tracked point, velocities are extracted, and a 2D AR model is fitted. We then test the independence between the noise and velocity to determine Time\u2019s Arrow at the trajectory level.\nInferring causal direction of AR process is only possible when the noise is non-Gaussian, and when noise in only one\ntemporal direction is independent. We define a valid trajectory to be one which spans at least 50 frames, for which noise in at least one direction is non-Gaussian as determined by a normality test, and for which the p-value test in one time-direction gives p < 0.05 whereas in the other it gives p > 0.05 + \u03b4 for some minimal gap \u03b4 (i.e. exactly one direction fails the null hypothesis test).\nAll valid trajectories are classified as forward or backward according to their p-value scores. Ideally, all valid trajectories for one video should imply the same direction of time, but in practice, tracking can be noisy in a way that violates the time-series model assumption. For this, we reject the videos with fewer than N valid trajectories, where N \u2265 1. We classify the accepted videos by a majority vote among the valid trajectories. We thus get a binary classifier (with the possibility of rejection) at video level. While hypothesis testing is used to classify single trajectories, the overall procedure is not a hypothesis test, and thus issues of multiple testing do not arise. The hypothesis testing based trajectory classifiers can be seen as weak classifiers for video and the voting makes a strong classifier.\nExperimental results: The desirability of using a large gap \u03b4 in the p-value test, and requiring many valid trajectories per video must be traded off against the need not to reject excessively many videos. For large values (\u03b4,N) = (0.24, 13), only 5 videos are accepted, but 4 are classified correctly (80%). For lower parameter values, (\u03b4,N) = (0.15, 4), 101 videos are accepted, of which 58 are correctly classified (58%). Figure 8 shows more of this trade-off of between accuracy and number of videos classified."
        },
        {
            "heading": "6. Discussion",
            "text": "We have addressed a new and very fundamental problem of telling which direction time is flowing in a video, and presented three complementary methods for determining this direction. Our results indicate that the statistics of natural videos are not symmetric under time reversal: for if they were, then for each video, there would be an equally likely video where time is reversed, so there would be no way to solve the classification problem better than chance\n(assuming our training sample is unbiased / representative of the prior).\nOf course, each method can be improved. For example, the flow word method performed well above chance on this video set, but its performance can doubtlessly be improved by extensions to use multi-resolution in both space and time. State of the art features developed for other tasks with a temporal dimension, such as actions and activity recognition, could also be employed [8, 23].\nThe causality method captures an essential aspect of going forward in time, and can be extended to longer temporal intervals \u2013 for example to capture events that are causal but delayed, e.g. a tree branch oscillating some time after a person has brushed past it.\nSimilarly, the AR method as applied in our experiments is based on rather specific assumptions (2nd order linear AR model with independent non-Gaussian noise). If these assumptions hold true for a given video, we can expect a large p-value in the forward direction and a small one backward, and thus a big difference in p-values. This case turns out to be rare in our videos, but if it occurs, the method works rather well. If not, then both p-values should be small, and their difference is also small. Our method will reject such cases \u2013 rightly so, for if it is forced to decide these cases, then we find performance to be almost at chance level.\nMore generally, we have deliberately concentrated on low-level statistics in this work, but priors and classifiers can also be developed for high-level semantic information (like the fact that animals move head-forwards) to complement the low-level analysis.\nAcknowledgements. This work was supported in the UK by ERC grant VisRec no. 228180, in China by 973 Program (2013CB329503), NSFC Grant no. 91120301, and in the US by ONR MURI grant N00014-09-1-1051 and NSF CGV-1111415."
        }
    ],
    "title": "Seeing the Arrow of Time",
    "year": 2014
}