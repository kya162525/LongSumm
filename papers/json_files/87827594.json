{
    "abstractText": "An efficient learner is one who reuses what they already know to tackle a new problem. For a machine learner, this means understanding the similarities amongst datasets. In order to do this, one must take seriously the idea of working with datasets, rather than datapoints, as the key objects to model. Towards this goal, we demonstrate an extension of a variational autoencoder that can learn a method for computing representations, or statistics, of datasets in an unsupervised fashion. The network is trained to produce statistics that encapsulate a generative model for each dataset. Hence the network enables efficient learning from new datasets for both unsupervised and supervised tasks. We show that we are able to learn statistics that can be used for: clustering datasets, transferring generative models to new datasets, selecting representative samples of datasets and classifying previously unseen classes.",
    "authors": [
        {
            "affiliations": [],
            "name": "Harrison Edwards"
        }
    ],
    "id": "SP:618c21d11ef6147957919ac041292c728d16324d",
    "references": [
        {
            "authors": [
                "Mart\u00edn Abadi",
                "Ashish Agarwal",
                "Paul Barham",
                "Eugene Brevdo",
                "Zhifeng Chen"
            ],
            "title": "TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL http: //tensorflow.org/. Software available from tensorflow.org",
            "year": 2015
        },
        {
            "authors": [
                "Veronika Cheplygina",
                "David M.J. Tax",
                "Marco Loog"
            ],
            "title": "On classification with bags, groups and sets",
            "venue": "Pattern Recognition Letters,",
            "year": 2015
        },
        {
            "authors": [
                "S. Chopra",
                "R. Hadsell",
                "Y. LeCun"
            ],
            "title": "Learning a similarity metric discriminatively, with application to face verification",
            "venue": "In Computer Vision and Pattern Recognition,",
            "year": 2005
        },
        {
            "authors": [
                "Theodoros Evgeniou",
                "Massimiliano Pontil"
            ],
            "title": "Regularized multi\u2013task learning",
            "venue": "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,",
            "year": 2004
        },
        {
            "authors": [
                "Kenji Fukumizu",
                "Le Song",
                "Arthur Gretton"
            ],
            "title": "Kernel Bayes\u2019 rule: Bayesian inference with positive definite kernels",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2013
        },
        {
            "authors": [
                "Thomas Gartner",
                "Peter A. Flach",
                "Adam Kowalczyk",
                "Alex J. Smola"
            ],
            "title": "Multi-instance kernels",
            "venue": "Proc. 19th International Conf. on Machine Learning,",
            "year": 2002
        },
        {
            "authors": [
                "Karol Gregor",
                "Ivo Danihelka",
                "Alex Graves",
                "Danilo Rezende",
                "Daan Wierstra"
            ],
            "title": "Draw: A recurrent neural network for image generation",
            "venue": "In Proceedings of The 32nd International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Karol Gregor",
                "Frederic Besse",
                "Danilo Jimenez Rezende",
                "Ivo Danihelka",
                "Daan Wierstra"
            ],
            "title": "Towards conceptual compression",
            "venue": "arXiv preprint arXiv:1604.08772,",
            "year": 2016
        },
        {
            "authors": [
                "Sergey Ioffe",
                "Christian Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "In Proceedings of The 32nd International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Max Jaderberg",
                "Karen Simonyan",
                "Andrew Zisserman"
            ],
            "title": "Spatial transformer networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2015
        },
        {
            "authors": [
                "Bai Jiang",
                "Tung-yu Wu",
                "Charles Zheng",
                "Wing H Wong"
            ],
            "title": "Learning summary statistic for approximate Bayesian computation via deep neural network",
            "venue": "arXiv preprint arXiv:1510.02175,",
            "year": 2015
        },
        {
            "authors": [
                "Matthew J Johnson",
                "David Duvenaud",
                "Alexander B Wiltschko",
                "Sandeep R Datta",
                "Ryan P Adams"
            ],
            "title": "Structured vaes: Composing probabilistic graphical models and variational autoencoders",
            "venue": "arXiv preprint arXiv:1603.06277,",
            "year": 2016
        },
        {
            "authors": [
                "Casper Kaae S\u00f8nderby",
                "Tapani Raiko",
                "Lars Maal\u00f8e",
                "S\u00f8ren Kaae S\u00f8nderby",
                "Ole Winther"
            ],
            "title": "How to train deep variational autoencoders and probabilistic ladder networks",
            "venue": "arXiv preprint arXiv:1602.02282,",
            "year": 2016
        },
        {
            "authors": [
                "Diederik Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding variational Bayes",
            "venue": "In Proceedings of the 2nd International Conference on Learning Representations (ICLR),",
            "year": 2014
        },
        {
            "authors": [
                "Gregory Koch"
            ],
            "title": "Siamese neural networks for one-shot image recognition",
            "venue": "Doctoral dissertation, University of Toronto,",
            "year": 2015
        },
        {
            "authors": [
                "Brenden M Lake",
                "Ruslan Salakhutdinov",
                "Joshua B Tenenbaum"
            ],
            "title": "Human-level concept learning through probabilistic program induction",
            "year": 2015
        },
        {
            "authors": [
                "Alex Lamb",
                "Vincent Dumoulin",
                "Aaron Courville"
            ],
            "title": "Discriminative regularization for generative models",
            "venue": "arXiv preprint arXiv:1602.03220,",
            "year": 2016
        },
        {
            "authors": [
                "Neil D Lawrence",
                "John C Platt"
            ],
            "title": "Learning to learn with the informative vector machine",
            "venue": "In Proceedings of the twenty-first international conference on Machine learning,",
            "year": 2004
        },
        {
            "authors": [
                "Yann LeCun",
                "L\u00e9on Bottou",
                "Yoshua Bengio",
                "Patrick Haffner"
            ],
            "title": "Gradient-based learning applied to document recognition",
            "venue": "Proceedings of the IEEE,",
            "year": 1998
        },
        {
            "authors": [
                "Lars Maal\u00f8e",
                "Casper Kaae S\u00f8nderby",
                "S\u00f8ren Kaae S\u00f8nderby",
                "Ole Winther"
            ],
            "title": "Auxiliary deep generative models",
            "venue": "arXiv preprint arXiv:1602.05473,",
            "year": 2016
        },
        {
            "authors": [
                "Yishu Miao",
                "Lei Yu",
                "Phil Blunsom"
            ],
            "title": "Neural variational inference for text processing",
            "venue": "arXiv preprint arXiv:1511.06038,",
            "year": 2015
        },
        {
            "authors": [
                "Krikamol Muandet",
                "Kenji Fukumizu",
                "Francesco Dinuzzo",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Learning from distributions via support measure machines",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2012
        },
        {
            "authors": [
                "Sinno Jialin Pan",
                "Qiang Yang"
            ],
            "title": "A survey on transfer learning",
            "venue": "Knowledge and Data Engineering, IEEE Transactions on,",
            "year": 2010
        },
        {
            "authors": [
                "Barnab\u00e1s P\u00f3czos",
                "Liang Xiong",
                "Dougal J Sutherland",
                "Jeff Schneider"
            ],
            "title": "Support distribution machines",
            "venue": "Technical Report,",
            "year": 2012
        },
        {
            "authors": [
                "Danilo Jimenez Rezende",
                "Shakir Mohamed",
                "Daan Wierstra"
            ],
            "title": "Stochastic backpropagation and approximate inference in deep generative models",
            "venue": "In Proceedings of The 31st International Conference on Machine Learning,",
            "year": 2014
        },
        {
            "authors": [
                "Danilo Jimenez Rezende",
                "Shakir Mohamed",
                "Ivo Danihelka",
                "Karol Gregor",
                "Daan Wierstra"
            ],
            "title": "One-shot generalization in deep generative models",
            "venue": "arXiv preprint arXiv:1603.05106,",
            "year": 2016
        },
        {
            "authors": [
                "L. Theis",
                "A. van den Oord",
                "M. Bethge"
            ],
            "title": "A note on the evaluation of generative models",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2016
        },
        {
            "authors": [
                "Oriol Vinyals",
                "Samy Bengio",
                "Manjunath Kudlur"
            ],
            "title": "Order matters: sequence to sequence for sets",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2016
        },
        {
            "authors": [
                "Lior Wolf",
                "Tal Hassner",
                "Itay Maoz"
            ],
            "title": "Face recognition in unconstrained videos with matched background similarity",
            "venue": "In Computer Vision and Pattern Recognition (CVPR),",
            "year": 2011
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The machine learning community is well-practised at learning representations of data-points and sequences. A middle-ground between these two is representing, or summarizing, datasets - unordered collections of vectors, such as photos of a particular person, recordings of a given speaker or a document as a bag-of-words. Where these sets take the form of i.i.d samples from some distribution, such summaries are called statistics. We explore the idea of using neural networks to learn statistics and we refer to our approach as the neural statistician. The key result of our approach is a statistic network that takes as input a set of vectors and outputs a vector of summary statistics specifying a generative model of that set. The advantages of our approach are that it is:\n\u2022 Unsupervised: It provides principled and unsupervised way to learn summary statistics as a variational encoder of a generative model. \u2022 Data efficient: If one has a large number of small but related datasets, modelling the datasets jointly\nenables us to gain statistical strength. \u2022 Parameter Efficient: By using summary statistics instead of say categorical labellings of each\ndataset, we decouple the number of parameters of the model from the number of datasets. \u2022 Capable of small-shot learning: If the datasets correspond to examples from different classes, class\nembeddings, (summary statistics associated with examples from a class) allow us to handle new classes at test time."
        },
        {
            "heading": "2 Problem Statement",
            "text": "We are given datasets Di for i \u2208 I. Each dataset Di = {x1, . . . , xki} consists of a number of i.i.d samples from an associated distribution pi over Rn. The task can be split into learning and inference components. The learning component is to produce a generative model p\u0302i for each dataset Di. We assume there is a common underlying generative process p such that pi = p(\u00b7|ci) for ci \u2208 Rl drawn from p(c). We refer to c as the context. The inference component is to give an approximate posterior over the context q(c|D) for a given dataset produced by a statistic network.\nar X\niv :1\n60 6.\n02 18\n5v 1\n[ st\nat .M\nL ]\n7 J\nun 2\n01 6"
        },
        {
            "heading": "3 Neural Statistician",
            "text": "In order to exploit the assumption of a hierarchical generative process over datasets we will use a \u2018parameter-transfer approach\u2019 (see [25]) to extend the variational autoencoder model of Kingma and Welling [16]."
        },
        {
            "heading": "3.1 Variational Autoencoder",
            "text": "The variational autoencoder is a latent variable model p(x|z; \u03b8) with parameters \u03b8. For each observed x, a corresponding latent variable z is drawn from p(z) so that\np(x) = \u222b p(x|z; \u03b8)p(z) dz (1)\nThe generative parameters \u03b8 are learned by introducing a recognition network q(z|x;\u03c6) with parameters \u03c6. The recognition network gives an approximate posterior over the latent variables that can then be used to give a variational lower bound on the log-likelihood\nLx = Eq(z|x,\u03c6) [log p(x|z; \u03b8)]\u2212DKL (q(z|x;\u03c6)\u2016p(z)) (2)\nWe can then optimize Lx with respect to \u03c6 and \u03b8 using the reparameterization trick introduced by Kingma and Welling [16] and Rezende et al. [27] to take gradients."
        },
        {
            "heading": "3.2 Basic Model",
            "text": "We extend this model as shown on the left in Figure 3 to include a latent variable c, the context, that varies per dataset D. Now the likelihood of a particular dataset D is given by\np(D) = \u222b p(c) [\u220f x\u2208D \u222b p(x|z; \u03b8)p(z|c; \u03b8) dz ] dc (3)\nThe prior p(c) is chosen to be a spherical Gaussian with zero-mean and unit variances. The conditional p(z|c; \u03b8) is Gaussian with diagonal covariance, where the mean and variance depend on c through a neural network. Similarly the observation model p(x|z; \u03b8) will be a simple likelihood function appropriate to the data modality with dependence on z parameterized by a neural network. We use approximate inference networks q(z|x, c, \u03c6), q(c|D;\u03c6) with parameters \u03c6 to calculate and optimize a variational lower bound on the log-likelihood, where again the likelihood forms are diagonal Gaussians parameterized by neural networks:\nLD = Eq(c|D;\u03c6) [\u2211 x\u2208d Eq(z,|c,x;\u03c6) [log p(x|z; \u03b8)]\u2212DKL (q(z|c, x;\u03c6)\u2016p(z|c; \u03b8)) ] \u2212DKL (q(c|D;\u03c6)\u2016p(c)) (4)\nNote that q(c|D,\u03c6) accepts as input a dataset D and we refer to this as the statistic network. We describe this in Subsection 3.4.\n3.3 Full Model Algorithm 1 Sampling a Dataset of size k\nsample c \u223c p(c) for i = 1 to k do\nsample zi,L \u223c p(zL|c, \u03b8) for j = L\u2212 1 to 1 do\nsample zi,j \u223c p(zj |zi,j+1, c, \u03b8) end for sample xi \u223c p(x|zi,1, . . . , zi,L, c, \u03b8)\nend for The basic model works well for modelling simple datasets, but struggles when the datasets have complex internal structure. To increase the sophistication of the model we use multiple stochastic layers z1, . . . , zk and introduce skipconnections for both the inference and generative networks. The model is shown graphically in Figure 3. The probability of a dataset D is then given by\np(D) = \u222b p(c) \u220f x\u2208D \u222b p(x|c, z1:L; \u03b8)p(zL|c; \u03b8) L\u22121\u220f i=1 p(zi|zi+1, c; \u03b8) dz1:L dc (5)\nand the generative process for the full model is described in Algorithm 1. The full approximate posterior factorizes analogously as\nq(c, z1:L|D;\u03c6) = q(c|D;\u03c6) \u220f x\u2208D q(zL|x, c;\u03c6) L\u22121\u220f i=1 q(zi|zi+1, x, c;\u03c6) (6)\nFor convenience we give the variational lower bound as sum of a three parts\nLD = RD + CD + LD (7)\nthe reconstruction term RD = Eq(c|D;\u03c6) \u2211 x\u2208D Eq(z1:L|c,x;\u03c6) log p(x|z1:L, c; \u03b8) (8)\nthe context divergence CD = DKL (q(c|D;\u03c6)\u2016p(c)) (9)\nand the latent divergences\nLD = Eq(c,z1:L|D;\u03c6) [\u2211 x\u2208D DKL (q(zL|c, x;\u03c6)\u2016p(zL|c; \u03b8))\n+ L\u22121\u2211 i=1 DKL (q(zi|zi+1, c, x;\u03c6)\u2016p(zi|zi+1, c; \u03b8))\n] (10)\nThe skip-connections p(zi|zi+1, c, \u03b8) and q(zi|zi+1, x) allow the context to specify a more precise distribution for each latent variable by explaining-away more generic aspects of the dataset at each stochastic layer. This architecture was inspired by recent work on probabilistic ladder networks in Kaae S\u00f8nderby et al. [14]. Complementing these are the skip-connections from each latent variable to the observation p(x|z1:L, c; \u03b8), the intuition here is that each stochastic layer can focus on representing a certain level of abstraction, since its information does not need to be copied into the next layer, a similar approach was used in Maal\u00f8e et al. [22]. Note that we are optimizing over many datasets D: we want to maximize the expectation of LD over all datasets. We do this optimization using stochastic gradient descent. In contrast to a variational autoencoder where a minibatch would consist of a subsample of datapoints from the dataset, we use minibatches consisting of a subsample of datasets - tensors of shape (batch size, sample size, number of features)."
        },
        {
            "heading": "3.4 Statistic Network",
            "text": "In addition to the standard recognition networks we use a statistic network q(c|D;\u03c6) to give an approximate posterior over the context c given a dataset D = {x1, . . . , xk} . This is a feedforward neural network consisting of three main elements: \u2022 An instance encoder E that takes each individual datapoint xi to a vector ei = E(xi).\n\u2022 An exchangeable instance pooling layer that collapses the matrix (e1, . . . , ek) to a single prestatistic vector v. Examples include elementwise means, sums, products, geometric means and maximum. We use the sample mean for all experiments. \u2022 A final post-pooling network that takes v to a parameterization of a diagonal Gaussian. We note that the humble sample mean already gives the statistic network a great deal of representational power due to the fact that the instance encoder can learn a representation where averaging makes sense. For example since the instance encoder can approximate a polynomial on a compact domain, and so can the post-pooling network, a statistic network can approximate any moment of a distribution."
        },
        {
            "heading": "4 Related Work",
            "text": "Due to the general nature of the problem considered, our work touches on many different topics which we now attempt to summarize.\nTopics models and graphical models We note the resemblance of the graphical model in Figure 3 to the form of a topic model. In contrast to traditional topic models we do not use discrete latent variables, or restrict to discrete data. In addition we use more flexible conditional distributions and dependency structures parameterized by deep neural networks, although recent work has explored this for document models in Miao et al. [23]. Along related lines are \u2018structured variational autoencoders\u2019 [13] where they treat the general problem of integrating graphical models with variational autoencoders.\nTransfer learning There is a considerable literature on transfer learning, for a survey see Pan and Yang [25]. There they discuss \u2018parameter-transfer\u2019 approaches whereby parameters or priors are shared across datasets, and our work fits into that paradigm. For examples see Lawrence and Platt [20] where share they priors between Gaussian processes, and Evgeniou and Pontil [5] where they take an SVM-like approach to share kernels.\nOne-shot Learning Learning quickly from small amounts of data is a topic of great interest. In particular we have seen Lake et al. [18] where they use Bayesian program induction, and [17] where they train a Siamese ([3]) convolutional network for one-shot image classification We note the relation to the recent work [28] in which they use a conditional recurrent variational autoencoder capable of one-shot generalization by taking as extra input a conditioning data point. The important differences here are that we jointly model datasets and datapoints and consider datasets of any size.\nMultiple-Instance Learning There is previous work on classifying sets in multiple-instance learning, for a useful survey see Cheplygina et al. [2]. Typical approaches involve adapting kernel based methods such as support measure machines [24], support distribution machines [26] and multiple-instance-kernels [7].\nSet2Seq A highly related work is Vinyals et al. [31] where they explore architectures for mapping sets to sequences. There they use an LSTM to repeatedly compute weighted-averages of the datapoints. They use this to tackle problems such as sorting a list of numbers. The main difference between their work and ours is that they primarily consider supervised problems, whereas we present a general unsupervised method for learning representations of sets of i.i.d instances. In future work we may also explore recurrently computing statistics.\nABC There has also been work on learning summary statistics for Approximate Bayesian Computation by either learning to predict the parameters generating a sample as a supervised problem, or by using kernel embeddings as infinite dimensional summary statistics. See [6] for instance for kernel-based approaches. More recently Jiang et al. [12] used deep neural networks to predict the parameters generating the data. The crucial differences are that their problem is supervised, they do not leverage any exchangeability properties the data may have, nor can it deal with varying sample sizes."
        },
        {
            "heading": "5 Experimental Results",
            "text": "Given an input set x1, . . . xk we can use the statistic network to calculate an approximate posterior over contexts q(c|x1, . . . , xk;\u03c6). Each context c specifies a generative model p(x|c; \u03b8). To get samples from the model we set c to the mean of the approximate posterior and then sample directly\nfrom the conditional distributions. We use the Adam optimization algorithm [15] for all experiments, batch normalization [10] for models with mulitple stochastic layers and we always use a batch size of 16. We primarily use the Theano [29] framework with the Lasagne [4] library, but the final experiments with face data were done using Tensorflow [1]."
        },
        {
            "heading": "5.1 Simple 1-D Distributions",
            "text": "In our first experiment we wanted to know if the neural statistician will learn to cluster synthetic 1-D datasets by distribution family. We generated a collection of synthetic 1-D datasets each containing 200 samples. Datasets consist of samples from either an Exponential, Gaussian, Uniform or Laplacian distribution with equal probability. Means and variances are sampled from U [\u22121, 1] and U [0.5, 2] respectively. Training data contains 10K sets. We used a model with a single stochastic layer with 32 units for z and 3 units for c. We used three dense layers before and after pooling in the statistic network each with 128 units with Rectified Linear Unit (ReLU) activations. For q(z|x, c) and p(x|z, c) we used three dense layers with ReLU activations and 128 units. Figure 2 on the left shows a 3-D scatter plot of the summary statistics learned. Notice that the different families of distribution cluster. It is interesting to observe that the Exponential cluster is differently orientated to the others, perhaps reflecting the fact that it is the only non-symmetric distribution. We also see that between the Gaussian and Laplacian cluster there is an area of ambiguity which is as one might expect."
        },
        {
            "heading": "5.2 Bernoulli Data with Varying Sample Size",
            "text": "If we observe one coin flip come up heads we gain little information about the coin; if we see fifty heads in a row we are all but certain that the coin is biased. But in both cases the observed proportion of heads is 100%. The lesson is that the sample size can be important for accurate inference of the generative parameters, and so a neural statistician should be able to take sample size into account. The problem is that the sample mean records only the relative proportions of values, for instance: the sample mean s\u0304 = 1k \u2211k i=1 xi is equal to the sample mean of the repeated data 1 kr \u2211k i=1 \u2211r j=1 xi is equal to the sample mean of the single datapoint x1 = s\u0304. The approximate posterior over the context given by the statistic network is a function of a sample mean. Hence the approximate posterior is also partially invariant to the sample size in the above sense. This means that the statistic network q(c|D) has no way to represent uncertainty about the context resulting from differing sample sizes. In order to handle uncertainty over the context resulting from differing sample sizes, we augment our recognition model by introducing a prior-interpolation layer. The prior-interpolation layer modifies the output of the statistic network q(c|D) described in Subsection 3.4 as follows: given that the output of q(c|D) is \u00b5c and \u03c32c parameterizing the approximate posterior N (c;\u00b5c, \u03c32c ), we define \u00b5\u2217 and (\u03c32c ) \u2217, the interpolated parameters by\n\u00b5\u2217c = f\u03b1(n) \u00b7 0 + (1\u2212 f\u03b1(n)) \u00b7 \u00b5c and (\u03c32c )\u2217 = f\u03b2(n) \u00b7 1 + (1\u2212 f\u03b2(n)) \u00b7 \u03c32c (11)\ngiving an interpolated approximate posterior N (c;\u00b5\u2217c , (\u03c32c )\u2217), where n is the sample size, f\u03b1, f\u03b2 are monotonically decreasing functions with outputs in [0, 1] with parameters \u03b1 and \u03b2 (learned through optimizing the variational lower bound in Equation 7), and such that f\u03b1(0) = f\u03b2(0) = 1 and f\u03b1(n), f\u03b2(n) \u2192 0 as n \u2192 \u221e. This allows the statistic network to interpolate its proposed approximate posterior coordinate-wise with the prior p(c) = N (c; 0, 1). To demonstrate that this simple intervention can work we perform an experiment on synthetic data where we can calculate the true posterior analytically. The datasets consist of samples from Bernoulli distributions with probability p \u223c U [0, 1]. Sample sizes are between 1 and 20 with equal probability. We chose f\u03b1(n) = exp(\u2212 exp(\u03b1) \u00b7 n) and f\u03b2(n) = exp(\u2212 exp(\u03b2) \u00b7 n) as our prior-interpolation layer. We use a single stochastic layer with 2 units for c and 2 units for z. There is no instance encoder in the statistic network, the pre-statistic vector is simply the average of each dataset. There are three dense post-pooling layers with 16 ReLU units. The other subnetworks each have 2 dense layers with 32 ReLU units. We use a Bernoulli likelihood for the decoder p(x|z, c). Since the Beta distribution is a conjugate prior for the Bernoulli, we know the exact posterior for p given observations d. In Figure 2 on the right we compare samples from the approximate posterior given by the model and the true posterior across a range of sample sizes, and we see that the model is indeed able to account for uncertainty. In order to sample the probability from our model we sample a context from the approximate posterior and then sample x \u223c p(x|c; \u03b8) 100 times and average the result to give p. This simple modification improves the average log-likelihood per data point from \u22120.627 when training without the prior-interpolation layer to \u22120.597, for comparison the true likelihood of the test data is \u22120.595. 5.3 Spatial MNIST\nBuilding on the previous experiments we investigate 2-D datasets that have complex structure, but the datapoints contain little information by themselves. We created a dataset called spatial MNIST. In spatial MNIST each image from MNIST [21] is turned into a dataset by interpreting the normalised pixel intensities as a probability density and sampling coordinate values. An example is shown in Figure 3. This creates two-dimensional spatial datasets. We used a sample size of 50. Note that since the pixel coordinates are discrete, it is necessary to dequantize them by adding uniform noise u \u223c U [0, 1] to the coordinates if one models them as real numbers, else you can get arbitrarily high densities (see [30] for a discussion of this point).\nWe used 3 stochastic layers with 2 units for each z and 64 units for c. Each subnetwork contained 3 dense layers with 256 ReLU units each. We used a Gaussian likelihood for p(x|z1:3, c \u03b8) . In addition to being able to sample from the model conditioned on a set of inputs, we can also to summarize a dataset by choosing a subset S \u2286 d to minimise the KL divergence of q(C|d;\u03c6) from q(C|S;\u03c6). We do this greedily by iteratively discarding points from the full sample. The results are shown in Figure 5.3. We see that the model is capable of handling complex arrangements of datapoints. We also see that it can select sensible subsets of a dataset as a summary, notice that the model tends to select \u2018critical points\u2019 of the shape just as a human might do."
        },
        {
            "heading": "5.4 MNIST",
            "text": "We now move on to modelling high dimensional data. A natural source of such datasets are images of a particular class. We begin with sets of MNIST digits of the same class with sample sizes of 10. We\ntreat the data as binary by interpreting the pixel intensities as Bernoulli probabilities and sampling. We resample the training data at each epoch to combat overfitting following [14]. Since the MNIST digits exhibit relatively small amounts of variation, we use a single stochastic layer with 32 units for both z and c. The statistic network has three pre and post pooling dense layers with 64 ReLU units each. The other subnetworks each have three dense layers with 256 ReLU units each. We used a Bernoulli likelihood for the decoder. Conditioned samples are shown in Figure 5 on the right. The samples appear well-formed and so we have demonstrated that it is possible to flexibly condition generative models of high-dimensional data on-the-fly."
        },
        {
            "heading": "5.5 Omniglot",
            "text": "Next we work with the OMNIGLOT data [18]. This contains 1628 classes of handwritten characters but with just 20 examples per class. This makes it an excellent test-bed for transfer / small-shot learning. Here the datasets correspond to samples from a given class, and we use a sample size of 5. We save 5 examples per class for the test set, and in addition we hold out 20 entire classes to test generalization to new classes. We created new classes with data augmentation by rotating and reflecting characters. We resized the images to 28\u00d7 28. As with MNIST we treated the data as binary and sampled a binarization of the image at each epoch. We used three stochastic layers with 16 units for each z and 64 units for c. The decoder p(x|z1, z2, z3, c) consisted of: three dense layers with 256 ReLU units, followed a dense layer H with 10\u00d7 7\u00d7 7 linear units, followed by ReLU convolutional layers with 64\u2192 64\u2192 32 filters of size 3\u00d7 3 interspersed with 2\u00d7 2 upsampling layers, followed by a 1\u00d7 1 convolution with a single filter and a sigmoid activation. The output was then passed through a spatial transformer network [11] effecting an affine change of coordinates with parameters a linear projection of H . The decoder used a Bernoulli likelihood. All other subnetworks had 3 dense layers with 256 ReLU units. In Figure 5 on the left we show samples from the model conditioned on classes seen in the training data. The good quality of the samples indicate that the model is indeed able to represent a large variety of different datasets. In Figure 6 we show two examples of small-shot learning by conditioning on samples of unseen characters from OMNIGLOT, and conditioning on samples of digits from MNIST. The OMNIGLOT samples are mostly of a high-quality, and this shows that the neural statistician can generalize even to new datasets. The transfer to MNIST represents a harder task and produces less attractive samples, but they are still recognisably the correct digit. We believe that the results could be improved further through use of a convolutional encoder and/or more aggressive data augmentation. As a further test we considered small-shot classification of both unseen OMNIGLOT characters and MNIST digits. Given a sets of labelled examples of each class D0, . . . , D9 for MNIST say we computed the approximate posteriors q(C|Di;\u03c6) using the statistic network. Then for each test image x we also computed the posterior q(C|x;\u03c6) and classified it according to training dataset Di minimizing the KL divergence from the test context to the training context. We tried this with either 1 or 5 labelled examples per class. We implemented two simple baselines, 1-nearest neighbour on the raw features, and 1-nearest neighbour on features learned by an autoencoder trained on the OMNIGLOT training data. The autoencoder had 5 hidden layers with 256,256,64,256,256 respective\nReLU units, the middle layer was used as the feature embedding. The results are shown in Table 1. We also include similar results from Koch [17] a discriminatively trained model, but note that they are not directly comparable since they used different splits of characters when training their model. We include their results to give a rough idea of how our model compares to one trained for this task. We find that the performance is remarkably good, especially considering that the model itself was not discriminatively trained."
        },
        {
            "heading": "5.6 Youtube Faces",
            "text": "Finally, we provide a proof of concept for generating faces of a particular person. We use the Youtube Faces Database from Wolf et al. [32]. It contains 3, 245 videos of 1, 595 different people. We use the aligned and cropped to face version, resize to 64\u00d7 64. The validation and test sets contain 100 unique people each, and there is no overlap of persons between data splits. The sets were created by\nsampling frames randomly without replacement from each video, we use a set size of 5 frames. We resample the sets for the training data each epoch. Our architecture for this problem is based on one presented in [19]. We used a single stochastic layer with 500 dimensional latent c and 16 dimensional z variable. The statistic network and the inference network q(z|x, c) share a common convolutional encoder consisting of layers with (filter sizes, numbers of filters, strides) given by (3,32,1), (3,32,1), (2,32,2), (3,64,1), (3,64,1), (2,64,2),(3,128,1), (3,128,1), (2,128,2), (3,256,1), (3,256,1), (2,256,2) respectively, all with ReLU activations. The statistic network continues with a dense ReLU layer with 1000 units, followed by average pooling and a dense layer to c. The inference network continues by concatenating with the sample from the statistic network, then a 1000 unit ReLU layer then a linear mapping to z. The decoder begins with a 1000 unit ReLU layer, followed by a linear layer with 256 \u00d7 8 \u00d7 8 units followed by convolutional layers with (filter sizes, numbers of filters, strides) given by (3,256,1), (3,256,1), (3,256,2), (3,128,1), (3,128,1), (2,128,2), (3,64,1), (3,64,1),(2,64,2), (3,32,1), (3,32,1), (2,32,2), (1,3,1) respectively, with stride 1 layers being convolutions and stride 2 layers being transpose-convolutional layers. All activations are ReLUs except for the last layer which is a sigmoid. The likelihood function is a Gaussian, but where the variance parameters are shared across all datapoints, this was found to make training faster and more stable. The results are shown in Figure 7. Whilst there is room for improvement, we see that it is possible to specify a highly complex distribution on-the-fly with a set of photos of a previously unseen person. The samples conditioned on an input set have a reasonable likeness of the input faces. We also show the ability of the model to generate new datasets and see that the samples have a consistent identity and varied poses. Our approach can easily benefit from advances in deep generative models by upgrading our base generative model, and so in the future we could for instance use a recurrent generative model as in [8], [28], [9]."
        },
        {
            "heading": "6 Conclusions and Future Work",
            "text": "Our goal was to demonstrate that it is both possible and profitable to work at a level of abstraction of datasets rather than just datapoints. We have shown how it is possible to learn to represent datasets using a statistic network, and that these statistics enable highly flexible and efficient models that can do transfer learning, small shot classification, cluster distributions, summarize datasets and more. Avenues for future research are engineering, methodological and application based. In terms of engineering we believe that there are gains to be had by more thorough exploration of different (larger) architectures. In terms of methodology we want to look at: improved methods of representing uncertainty resulting from sample size; models explicitly designed trained for small-shot classification; supervised and semi-supervised approaches to classifiying either datasets or datapoints within the dataset. One\nadvantage we have yet to explore is that by specifying classes implicitly in terms of sets, we can combine multiple data sources with potentially different labels, or multiple labels. We can also easily train on any unlabelled data because this corresponds to sets of size one. We also want to consider questions such as: What are desirable properties for statistics to have as representations? How can we enforce these? Can we use ideas from classical work on estimators? In terms of applications we are interested in applying this framework to learning embeddings of speakers for speech problems or customer embeddings in commercial problems."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported in part by the EPSRC Centre for Doctoral Training in Data Science, funded by the UK Engineering and Physical Sciences Research Council (grant EP/L016427/1) and the University of Edinburgh."
        }
    ],
    "title": "Towards a Neural Statistician",
    "year": 2022
}