{
    "abstractText": "Perception and expression of emotion are key factors to the success of dialogue systems or conversational agents. However, this problem has not been studied in large-scale conversation generation so far. In this paper, we propose Emotional Chatting Machine (ECM) that can generate appropriate responses not only in content (relevant and grammatical) but also in emotion (emotionally consistent). To the best of our knowledge, this is the first work that addresses the emotion factor in large-scale conversation generation. ECM addresses the factor using three new mechanisms that respectively (1) models the high-level abstraction of emotion expressions by embedding emotion categories, (2) captures the change of implicit internal emotion states, and (3) uses explicit emotion expressions with an external emotion vocabulary. Experiments show that the proposed model can generate responses appropriate not only in content but also in emotion.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hao Zhou"
        },
        {
            "affiliations": [],
            "name": "Minlie Huang"
        },
        {
            "affiliations": [],
            "name": "Tianyang Zhang"
        },
        {
            "affiliations": [],
            "name": "Xiaoyan Zhu"
        },
        {
            "affiliations": [],
            "name": "Bing Liu"
        }
    ],
    "id": "SP:e661f677dc2c8302f810a0a93f370235fe873713",
    "references": [
        {
            "authors": [
                "Danieli Alam",
                "F. Riccardi 2017] Alam",
                "M. Danieli",
                "G. Riccardi"
            ],
            "title": "Annotating and modeling empathy in spoken conversations",
            "venue": "CoRR abs/1705.04839",
            "year": 2017
        },
        {
            "authors": [
                "Cho Bahdanau",
                "D. Bengio 2014] Bahdanau",
                "K. Cho",
                "Y. Bengio"
            ],
            "title": "Neural machine translation by jointly learning to align and translate",
            "venue": "CoRR abs/1409.0473",
            "year": 2014
        },
        {
            "authors": [
                "Cagan, T.",
                "Frank"
            ],
            "title": "S",
            "venue": "L.; and Tsarfaty, R.",
            "year": 2017
        },
        {
            "authors": [
                "Cho"
            ],
            "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
            "venue": "CoRR abs/1406.1078",
            "year": 2014
        },
        {
            "authors": [
                "Chung"
            ],
            "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
            "venue": "CoRR abs/1412.3555",
            "year": 2014
        },
        {
            "authors": [
                "Fleiss"
            ],
            "title": "J",
            "venue": "L.",
            "year": 1971
        },
        {
            "authors": [
                "Ghosh"
            ],
            "title": "Affect-lm: A neural language model for customizable affective text generation",
            "year": 2017
        },
        {
            "authors": [
                "Fern\u00e1ndez Graves",
                "A. Schmidhuber 2005] Graves",
                "S. Fern\u00e1ndez",
                "J. Schmidhuber"
            ],
            "title": "Bidirectional lstm networks for improved phoneme classification and recognition",
            "venue": "In ICANN,",
            "year": 2005
        },
        {
            "authors": [
                "Gross"
            ],
            "title": "J",
            "venue": "J.",
            "year": 1998
        },
        {
            "authors": [
                "J. Gu",
                "Z. Lu",
                "H. Li",
                "Li"
            ],
            "title": "V",
            "venue": "O.",
            "year": 2016
        },
        {
            "authors": [
                "Herzig"
            ],
            "title": "Neural response generation for customer service based on personality traits",
            "venue": "In Proceedings of the 10th International Conference on Natural Language Generation,",
            "year": 2017
        },
        {
            "authors": [
                "S. Hochreiter"
            ],
            "title": "and Schmidhuber",
            "venue": "J.",
            "year": 1997
        },
        {
            "authors": [
                "Hochschild"
            ],
            "title": "A",
            "venue": "R.",
            "year": 1979
        },
        {
            "authors": [
                "Z. Hu",
                "Z. Yang",
                "X. Liang",
                "R. Salakhutdinov",
                "Xing"
            ],
            "title": "E",
            "venue": "P.",
            "year": 2017
        },
        {
            "authors": [
                "Li"
            ],
            "title": "2016a. A diversity-promoting objective function for neural conversation models",
            "year": 2016
        },
        {
            "authors": [
                "Li"
            ],
            "title": "2016b. A personabased neural conversation model",
            "year": 2016
        },
        {
            "authors": [
                "Liu"
            ],
            "title": "How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation",
            "year": 2016
        },
        {
            "authors": [
                "B. Martinovski"
            ],
            "title": "and Traum",
            "venue": "D.",
            "year": 2003
        },
        {
            "authors": [
                "J.D. Mayer"
            ],
            "title": "and Salovey",
            "venue": "P.",
            "year": 1997
        },
        {
            "authors": [
                "Mikolov"
            ],
            "title": "Recurrent neural network based language model",
            "venue": "In Interspeech,",
            "year": 2010
        },
        {
            "authors": [
                "Miller"
            ],
            "title": "A",
            "venue": "H.; Fisch, A.; Dodge, J.; Karimi, A.; Bordes, A.; and Weston, J.",
            "year": 2016
        },
        {
            "authors": [
                "Mou"
            ],
            "title": "Sequence to backward and forward sequences: A content-introducing approach to generative short-text conversation",
            "year": 2016
        },
        {
            "authors": [
                "T. Partala"
            ],
            "title": "and Surakka",
            "venue": "V.",
            "year": 2004
        },
        {
            "authors": [
                "R.W. Picard"
            ],
            "title": "and Picard",
            "venue": "R.",
            "year": 1997
        },
        {
            "authors": [
                "T.S. Polzin"
            ],
            "title": "and Waibel",
            "venue": "A.",
            "year": 2000
        },
        {
            "authors": [
                "H. Prendinger"
            ],
            "title": "and Ishizuka",
            "venue": "M.",
            "year": 2005
        },
        {
            "authors": [
                "Mori Prendinger",
                "H. Ishizuka 2005] Prendinger",
                "J. Mori",
                "M. Ishizuka"
            ],
            "title": "Using human physiology to evaluate subtle expressivity of a virtual quizmaster in a mathematical game. International journal of human-computer studies 62(2):231\u2013245",
            "year": 2005
        },
        {
            "authors": [
                "A. Ritter",
                "C. Cherry",
                "Dolan"
            ],
            "title": "W",
            "venue": "B.",
            "year": 2011
        },
        {
            "authors": [
                "I.V. Serban",
                "A. Sordoni",
                "Y. Bengio",
                "Courville"
            ],
            "title": "A",
            "venue": "C.; and Pineau, J.",
            "year": 2015
        },
        {
            "authors": [
                "I.V. Serban",
                "A. Sordoni",
                "Y. Bengio",
                "Courville"
            ],
            "title": "A",
            "venue": "C.; and Pineau, J.",
            "year": 2016
        },
        {
            "authors": [
                "Lu Shang",
                "L. Li 2015] Shang",
                "Z. Lu",
                "H. Li"
            ],
            "title": "Neural responding machine for short-text conversation",
            "year": 2015
        },
        {
            "authors": [
                "I. Sutskever",
                "O. Vinyals",
                "Le"
            ],
            "title": "Q",
            "venue": "V.",
            "year": 2014
        },
        {
            "authors": [
                "O. Vinyals"
            ],
            "title": "and Le",
            "venue": "Q.",
            "year": 2015
        },
        {
            "authors": [
                "Xing"
            ],
            "title": "Topic aware neural response generation",
            "year": 2017
        },
        {
            "authors": [],
            "title": "Affective lexicon ontology",
            "venue": "Journal of information",
            "year": 2008
        }
    ],
    "sections": [
        {
            "heading": "Introduction",
            "text": "As a vital part of human intelligence, emotional intelligence is defined as the ability to perceive, integrate, understand, and regulate emotions (Mayer and Salovey 1997). It has been a long-term goal of artificial intelligence to enable a machine to understand affect and emotion (Picard and Picard 1997). To create a chatbot capable of communicating with a user at the human level, it is necessary to equip the machine with the ability of perceiving and expressing emotions.\nExisting studies show that addressing affect and emotion in dialogue systems or conversational agents can enhance user satisfaction (Prendinger, Mori, and Ishizuka 2005),\nand lead to fewer breakdowns in dialogues (Martinovski and Traum 2003). Some initial work has also been done on adjusting dialogue behaviors to suit users\u2019 emotional states (Polzin and Waibel 2000), and on generating responses to users\u2019 utterances at both the content- and affect-related levels (Skowron 2010).\nHowever, these studies, mostly inspired by psychology findings, are either rule-based or limited to small-scale data. Recently, neural models trained on large-scale data\n\u2217Corresponding author: Minlie Huang, aihuang@tsinghua.edu .cn Copyright c\u00a9 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nhave advanced open-domain conversation generation significantly (Ritter, Cherry, and Dolan 2011; Vinyals and Le 2015; Shang, Lu, and Li 2015; Serban et al. 2016). Most of these models aim to improve the content quality of conversation generation (Gu et al. 2016; Li et al. 2016a; Xing et al. 2017; Mou et al. 2016; Li et al. 2016b). To the best of our knowledge, the emotion factor has not been addressed in existing neural models for conversation generation. Table 1 shows some examples with/without considering emotions. We can see that our ECM model is emotionally involved and more empathetic.\nThere are several challenges in addressing the emotion factor in large-scale conversation generation. First, highquality emotion-labeled data are difficult to obtain in a largescale corpus, as emotion annotation is a fairly subjective task and emotion classification is also challenging. Second, it is difficult to consider emotions in a natural and coherent way because we need to balance grammaticality and expressions of emotions, as argued in (Ghosh et al. 2017). Last, simply embedding emotion information in existing neural models, as shown in our experiments, cannot produce desirable emotional responses but just hard-to-perceive general expressions (which contain only common words that are quite implicit or ambiguous about emotions, and amount to 73.7% of all emotional responses in our dataset).\nIn this paper, we address the problem of generating emotional responses in open-domain conversational systems and propose an emotional chatting machine (ECM for short). To obtain large-scale emotion-labeled data for ECM, we train a neural classifier on a manually annotated corpus. The classifier is used to annotate large-scale conversation data automatically for the training of ECM. To express emotion naturally and coherently in a sentence, we design a sequence-\nar X\niv :1\n70 4.\n01 07\n4v 4\n[ cs\n.C L\n] 1\nJ un\n2 01\n8\nto-sequence generation model equipped with new mechanisms for emotion expression generation, namely, emotion category embedding for capturing high-level abstraction of emotion expressions, an internal emotion state for balancing grammaticality and emotion dynamically, and an external emotion memory to help generate more explicit and unambiguous emotional expressions.\nIn summary, this paper makes the following contributions:\n\u2022 It proposes to address the emotion factor in large-scale conversation generation. To the best of our knowledge, this is the first work on the topic.\n\u2022 It proposes an end-to-end framework (called ECM) to incorporate the emotion influence in large-scale conversation generation. It has three novel mechanisms: emotion category embedding, an internal emotion memory, and an external memory.\n\u2022 It shows that ECM can generate responses with higher content and emotion scores than the traditional seq2seq model. We believe that future work such as the empathetic computer agent and the emotion interaction model can be carried out based on ECM."
        },
        {
            "heading": "Related Work",
            "text": "In human-machine interactions, the ability to detect signs of human emotions and to properly react to them can enrich communication. For example, display of empathetic emotional expressions enhanced users\u2019 performance (Partala and Surakka 2004), and led to an increase in user satisfaction (Prendinger, Mori, and Ishizuka 2005). Experiments in (Prendinger and Ishizuka 2005) showed that an empathetic computer agent can contribute to a more positive perception of the interaction. In (Martinovski and Traum 2003), the authors showed that many breakdowns could be avoided if the machine was able to recognize the emotional state of the user and responded to it sensitively. The work in (Polzin and Waibel 2000) presented how dialogue behaviors can be adjusted to users\u2019 emotional states. Skowron (2010) proposed conversational systems, called affect listeners, that can respond to users\u2019 utterances both at the content- and affectrelated level.\nThese works, mainly inspired by psychological findings, are either rule-based, or limited to small data, making them difficult to apply to large-scale conversation generation. Recently, sequence-to-sequence generation models (Sutskever, Vinyals, and Le 2014; Bahdanau, Cho, and Bengio 2014) have been successfully applied to large-scale conversation generation (Vinyals and Le 2015), including neural responding machine (Shang, Lu, and Li 2015), hierarchical recurrent models (Serban et al. 2015), and many others. These models focus on improving the content quality of the generated responses, including diversity promotion (Li et al. 2016a), considering additional information (Xing et al. 2017; Mou et al. 2016; Li et al. 2016b; Herzig et al. 2017), and handing unknown words (Gu et al. 2016).\nHowever, no work has addressed the emotion factor in large-scale conversation generation. There are several studies that generate text from controllable variables. (Hu et al.\n2017) proposed a generative model which can generate sentences conditioned on certain attributes of the language such as sentiment and tenses. Affect Language Model was proposed in (Ghosh et al. 2017) to generate text conditioned on context words and affect categories. (Cagan, Frank, and Tsarfaty 2017) incorporated the grammar information to generate comments for a document using sentiment and topics. Our work is different in two main aspects: 1) prior studies are heavily dependent on linguistic tools or customized parameters in text generation, while our model is fully datadriven without any manual adjustment; 2) prior studies are unable to model multiple emotion interactions between the input post and the response, instead, the generated text simply continues the emotion of the leading context."
        },
        {
            "heading": "Emotional Chatting Machine",
            "text": ""
        },
        {
            "heading": "Background: Encoder-decoder Framework",
            "text": "Our model is based on the encoder-decoder framework of the general sequence-to-sequence (seq2seq for short) model (Sutskever, Vinyals, and Le 2014). It is implemented with gated recurrent units (GRU) (Cho et al. 2014; Chung et al. 2014). The encoder converts the post sequence X = (x1, x2, \u00b7 \u00b7 \u00b7 , xn) to hidden representations h = (h1,h2, \u00b7 \u00b7 \u00b7 ,hn), which is defined as:\nht = GRU(ht\u22121, xt). (1)\nThe decoder takes as input a context vector ct and the embedding of a previously decoded word e(yt\u22121) to update its state st using another GRU:\nst = GRU(st\u22121, [ct; e(yt\u22121)]), (2)\nwhere [ct; e(yt\u22121)] is the concatenation of the two vectors, serving as the input to the GRU cell. The context vector ct is designed to dynamically attend on key information of the input post during decoding (Bahdanau, Cho, and Bengio 2014). Once the state vector st is obtained, the decoder generates a token by sampling from the output probability distribution ot computed from the decoder\u2019s state st as follows:\nyt \u223c ot = P (yt | y1, y2, \u00b7 \u00b7 \u00b7 , yt\u22121, ct), (3) = softmax(Wost). (4)"
        },
        {
            "heading": "Task Definition and Overview",
            "text": "Our problem is formulated as follows: Given a post X = (x1, x2, \u00b7 \u00b7 \u00b7 , xn) and an emotion category e of the response to be generated (explained below), the goal is to generate a response Y = (y1, y2, \u00b7 \u00b7 \u00b7 , ym) that is coherent with the emotion category e. Essentially, the model estimates the probability: P (Y |X, e) = \u220fm t=1 P (yt|y<t,X, e). The emotion categories are {Angry, Disgust, Happy, Like, Sad, Other}, adopted from a Chinese emotion classification challenge task.1\n1The taxonomy comes from http://tcci.ccf.org.cn/confere -nce/2014/dldoc/evatask1.pdf\nIn our problem statement, we assume that the emotion category of the to-be-generated response is given, because emotions are highly subjective. Given a post, there may be multiple emotion categories that are suitable for its response, depending on the attitude of the respondent. For example, for a sad story, someone may respond with sympathy (as a friend), someone may feel angry (as an irritable stranger), yet someone else may be happy (as an enemy). Flexible emotion interactions between a post and a response are an important difference from the previous studies (Hu et al. 2017; Ghosh et al. 2017; Cagan, Frank, and Tsarfaty 2017), which use the same emotion or sentiment for response as that in the input post.\nThus, due to this subjectivity of emotional responses, we choose to focus on solving the core problem: generating an emotional response given a post and an emotion category of the response. Our model thus works regardless the response emotion category. Note that there can be multiple ways to enable a chatbot to choose an emotion category for response. One way is to give the chatbot a personality and some background knowledge. Another way is to use the training data to find the most frequent response emotion category for the emotion in the given post and use that as the response emotion. This method is reasonable as it reflects the general emotion of the people. We leave this study to our future work.\nBuilding upon the generation framework discussed in the previous section, we propose the Emotional Chatting Machine (ECM) to generate emotion expressions using three mechanisms: First, since the emotion category is a highlevel abstraction of an emotion expression, ECM embeds the emotion category and feeds the emotion category embedding to the decoder. Second, we assume that during decoding, there is an internal emotion state, and in order to capture the implicit change of the state and to balance the weights between the grammar state and the emotion state dynamically, ECM adopts an internal memory module. Third, an explicit expression of an emotion is modeled through an explicit selection of a generic (non-emotion) or emotion word by an external memory module.\nAn overview of ECM is given in Figure 1. In the training process, the corpus of post-response pairs is fed to an emotion classifier to generate the emotion label of each response, and then ECM is trained on the data of triples: posts,\nresponses and emotion labels of responses. In the inference process, a post is fed to ECM to generate emotional responses conditioned on different emotion categories."
        },
        {
            "heading": "Emotion Category Embedding",
            "text": "Since an emotion category (for instance, Angry, Disgust, Happy) provides a high-level abstraction of an emotion expression, the most intuitive approach to modeling emotion in response generation is to take as additional input the emotion category of a response to be generated. Each emotion category is represented by a real-valued, low dimensional vector. For each emotion category e, we randomly initialize the vector of an emotion category ve, and then learn the vectors of the emotion category through training. The emotion category embedding ve, along with word embedding e(yt\u22121), and the context vector ct, are fed into the decoder to update the decoder\u2019s state st:\nst = GRU(st\u22121, [ct; e(yt\u22121);ve]). (5)\nBased on st, the decoding probability distribution can be computed accordingly by Eq. 4 to generate the next token yt."
        },
        {
            "heading": "Internal Memory",
            "text": "The method presented in the preceding section is rather static: the emotion category embedding will not change during the generation process which may sacrifice grammatical correctness of sentences as argued in (Ghosh et al. 2017). Inspired by the psychological findings that emotional responses are relatively short lived and involve changes (Gross 1998; Hochschild 1979), and the dynamic emotion situation in emotional responses (Alam, Danieli, and Riccardi 2017), we design an internal memory module to capture the emotion dynamics during decoding. We simulate the process of expressing emotions as follows: there is an internal emotion state for each category before the decoding process starts; at each step the emotion state decays by a certain amount; once the decoding process is completed, the emotion state should decay to zero indicating the emotion is completely expressed.\nThe detailed process of the internal memory module is illustrated in Figure 2. At each step t, ECM computes a read\ngate grt with the input of the word embedding of the previously decoded word e(yt\u22121), the previous state of the decoder st\u22121, and the current context vector ct. A write gate gwt is computed on the decoder\u2019s state vector st. The read gate and write gate are defined as follows:\ngrt = sigmoid(W r g[e(yt\u22121); st\u22121; ct]), (6)\ngwt = sigmoid(W w g st). (7)\nThe read and write gates are then used to read from and write into the internal memory, respectively. Hence, the emotion state is erased by a certain amount (by gwt ) at each step. At the last step, the internal emotion state will decay to zero. This process is formally described as below:\nM Ir,t = g r t \u2297M Ie,t, (8)\nM Ie,t+1 = g w t \u2297M Ie,t, (9)\nwhere \u2297 is element-wise multiplication, r/w denotes read/write respectively, and I means Internal. GRU updates its state st conditioned on the previous target word e(yt\u22121), the previous state of the decoder st\u22121, the context vector ct, and the emotion state update M Ir,t, as follows:\nst = GRU(st\u22121, [ct; e(yt\u22121);M I r,t]). (10)\nBased on the state, the word generation distribution ot can be obtained with Eq. 4, and the next word yt can be sampled. After generating the next word, M Ie,t+1 is written back to the internal memory. Note that if Eq. 9 is executed many times, it is equivalent to continuously multiplying the matrix, resulting in a decay effect since 0 \u2264 sigmoid(\u00b7) \u2264 1. This is similar to a DELETE operation in memory networks (Miller et al. 2016)."
        },
        {
            "heading": "External Memory",
            "text": "In the internal memory module, the correlation between the change of the internal emotion state and selection of a word is implicit and not directly observable. As the emotion expressions are quite distinct with emotion words (Xu et al. 2008) contained in a sentence, such as lovely and awesome, which carry strong emotions compared to generic\n(non-emotion) words, such as person and day, we propose an external memory module to model emotion expressions explicitly by assigning different generation probabilities to emotion words and generic words. Thus, the model can choose to generate words from an emotion vocabulary or a generic vocabulary.\nThe decoder with an external memory is illustrated in Figure 3. Given the current state of the decoder st, the emotion softmax Pe(yt = we) and the generic softmax Pg(yt = wg) are computed over the emotion vocabulary which is read from the external memory and generic vocabulary, respectively. The type selector \u03b1t controls the weight of generating an emotion or a generic word. Finally, the next word yt is sampled from the next word probability, the concatenation of the two weighted probabilities. The process can be formulated as follows:\n\u03b1t = sigmoid(vu >st), (11)\nPg(yt = wg) = softmax(W o gst), (12) Pe(yt = we) = softmax(W o est), (13)\nyt \u223c ot = P (yt) = [ (1\u2212 \u03b1t)Pg(yt = wg)\n\u03b1tPe(yt = we)\n] , (14)\nwhere \u03b1t \u2208 [0, 1] is a scalar to balance the choice between an emotion word we and a generic word wg , Pg/Pe is the distribution over generic/emotion words respectively, and P (yt) is the final word decoding distribution. Note that the two vocabularies have no intersection, and the final distribution P (yt) is a concatenation of two distributions."
        },
        {
            "heading": "Loss Function",
            "text": "The loss function is the cross entropy error between the predicted token distribution ot and the gold distribution pt in the training corpus. Additionally, we apply two regularization terms: one on the internal memory, enforcing that the internal emotion state should decay to zero at the end of decoding, and the other on the external memory, constraining the selection of an emotional or generic word.\nThe loss on one sample < X,Y > (X = x1, x2, ..., xn, Y = y1, y2, ..., ym) is defined as:\nL(\u03b8) = \u2212 m\u2211 t=1 ptlog(ot)\u2212 m\u2211 t=1 qtlog(\u03b1t)+ \u2016M Ie,m \u2016,\n(15) where M Ie,m is the internal emotion state at the last step m, \u03b1t is the probability of choosing an emotion word or a generic word, and qt \u2208 {0, 1} is the true choice of an emotion word or a generic word in Y . The second term is used to supervise the probability of selecting an emotion or generic word. And the third term is used to ensure that the internal emotion state has been expressed completely once the generation is completed."
        },
        {
            "heading": "Data Preparation",
            "text": "Since there is no off-the-shelf data to train ECM, we firstly trained an emotion classifier using the NLPCC emotion classification dataset and then used the classifier to annotate the STC conversation dataset (Shang, Lu, and Li 2015) to construct our own experiment dataset. There are two steps in the data preparation process:\n1. Building an Emotion Classifier. We trained several classifiers on the NLPCC dataset and then chose the best classifier for automatic annotation. This dataset was used in challenging tasks of emotion classification in NLPCC20132 and NLPCC20143, consisting of 23,105 sentences collected from Weibo. It was manually annotated with 8 emotion categories: Angry, Disgust, Fear, Happy, Like, Sad, Surprise, and Other. After removing the infrequent classes (Fear (1.5%) and Surprise (4.4%)), we have six emotion categories, i.e., Angry, Disgust, Happy, Like, Sad and Other.\nWe then partitioned the NLPCC dataset into training, validation, and test sets with the ratio of 8:1:1. Several emotion classifiers were trained on the filtered dataset, including a lexicon-based classifier (Liu 2012) (we used the emotion lexicon in (Xu et al. 2008)), RNN (Mikolov et al. 2010), LSTM (Hochreiter and Schmidhuber 1997), and Bidirectional LSTM (Bi-LSTM) (Graves, Ferna\u0301ndez, and Schmidhuber 2005). Results in Table 2 show that all neural classifiers outperform the lexicon-based classifier, and the BiLSTM classifier obtains the best accuracy of 0.623.\n2. Annotating STC with Emotion. We applied the best classifier, Bi-LSTM, to annotate the STC Dataset with the six emotion categories. After annotation, we obtained an\n2http://tcci.ccf.org.cn/conference/2013/ 3http://tcci.ccf.org.cn/conference/2014/\nemotion-labeled dataset, which we call the Emotional STC (ESTC) Dataset. The statistics of the ESTC Dataset are shown in Table 3. Although the emotion labels for ESTC Dataset are noisy due to automatic annotation, this dataset is good enough to train the models in practice. As future work, we will study how the classification errors influence response generation."
        },
        {
            "heading": "Experiments",
            "text": ""
        },
        {
            "heading": "Implementation Details",
            "text": "We used Tensorflow4 to implement the proposed model5. The encoder and decoder have 2-layer GRU structures with 256 hidden cells for each layer and use different sets of parameters respectively. The word embedding size is set to 100. The vocabulary size is limited to 40,000. The embedding size of emotion category is set to 100. The internal memory is a trainable matrix of size 6\u00d7256 and the external memory is a list of 40,000 words containing generic words and emotion words (but emotion words have different markers). To generate diverse responses, we adopted beam search in the decoding process of which the beam size is set to 20, and then reranked responses by the generation probability after removing those containing UNKs, unknown words.\nWe used the stochastic gradient descent (SGD) algorithm with mini-batch. Batch size and learning rate are set to 128 and 0.5, respectively. To accelerate the training process, we trained a seq2seq model on the STC dataset with pre-trained word embeddings. And we then trained our model on the ESTC Dataset with parameters initialized by the parameters of the pre-trained seq2seq model. We ran 20 epoches, and the training stage of each model took about a week on a Titan X GPU machine."
        },
        {
            "heading": "Baselines",
            "text": "As aforementioned, this paper is the first work to address the emotion factor in large-scale conversation generation. We did not find closely-related baselines in the literature. AffectLM (Ghosh et al. 2017) cannot be our baseline because it is unable to generate responses of different emotions for the same post. Instead, it simply copies and uses the emotion of the input post. Moreover, it depends heavily on linguistic resources and needs manual parameter adjustments.\n4https://github.com/tensorflow/tensorflow 5https://github.com/tuxchow/ecm\nNevertheless, we chose two suitable baselines: a general seq2seq model (Sutskever, Vinyals, and Le 2014), and an emotion category embedding model (Emb) created by us where the emotion category is embedded into a vector, and the vector serves as an input to every decoding position, similar to the idea of user embedding in (Li et al. 2016b). As emotion category is a high-level abstraction of emotion expressions, this is a proper baseline for our model."
        },
        {
            "heading": "Automatic Evaluation",
            "text": "Metrics: As argued in (Liu et al. 2016), BLEU is not suitable for measuring conversation generation due to its low correlation with human judgment. We adopted perplexity to evaluate the model at the content level (whether the content is relevant and grammatical). To evaluate the model at the emotion level, we adopted emotion accuracy as the agreement between the expected emotion category (as input to the model) and the predicted emotion category of a generated response by the emotion classifier.\nResults: The results are shown in Table 4. As can be seen, ECM obtains the best performance in emotion accuracy, and the performance in perplexity is better than Seq2Seq but worse than Emb. This may be because the loss function of ECM is supervised not only on perplexity, but also on the selection of generic or emotion words (see Eq.15). In practice, emotion accuracy is more important than perplexity considering that the generated sentences are already fluent and grammatical with the perplexity of 68.0.\nIn order to investigate the influence of different modules, we conducted ablation tests where one of the three modules was removed from ECM each time. As we can see, ECM without the external memory achieves the best performance in perplexity. Our model can generate responses without sacrificing grammaticality by introducing the internal memory, where the module can balance the weights between grammar and emotion dynamically. After removing the external memory, the emotion accuracy decreases the most, indicating the external memory leads to a higher emotion accuracy since it explicitly chooses the emotion words. Note that the emotion accuracy of Seq2Seq is extremely low because it generates the same response for different emotion categories."
        },
        {
            "heading": "Manual Evaluation",
            "text": "In order to better understand the quality of the generated responses from the content and emotion perspectives, we performed manual evaluation. Given a post and an emotion cat-\negory, responses generated from all the models were randomized and presented to three human annotators.\nMetrics: Annotators were asked to score a response in terms of Content (rating scale is 0,1,2) and Emotion (rating scale is 0,1), and also to state a preference between any two systems. Content is defined as whether the response is appropriate and natural to a post and could plausibly have been produced by a human, which is a widely accepted metric adopted by researchers and conversation challenging tasks, as proposed in (Shang, Lu, and Li 2015). Emotion is defined as whether the emotion expression of a response agrees with the given emotion category.\nAnnotation Statistics: We randomly sampled 200 posts from the test set. For each model we generated 1,200 responses in total: for Seq2Seq, we generated the top 6 responses for each post, and for Emb and ECM, we generated the top responses corresponding to the 6 emotion categories.\nWe calculated the Fleiss\u2019 kappa (Fleiss 1971) to measure inter-rater consistency. Fleiss\u2019 kappa for Content and Emotion is 0.441 and 0.757, indicating \u201cModerate agreement\u201d and \u201cSubstantial agreement\u201d respectively.\nResults: The results are shown in Table 6. ECM with all options outperforms the other methods in both metrics significantly (2-tailed t-test, p < 0.05 for Content, and p < 0.005 for Emotion). After incorporating the internal memory and the external memory modules, the performance of ECM in Emotion is improved comparing to Emb, indicating our model can generate more explicit expressions of emotion. Besides, the performance in Content is improved from 1.256 of Emb to 1.299 of ECM, which shows the ability of ECM to control the weight of emotion and generate responses appropriate in content.\nFor all emotion categories, the performance of ECM in Emotion outperforms the other methods. However, the performances of ECM in Content is worse than baselines in Disgust and Angry categories, due to the fact that there are not sufficient training data for the two categories. For instance, the Angry category has 234,635 responses in our ESTC Dataset, much less than the other categories.\nTo evaluate whether ECM can generate responses that are appropriate not only in content but also in emotion, we present results in Table 5 by considering content and emotion scores simultaneously6. As we can see, 27.2% of the responses generated by ECM have a Content score of 2 and an Emotion score of 1, while only 22.8% for Emb and 9.0%\n6Note that Content and Emotion are two independent metrics.\nfor Seq2Seq. These indicate that ECM is better in generating high-quality responses in both content and emotion.\nPreference Test: In addition, emotion models (Emb and ECM) are much more preferred than Seq2Seq, and ECM is also significantly (2-tailed t-test, p < 0.001) preferred by annotators against other methods as shown in Table 7. The diverse emotional responses are more attractive to users than the generic responses generated by the Seq2Seq model. And with the explicitly expressions of emotions as well as the appropriateness in content, ECM is much more preferred."
        },
        {
            "heading": "Analysis of Emotion Interaction and Case Study",
            "text": "Figure 5 visualizes the emotion interaction patterns of the posts and responses in the ESTC Dataset. An emotion interaction pattern (EIP) is defined as < ep, er >, the pair of emotion categories of the post and its response. The value of an EIP is the conditional probability P (er|ep) = P (er, ep)/P (ep). An EIP marked with a darker color occurs more frequently than a lighter color. From the figure, we can make a few observations. First, frequent EIPs show that there are some major responding emotions given a post emo-\ntion category. For instance, when a post expresses Happy, the responding emotion is typically Like or Happy. Second, the diagonal patterns indicate emotional empathy, a common type of emotion interaction. Third, there are also other EIPs for a post, indicating that emotion interactions in conversation are quite diverse, as mentioned earlier. Note that class Other has much more data than other classes (see Table 3), indicating that EIPs are biased toward this class (the first column of Figure 5), due to the data bias and the emotion classification errors.\nWe present some examples in Figure 4. As can be seen, for a given post, there are multiple emotion categories that are suitable for its response in conversation. Seq2Seq generates a response with a random emotion. ECM can generate\nemotional responses conditioned on every emotion category. All these responses are appropriate to the post, indicating the existence of multiple EIPs and the reason why an emotion category should be specified as an input to our system.\nWe can see that ECM can generate appropriate responses if the pre-specified emotion category and the emotion of the post belong to one of the frequent EIPs. Colored words show that ECM can explicitly express emotion by applying the external memory which can choose a generic (non-emotion) or emotion word during decoding. For low-frequency EIPs such as < Happy ,Disgust > and < Happy ,Angry > as shown in the last two lines of Figure 4, responses are not appropriate to the emotion category due to the lack of training data and/or the errors caused by the emotion classifier."
        },
        {
            "heading": "Conclusion and Future Work",
            "text": "In this paper, we proposed the Emotional Chatting Machine (ECM) to model the emotion influence in large-scale conversation generation. Three mechanisms were proposed to model the emotion factor, including emotion category embedding, internal emotion memory, and external memory. Objective and manual evaluation show that ECM can generate responses appropriate not only in content but also in emotion.\nIn our future work, we will explore emotion interactions with ECM: instead of specifying an emotion class, the model should decide the most appropriate emotion category for the response. However, this may be challenging since such a task depends on the topics, contexts, or the mood of the user."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work was partly supported by the National Science Foundation of China under grant No.61272227/61332007, and a joint project with Sogou. We would like to thank our collaborators, Jingfang Xu and Haizhou Zhao."
        }
    ],
    "title": "Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory",
    "year": 2018
}