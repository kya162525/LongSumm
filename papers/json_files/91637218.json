{
    "abstractText": "Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state-of-the-art results on several types of tasks and datasets: question answering (Facebook\u2019s bAbI dataset), text classification for sentiment analysis (Stanford Sentiment Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The training for these different tasks relies exclusively on trained word vector representations and input-question-answer triplets.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ankit Kumar"
        },
        {
            "affiliations": [],
            "name": "Peter Ondruska"
        },
        {
            "affiliations": [],
            "name": "Mohit Iyyer"
        },
        {
            "affiliations": [],
            "name": "James Bradbury"
        },
        {
            "affiliations": [],
            "name": "Ishaan Gulrajani"
        },
        {
            "affiliations": [],
            "name": "Victor Zhong"
        },
        {
            "affiliations": [],
            "name": "Romain Paulus"
        },
        {
            "affiliations": [],
            "name": "Richard Socher"
        }
    ],
    "id": "SP:78e7edb4f051d9af41d566e31d57539c9a163144",
    "references": [
        {
            "authors": [
                "J. Andreas",
                "M. Rohrbach",
                "T. Darrell",
                "D. Klein"
            ],
            "title": "Learning to Compose Neural Networks for Question Answering",
            "venue": "arXiv preprint arXiv:1601.01705,",
            "year": 2016
        },
        {
            "authors": [
                "D. Bahdanau",
                "K. Cho",
                "Y. Bengio"
            ],
            "title": "Neural machine translation by jointly learning to align and translate",
            "venue": "CoRR, abs/1409.0473,",
            "year": 2014
        },
        {
            "authors": [
                "A. Bordes",
                "X. Glorot",
                "J. Weston",
                "Y. Bengio"
            ],
            "title": "Joint Learning of Words and Meaning Representations for Open-Text",
            "venue": "Semantic Parsing. AISTATS,",
            "year": 2012
        },
        {
            "authors": [
                "S.R. Bowman",
                "C. Potts",
                "C.D. Manning"
            ],
            "title": "Recursive neural networks for learning logical semantics",
            "venue": "CoRR, abs/1406.1827,",
            "year": 2014
        },
        {
            "authors": [
                "X. Chen",
                "C.L. Zitnick"
            ],
            "title": "Learning a recurrent visual representation for image caption generation",
            "venue": "arXiv preprint arXiv:1411.5654,",
            "year": 2014
        },
        {
            "authors": [
                "K. Cho",
                "B. van Merrienboer",
                "D. Bahdanau",
                "Y. Bengio"
            ],
            "title": "On the properties of neural machine translation: Encoder-decoder approaches",
            "venue": "CoRR, abs/1409.1259,",
            "year": 2014
        },
        {
            "authors": [
                "K. Cho",
                "B. van Merrienboer",
                "C. Gulcehre",
                "D. Bahdanau",
                "F. Bougares",
                "H. Schwenk",
                "Y. Bengio"
            ],
            "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
            "venue": "In EMNLP,",
            "year": 2014
        },
        {
            "authors": [
                "J. Chung",
                "\u00c7. G\u00fcl\u00e7ehre",
                "K. Cho",
                "Y. Bengio"
            ],
            "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
            "venue": "CoRR, abs/1412.3555,",
            "year": 2014
        },
        {
            "authors": [
                "J.A. Dusek",
                "H. Eichenbaum"
            ],
            "title": "The hippocampus and memory for orderly stimulusrelations",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 1997
        },
        {
            "authors": [
                "H. Eichenbaum",
                "N.J. Cohen"
            ],
            "title": "From Conditioning to Conscious Recollection: Memory Systems of the Brain (Oxford Psychology)",
            "year": 2004
        },
        {
            "authors": [
                "J.L. Elman"
            ],
            "title": "Distributed representations, simple recurrent networks, and grammatical structure",
            "venue": "Machine Learning,",
            "year": 1991
        },
        {
            "authors": [
                "S. Heckers",
                "M. Zalesak",
                "A.P. Weiss",
                "T. Ditman",
                "D. Titone"
            ],
            "title": "Hippocampal activation during transitive inference in humans",
            "venue": "Hippocampus,",
            "year": 2004
        },
        {
            "authors": [
                "K.M. Hermann",
                "T. Ko\u010disk\u00fd",
                "E. Grefenstette",
                "L. Espeholt",
                "W. Kay",
                "M. Suleyman",
                "P. Blunsom"
            ],
            "title": "Teaching machines to read and comprehend",
            "venue": "In NIPS,",
            "year": 2015
        },
        {
            "authors": [
                "S. Hochreiter",
                "J. Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural Computation,",
            "year": 1997
        },
        {
            "authors": [
                "Howard",
                "Marc W",
                "Kahana",
                "Michael J"
            ],
            "title": "A distributed representation of temporal context",
            "venue": "Journal of Mathematical Psychology,",
            "year": 2002
        },
        {
            "authors": [
                "O. Irsoy",
                "C. Cardie"
            ],
            "title": "Modeling compositionality with multiplicative recurrent neural networks",
            "venue": "In ICLR,",
            "year": 2015
        },
        {
            "authors": [
                "M. Iyyer",
                "J. Boyd-Graber",
                "L. Claudino",
                "R. Socher",
                "H. Daum\u00e9 III"
            ],
            "title": "A neural network for factoid question answering over paragraphs",
            "venue": "In EMNLP,",
            "year": 2014
        },
        {
            "authors": [
                "A. Joulin",
                "T. Mikolov"
            ],
            "title": "Inferring algorithmic patterns with stack-augmented recurrent nets",
            "venue": "In NIPS,",
            "year": 2015
        },
        {
            "authors": [
                "N. Kalchbrenner",
                "E. Grefenstette",
                "P. Blunsom"
            ],
            "title": "A convolutional neural network for modelling sentences",
            "venue": "In ACL,",
            "year": 2014
        },
        {
            "authors": [
                "A. Karpathy",
                "L. Fei-Fei"
            ],
            "title": "Deep visual-semantic alignments for generating image descriptions",
            "venue": "In CVPR,",
            "year": 2015
        },
        {
            "authors": [
                "Y. Kim"
            ],
            "title": "Convolutional neural networks for sentence classification",
            "venue": "In EMNLP,",
            "year": 2014
        },
        {
            "authors": [
                "P. Kingma",
                "Ba",
                "Jimmy"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "CoRR, abs/1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Q.V. Le",
                "T. Mikolov"
            ],
            "title": "Distributed representations of sentences and documents",
            "venue": "In ICML,",
            "year": 2014
        },
        {
            "authors": [
                "M. Malinowski",
                "M. Fritz"
            ],
            "title": "A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input",
            "venue": "In NIPS,",
            "year": 2014
        },
        {
            "authors": [
                "M.P. Marcus",
                "M.A. Marcinkiewicz",
                "B. Santorini"
            ],
            "title": "Building a large annotated corpus of english: The penn treebank",
            "venue": "Computational Linguistics,",
            "year": 1993
        },
        {
            "authors": [
                "T. Mikolov",
                "G. Zweig"
            ],
            "title": "Context dependent recurrent neural network language model",
            "venue": "In SLT, pp. 234\u2013239",
            "year": 2012
        },
        {
            "authors": [
                "A. Passos",
                "V. Kumar",
                "A. McCallum"
            ],
            "title": "Lexicon infused phrase embeddings for named entity resolution",
            "venue": "In Conference on Computational Natural Language Learning",
            "year": 2014
        },
        {
            "authors": [
                "J. Pennington",
                "R. Socher",
                "C.D. Manning"
            ],
            "title": "Glove: Global vectors for word representation",
            "venue": "In EMNLP,",
            "year": 2014
        },
        {
            "authors": [
                "R. Socher",
                "S. Gershman",
                "A. Perotte",
                "P. Sederberg",
                "D. Blei",
                "K. Norman"
            ],
            "title": "A bayesian analysis of dynamics in free recall",
            "year": 2009
        },
        {
            "authors": [
                "R. Socher",
                "E.H. Huang",
                "J. Pennington",
                "A.Y. Ng",
                "C.D. Manning"
            ],
            "title": "Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection",
            "venue": "In NIPS,",
            "year": 2011
        },
        {
            "authors": [
                "R. Socher",
                "A. Perelygin",
                "J. Wu",
                "J. Chuang",
                "C. Manning",
                "A. Ng",
                "C. Potts"
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "In EMNLP,",
            "year": 2013
        },
        {
            "authors": [
                "A. S\u00f8gaard"
            ],
            "title": "Semisupervised condensed nearest neighbor for part-of-speech tagging",
            "venue": "In ACL-HLT,",
            "year": 2011
        },
        {
            "authors": [
                "M.F. Stollenga",
                "J. Masci",
                "F. Gomez",
                "J. Schmidhuber"
            ],
            "title": "Deep Networks with Internal Selective Attention through Feedback Connections",
            "venue": "In NIPS,",
            "year": 2014
        },
        {
            "authors": [
                "I. Sutskever",
                "O. Vinyals",
                "Q.V. Le"
            ],
            "title": "Sequence to sequence learning with neural networks",
            "venue": "In NIPS,",
            "year": 2014
        },
        {
            "authors": [
                "K.S. Tai",
                "R. Socher",
                "C.D. Manning"
            ],
            "title": "Improved semantic representations from tree-structured long shortterm memory networks",
            "venue": "In ACL,",
            "year": 2015
        },
        {
            "authors": [
                "J. Weston",
                "A. Bordes",
                "S. Chopra",
                "T. Mikolov"
            ],
            "title": "Towards ai-complete question answering: A set of prerequisite toy",
            "venue": "tasks. CoRR,",
            "year": 2015
        },
        {
            "authors": [
                "K. Xu",
                "J. Ba",
                "R. Kiros",
                "K. Cho",
                "A.C. Courville",
                "R. Salakhutdinov",
                "R.S. Zemel",
                "Y. Bengio"
            ],
            "title": "Show, attend and tell: Neural image caption generation with visual attention",
            "venue": "CoRR, abs/1502.03044,",
            "year": 2015
        },
        {
            "authors": [
                "Z. Yang",
                "X. He",
                "J. Gao",
                "L. Deng",
                "A. Smola"
            ],
            "title": "Stacked attention networks for image question answering",
            "venue": "arXiv preprint arXiv:1511.02274,",
            "year": 2015
        },
        {
            "authors": [
                "A. Yates",
                "M. Banko",
                "M. Broadhead",
                "M.J. Cafarella",
                "O. Etzioni",
                "S. Soderland"
            ],
            "title": "Textrunner: Open information extraction on the web",
            "venue": "In HLT-NAACL (Demonstrations),",
            "year": 2007
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Question answering (QA) is a complex natural language processing task which requires an understanding of the meaning of a text and the ability to reason over relevant facts. Most, if not all, tasks in natural language processing can be cast as a question answering problem: high level tasks like machine translation (What is the translation into French?); sequence modeling tasks like named entity recognition (Passos et al., 2014) (NER) (What are the named entity tags in this sentence?) or part-of-speech tagging (POS) (What are the part-of-speech tags?); classification problems like sentiment analysis (Socher et al., 2013)\nCopyright 2016 by the author(s).\nI: Jane went to the hallway. I: Mary walked to the bathroom. I: Sandra went to the garden. I: Daniel went back to the garden. I: Sandra took the milk there. Q: Where is the milk? A: garden I: It started boring, but then it got interesting. Q: What\u2019s the sentiment? A: positive Q: POS tags? A: PRP VBD JJ , CC RB PRP VBD JJ .\nFigure 1. Example inputs and questions, together with answers generated by a dynamic memory network trained on the corresponding task. In sequence modeling tasks, an answer mechanism is triggered at each input word instead of only at the end.\n(What is the sentiment?); even multi-sentence joint classification problems like coreference resolution (Who does \u201dtheir\u201d refer to?).\nWe propose the Dynamic Memory Network (DMN), a neural network based framework for general question answering tasks that is trained using raw input-question-answer triplets. Generally, it can solve sequence tagging tasks, classification problems, sequence-to-sequence tasks and question answering tasks that require transitive reasoning.\nThe DMN first computes a representation for all inputs and the question. The question representation then triggers an iterative attention process that searches the inputs and retrieves relevant facts. The DMN memory module then reasons over retrieved facts and provides a vector representation of all relevant information to an answer module which generates the answer.\nFig. 1 provides examples of inputs, questions and answers for tasks that are evaluated in this paper and for which a DMN achieves a new level of state-of-the-art performance.\nar X\niv :1\n50 6.\n07 28\n5v 5\n[ cs\n.C L\n] 5\nM ar\n2 01"
        },
        {
            "heading": "2. Dynamic Memory Networks",
            "text": "We now give an overview of the modules that make up the DMN. We then examine each module in detail and give intuitions about its formulation. A high-level illustration of the DMN is shown in Fig. 2.1.\nInput Module: The input module encodes raw text inputs from the task into distributed vector representations. In this paper, we focus on natural language related problems. In these cases, the input may be a sentence, a long story, a movie review, a news article, or several Wikipedia articles.\nQuestion Module: Like the input module, the question module encodes the question of the task into a distributed vector representation. For example, in the case of question answering, the question may be a sentence such as Where did the author first fly?. The representation is fed into the episodic memory module, and forms the basis, or initial state, upon which the episodic memory module iterates.\nEpisodic Memory Module: Given a collection of input representations, the episodic memory module chooses which parts of the inputs to focus on through the attention mechanism. It then produces a \u201dmemory\u201d vector representation taking into account the question as well as the previous memory. Each iteration provides the module with newly relevant information about the input. In other words, the module has the ability to retrieve new information, in the form of input representations, which were thought to be irrelevant in previous iterations.\nAnswer Module: The answer module generates an answer from the final memory vector of the memory module.\nA detailed visualization of these modules is shown in Fig.3."
        },
        {
            "heading": "2.1. Input Module",
            "text": "In natural language processing problems, the input is a sequence of TI words w1, . . . , wTI . One way to encode the input sequence is via a recurrent neural network (Elman, 1991). Word embeddings are given as inputs to the recurrent network. At each time step t, the network updates its hidden state ht = RNN(L[wt], ht\u22121), where L is the embedding matrix and wt is the word index of the tth word of the input sequence.\nIn cases where the input sequence is a single sentence, the input module outputs the hidden states of the recurrent network. In cases where the input sequence is a list of sentences, we concatenate the sentences into a long list of word tokens, inserting after each sentence an end-of-sentence token. The hidden states at each of the end-of-sentence tokens are then the final representations of the input module. In subsequent sections, we denote the output of the input module as the sequence of TC fact representations c, whereby ct denotes the tth element in the output sequence\nof the input module. Note that in the case where the input is a single sentence, TC = TI . That is, the number of output representations is equal to the number of words in the sentence. In the case where the input is a list of sentences, TC is equal the number of sentences.\nChoice of recurrent network: In our experiments, we use a gated recurrent network (GRU) (Cho et al., 2014a; Chung et al., 2014). We also explored the more complex LSTM (Hochreiter & Schmidhuber, 1997) but it performed similarly and is more computationally expensive. Both work much better than the standard tanh RNN and we postulate that the main strength comes from having gates that allow the model to suffer less from the vanishing gradient problem (Hochreiter & Schmidhuber, 1997). Assume each time step t has an input xt and a hidden state ht. The internal mechanics of the GRU is defined as:\nzt = \u03c3 ( W (z)xt + U (z)ht\u22121 + b (z) )\n(1) rt = \u03c3 ( W (r)xt + U (r)ht\u22121 + b (r) ) (2)\nh\u0303t = tanh ( Wxt + rt \u25e6 Uht\u22121 + b(h) ) (3)\nht = zt \u25e6 ht\u22121 + (1\u2212 zt) \u25e6 h\u0303t (4)\nwhere \u25e6 is an element-wise product, W (z),W (r),W \u2208 RnH\u00d7nI and U (z), U (r), U \u2208 RnH\u00d7nH . The dimensions n are hyperparameters. We abbreviate the above computation with ht = GRU(xt, ht\u22121)."
        },
        {
            "heading": "2.2. Question Module",
            "text": "Similar to the input sequence, the question is also most commonly given as a sequence of words in natural language processing problems. As before, we encode the question via a recurrent neural network. Given a question\nof TQ words, hidden states for the question encoder at time t is given by qt = GRU(L[w Q t ], qt\u22121), L represents the word embedding matrix as in the previous section and wQt represents the word index of the tth word in the question. We share the word embedding matrix across the input module and the question module. Unlike the input module, the question module produces as output the final hidden state of the recurrent network encoder: q = qTQ ."
        },
        {
            "heading": "2.3. Episodic Memory Module",
            "text": "The episodic memory module iterates over representations outputted by the input module, while updating its internal episodic memory. In its general form, the episodic memory module is comprised of an attention mechanism as well as a recurrent network with which it updates its memory. During each iteration, the attention mechanism attends over the fact representations c while taking into consideration the question representation q and the previous memory mi\u22121 to produce an episode ei.\nThe episode is then used, alongside the previous memories mi\u22121, to update the episodic memory mi = GRU(ei,mi\u22121). The initial state of this GRU is initialized to the question vector itself: m0 = q. For some tasks, it is beneficial for episodic memory module to take multiple passes over the input. After TM passes, the final memory mTM is given to the answer module.\nNeed for Multiple Episodes: The iterative nature of this module allows it to attend to different inputs during each pass. It also allows for a type of transitive inference, since the first pass may uncover the need to retrieve additional facts. For instance, in the example in Fig. 3, we are asked Where is the football? In the first iteration, the model ought attend to sentence 7 (John put down the football.), as the question asks about the football. Only once the model sees that John is relevant can it reason that the second iteration should retrieve where John was. Similarly, a second pass may help for sentiment analysis as we show in the experiments section below.\nAttention Mechanism: In our work, we use a gating function as our attention mechanism. For each pass i, the mechanism takes as input a candidate fact ct, a previous memory mi\u22121, and the question q to compute a gate: git = G(ct,m i\u22121, q).\nThe scoring function G takes as input the feature set z(c,m, q) and produces a scalar score. We first define a large feature vector that captures a variety of similarities between input, memory and question vectors: z(c,m, q) =\n[ c,m, q, c \u25e6 q, c \u25e6m, |c\u2212 q|, |c\u2212m|, cTW (b)q, cTW (b)m ] ,\n(5) where \u25e6 is the element-wise product. The function G is a simple two-layer feed forward neural network\nG(c,m, q) = \u03c3 ( W (2) tanh ( W (1)z(c,m, q) + b(1) ) + b(2) ) . (6)\nSome datasets, such as Facebook\u2019s bAbI dataset, specify which facts are important for a given question. In those cases, the attention mechanism of the G function can be trained in a supervised fashion with a standard crossentropy cost function.\nMemory Update Mechanism: To compute the episode for pass i, we employ a modified GRU over the sequence of the inputs c1, . . . , cTC , weighted by the gates g\ni. The episode vector that is given to the answer module is the final state of the GRU. The equation to update the hidden states of the GRU at time t and the equation to compute the episode are, respectively:\nhit = g i tGRU(ct, h i t\u22121) + (1\u2212 git)hit\u22121 (7) ei = hiTC (8)\nCriteria for Stopping: The episodic memory module also has a signal to stop iterating over inputs. To achieve this, we append a special end-of-passes representation to the input, and stop the iterative attention process if this representation is chosen by the gate function. For datasets without explicit supervision, we set a maximum number of iterations. The whole module is end-to-end differentiable."
        },
        {
            "heading": "2.4. Answer Module",
            "text": "The answer module generates an answer given a vector. Depending on the type of task, the answer module is either triggered once at the end of the episodic memory or at each time step.\nWe employ another GRU whose initial state is initialized to the last memory a0 = mTM . At each timestep, it takes as input the question q, last hidden state at\u22121, as well as the previously predicted output yt\u22121.\nyt = softmax(W (a)at) (9)\nat = GRU([yt\u22121, q], at\u22121), (10)\nwhere we concatenate the last generated word and the question vector as the input at each time step. The output is trained with the cross-entropy error classification of the correct sequence appended with a special end-of-sequence token.\nIn the sequence modeling task, we wish to label each word in the original sequence. To this end, the DMN is run in the same way as above over the input words. For word t, we replace Eq. 8 with ei = hit. Note that the gates for the first pass will be the same for each word, as the question\nis the same. This allows for speed-up in implementation by computing these gates only once. However, gates for subsequent passes will be different, as the episodes are different."
        },
        {
            "heading": "2.5. Training",
            "text": "Training is cast as a supervised classification problem to minimize cross-entropy error of the answer sequence. For datasets with gate supervision, such as bAbI, we add the cross-entropy error of the gates into the overall cost. Because all modules communicate over vector representations and various types of differentiable and deep neural networks with gates, the entire DMN model can be trained via backpropagation and gradient descent."
        },
        {
            "heading": "3. Related Work",
            "text": "Given the many shoulders on which this paper is standing and the many applications to which our model is applied, it is impossible to do related fields justice.\nDeep Learning: There are several deep learning models that have been applied to many different tasks in NLP. For instance, recursive neural networks have been used for parsing (Socher et al., 2011), sentiment analysis (Socher et al., 2013), paraphrase detection (Socher et al., 2011) and question answering (Iyyer et al., 2014) and logical inference (Bowman et al., 2014), among other tasks. However, because they lack the memory and question modules, a single model cannot solve as many varied tasks, nor tasks that require transitive reasoning over multiple sentences. Another commonly used model is the chain-structured recurrent neural network of the kind we employ above. Recurrent neural networks have been successfully used in language modeling (Mikolov & Zweig, 2012), speech recognition, and sentence generation from images (Karpathy & Fei-Fei, 2015). Also relevant is the sequence-to-sequence model used for machine translation by Sutskever et al. (Sutskever et al., 2014). This model uses two extremely large and deep LSTMs to encode a sentence in one language and then decode the sentence in another language. This sequence-to-sequence model is a special case of the DMN without a question and without episodic memory. Instead it maps an input sequence directly to an answer sequence.\nAttention and Memory: The second line of work that is very relevant to DMNs is that of attention and memory in deep learning. Attention mechanisms are generally useful and can improve image classification (Stollenga & J. Masci, 2014), automatic image captioning (Xu et al., 2015) and machine translation (Cho et al., 2014b; Bahdanau et al., 2014). Neural Turing machines use memory to solve algorithmic problems such as list sorting (Graves\net al., 2014). The work of recent months by Weston et al. on memory networks (Weston et al., 2015b) focuses on adding a memory component for natural language question answering. They have an input (I) and response (R) component and their generalization (G) and output feature map (O) components have some functional overlap with our episodic memory. However, the Memory Network cannot be applied to the same variety of NLP tasks since it processes sentences independently and not via a sequence model. It requires bag of n-gram vector features as well as a separate feature that captures whether a sentence came before another one.\nVarious other neural memory or attention architectures have recently been proposed for algorithmic problems (Joulin & Mikolov, 2015; Kaiser & Sutskever, 2015), caption generation for images (Malinowski & Fritz, 2014; Chen & Zitnick, 2014), visual question answering (Yang et al., 2015) or other NLP problems and datasets (Hermann et al., 2015).\nIn contrast, the DMN employs neural sequence models for input representation, attention, and response mechanisms, thereby naturally capturing position and temporality. As a result, the DMN is directly applicable to a broader range of applications without feature engineering. We compare directly to Memory Networks on the bAbI dataset (Weston et al., 2015a).\nNLP Applications: The DMN is a general model which we apply to several NLP problems. We compare to what, to the best of our knowledge, is the current state-of-the-art method for each task.\nThere are many different approaches to question answering: some build large knowledge bases (KBs) with open information extraction systems (Yates et al., 2007), some use neural networks, dependency trees and KBs (Bordes et al., 2012), others only sentences (Iyyer et al., 2014). A lot of other approaches exist. When QA systems do not produce the right answer, it is often unclear if it is because they do not have access to the facts, cannot reason over them or have never seen this type of question or phenomenon. Most QA dataset only have a few hundred questions and answers but require complex reasoning. They can hence not be solved by models that have to learn purely from examples. While synthetic datasets (Weston et al., 2015a) have problems and can often be solved easily with manual feature engineering, they let us disentangle failure modes of models and understand necessary QA capabilities. They are useful for analyzing models that attempt to learn everything and do not rely on external features like coreference, POS, parsing, logical rules, etc. The DMN is such a model. Another related model by Andreas et al. (2016) combines neural and logical reasoning for question answering over knowledge bases and visual question answering.\nSentiment analysis is a very useful classification task and recently the Stanford Sentiment Treebank (Socher et al., 2013) has become a standard benchmark dataset. Kim (Kim, 2014) reports the previous state-of-the-art result based on a convolutional neural network that uses multiple word vector representations. The previous best model for part-of-speech tagging on the Wall Street Journal section of the Penn Tree Bank (Marcus et al., 1993) was Sogaard (S\u00f8gaard, 2011) who used a semisupervised nearest neighbor approach. We also directly compare to paragraph vectors by (Le & Mikolov., 2014).\nNeuroscience: The episodic memory in humans stores specific experiences in their spatial and temporal context. For instance, it might contain the first memory somebody has of flying a hang glider. Eichenbaum and Cohen have argued that episodic memories represent a form of relationship (i.e., relations between spatial, sensory and temporal information) and that the hippocampus is responsible for general relational learning (Eichenbaum & Cohen, 2004). Interestingly, it also appears that the hippocampus is active during transitive inference (Heckers et al., 2004), and disruption of the hippocampus impairs this ability (Dusek & Eichenbaum, 1997).\nThe episodic memory module in the DMN is inspired by these findings. It retrieves specific temporal states that are related to or triggered by a question. Furthermore, we found that the GRU in this module was able to do some transitive inference over the simple facts in the bAbI dataset. This module also has similarities to the Temporal Context Model (Howard & Kahana, 2002) and its Bayesian extensions (Socher et al., 2009) which were developed to analyze human behavior in word recall experiments."
        },
        {
            "heading": "4. Experiments",
            "text": "We include experiments on question answering, part-ofspeech tagging, and sentiment analysis. The model is trained independently for each problem, while the architecture remains the same except for the answer module and input fact subsampling (words vs sentences). The answer module, as described in Section 2.4, is triggered either once at the end or for each token.\nFor all datasets we used either the official train, development, test splits or if no development set was defined, we used 10% of the training set for development. Hyperparameter tuning and model selection (with early stopping) is done on the development set. The DMN is trained via backpropagation and Adam (Kingma & Ba, 2014). We employ L2 regularization, and dropout on the word embeddings. Word vectors are pre-trained using GloVe (Pennington et al., 2014)."
        },
        {
            "heading": "4.1. Question Answering",
            "text": "The Facebook bAbI dataset is a synthetic dataset for testing a model\u2019s ability to retrieve facts and reason over them. Each task tests a different skill that a question answering model ought to have, such as coreference resolution, deduction, and induction. Showing an ability exists here is not sufficient to conclude a model would also exhibit it on real world text data. It is, however, a necessary condition.\nTraining on the bAbI dataset uses the following objective function: J = \u03b1ECE(Gates) + \u03b2ECE(Answers), where ECE is the standard cross-entropy cost and \u03b1 and \u03b2 are hyperparameters. In practice, we begin training with \u03b1 set to 1 and \u03b2 set to 0, and then later switch \u03b2 to 1 while keeping \u03b1 at 1. As described in Section 2.1, the input module outputs fact representations by taking the encoder hidden states at time steps corresponding to the end-of-sentence tokens. The gate supervision aims to select one sentence per pass; thus, we also experimented with modifying Eq. 8 to a simple softmax instead of a GRU. Here, we compute the final episode vector via: ei = \u2211T t=1 softmax(g i t)ct, where softmax(git) = exp(git)\u2211T\nj=1 exp(g i j)\n, and git here is the value of\nthe gate before the sigmoid. This setting achieves better results, likely because the softmax encourages sparsity and is better suited to picking one sentence at a time.\nWe list results in Table 1. The DMN does worse than the Memory Network, which we refer to from here on as MemNN, on tasks 2 and 3, both tasks with long input sequences. We suspect that this is due to the recurrent input sequence model having trouble modeling very long inputs. The MemNN does not suffer from this problem as it views each sentence separately. The power of the episodic memory module is evident in tasks 7 and 8, where the DMN significantly outperforms the MemNN. Both tasks require the model to iteratively retrieve facts and store them in a representation that slowly incorporates more of the relevant information of the input sequence. Both models do poorly on tasks 17 and 19, though the MemNN does better. We suspect this is due to the MemNN using n-gram vectors and sequence position features."
        },
        {
            "heading": "4.2. Text Classification: Sentiment Analysis",
            "text": "The Stanford Sentiment Treebank (SST) (Socher et al., 2013) is a popular dataset for sentiment classification. It provides phrase-level fine-grained labels, and comes with a train/development/test split. We present results on two formats: fine-grained root prediction, where all full sentences (root nodes) of the test set are to be classified as either very negative, negative, neutral, positive, or very positive, and binary root prediction, where all non-neutral full sentences of the test set are to be classified as either positive or negative. To train the model, we use all full sentences as well as subsample 50% of phrase-level labels every epoch. During evaluation, the model is only evaluated on the full sentences (root setup). In binary classification, neutral phrases are removed from the dataset. The DMN achieves state-ofthe-art accuracy on the binary classification task, as well as on the fine-grained classification task.\nIn all experiments, the DMN was trained with GRU sequence models. It is easy to replace the GRU sequence model with any of the models listed above, as well as in-\ncorporate tree structure in the retrieval process."
        },
        {
            "heading": "4.3. Sequence Tagging: Part-of-Speech Tagging",
            "text": "Part-of-speech tagging is traditionally modeled as a sequence tagging problem: every word in a sentence is to be classified into its part-of-speech class (see Fig. 1). We evaluate on the standard Wall Street Journal dataset (Marcus et al., 1993). We use the standard splits of sections 0-18 for training, 19-21 for development and 22-24 for test sets (S\u00f8gaard, 2011). Since this is a word level tagging task, DMN memories are classified at each time step corresponding to each word. This is described in detail in Section 2.4\u2019s discussion of sequence modeling.\nWe compare the DMN with the results in (S\u00f8gaard, 2011). The DMN achieves state-of-the-art accuracy with a single model, reaching a development set accuracy of 97.5. Ensembling the top 4 development models, the DMN gets to 97.58 dev and 97.56 test accuracies, achieving a slightly higher new state-of-the-art (Table 3)."
        },
        {
            "heading": "4.4. Quantitative Analysis of Episodic Memory Module",
            "text": "The main novelty of the DMN architecture is in its episodic memory module. Hence, we analyze how important the episodic memory module is for NLP tasks and in particular how the number of passes over the input affect accuracy.\nTable 4 shows the accuracies on a subset of bAbI tasks as well as on the Stanford Sentiment Treebank. We note that for several of the hard reasoning tasks, multiple passes over the inputs are crucial to achieving high performance. For sentiment the differences are smaller. However, two passes outperform a single pass or zero passes. In the latter case, there is no episodic memory at all and outputs are passed directly from the input module to the answer module. We note that, especially complicated examples are more often correctly classified with 2 passes but many examples in sentiment contain only simple sentiment words and no negation or misleading expressions. Hence the need to have a complicated architecture for them is small. The same is true for POS tagging. Here, differences in accuracy are less than 0.1 between different numbers of passes.\nNext, we show that the additional correct classifications are\nhard examples with mixed positive/negative vocabulary."
        },
        {
            "heading": "4.5. Qualitative Analysis of Episodic Memory Module",
            "text": "Apart from a quantitative analysis, we also show qualitatively what happens to the attention during multiple passes. We present specific examples from the experiments to illustrate that the iterative nature of the episodic memory module enables the model to focus on relevant parts of the input. For instance, Table 5 shows an example of what the DMN focuses on during each pass of a three-iteration scan on a question from the bAbI dataset.\nWe also evaluate the episodic memory module for sentiment analysis. Given that the DMN performs well with both one iteration and two iterations, we study test examples where the one-iteration DMN is incorrect and the twoepisode DMN is correct. Looking at the sentences in Fig. 4 and 5, we make the following observations:\n1. The attention of the two-iteration DMN is generally much more focused compared to that of the oneiteration DMN. We believe this is due to the fact that with fewer iterations over the input, the hidden states of the input module encoder have to capture more of the content of adjacent time steps. Hence, the attention mechanism cannot only focus on a few key time steps. Instead, it needs to pass all necessary information to the answer module from a single pass.\n2. During the second iteration of the two-iteration DMN, the attention becomes significantly more focused on relevant key words and less attention is paid to strong sentiment words that lose their sentiment in context. This is exemplified by the sentence in Fig. 5 that includes the very positive word \u201dbest.\u201d In the first iteration, the word \u201dbest\u201d dominates the attention scores (darker color means larger score). However, once its context, \u201dis best described\u201d, is clear, its relevance is diminished and \u201dlukewarm\u201d becomes more important.\nWe conclude that the ability of the episodic memory mod-\nule to perform multiple passes over the data is beneficial. It provides significant benefits on harder bAbI tasks, which require reasoning over several pieces of information or transitive reasoning. Increasing the number of passes also slightly improves the performance on sentiment analysis, though the difference is not as significant. We did not attempt more iterations for sentiment analysis as the model struggles with overfitting with three passes."
        },
        {
            "heading": "5. Conclusion",
            "text": "The DMN model is a potentially general architecture for a variety of NLP applications, including classification, question answering and sequence modeling. A single architecture is a first step towards a single joint model for multiple NLP problems. The DMN is trained end-to-end with one, albeit complex, objective function. Future work will explore additional tasks, larger multi-task models and multimodal inputs and questions."
        }
    ],
    "title": "Ask Me Anything:Dynamic Memory Networks for Natural Language Processing",
    "year": 2016
}