{
    "abstractText": "A central challenge in semantic parsing is handling the myriad ways in which knowledge base predicates can be expressed. Traditionally, semantic parsers are trained primarily from text paired with knowledge base information. Our goal is to exploit the much larger amounts of raw text not tied to any knowledge base. In this paper, we turn semantic parsing on its head. Given an input utterance, we first use a simple method to deterministically generate a set of candidate logical forms with a canonical realization in natural language for each. Then, we use a paraphrase model to choose the realization that best paraphrases the input, and output the corresponding logical form. We present two simple paraphrase models, an association model and a vector space model, and train them jointly from question-answer pairs. Our system PARASEMPRE improves stateof-the-art accuracies on two recently released question-answering datasets.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jonathan Berant"
        },
        {
            "affiliations": [],
            "name": "Percy Liang"
        }
    ],
    "id": "SP:172d7f5118df9082b77bf0395412604a06afce51",
    "references": [
        {
            "authors": [
                "J. Berant",
                "A. Chou",
                "R. Frostig",
                "P. Liang."
            ],
            "title": "Semantic parsing on Freebase from question-answer pairs",
            "venue": "Empirical Methods in Natural Language Processing (EMNLP).",
            "year": 2013
        },
        {
            "authors": [
                "Q. Cai",
                "A. Yates."
            ],
            "title": "Large-scale semantic parsing via schema matching and lexicon extension",
            "venue": "Association for Computational Linguistics (ACL).",
            "year": 2013
        },
        {
            "authors": [
                "M. Chang",
                "D. Goldwasser",
                "D. Roth",
                "V. Srikumar."
            ],
            "title": "Discriminative learning over constrained latent representations",
            "venue": "North American Association for Computational Linguistics (NAACL).",
            "year": 2010
        },
        {
            "authors": [
                "I. Dagan",
                "D. Roth",
                "M. Sammons",
                "F.M. Zanzotto."
            ],
            "title": "Recognizing Textual Entailment: Models and Applications",
            "venue": "Morgan and Claypool Publishers.",
            "year": 2013
        },
        {
            "authors": [
                "R. Dale",
                "S. Geldof",
                "J. Prost."
            ],
            "title": "Coral: using natural language generation for navigational assistance",
            "venue": "Australasian computer science conference, pages 35\u201344.",
            "year": 2003
        },
        {
            "authors": [
                "D. Das",
                "N.A. Smith."
            ],
            "title": "Paraphrase identification as probabilistic quasi-synchronous recognition",
            "venue": "Association for Computational Linguistics (ACL), pages 468\u2013476.",
            "year": 2009
        },
        {
            "authors": [
                "B. Dolan",
                "C. Quirk",
                "C. Brockett."
            ],
            "title": "Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources",
            "venue": "International Conference on Computational Linguistics (COLING).",
            "year": 2004
        },
        {
            "authors": [
                "J. Duchi",
                "E. Hazan",
                "Y. Singer."
            ],
            "title": "Adaptive subgradient methods for online learning and stochastic optimization",
            "venue": "Conference on Learning Theory (COLT).",
            "year": 2010
        },
        {
            "authors": [
                "A. Fader",
                "S. Soderland",
                "O. Etzioni."
            ],
            "title": "Identifying relations for open information extraction",
            "venue": "Empirical Methods in Natural Language Processing (EMNLP).",
            "year": 2011
        },
        {
            "authors": [
                "A. Fader",
                "L. Zettlemoyer",
                "O. Etzioni."
            ],
            "title": "Paraphrase-driven learning for open question answering",
            "venue": "Association for Computational Linguistics (ACL).",
            "year": 2013
        },
        {
            "authors": [
                "C. Fellbaum."
            ],
            "title": "WordNet: An Electronic Lexical Database",
            "venue": "MIT Press.",
            "year": 1998
        },
        {
            "authors": [
                "J.R. Finkel",
                "T. Grenager",
                "C. Manning."
            ],
            "title": "Incorporating non-local information into information extraction systems by Gibbs sampling",
            "venue": "Association for Computational Linguistics (ACL), pages 363\u2013370.",
            "year": 2005
        },
        {
            "authors": [
                "Google."
            ],
            "title": "Freebase data dumps (2013-0609)",
            "venue": "https://developers.google.com/ freebase/data.",
            "year": 2013
        },
        {
            "authors": [
                "A. Haghighi",
                "A.Y. Ng",
                "C.D. Manning."
            ],
            "title": "Robust textual inference via graph matching",
            "venue": "Empirical Methods in Natural Language Processing (EMNLP).",
            "year": 2005
        },
        {
            "authors": [
                "S. Harabagiu",
                "A. Hickl."
            ],
            "title": "Methods for using textual entailment in open-domain question answering",
            "venue": "Association for Computational Linguistics (ACL).",
            "year": 2006
        },
        {
            "authors": [
                "M. Heilman",
                "N.A. Smith."
            ],
            "title": "Tree edit models for recognizing textual entailments, paraphrases, and answers to questions",
            "venue": "Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL), pages",
            "year": 2010
        },
        {
            "authors": [
                "E.H. Huang",
                "R. Socher",
                "C.D. Manning",
                "A.Y. Ng."
            ],
            "title": "Improving word representations via global context and multiple word prototypes",
            "venue": "Association for Computational Linguistics (ACL).",
            "year": 2012
        },
        {
            "authors": [
                "D. Klein",
                "C. Manning."
            ],
            "title": "Accurate unlexicalized parsing",
            "venue": "Association for Computational Linguistics (ACL), pages 423\u2013430.",
            "year": 2003
        },
        {
            "authors": [
                "T. Kwiatkowski",
                "L. Zettlemoyer",
                "S. Goldwater",
                "M. Steedman."
            ],
            "title": "Inducing probabilistic CCG grammars from logical form with higher-order unification",
            "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 1223\u20131233.",
            "year": 2010
        },
        {
            "authors": [
                "T. Kwiatkowski",
                "E. Choi",
                "Y. Artzi",
                "L. Zettlemoyer."
            ],
            "title": "Scaling semantic parsers with on-the-fly ontology matching",
            "venue": "Empirical Methods in Natural Language Processing (EMNLP).",
            "year": 2013
        },
        {
            "authors": [
                "P. Liang."
            ],
            "title": "Lambda dependency-based compositional semantics",
            "venue": "Technical report, ArXiv.",
            "year": 2013
        },
        {
            "authors": [
                "T. Lin",
                "Mausam",
                "O. Etzioni."
            ],
            "title": "Entity linking at web scale",
            "venue": "Knowledge Extraction Workshop (AKBC-WEKEX).",
            "year": 2012
        },
        {
            "authors": [
                "T. Mikolov",
                "K. Chen",
                "G. Corrado",
                "Jeffrey."
            ],
            "title": "Efficient estimation of word representations in vector space",
            "venue": "Technical report, ArXiv.",
            "year": 2013
        },
        {
            "authors": [
                "F.J. Och",
                "H. Ney."
            ],
            "title": "The alignment template approach to statistical machine translation",
            "venue": "Computational Linguistics, 30:417\u2013449.",
            "year": 2004
        },
        {
            "authors": [
                "P. Piwek",
                "K.E. Boyer."
            ],
            "title": "Varieties of question generation: Introduction to this special issue",
            "venue": "Dialogue and Discourse, 3:1\u20139.",
            "year": 2012
        },
        {
            "authors": [
                "E. Reiter",
                "S. Sripada",
                "J. Hunter",
                "J. Yu",
                "I. Davy."
            ],
            "title": "Choosing words in computer-generated weather forecasts",
            "venue": "Artificial Intelligence, 167:137\u2013 169.",
            "year": 2005
        },
        {
            "authors": [
                "L. Romano",
                "M. kouylekov",
                "I. Szpektor",
                "I. Dagan",
                "A. Lavelli."
            ],
            "title": "Investigating a generic paraphrase-based approach for relation extraction",
            "venue": "Proceedings of ECAL.",
            "year": 2006
        },
        {
            "authors": [
                "R. Socher",
                "E.H. Huang",
                "J. Pennin",
                "C.D. Manning",
                "A. Ng."
            ],
            "title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection",
            "venue": "Advances in Neural Information Processing Systems (NIPS), pages 801\u2013809.",
            "year": 2011
        },
        {
            "authors": [
                "A. Stern",
                "I. Dagan."
            ],
            "title": "A confidence model for syntactically-motivated entailment proofs",
            "venue": "Recent Advances in Natural Language Processing, pages 455\u2013462.",
            "year": 2011
        },
        {
            "authors": [
                "K. Toutanova",
                "C.D. Manning."
            ],
            "title": "Featurerich part-of-speech tagging with a cyclic dependency network",
            "venue": "Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL).",
            "year": 2003
        },
        {
            "authors": [
                "R. Turner",
                "Y. Sripada",
                "E. Reiter."
            ],
            "title": "Generating approximate geographic descriptions",
            "venue": "European Workshop on Natural Language Generation, pages 42\u201349.",
            "year": 2009
        },
        {
            "authors": [
                "S. Wan",
                "M. Dras",
                "R. Dale",
                "C. Paris."
            ],
            "title": "Using dependency-based features to take the \u201cpara-farce\u201d out of paraphrase",
            "venue": "Australasian Language Technology Workshop.",
            "year": 2006
        },
        {
            "authors": [
                "M. Wang",
                "C.D. Manning."
            ],
            "title": "Probabilistic treeedit models with structured latent variables for textual entailment and question answering",
            "venue": "The International Conference on Computational Linguistics, pages 1164\u20131172.",
            "year": 2010
        },
        {
            "authors": [
                "Y.W. Wong",
                "R.J. Mooney."
            ],
            "title": "Learning synchronous grammars for semantic parsing with lambda calculus",
            "venue": "Association for Computational Linguistics (ACL), pages 960\u2013967.",
            "year": 2007
        },
        {
            "authors": [
                "M. Zelle",
                "R.J. Mooney."
            ],
            "title": "Learning to parse database queries using inductive logic proramming",
            "venue": "Association for the Advancement of Artificial Intelligence (AAAI), pages 1050\u20131055.",
            "year": 1996
        },
        {
            "authors": [
                "L.S. Zettlemoyer",
                "M. Collins."
            ],
            "title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars",
            "venue": "Uncertainty in Artificial Intelligence (UAI), pages 658\u2013 666.",
            "year": 2005
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "We consider the semantic parsing problem of mapping natural language utterances into logical forms to be executed on a knowledge base (KB) (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). Scaling semantic parsers to large knowledge bases has attracted substantial attention recently (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), since it drives applications such as question answering (QA) and information extraction (IE).\nSemantic parsers need to somehow associate natural language phrases with logical predicates, e.g., they must learn that the constructions \u201cWhat\nWhat party did Clay establish?\nparaphrase model\nWhat political party founded by Henry Clay? ... What event involved the people Henry Clay?\nType.PoliticalParty u Founder.HenryClay ... Type.Event u Involved.HenryClay\ndoes X do for a living?\u201d, \u201cWhat is X\u2019s profession?\u201d, and \u201cWho is X?\u201d, should all map to the logical predicate Profession. To learn these mappings, traditional semantic parsers use data which pairs natural language with the KB. However, this leaves untapped a vast amount of text not related to the KB. For instance, the utterances \u201cWhere is ACL in 2014?\u201d and \u201cWhat is the location of ACL 2014?\u201d cannot be used in traditional semantic parsing methods, since the KB does not contain an entity ACL2014, but this pair clearly contains valuable linguistic information. As another reference point, out of 500,000 relations extracted by the ReVerb Open IE system (Fader et al., 2011), only about 10,000 can be aligned to Freebase (Berant et al., 2013).\nIn this paper, we present a novel approach for semantic parsing based on paraphrasing that can exploit large amounts of text not covered by the KB (Figure 1). Our approach targets factoid questions with a modest amount of compositionality. Given an input utterance, we first use a simple deterministic procedure to construct a manageable set of candidate logical forms (ideally, we would generate canonical utterances for all possible logical forms, but this is intractable). Next, we heuris-\ntically generate canonical utterances for each logical form based on the text descriptions of predicates from the KB. Finally, we choose the canonical utterance that best paraphrases the input utterance, and thereby the logical form that generated it. We use two complementary paraphrase models: an association model based on aligned phrase pairs extracted from a monolingual parallel corpus, and a vector space model, which represents each utterance as a vector and learns a similarity score between them. The entire system is trained jointly from question-answer pairs only.\nOur work relates to recent lines of research in semantic parsing and question answering. Kwiatkowski et al. (2013) first maps utterances to a domain-independent intermediate logical form, and then performs ontology matching to produce the final logical form. In some sense, we approach the problem from the opposite end, using an intermediate utterance, which allows us to employ paraphrasing methods (Figure 2). Fader et al. (2013) presented a QA system that maps questions onto simple queries against Open IE extractions, by learning paraphrases from a large monolingual parallel corpus, and performing a single paraphrasing step. We adopt the idea of using paraphrasing for QA, but suggest a more general paraphrase model and work against a formal KB (Freebase).\nWe apply our semantic parser on two datasets: WEBQUESTIONS (Berant et al., 2013), which contains 5,810 question-answer pairs with common questions asked by web users; and\nFREE917 (Cai and Yates, 2013), which has 917 questions manually authored by annotators. On WEBQUESTIONS, we obtain a relative improvement of 12% in accuracy over the state-of-the-art, and on FREE917 we match the current best performing system. The source code of our system PARASEMPRE is released at http://www-nlp.stanford.edu/ software/sempre/."
        },
        {
            "heading": "2 Setup",
            "text": "Our task is as follows: Given (i) a knowledge base K, and (ii) a training set of question-answer pairs {(xi, yi)}ni=1, output a semantic parser that maps new questions x to answers y via latent logical forms z. Let E denote a set of entities (e.g., BillGates), and let P denote a set of properties (e.g., PlaceOfBirth). A knowledge base K is a set of assertions (e1, p, e2) \u2208 E \u00d7 P \u00d7 E (e.g., (BillGates, PlaceOfBirth, Seattle)). We use the Freebase KB (Google, 2013), which has 41M entities, 19K properties, and 596M assertions.\nTo query the KB, we use a logical language called simple \u03bb-DCS. In simple \u03bb-DCS, an entity (e.g., Seattle) is a unary predicate (i.e., a subset of E) denoting a singleton set containing that entity. A property (which is a binary predicate) can be joined with a unary predicate; e.g., Founded.Microsoft denotes the entities that are Microsoft founders. In PlaceOfBirth.Seattle u Founded.Microsoft, an intersection operator allows us to denote the set of Seattle-born Microsoft founders. A reverse operator reverses the order of arguments: R[PlaceOfBirth].BillGates denotes Bill Gates\u2019s birthplace (in contrast to PlaceOfBirth.Seattle). Lastly, count(Founded.Microsoft) denotes set cardinality, in this case, the number of Microsoft founders. The denotation of a logical form z with respect to a KB K is given by JzKK. For a formal description of simple \u03bb-DCS, see Liang (2013) and Berant et al. (2013)."
        },
        {
            "heading": "3 Model overview",
            "text": "We now present the general framework for semantic parsing via paraphrasing, including the model and the learning algorithm. In Sections 4 and 5, we provide the details of our implementation.\nCanonical utterance construction Given an utterance x and the KB, we construct a set of candi-\ndate logical forms Zx, and then for each z \u2208 Zx generate a small set of canonical natural language utterances Cz . Our goal at this point is only to generate a manageable set of logical forms containing the correct one, and then generate an appropriate canonical utterance from it. This strategy is feasible in factoid QA where compositionality is low, and so the size of Zx is limited (Section 4).\nParaphrasing We score the canonical utterances in Cz with respect to the input utterance x using a paraphrase model, which offers two advantages. First, the paraphrase model is decoupled from the KB, so we can train it from large text corpora. Second, natural language utterances often do not express predicates explicitly, e.g., the question \u201cWhat is Italy\u2019s money?\u201d expresses the binary predicate CurrencyOf with a possessive construction. Paraphrasing methods are well-suited for handling such text-to-text gaps. Our framework accommodates any paraphrasing method, and in this paper we propose an association model that learns to associate natural language phrases that co-occur frequently in a monolingual parallel corpus, combined with a vector space model, which learns to score the similarity between vector representations of natural language utterances (Section 5).\nModel We define a discriminative log-linear model that places a probability distribution over pairs of logical forms and canonical utterances (c, z), given an utterance x:\np\u03b8(c, z | x) = exp{\u03c6(x, c, z)>\u03b8}\u2211\nz\u2032\u2208Zx,c\u2032\u2208Cz exp{\u03c6(x, c\u2032, z\u2032)>\u03b8} ,\nwhere \u03b8 \u2208 Rb is the vector of parameters to be learned, and \u03c6(x, c, z) is a feature vector extracted from the input utterance x, the canonical utterance c, and the logical form z. Note that the candidate set of logical forms Zx and canonical utterances Cx are constructed during the canonical utterance construction phase.\nThe model score decomposes into two terms:\n\u03c6(x, c, z)>\u03b8 = \u03c6pr(x, c) >\u03b8pr + \u03c6lf(x, z) >\u03b8lf,\nwhere the parameters \u03b8pr define the paraphrase model (Section 5), which is based on features extracted from text only (the input and canonical utterance). The parameters \u03b8lf correspond to semantic parsing features based on the logical form and\ninput utterance, and are briefly described in this section.\nMany existing paraphrase models introduce latent variables to describe the derivation of c from x, e.g., with transformations (Heilman and Smith, 2010; Stern and Dagan, 2011) or alignments (Haghighi et al., 2005; Das and Smith, 2009; Chang et al., 2010). However, we opt for a simpler paraphrase model without latent variables in the interest of efficiency.\nLogical form features The parameters \u03b8lf correspond to the following features adopted from Berant et al. (2013). For a logical form z, we extract the size of its denotation JzKK. We also add all binary predicates in z as features. Moreover, we extract a popularity feature for predicates based on the number of instances they have in K. For Freebase entities, we extract a popularity feature based on the entity frequency in an entity linked subset of Reverb (Lin et al., 2012). Lastly, Freebase formulas have types (see Section 4), and we conjoin the type of z with the first word of x, to capture the correlation between a word (e.g., \u201cwhere\u201d) with the Freebase type (e.g., Location).\nLearning As our training data consists of question-answer pairs (xi, yi), we maximize the log-likelihood of the correct answer. The probability of an answer y is obtained by marginalizing over canonical utterances c and logical forms z whose denotation is y. Formally, our objective function O(\u03b8) is as follows:\nO(\u03b8) = n\u2211 i=1 log p\u03b8(yi | xi)\u2212 \u03bb\u2016\u03b8\u20161,\np\u03b8(y | x) = \u2211\nz\u2208Zx:y=JzKK\n\u2211 c\u2208Cz p\u03b8(c, z | x).\nThe strength \u03bb of the L1 regularizer is set based on cross-validation. We optimize the objective by initializing the parameters \u03b8 to zero and running AdaGrad (Duchi et al., 2010). We approximate the set of pairs of logical forms and canonical utterances with a beam of size 2,000."
        },
        {
            "heading": "4 Canonical utterance construction",
            "text": "We construct canonical utterances in two steps. Given an input utterance x, we first construct a set of logical forms Zx, and then generate canonical utterances from each z \u2208 Zx. Both steps are performed with a small and simple set of deterministic rules, which suffices for our datasets, as\nthey consist of factoid questions with a modest amount of compositional structure. We describe these rules below for completeness. Due to its soporific effect though, we advise the reader to skim it quickly.\nCandidate logical forms We consider logical forms defined by a set of templates, summarized in Table 1. The basic template is a join of a binary and an entity, where a binary can either be one property p.e (#1 in the table) or two properties p1.p2.e (#2). To handle cases of events involving multiple arguments (e.g., \u201cWho did Brad Pitt play in Troy?\u201d), we introduce the template p.(p1.e1 u p2.e2) (#3), where the main event is modified by more than one entity. Logical forms can be further modified by a unary \u201cfilter\u201d, e.g., the answer to \u201cWhat composers spoke French?\u201d is a set of composers, i.e., a subset of all people (#4). Lastly, we handle aggregation formulas for utterances such as \u201cHow many teams are in the NCAA?\u201d (#5).\nTo construct candidate logical forms Zx for a given utterance x, our strategy is to find an entity in x and grow the logical form from that entity. As we show later, this procedure actually produces a set with better coverage than constructing logical forms recursively from spans of x, as is done in traditional semantic parsing. Specifically, for every span of x, we take at most 10 entities whose Freebase descriptions approximately match the span. Then, we join each entity e with all type-compatible1 binaries b, and add these logical forms to Zx (#1 and #2).\nTo construct logical forms with multiple entities (#3) we do the following: For any logical form z = p.p1.e1, where p1 has type signature (t1, \u2217), we look for other entities e2 that were matched in x. Then, we add the logical form p.(p1.e1 u p2.e2), if there exists a binary p2 with a compatible type signature (t1, t2), where t2 is one of e2\u2019s types. For example, for the logical form Character.Actor.BradPitt, if we match the entity Troy in x, we obtain Character.(Actor.BradPitt u Film.Troy). We further modify logical forms by intersecting with a unary filter (#4): given a formula z with some Freebase type (e.g., People), we look at all Freebase sub-types t (e.g., Composer), and\n1Entities in Freebase are associated with a set of types, and properties have a type signature (t1, t2) We use these types to compute an expected type t for any logical form z.\ncheck whether one of their Freebase descriptions (e.g., \u201ccomposer\u201d) appears in x. If so, we add the formula Type.t u z to Zx. Finally, we check whether x is an aggregation formula by identifying whether it starts with phrases such as \u201chow many\u201d or \u201cnumber of\u201d (#5).\nOn WEBQUESTIONS, this results in 645 formulas per utterance on average. Clearly, we can increase the expressivity of this step by expanding the template set. For example, we could handle superlative utterances (\u201cWhat NBA player is tallest?\u201d) by adding a template with an argmax operator.\nUtterance generation While mapping general language utterances to logical forms is hard, we observe that it is much easier to generate a canonical natural language utterances of our choice given a logical form. Table 2 summarizes the rules used to generate canonical utterances from the template p.e. Questions begin with a question word, are followed by the Freebase description of the expected answer type (d(t)), and followed by Freebase descriptions of the entity (d(e)) and binary (d(p)). To fill in auxiliary verbs, determiners, and prepositions, we parse the description d(p) into one of NP, VP, PP, or NP VP. This determines the generation rule to be used.\nEach Freebase property p has an explicit property p\u2032 equivalent to the reverse R[p] (e.g., ContainedBy and R[Contains]). For each logical form z, we also generate using equivalent logical forms where p is replaced with R[p\u2032]. Reversed formulas have different generation rules, since entities in these formulas are in the subject position rather than object position.\nWe generate the description d(t) from the Freebase description of the type of z (this handles #4). For the template p1.p2.e (#2), we have a similar set of rules, which depends on the syntax of d(p1) and d(p2) and is omitted for brevity. The template p.(p1.e1 u p2.e2) (#3) is generated by appending the prepositional phrase in d(e2), e.g, \u201cWhat character is the character of Brad Pitt in Troy?\u201d. Lastly, we choose the question phrase \u201cHow many\u201d for aggregation formulas (#5), and \u201cWhat\u201d for all other formulas.\nWe also generate canonical utterances using an alignment lexicon, released by Berant et al. (2013), which maps text phrases to Freebase binary predicates. For a binary predicate b mapped from text phrase d(b), we generate the utterance\nWH d(t) d(b) d(e) ?. On the WEBQUESTIONS dataset, we generate an average of 1,423 canonical utterances c per input utterance x. In Section 6, we show that an even simpler method of generating canonical utterances by concatenating Freebase descriptions hurts accuracy by only a modest amount."
        },
        {
            "heading": "5 Paraphrasing",
            "text": "Once the candidate set of logical forms paired with canonical utterances is constructed, our problem is reduced to scoring pairs (c, z) based on a paraphrase model. The NLP paraphrase literature is vast and ranges from simple methods employing surface features (Wan et al., 2006), through vector space models (Socher et al., 2011), to latent variable models (Das and Smith, 2009; Wang and Manning, 2010; Stern and Dagan, 2011).\nIn this paper, we focus on two paraphrase models that emphasize simplicity and efficiency. This is important since for each question-answer pair, we consider thousands of canonical utterances as potential paraphrases. In contrast, traditional paraphrase detection (Dolan et al., 2004) and Recognizing Textual Entailment (RTE) tasks (Dagan et al., 2013) consider examples consisting of only a single pair of candidate paraphrases.\nOur paraphrase model decomposes into an association model and a vector space model:\n\u03c6pr(x, c) >\u03b8pr = \u03c6as(x, c) >\u03b8as + \u03c6vs(x, c) >\u03b8vs."
        },
        {
            "heading": "5.1 Association model",
            "text": "The goal of the association model is to determine whether x and c contain phrases that are likely to be paraphrases. Given an utterance x = \u3008x0, x1, .., xn\u22121\u3009, we denote by xi:j the span from token i to token j. For each pair of utterances (x, c), we go through all spans of x and c and identify a set of pairs of potential paraphrases (xi:j , ci\u2032:j\u2032), which we call associations. (We will describe how associations are identified shortly.) We then define features on each association; the weighted combination of these features yields a score. In this light, associations can be viewed as soft paraphrase rules. Figure 3 presents examples of associations extracted from a paraphrase pair and visualizes the learned scores. We can see that our model learns a positive score for associating \u201ctype\u201d with \u201cgenres\u201d, and a negative score for associating \u201cis\u201d with \u201cplay\u201d.\nWe define associations in x and c primarily by looking up phrase pairs in a phrase table constructed using the PARALEX corpus (Fader et al., 2013). PARALEX is a large monolingual parallel\ncorpora, containing 18 million pairs of question paraphrases from wikianswers.com, which were tagged as having the same meaning by users. PARALEX is suitable for our needs since it focuses on question paraphrases. For example, the phrase \u201cdo for a living\u201d occurs mostly in questions, and we can extract associations for this phrase from PARALEX. Paraphrase pairs in PARALEX are word-aligned using standard machine translation methods. We use the word alignments to construct a phrase table by applying the consistent phrase pair heuristic (Och and Ney, 2004) to all 5-grams. This results in a phrase table with approximately 1.3 million phrase pairs. We let A denote this set of mined candidate associations.\nFor a pair (x, c), we also consider as candidate associations the set B (represented implicitly), which contains token pairs (xi, ci\u2032) such that xi and ci\u2032 share the same lemma, the same POS tag, or are linked through a derivation link on WordNet (Fellbaum, 1998). This allows us to learn paraphrases for words that appear in our datasets but are not covered by the phrase table, and to handle nominalizations for phrase pairs such as \u201cWho designed the game of life?\u201d and \u201cWhat game designer is the designer of the game of life?\u201d.\nOur model goes over all possible spans of x and c and constructs all possible associations from A and B. This results in many poor associations (e.g., \u201cplay\u201d and \u201cthe\u201d), but as illustrated in Figure 3, we learn weights that discriminate good from bad associations. Table 3 specifies the full set of features. Note that unlike standard paraphrase detection and RTE systems, we use lexicalized features, firing approximately 400,000 features on WEBQUESTIONS. By extracting POS features, we obtain soft syntactic rules, e.g., the feature \u201cJJ N \u2227 N\u201d indicates that omitting adjectives before nouns is possible. Once associations are constructed, we mark tokens in x and c that were not part of any association, and extract\ndeletion features for their lemmas and POS tags. Thus, we learn that deleting pronouns is acceptable, while deleting nouns is not.\nTo summarize, the association model links phrases of two utterances in multiple overlapping ways. During training, the model learns which associations are characteristic of paraphrases and which are not."
        },
        {
            "heading": "5.2 Vector space model",
            "text": "The association model relies on having a good set of candidate associations, but mining associations suffers from coverage issues. We now introduce a vector space (VS) model, which assigns a vector representation for each utterance, and learns a scoring function that ranks paraphrase candidates.\nWe start by constructing vector representations of words. We run the WORD2VEC tool (Mikolov et al., 2013) on lower-cased Wikipedia text (1.59 billion tokens), using the CBOW model with a window of 5 and hierarchical softmax. We also experiment with publicly released word embeddings (Huang et al., 2012), which were trained using both local and global context. Both result in kdimensional vectors (k = 50). Next, we construct a vector vx \u2208 Rk for each utterance x by simply averaging the vectors of all content words (nouns, verbs, and adjectives) in x.\nWe can now estimate a paraphrase score for two utterances x and c via a weighted combination of the components of the vector representations:\nv>xWvc = k\u2211\ni,j=1\nwijvx,ivc,j\nwhere W \u2208 Rk\u00d7k is a parameter matrix. In terms of our earlier notation, we have \u03b8vs = vec(W ) and \u03c6vs(x, c) = vec(vxv>c ), where vec(\u00b7) unrolls a matrix into a vector. In Section 6, we experiment with W equal to the identity matrix, constraining W to be diagonal, and learning a full W matrix.\nThe VS model can identify correct paraphrases in cases where it is hard to directly associate phrases from x and c. For example, the answer to \u201cWhere is made Kia car?\u201d (from WEBQUESTIONS), is given by the canonical utterance \u201cWhat city is Kia motors a headquarters of?\u201d. The association model does not associate \u201cmade\u201d and \u201cheadquarters\u201d, but the VS model is able to determine that these utterances are semantically related. In other cases, the VS model cannot distinguish correct paraphrases from incorrect ones. For\nexample, the association model identifies that the paraphrase for \u201cWhat type of music did Richard Wagner Play?\u201d is \u201cWhat is the musical genres of Richard Wagner?\u201d, by relating phrases such as \u201ctype of music\u201d and \u201cmusical genres\u201d. The VS model ranks the canonical utterance \u201cWhat composition has Richard Wagner as lyricist?\u201d higher, as this utterance is also in the music domain. Thus, we combine the two models to benefit from their complementary nature.\nIn summary, while the association model aligns particular phrases to one another, the vector space model provides a soft vector-based representation for utterances."
        },
        {
            "heading": "6 Empirical evaluation",
            "text": "In this section, we evaluate our system on WEBQUESTIONS and FREE917. After describing the setup (Section 6.1), we present our main empirical results and analyze the components of the system (Section 6.2)."
        },
        {
            "heading": "6.1 Setup",
            "text": "We use the WEBQUESTIONS dataset (Berant et al., 2013), which contains 5,810 question-answer pairs. This dataset was created by crawling questions through the Google Suggest API, and then obtaining answers using Amazon Mechanical Turk. We use the original train-test split, and divide the training set into 3 random 80%\u201320% splits for development. This dataset is characterized by questions that are commonly asked on the web (and are not necessarily grammatical), such as \u201cWhat character did Natalie Portman play in Star Wars?\u201d and \u201cWhat kind of money to take to Bahamas?\u201d.\nThe FREE917 dataset contains 917 questions, authored by two annotators and annotated with logical forms. This dataset contains questions on rarer topics (for example, \u201cWhat is the engine in a 2010 Ferrari California?\u201d and \u201cWhat was the cover price of the X-men Issue 1?\u201d), but the phrasing of questions tends to be more rigid compared to WEBQUESTIONS. Table 4 provides some statistics on the two datasets. Following Cai and Yates (2013), we hold out 30% of the data for the\nfinal test, and perform 3 random 80%-20% splits of the training set for development. Since we train from question-answer pairs, we collect answers by executing the gold logical forms against Freebase.\nWe execute \u03bb-DCS queries by converting them into SPARQL and executing them against a copy of Freebase using the Virtuoso database engine. We evaluate our system with accuracy, that is, the proportion of questions we answer correctly. We run all questions through the Stanford CoreNLP pipeline (Toutanova and Manning, 2003; Finkel et al., 2005; Klein and Manning, 2003).\nWe tuned the L1 regularization strength, developed features, and ran analysis experiments on the development set (averaging across random splits). On WEBQUESTIONS, without L1 regularization, the number of non-zero features was 360K; L1 regularization brings it down to 17K."
        },
        {
            "heading": "6.2 Results",
            "text": "We compare our system to Cai and Yates (2013) (CY13), Berant et al. (2013) (BCFL13), and Kwiatkowski et al. (2013) (KCAZ13). For BCFL13, we obtained results using the SEMPRE package2 and running Berant et al. (2013)\u2019s system on the datasets.\nTable 5 presents results on the test set. We achieve a substantial relative improvement of 12% in accuracy on WEBQUESTIONS, and match the best results on FREE917. Interestingly, our system gets an oracle accuracy of 63% on WEBQUESTIONS compared to 48% obtained by BCFL13, where the oracle accuracy is the fraction of questions for which at least one logical form in the candidate set produced by the system is correct. This demonstrates that our method for constructing candidate logical forms is reasonable. To further examine this, we ran BCFL13 on the development set, allowing it to use only predicates from logical forms suggested by our logical form construction step. This improved oracle accuracy on the development set to 64.5%, but accuracy was 32.2%. This shows that the improvement in accuracy should not be attributed only to better logical form generation, but also to the paraphrase model.\nWe now perform more extensive analysis of our system\u2019s components and compare it to various baselines.\nComponent ablation We ablate the association model, the VS model, and the entire paraphrase\n2http://www-nlp.stanford.edu/software/sempre/\nmodel (using only logical form features). Table 5 shows that our full system obtains highest accuracy, and that removing the association model results in a much larger degradation compared to removing the VS model.\nUtterance generation Our system generates relatively natural utterances from logical forms using simple rules based on Freebase descriptions (Section 4). We now consider simply concatenating Freebase descriptions. For example, the logical form R[PlaceOfBirth].ElvisPresley would generate the utterance \u201cWhat location Elvis Presley place of birth?\u201d. Row SIMPLEGEN in Table 6 demonstrates that we still get good results in this setup. This is expected given that our paraphrase models are not sensitive to the syntactic structure of the generated utterance.\nVS model Our system learns parameters for a full W matrix. We now examine results when learning parameters for a full matrix W , a diagonal matrix W , and when setting W to be the identity matrix. Table 6 (third section) illustrates that learning a full matrix substantially improves accuracy. Figure 4 gives an example for a correct paraphrase pair, where the full matrix model boosts the overall model score. Note that the full matrix assigns a high score for the phrases \u201cofficial language\u201d and \u201cspeak\u201d compared to the simpler models, but other pairs are less interpretable.\nBaselines We also compared our system to the following implemented baselines:\n\u2022 JACCARD: We compute the Jaccard score between the tokens of x and c and define \u03c6pr(x, c) to be this single feature. \u2022 EDIT: We compute the token edit distance\nbetween x and c and define \u03c6pr(x, c) to be this single feature. \u2022 WDDC06: We re-implement 13 features\nfrom Wan et al. (2006), who obtained close to state-of-the-art performance on the Microsoft Research paraphrase corpus.3 Table 6 demonstrates that we improve performance over all baselines. Interestingly, JACCARD and WDDC06 obtain reasonable performance on FREE917 but perform much worse on WEBQUESTIONS. We surmise this is because questions in FREE917 were generated by annotators prompted by Freebase facts, whereas questions in WEBQUESTIONS originated independently of Freebase. Thus, word choice in FREE917 is often close to the generated Freebase descriptions, allowing simple baselines to perform well.\nError analysis We sampled examples from the development set to examine the main reasons PARASEMPRE makes errors. We notice that in many cases the paraphrase model can be further improved. For example, PARASEMPRE suggests\n3We implement all features that do not require dependency parsing.\nthat the best paraphrase for \u201cWhat company did Henry Ford work for?\u201d is \u201cWhat written work novel by Henry Ford?\u201d rather than \u201cThe employer of Henry Ford\u201d, due to the exact match of the word \u201cwork\u201d. Another example is the question \u201cWhere is the Nascar hall of fame?\u201d, where PARASEMPRE suggests that \u201cWhat hall of fame discipline has Nascar hall of fame as halls of fame?\u201d is the best canonical utterance. This is because our simple model allows to associate \u201chall of fame\u201d with the canonical utterance three times. Entity recognition also accounts for many errors, e.g., the entity chosen in \u201cwhere was the gallipoli campaign waged?\u201d is Galipoli and not GalipoliCampaign. Last, PARASEMPRE does not handle temporal information, which causes errors in questions like \u201cWhere did Harriet Tubman live after the civil war?\u201d"
        },
        {
            "heading": "7 Discussion",
            "text": "In this work, we approach the problem of semantic parsing from a paraphrasing viewpoint. A fundamental motivation and long standing goal of the paraphrasing and RTE communities has been to cast various semantic applications as paraphrasing/textual entailment (Dagan et al., 2013). While it has been shown that paraphrasing methods are useful for question answering (Harabagiu and Hickl, 2006) and relation extraction (Romano et al., 2006), this is, to the best of our knowledge, the first paper to perform semantic parsing through paraphrasing. Our paraphrase model emphasizes simplicity and efficiency, but the framework is agnostic to the internals of the paraphrase method.\nOn the semantic parsing side, our work is most related to Kwiatkowski et al. (2013). The main challenge in semantic parsing is coping with the mismatch between language and the KB. In both Kwiatkowski et al. (2013) and this work, an intermediate representation is employed to handle the mismatch, but while they use a logical representation, we opt for a text-based one. Our choice allows us to benefit from the parallel monolingual corpus PARALEX and from word vectors trained on Wikipedia. We believe that our approach is particularly suitable for scenarios such as factoid question answering, where the space of logical forms is somewhat constrained and a few generation rules suffice to reduce the problem to paraphrasing.\nOur work is also related to Fader et al. (2013),\nwho presented a paraphrase-driven question answering system. One can view this work as a generalization of Fader et al. along three dimensions. First, Fader et al. use a KB over natural language extractions rather than a formal KB and so querying the KB does not require a generation step \u2013 they paraphrase questions to KB entries directly. Second, they suggest a particular paraphrasing method that maps a test question to a question for which the answer is already known in a single step. We propose a general paraphrasing framework and instantiate it with two paraphrase models. Lastly, Fader et al. handle queries with only one property and entity whereas we generalize to more types of logical forms.\nSince our generated questions are passed to a paraphrase model, we took a very simple approach, mostly ensuring that we preserved the semantics of the utterance without striving for the most fluent realization. Research on generation (Dale et al., 2003; Reiter et al., 2005; Turner et al., 2009; Piwek and Boyer, 2012) typically focuses on generating natural utterances for human consumption, where fluency is important.\nIn conclusion, the main contribution of this paper is a novel approach for semantic parsing based on a simple generation procedure and a paraphrase model. We achieve state-of-the-art results on two recently released datasets. We believe that our approach opens a window of opportunity for learning semantic parsers from raw text not necessarily related to the target KB. With more sophisticated generation and paraphrase, we hope to tackle compositionally richer utterances."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank Kai Sheng Tai for performing the error analysis. Stanford University gratefully acknowledges the support of the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract no. FA8750-13-2-0040. Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the DARPA, AFRL, or the US government. The second author is supported by a Google Faculty Research Award."
        }
    ],
    "title": "Semantic Parsing via Paraphrasing",
    "year": 2014
}