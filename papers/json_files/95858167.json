{
    "abstractText": "We discuss a key problem in information extraction which deals with wrapper failures due to changing content templates. A good proportion of wrapper failures are due to HTML templates changing to cause wrappers to become incompatible after element inclusion or removal in a DOM (Tree representation of HTML). We perform a large-scale empirical analyses of the causes of shift and mathematically quantify the levels of domain difficulty based on entropy. We propose the XTreePath annotation method to captures contextual node information from the training DOM. We then utilize this annotation in a supervised manner at test time with our proposed Recursive Tree Matching method which locates nodes most similar in context recursively using the tree edit distance. The search is based on a heuristic function that takes into account the similarity of a tree compared to the structure that was present in the training data. We evaluate XTreePath using 117,422 pages from 75 diverse websites in 8 vertical markets. Our XTreePath method consistently outperforms XPath and a current commercial system in terms of successful extractions in a blackbox test. We make our code and datasets publicly available online.",
    "authors": [
        {
            "affiliations": [],
            "name": "Joseph Paul Cohen"
        },
        {
            "affiliations": [],
            "name": "Wei Ding"
        },
        {
            "affiliations": [],
            "name": "Abraham Bagherjeiran"
        }
    ],
    "id": "SP:c113b0ae069f0bc1450d6a7a390d16eedc2e1e77",
    "references": [
        {
            "authors": [
                "Tobias Anton"
            ],
            "title": "XPath-Wrapper Induction by generalizing tree traversal patterns",
            "venue": "Lernen, Wissensentdeckung und Adaptivitt (LWA)",
            "year": 2005
        },
        {
            "authors": [
                "Boris Chidlovskii"
            ],
            "title": "Information extraction from tree documents by learning subtree delimiters",
            "venue": "IJCAI Workshop on Information Integration on the Web",
            "year": 2003
        },
        {
            "authors": [
                "Nilesh Dalvi",
                "Philip Bohannon",
                "Fei Sha"
            ],
            "title": "Robust web extraction: an approach based on a probabilistic tree-edit model",
            "venue": "In International Conference on Management of Data",
            "year": 2009
        },
        {
            "authors": [
                "Nilesh Dalvi",
                "Ravi Kumar",
                "Mohamed Soliman"
            ],
            "title": "Automatic wrappers for large scale web extraction",
            "venue": "Proceedings of the VLDB Endowment",
            "year": 2011
        },
        {
            "authors": [
                "Yixiang Fang",
                "Xiaoqin Xie",
                "Xiaofeng Zhang",
                "Reynold Cheng",
                "Zhiqiang Zhang"
            ],
            "title": "STEM: a suffix tree-based method for web data records extraction. Knowledge and Information Systems (2017)",
            "year": 2017
        },
        {
            "authors": [
                "S Flesca",
                "G Manco",
                "E Masciari",
                "E Rende",
                "A Tagarelli"
            ],
            "title": "Web wrapper induction: a brief survey",
            "venue": "AI Communications",
            "year": 2004
        },
        {
            "authors": [
                "P Gulhane",
                "A Madaan",
                "R Mehta",
                "J Ramamirtham",
                "R Rastogi",
                "S Satpal",
                "S H Sengamedu",
                "A Tengli",
                "C Tiwari"
            ],
            "title": "Web-scale information extraction with vertex",
            "venue": "In International Conference on Data Engineering. IEEE. https: //doi.org/10.1109/ICDE.2011.5767842",
            "year": 2011
        },
        {
            "authors": [
                "Qiang Hao",
                "Rui Cai",
                "Yanwei Pang",
                "Lei Zhang"
            ],
            "title": "From one tree to a forest: a unified solution for structured web data extraction",
            "venue": "In International Conference on Research and Development in Information Retrieval",
            "year": 2011
        },
        {
            "authors": [
                "Nitin Jindal",
                "Bing Liu"
            ],
            "title": "A Generalized Tree Matching Algorithm Considering Nested Lists for Web Data Extraction",
            "venue": "The SIAM International Conference on Data",
            "year": 2010
        },
        {
            "authors": [
                "M Kayed",
                "M R Girgis",
                "K F Shaalan"
            ],
            "title": "A survey of web information extraction systems",
            "venue": "IEEE Transactions on Knowledge and Data Engineering",
            "year": 2006
        },
        {
            "authors": [
                "Raymond Kosala",
                "Jan Van den Bussche",
                "Maurice Bruynooghe",
                "Hendrik Blockeel"
            ],
            "title": "Information Extraction in Structured Documents Using Tree Automata Induction",
            "year": 2002
        },
        {
            "authors": [
                "N Kushmerick"
            ],
            "title": "Wrapper induction: Efficiency and expressiveness",
            "venue": "Artificial Intelligence",
            "year": 2000
        },
        {
            "authors": [
                "Nicholas Kushmerick",
                "Daniel S. Weld",
                "Robert Doorenbos"
            ],
            "title": "Wrapper induction for information extraction",
            "year": 1997
        },
        {
            "authors": [
                "Samiah Jan Nasti",
                "M. Asghar",
                "Muheet Ahmad Butt"
            ],
            "title": "A Comparative Study on Web Data Extraction Approaches",
            "venue": "International Journal of Engineering Science and Computing",
            "year": 2016
        },
        {
            "authors": [
                "Adi Omari",
                "Sharon Shoham",
                "Eran Yahav"
            ],
            "title": "Synthesis of Forgiving Data Extractors",
            "venue": "International Conference on Web Search and Data Mining (2017)",
            "year": 2017
        },
        {
            "authors": [
                "Aditya Parameswaran",
                "N Dalvi",
                "H Garcia-Molina",
                "R Rastogi"
            ],
            "title": "Optimal Schemes for Robust Web Extraction",
            "venue": "In International Conference on Very Large Data Bases",
            "year": 2011
        },
        {
            "authors": [
                "D C Reis",
                "Paulo B Golgher",
                "A S Silva",
                "a F Laender"
            ],
            "title": "Automatic web news extraction using tree edit distance",
            "venue": "In World Wide Web",
            "year": 2004
        },
        {
            "authors": [
                "W Yang"
            ],
            "title": "Identifying syntactic differences between two programs",
            "venue": "Software - Practice and Experience",
            "year": 1991
        },
        {
            "authors": [
                "Y Zhai",
                "B Liu"
            ],
            "title": "Web data extraction based on partial tree alignment",
            "venue": "In World Wide Web",
            "year": 2005
        },
        {
            "authors": [
                "S Zheng",
                "R Song",
                "J R Wen",
                "C L Giles"
            ],
            "title": "Efficient record-level wrapper induction",
            "venue": "In Conference on Information and knowledge Management",
            "year": 2009
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "We address a key problem in information extraction which deals withwrapper failures due to changing content templates. Awrapper is best described by Kushmerick as a \u201cprocedure, specific to a single information resource, that translates a [webpage] query response to relational form\u201d [13]. Wrappers are required because much of the desired data on the Internet is presented using HTML templates instead of well formed (XML or JSON) data or unstructured freeform text [6]. Extracting data, such as stock, flight, and product information, from websites that use HTML templates is difficult because wrapper methods have difficulty dealing with changes to HTML structure.\nWe believe a good proportion of wrapper failures are due to HTML templates changing and cause wrappers to become incompatible after the element inclusion or removal of DOM (Tree representation of HTML). For example, an additional \u201cOn Sale\u201d element is included above a product. This shift [3, 19] may require manual\nretraining of wrappers which is a burden to users. Empirically we find over 50% of our web sample contains shift at various levels and we have a detailed discussion about shift in Section \u00a73.\nTo handle the problem of shift we need to take in more information from the DOM [2, 4, 7, 11, 15, 19]. Instead of only extracting statistics from the training data we extract an entire sub-tree structure, like Reis [17], and use this during wrapper induction to create a supervised information extraction method.\nFigure 1 gives an example of XTreePath. We start by finding the least common ancestor of the nodes of interest (product name, price, etc) and extract a relative XPath as well as the nodes of the DOM for every step of that XPath. We then use this XTreePath with our Recursive Tree Matching method which performs a heuristic graph search in the target DOM to find the most similar sub-trees to those in the training data. Because we are matching sub-trees we can handle large horizontal and vertical shifts as long as some unique traits about the DOM are preserved.\nFurthermore, XTreePath is compatible by design, which complements XPath but does not replace it. We only utilize the recursive tree matching lookup process after an XPath has failed, which minimizes runtime. This allows an XPath-based method to adopt XTreePath without sacrificing existing speed or quality. Existing research has discovered many methods to construct robust XPaths a priori. Our presented method can exist in parallel with these methods as they are continually advanced in order to achieve better overall accuracy.\nar X\niv :1\n50 5.\n01 30\n3v 3\n[ cs\n.I R\n] 2\n7 D\nec 2\n01 7\nThus, our main contributions are as follows:\n\u2022 Method:We propose the XTreePath method which is a generalization of XPaths where tree structure is also stored and used during wrapper induction. \u2022 Algorithm Design: We propose a Recursive Tree Matching method and a dynamic programming solution to perform wrapper induction. \u2022 Theoretical Analysis:We formally define and analyze the problem of shift theoretically and empirically. We mathematically quantify the levels of domain difficulty based on entropy and study a large representative dataset. \u2022 Robustness:We evaluate XTreePath using 117,422 pages from 75 diverse websites in 8 vertical markets. \u2022 Reproducibility: Our code and data are open-sourced at http: //kdl.cs.umb.edu/w/datasets/."
        },
        {
            "heading": "2 RELATEDWORK",
            "text": "In the field of information extraction there are two primary categories of annotation learning: supervised and unsupervised approaches. We take a supervised approach and combine positional (XPath [1]) and ontological (Tree Structure) concepts [10, 12, 14] together.\nIn Dalvi and Parameswaran [3, 4, 16] focused on supervised annotation learning algorithms tolerant to noise in the training data. They enumerate many XPath wrappers using a probabilistic ranking system to pick the best one. The output of these methods is XPath annotations which differentiates our method as XTreePath would complement this method and not replace it.\nThe context of nodes has been used to create annotations themselves but not during extraction as our method does. In 2009 Zheng [20] and Fang [5] used a \u201cbroom\u201d structure inside the HTML DOM to represent both records and generated wrappers. This work was motivated to capture lists of products instead of creating wrappers tolerant to shift.\nTree similarity has been used in unsupervised information extraction approaches to find common sub-trees in websites by Zhai in 2005 [19] and used as a method to locate lists from web pages by Jindal in 2010 [9]. These methods focus on locating interesting data but not in a way of imposing a label as our method XTreePath does.\nA break away from just carrying XPaths forward from the training was discussed by Omari [15]. In this work they would learn a decision decision tree to predict which XPath fragments should be used at test time.\nMoving beyond XPath is not a new concept. There has been work on tree automata induction by [11] which aims to learn a deterministic finite automata which will process the DOM tree and accept nodes which contain the information of interest. The mechanics of this are very different but we can also argue that a DFA does not take the neighborhood of the tree into account when accepting a node.\nPossibly the most similar approach to our method was proposed by Reis[17]. Here they use the tree edit distance to determine if a webpage contains a specific type of content (news) based on tree structure. We take this concept and structure it into a generalized\nXPath representation that can be used for supervised information extraction."
        },
        {
            "heading": "3 SHIFT ANALYSIS",
            "text": "A shift of a web page occurs when a modification of the page causes the inclusion, removal, or substitution of DOM elements which changes the DOM tree representation. Not all shifts are bad. A shift only becomes a problem when it causes wrappers to become incompatible and return no result or an incorrect result. Here compatibility is defined as whether the DOM structure of a wrapper matches the DOM structure of a web page.\nOur method is based on the principle that all shifts can be broken down into a combination of vertical and horizontal shifts. We can take advantage of this by considering all tree permutations. Given a tree T with nodes t \u2208 T , each node is a sub-tree and has a parent t .parent and a set of children t .children. A tree contains many paths p with elements pi \u2208 T , when the path travels down the tree: pi+1 \u2208 pi .children.\nDefinition 3.1. A vertical shift is a tree modification where a node is inserted on the path from the root to the target element. Formally, for some path p = {p1, . . . ,pi ,pi+1, . . .pn }, a vertical shift occurs when some new node, lets call s , is inserted and\np\u2032 = {p1, . . . ,pi , s,pi+1, . . . ,pn }\nor, if a node is removed,\np\u2032 = {p1, . . . ,pi ,pi+2, . . . ,pn }.\nA vertical shift causes an insertion or removal in an XPath, see the example in Figure 2.\nDefinition 3.2. A horizontal shift is a tree modification where a sibling element of a node is inserted. Formally, for some path\np = {p1, . . . ,pi\u22121,pi ,pi+1, . . . ,pn }, a horizontal shift occurs when some new node, lets call s where s , pi , is inserted p\u2032 = {p1, . . . ,pi\u22121, s,pi+1, . . . ,pn }. pi may still exist in the tree but a different node now connects the two path segments p1, . . . ,pi\u22121 and pi+1, . . . ,pn .\nA horizontal shift causes the index of a node to change, see the example in Figure 3 .\nWith formal definition of shifts, we study a large dataset of 117,422 pages from 75 websites in 8 vertical markets described in Section \u00a75 to empirically analyze possible reasons for shifts.\nOne indicator of shifts is when multiple XPaths are needed to extract the same attribute from different web pages of the same website. The probability of a domain requiring multiple compatible XPaths decreases as the number of XPaths increases. Even with this good news, 116 out of the 231 attributes have more than one XPath associated with them. The most difficult domain and attribute are barnesandnoble\u2019s title with 270 unique XPaths required. Inspecting the mean XPaths required for each domain we can observe slight chunks which would imply possible clusters and maybe some similarities between the websites. We identify three main possible reasons for shifts: \u2022 Inconsistent templates : A website might present items to a user differently depending on a property of that item. For example, an item on sale may have a graphic inserted which shifts the DOM. In our dataset, collegeboard.com uses different templates for public and private universities that results in shifts. \u2022 Temporal changes : Over time the developer may change the site to fix a bug, add a feature, or perform a redesign. Changes can be related to user tracking, advertising, or updated template software. \u2022 DOM cleaner inconsistencies : A DOM cleaner (details in Section \u00a75.1) needs to make assumptions when converting semistructured HTML into XHTML. If the HTML is ambiguous this\nprocess will result in a DOM tree that does notmatch the intended DOM and will appear as a shift. In our empirical study, we observe large groups of unique XPaths are due to inconsistent templates and small groups of unique XPaths (less than 4) are usually due to DOM cleaner inconsistencies.\nOur next step is to mathematically quantify the difficulty in maintaining a wrapper in order to measure the disorder of the domain.\nDefinition 3.3. Attribute difficulty represents the disorder of the attribute locationswith respect to XPath annotations in a sample set of pages from a domain. The probability that a particular XPath for an attribute will be compatible with pages from a domain given a sample of pages is formalized as:p(x) = compatible pagestotal pages . We define attribute difficulty in Eq 1:\nH (attribute) = \u2212 \u2211\nxpaths\np(xpath) logp(xpath) (1)\nWhen looking now at Table 1 the attribute difficulty can be used to quantify the difference between the two attributes presented for the domain deepdiscount. Intuitively title appears more difficult than author due to the long list of unique XPaths. Attribute difficulty confirms this with 0.93 for title and 0.65 for author. One strength of the difficulty analysis is that it weights each XPath to take into account outliers that are only compatible with 1 or 2 webpages. This is important because these outliers will not significantly impact accuracy and therefore should not impact the difficulty.\nDefinition 3.4. The domain difficulty is the mean difficulty of it\u2019s attributes.\nInspecting domain difficulty can provide insight into where the current XPath method fails to solve the problem. In Figure 4 we look at the F1-Score (measure for classification accuracy; the higher the better, explained in \u00a75) versus the domain difficulty (a low value means less shifts occur). The plot reinforces the intuition that XPath usually works well on domains with low difficulty. Also we can confirm that when the difficulty is high XPath does not perform as well. This makes sense because higher difficulty means that there is more disorder in the set of XPaths for that domain which causes them to fail."
        },
        {
            "heading": "A linear trend-line is drawn to show the trend of the rela-",
            "text": "tionship."
        },
        {
            "heading": "4 XTREEPATH (XPATH+TREEPATHS)",
            "text": "We now utilize the knowledge gained from our shift analysis that more attribute difficulty means lower performance of XPath annotations. Then we will explain how our Recursive Tree Matching algorithm searches node by node using similarity of sub-trees to train examples to simultaneously accommodate for vertical and horizontal shifts. In order to store the relevant data for searching we first explain tree paths."
        },
        {
            "heading": "4.1 TreePaths",
            "text": "We seek for an efficient wrapper method that can fix incompatible wrappers automatically when shifts happen. In order to have enough information to fix wrappers in an automated way we extract not only the direct indexing into the document but also the context of those elements. Instead of consulting the entire training set to repair a wrapper we store the tree structure immediately surrounding the target data. The algorithm starts building the tree path at the least common ancestor (LCA) of the target elements.\nDefinition 4.1. Least common ancestor (LCA) of target elements: The least common ancestor is an element that exists on every path from the root to each target element. The LCA is unique in that there is no other common element that is closer to every target element.\nDefinition 4.2. A tree path is identified by \u03c4 . It is a sequence of trees in an HTML Document Object Model (DOM) starting from the least common ancestor (\u03c40) and ending at the target element (\u03c4n ) as follows:\nTreePath = \u03c4 = \u27e8\u03c40,\u03c41, . . . ,\u03c4n\u27e9,\u03c4i \u2208 DOM\nA tree path is an extension of the XPath concept. With XPath, the elements of the path are tag names that describe the sequence to the target. In contrast, a tree path includes an entire sub-tree starting from each element in the sequence to the target. This is to provide sufficient contextual information from which each element was extracted to aid in the wrapper recovery later. An example tree path of length four is shown in Figure 1.\nExtracting tree paths from an HTML DOM is shown in Algorithm 1. First, we find the least common ancestor (LCA) between all the labeled elements. Next, starting from the target element, each element is added to a vector and then it\u2019s parent element and so on until the LCA is reached. Next, we add the LCA because it was\nnot added in the above loop. Finally the elements are reversed and returned so that they start with the LCA and end with the target element.\nAlgorithm 1: Build Tree Path From Training DOM Input: Training DOM dom\nLabeled elements E = {e1, . . . , ek } Target element e\nOutput: Tree Path \u03c4 1 domLCA = LCA(E,dom) 2 while domLCA , e do 3 \u03c4 .add(e) e = e .\u0434etParent() 4 \u03c4 .add(e .\u0434etParent()) 5 return \u03c4R"
        },
        {
            "heading": "4.2 Recursive Tree Matching",
            "text": "We learned from analyzing shifts causing incompatibility that the majority are composed only a very small number of vertical and horizontal shifts. With our proposed Recursive Tree Matching, we jump over these shifts by matching sub-trees on each side of the shift. The LCA, which is the root of the tree path, provides a starting point and allows us to ignore shifts that have occurred outside where the target data is. Starting here also allows us to reduce the complexity of the search. The proceeding elements of the tree path are matched to their most similar nodes in order to align trees that existed previously unshifted. The objective function is shown in Eq 2. Here the maximum matching sequence of e (DOM elements) to the data contained in each \u03c4i is found. This maximization is iterative with constraints which requires two maximization sections.\nmax ( \u2211 \u03c4i \u2208\u03c4 {ei+1 = ar\u0434maxe \u2208ei (match(\u03c4i , e))} ) (2)\nThe algorithmic detail, including the dynamic programming heuristic function, of Recursive Tree Matching is shown in Algorithm 2. In this pseudo code a reference to an element of the DOM is kept as d and updated as the search progresses. Line 4 is the core where each element of the tree path \u03c4i is matched to its most similar DOM element ind . HTML Tree similarity is calculated using a modified Simple Tree Matching algorithm [18] which is designed to deal with HTML specifically by taking into account the class, style, id, name attributes of each node. If a max similarity is 0 then the element is considered not found. Using this method we perform a heuristic search through the tree using concise information from the training data.\nA demonstration of the recursive tree matching process is shown in Figure 6. The shifted HTML DOM presented in Figure 5 is searched using the Recursive Tree Matching method with the tree path we extracted from original DOM in Figure 5 shown in Figure 1. We first start by trying to directly look up the target data using the original sequence of elements. This results in a failure causing the algorithm to perform wrapper recovery.\nRecovery starts by searching every element in the HTML DOM for the sub-tree that has the highest similarity to \u03c40 (the LCA).\nAlgorithm 2:Wrapper Induction (Recursive Tree Matching) Input: Tree path \u03c4\nHTML DOM dom Output: Resulting data\n1 d \u2190 dom 2 for \u03c4i \u2208 \u03c4 do 3 for e \u2208 d do 4 d = ar\u0434maxe (html_tree_match(\u03c4i , e)) 5 //If match is 0 then not found\n6 return d\n7 html_tree_match(a,b) : 8 if a and \u03c4i contain distinct symbols then 9 return 0"
        },
        {
            "heading": "10 else",
            "text": "11 m \u2190 the number of first-level sub-trees of a 12 n \u2190 the number of first-level sub-trees of b 13 M[i, 0] \u2190 0 for i = 0, . . . ,m 14 M[0, j] \u2190 0 for j = 0, . . . ,n 15 for i = 1 tom do 16 for i = 1 to n do 17 x \u2190 M[i, j \u2212 1] 18 y \u2190 M[i \u2212 1, j] 19 z \u2190 M[i \u2212 1, j \u2212 1] + html_tree_match(ai ,bj ) 20 M[i, j] \u2190max(x ,y, z)\n21 for attr \u2208 {class, style, id,name} do 22 if aattr == battr then 23 attrMatch \u2190 attrMatch + 0.25;\n24 returnM[m,n] + (attrMatch \u2217 0.5) + 1\nIn Figure 6 the element /div has a similarity score of 7 which is higher than all other sub-trees. The similarity is calculated using the html_tree_match method. The score of 7 is calculated as the maximum overlap of one tree with the other given the liberty of horizontal shifts.\nOnce we have focused on /div[1], this sub-tree is now searched using the second element of the tree path. A similarity score of 5 yields /div[2] as the most similar sub-tree. Next, the algorithm will find a most similar sub-tree by jumping over the element /div[1] to find /div[1]/div has a higher similarity. This search will result in the /span element being located and the wrapper successfully repaired. This example showcases the power of Recursive Tree Matching method in dealing with addition of identical trees and the extension of trees."
        },
        {
            "heading": "4.3 Complexity",
            "text": "The worst case complexity of the recursive tree matching method is O(|\u03c4 |n1n2) where n1 and n2 are the number of elements in the training and test DOM trees. This is derived from the Simple Tree Matching (STM) complexity, which is reduced using dynamic programming, beingO(n1n2). There are |\u03c4 | iterations of the algorithm, each needing to perform an STM search. The cost at each iteration\nwill be smaller than the previous but for this analysis we round up. Also, in our method we reduce the initial size of n1 by selecting the LCA instead of the root element. Empirically the initial n1 value is very small, about 30."
        },
        {
            "heading": "5 EVALUATION",
            "text": "In this section we aim to show that XTreePaths can be utilized to complement and outperform the existing dominant method XPath. The method presented has no parameters that require tuning. Our goal is to design robust and practical method which can be easily extended under different scenarios.\nIn order to test the robustness of the methods, the percentage of the dataset used for training is varied. This allows a comprehensive comparison between the following three methods:\n\u2022 XPath : Each training example has an XPath to be the target node that is combined into a set of possible paths. Each path is attempted on the testing examples until there is a valid path. \u2022 XTreePath : Firstly XPaths are attempted. If it is not successful then a tree paths is used to attempt recovery. \u2022 TreePath : Only a tree path is used without XPath. A tree path is extracted from each training example starting from the LCA\nof the target elements for that domain. Then Recursive Tree Matching is used to search the DOM tree. \u2022 ScrapingHub : The web service offered at scrapinghub.com is used as a blackbox to evaluate the state of the art offered commercially by industry. (No authors are affiliated with ScrapingHub)\nTo compare XTreePath and XPath we use a large dataset built by Qiang Hao [8] to benchmark per-vertical wrapper repair instead of per-domain. This dataset contains a total of 117,422 pages from 75 diverse websites in 8 vertical markets that covers a broad range of topics from university rankings to NBA players. The composition is displayed in Table 2. For each vertical market a set of (3-5) common attributes are labelled on every page. We make our data and code available for comparison at http://kdl.cs.umb.edu/w/datasets/."
        },
        {
            "heading": "5.1 Working With Data",
            "text": "It is important to document the difficulties of dealing with datasets in this field in order to ensure that the benchmark datasets used here can be utilized by other researchers.\nThe main issue is that HTML in the wild does not always map to the same DOM representation, it is highly dependent on the HTML Cleaner used. There is no standard mapping to convert HTML into\na properly formatted XHTML file. There are common algorithms used by browsers but there is no agreed specification of how they convert HTML to XHTML. Different cleaners convert HTML in different ways leading to an incompatibility of XPath annotations. A training set made with one cleaning engine will not work using another engine.\nThe libraries used for our work were chosen as the most reliable and competentmethods after a comparative study onmajor libraries was performed. The complete list of libraries we evaluated are labeled in Table 3 as having the following properties:\n\u2022 Cleaning - These are used to convert from a non-standardHTML file to an XHTML file. The corrections include tag closing, namespace filtering, and tag nesting. This is required because most HTML on the Internet is non-standard. \u2022 Representation - This library provides things such as DOM traversal, insertion, and removal. This library is used to build sub-trees and represent namespaces. Most of these libraries are not tolerant to non-standard HTML and require cleaning before they can turn HTML into a DOM. \u2022 Query - These methods can include XPath, XQuery, XML-GL, or XML-QL.\nIn this paper, our research is done using the well supported open source libraries JTidy and Dom4j. These libraries are written in Java and support multi-threading. JTidy\u2019s performed consistently for cleaning and intergraded cleanly into Dom4j. Dom4j has a clean query interface to lookup using XPath as well as a clean Representation interface for implementing tree paths and recursive tree matching. Other Java libraries such as JSoup and TagSoup are designed for their own query language instead of XPath and exposing a DOM.\nA few pages today retrieve their content using JavaScript once the page is loaded. This means the HTML retrieved with the initial GET request does not contain the full product information. A way to solve this problem is to use a library that will run the JavaScript on the page or to scrape the data using a browser after it has run the JavaScript code. It is better to get JavaScript out of the way during scraping due to the need to make AJAX calls to retrieve data that may be missing at a later date. For this reason we retrieve the pages using the FireFox web browser which will evaluate JavaScript as expected by the web developer.\nWe use the standard machine learning metrics precision, recall, and F1-score as the evaluation metrics. In this domain, a true positive (tp) is an extraction of the correct data (verified using labeled data), a false positive (fp) is an extraction that resulted in the wrong\ndata (something other than the correct data), and a false negative (fn) is an extraction that resulted in an error or \u201cnot found\u201d. Errors are caused by the lookup reaching a dead end. The following formulae are used: precision = tptp+f p , recall = tp tp+f n , and F1 = 2 \u00b7 precision \u00b7r ecallprecision+r ecall"
        },
        {
            "heading": "5.2 Difficulty Correlation",
            "text": "Using our new domain entropy measurement introduced in Section \u00a73we plot the entropy per domain over all vertical markets in Figure 7 for XPath and XTreePath. As the entropy increases XTreePath is able to maintain performance while XPath degrades. The higher a domain entropy value is, the more changes in HTML elements occur. XTreePath is more robust than XPath when dealing with shifts."
        },
        {
            "heading": "5.3 Limiting Training Examples",
            "text": "We evaluate the ability to recover from shift on each domain by splitting the pages of each domain into training and testing sets\nat various percentages. The pages of each domain are chosen randomly to simulate the different possible situations that could be encountered.\nIn the following experiments each method is trained on a percentage of the dataset. In these experiments; 10% percentage trained means that only 10% of the pages from a domain are used in training to build XPaths and XTreePaths. These are used to extract data from the remaining 90% of the pages.\nIn Figure 8 the aggregate recall is plotted against the percentage trained. This analyses how many wrappers are saved from needing to be relearned by using the different methods. We can observe the combination of XPath and tree paths as XTreePath achieves a significant increase which confirms they are complementing each other. This is important because these methods do not directly replace each other and together are able to provide a more robust data extraction pipeline. Also, it is important to note the largest increase is with a lower percentage of training data. This is desired because the algorithm can perform even if a small number of pages have been collected which is often the case. This is because every annotated training page is a cost to the system. Also, some data sources will increase in size over time causing the trained percentage will shrink over time.\nNext we evaluate the aggregate precision in Figure 9. The most interesting result here is that as the training percentage is increased, the precision is reduced for XPath and tree paths. As more examples are learned, XPath and tree path have more data to try which\nresults in higher false positives. When the methods are combined in XTreePath the same number of false positives exists but the number of true positives increases and allows the precision formula to grow.\nThe aggregate F1-Score is shown in Figure 10. Here the F1-Score of XTreePath consistently outperform XPath and tree paths alone. The advantage of XPath in precision is countered by it\u2019s low recall."
        },
        {
            "heading": "5.4 Performance Per Vertical",
            "text": "We are interested to know how the proposed XTreePath perform in vastly different web domains. In Figure 11 we analyze the mean F1score of XPath and XTreePath in each vertical market. XTreePath performs consistently well against XPath in all vertical markets. We can draw from this analysis that book websites have more stable structure which allows XPaths to work consistently. We can also draw that restaurant and university sites have more dynamic structures with slight changes that can easily be accommodated for by XTreePath."
        },
        {
            "heading": "5.5 Industry Baseline",
            "text": "Finally, we compare our method to a current commercial solution, ScrapingHub, that tackles the same problem. This method is treated\nas a black box and we do not know how it works. In this evaluation both methods are trained on the same single example. Each method is then evaluated on the remaining examples from the domain.\nFigure 12 shows the comparison using 17 randomly selected domains. XTreePath ties or beats ScrapingHub on 12/17 domains. For the domain embark the problems arise from two faults happening at the same time. First, an XPath fails when locating the address (a span[6] is shifted to a span[8]). XTreePath recovers this broken XPath but then fails on the phone number attribute (for which the learned XPath would have worked but the system was already trying to recover the wrapper). The weakness is that when all the children look the same the tree similarity doesn\u2019t work. This happened to be a perfect storm for XTreePath but would easily be fixed by adding another training example.\nThe abebooks results are identical. Why can\u2019t we improve this result? The weakness is that tree paths cannot deal with the shift in this site because it is confused with matching tree structure. The shifted site only has it\u2019s data rearranged but the tree structure has not changed. Even with using the node attributes it cannot repair the wrappers because they are also the same."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "We have presented the XTreePath algorithm which is composed of XPath and tree paths together. Tree paths contain contextual information from training examples and are used by the recursive tree matching search algorithm. We have shown that with a simple data structure, the tree path, we can conquer shifts in webpages and reduce manual retraining. We evaluate our method on a massive and publicly available dataset where XTreePath consistently outperforms XPath alone.\nA key advantage of the XTreePath method is that it complements existing methods and it does not need to replace them. We hope that this allows greater adoption in research and industry. Further work may utilize a semantic difference between trees by utilizing a cost matrix to weight differences in HTML elements unequally which would potentially increase accuracy but introduce additional parameters to test. To accelerate adoption we provide our easy to use implementation as open source and all the code necessary to evaluate it."
        },
        {
            "heading": "7 ACKNOWLEDGEMENTS",
            "text": "Partially funded by ThinkersR.Us, The University of Massachusetts Boston, The National Science Foundation Award Research Experiences for Undergraduates (NSF#0755376), and The National Science Foundation Graduate Research Fellowship Program (Grant No. DGE-1356104). This work utilized the supercomputing facilities managed by the Research Computing Department at the University of Massachusetts Boston as well as the resources provided by the Open Science Grid, which is supported by the National Science Foundation and the U.S. Department of Energy\u2019s Office of Science."
        }
    ],
    "title": "XTreePath: A generalization of XPath to handle real world structural variation",
    "year": 2017
}