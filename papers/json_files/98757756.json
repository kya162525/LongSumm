{
    "abstractText": "As data becomes the fuel driving technological and economic growth, a fundamental challenge is how to quantify the value of data in algorithmic predictions and decisions. For example, in healthcare and consumer markets, it has been suggested that individuals should be compensated for the data that they generate, but it is not clear what is an equitable valuation for individual data. In this work, we develop a principled framework to address data valuation in the context of supervised machine learning. Given a learning algorithm trained on n data points to produce a predictor, we propose data Shapley as a metric to quantify the value of each training datum to the predictor performance. Data Shapley uniquely satisfies several natural properties of equitable data valuation. We develop Monte Carlo and gradient-based methods to efficiently estimate data Shapley values in practical settings where complex learning algorithms, including neural networks, are trained on large datasets. In addition to being equitable, extensive experiments across biomedical, image and synthetic data demonstrate that data Shapley has several other benefits: 1) it is more powerful than the popular leave-one-out or leverage score in providing insight on what data is more valuable for a given learning task; 2) low Shapley value data effectively capture outliers and corruptions; 3) high Shapley value data inform what type of new data to acquire to improve the predictor.",
    "authors": [
        {
            "affiliations": [],
            "name": "Amirata Ghorbani"
        },
        {
            "affiliations": [],
            "name": "James Zou"
        }
    ],
    "id": "SP:b7a717233ec3ff37385ab1b06816d0ca375f5bb3",
    "references": [
        {
            "authors": [
                "I. Arrieta Ibarra",
                "L. Goff",
                "D. Jim\u00e9nez Hern\u00e1ndez",
                "J. Lanier",
                "E.G. Weyl"
            ],
            "title": "Should we treat data as labor? moving beyond\u2019free",
            "venue": "Moving Beyond\u2019Free\u2019(December",
            "year": 2017
        },
        {
            "authors": [
                "C. Beleites",
                "U. Neugebauer",
                "T. Bocklitz",
                "C. Krafft",
                "J. Popp"
            ],
            "title": "Sample size planning for classification models",
            "venue": "Analytica chimica acta,",
            "year": 2013
        },
        {
            "authors": [
                "J. Castro",
                "D. G\u00f3mez",
                "J. Tejada"
            ],
            "title": "Polynomial calculation of the shapley value based on sampling",
            "venue": "Computers & Operations Research,",
            "year": 2009
        },
        {
            "authors": [
                "J. Castro",
                "D. G\u00f3mez",
                "J. Tejada"
            ],
            "title": "Polynomial calculation of the shapley value based on sampling",
            "venue": "Computers & Operations Research,",
            "year": 2009
        },
        {
            "authors": [
                "J. Chen",
                "L. Song",
                "M.J. Wainwright",
                "M.I. Jordan"
            ],
            "title": "Lshapley and c-shapley: Efficient model interpretation for structured data",
            "venue": "arXiv preprint arXiv:1808.02610,",
            "year": 2018
        },
        {
            "authors": [
                "S. Cohen",
                "G. Dror",
                "E. Ruppin"
            ],
            "title": "Feature selection via coalitional game theory",
            "venue": "Neural Computation,",
            "year": 1939
        },
        {
            "authors": [
                "R.D. Cook"
            ],
            "title": "Detection of influential observation in linear regression",
            "venue": "Technometrics, 19(1):15\u201318,",
            "year": 1977
        },
        {
            "authors": [
                "R.D. Cook",
                "S. Weisberg"
            ],
            "title": "Residuals and influence in regression",
            "year": 1982
        },
        {
            "authors": [
                "A. Datta",
                "S. Sen",
                "Y. Zick"
            ],
            "title": "Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems",
            "venue": "In Security and Privacy (SP),",
            "year": 2016
        },
        {
            "authors": [
                "P. Dubey"
            ],
            "title": "On the uniqueness of the shapley value",
            "venue": "International Journal of Game Theory,",
            "year": 1975
        },
        {
            "authors": [
                "S.S. Fatima",
                "M. Wooldridge",
                "N.R. Jennings"
            ],
            "title": "A linear approximation method for the shapley value",
            "venue": "Artificial Intelligence,",
            "year": 2008
        },
        {
            "authors": [
                "B. Fr\u00e9nay",
                "M. Verleysen"
            ],
            "title": "Classification in the presence of label noise: a survey",
            "venue": "IEEE transactions on neural networks and learning systems,",
            "year": 2014
        },
        {
            "authors": [
                "J. Friedman",
                "T. Hastie",
                "R. Tibshirani"
            ],
            "title": "The elements of statistical learning, volume 1. Springer series in statistics",
            "year": 2001
        },
        {
            "authors": [
                "A. Ghorbani",
                "A. Abid",
                "J. Zou"
            ],
            "title": "Interpretation of neural networks is fragile",
            "venue": "arXiv preprint arXiv:1710.10547,",
            "year": 2017
        },
        {
            "authors": [
                "F. Gul"
            ],
            "title": "Bargaining foundations of shapley value",
            "venue": "Econometrica: Journal of the Econometric Society,",
            "year": 1989
        },
        {
            "authors": [
                "H. Hamers",
                "B. Husslage",
                "R. Lindelauf",
                "T Campen"
            ],
            "title": "A new approximation method for the shapley value applied to the wtc 9/11 terrorist attack",
            "venue": "Technical report,",
            "year": 2016
        },
        {
            "authors": [
                "P.W. Koh",
                "P. Liang"
            ],
            "title": "Understanding black-box predictions via influence functions",
            "venue": "arXiv preprint arXiv:1703.04730,",
            "year": 2017
        },
        {
            "authors": [
                "I Kononenko"
            ],
            "title": "An efficient explanation of individual classifications using game theory",
            "venue": "Journal of Machine Learning Research,",
            "year": 2010
        },
        {
            "authors": [
                "S.M. Lundberg",
                "Lee",
                "S.-I"
            ],
            "title": "A unified approach to interpreting model predictions",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "S.M. Lundberg",
                "G.G. Erion",
                "Lee",
                "S.-I"
            ],
            "title": "Consistent individualized feature attribution for tree ensembles",
            "venue": "arXiv preprint arXiv:1802.03888,",
            "year": 2018
        },
        {
            "authors": [
                "D. Mahajan",
                "R. Girshick",
                "V. Ramanathan",
                "K. He",
                "M. Paluri",
                "Y. Li",
                "A. Bharambe",
                "L. van der Maaten"
            ],
            "title": "Exploring the limits of weakly supervised pretraining",
            "venue": "arXiv preprint arXiv:1805.00932,",
            "year": 2018
        },
        {
            "authors": [
                "S. Maleki",
                "L. Tran-Thanh",
                "G. Hines",
                "T. Rahwan",
                "A. Rogers"
            ],
            "title": "Bounding the estimation error of samplingbased shapley value approximation",
            "venue": "arXiv preprint arXiv:1306.4265,",
            "year": 2013
        },
        {
            "authors": [
                "I. Mann",
                "L.S. Shapley"
            ],
            "title": "Values of large games. 6: Evaluating the electoral college exactly",
            "venue": "Technical report, RAND CORP SANTA MONICA CA,",
            "year": 1962
        },
        {
            "authors": [
                "V. Metsis",
                "I. Androutsopoulos",
                "G. Paliouras"
            ],
            "title": "Spam filtering with naive bayes-which naive bayes",
            "venue": "In CEAS,",
            "year": 2006
        },
        {
            "authors": [
                "T.P. Michalak",
                "K.V. Aadithya",
                "P.L. Szczepanski",
                "B. Ravindran",
                "N.R. Jennings"
            ],
            "title": "Efficient computation of the shapley value for game-theoretic network centrality",
            "venue": "Journal of Artificial Intelligence Research,",
            "year": 2013
        },
        {
            "authors": [
                "J.W. Milnor",
                "L.S. Shapley"
            ],
            "title": "Values of large games",
            "venue": "ii: Oceanic games. Mathematics of operations research,",
            "year": 1978
        },
        {
            "authors": [
                "E.A. Posner",
                "E.G. Weyl"
            ],
            "title": "Radical Markets: Uprooting Capitalism and Democracy for a Just Society",
            "year": 2018
        },
        {
            "authors": [
                "O. Russakovsky",
                "J. Deng",
                "H. Su",
                "J. Krause",
                "S. Satheesh",
                "S. Ma",
                "Z. Huang",
                "A. Karpathy",
                "A. Khosla",
                "M Bernstein"
            ],
            "title": "Imagenet large scale visual recognition challenge",
            "venue": "International Journal of Computer Vision,",
            "year": 2015
        },
        {
            "authors": [
                "L.S. Shapley"
            ],
            "title": "A value for n-person games",
            "venue": "Contributions to the Theory of Games,",
            "year": 1953
        },
        {
            "authors": [
                "L.S. Shapley",
                "Roth",
                "A. E"
            ],
            "title": "The Shapley value: essays in honor of Lloyd S",
            "year": 1988
        },
        {
            "authors": [
                "J. Steinhardt",
                "P.W.W. Koh",
                "P.S. Liang"
            ],
            "title": "Certified defenses for data poisoning attacks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "B. Strack",
                "J.P. DeShazo",
                "C. Gennings",
                "J.L. Olmo",
                "S. Ventura",
                "K.J. Cios",
                "J.N. Clore"
            ],
            "title": "Impact of hba1c measurement on hospital readmission rates: analysis of 70,000 clinical database patient",
            "venue": "records. BioMed research international,",
            "year": 2014
        },
        {
            "authors": [
                "C. Sudlow",
                "J. Gallacher",
                "N. Allen",
                "V. Beral",
                "P. Burton",
                "J. Danesh",
                "P. Downey",
                "P. Elliott",
                "J. Green",
                "M Landray"
            ],
            "title": "Uk biobank: an open access resource for identifying the causes of a wide range of complex diseases of middle and old age",
            "venue": "PLoS medicine,",
            "year": 2015
        },
        {
            "authors": [
                "C. Szegedy",
                "V. Vanhoucke",
                "S. Ioffe",
                "J. Shlens",
                "Z. Wojna"
            ],
            "title": "Rethinking the inception architecture for computer vision",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "H. Xiao",
                "K. Rasul",
                "R. Vollgraf"
            ],
            "title": "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms",
            "venue": "arXiv preprint arXiv:1708.07747,",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Data is valuable and it is the fuel that powers artificial intelligence. Increasingly in sectors such as health care and advertising, data generated by individuals is a key compo-\n1Department of Electrical Engineering, Stanford University, Stanford, CA, USA 2Department of Biomedical Data Science, Stanford University, Stanford, CA, USA. Correspondence to: James Zou <jamesz@stanford.edu>.\nProceedings of the 36 th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).\nnent of the market place, similar to labor and capital (Posner & Weyl, 2018). It has been suggested that certain data constitute individual property, and as such individuals should be compensated in exchange for these data (Regulation, 2018). Like labor and capital, a fundamental question is how to equitably value individual\u2019s data.\nWe focus on data valuation in the specific setting of supervised machine learning. In order to make sense of data value, we need three ingredients in our investigation: a fixed training data set, a learning algorithm, and a performance metric. The training data is a fixed set of n data points, {xi, yi}n1 , where xi and yi are the features and the label of point i, respectively. For our purpose, a learning algorithm A is a procedure that takes an arbitrary training set and produces a predictor. For example, A could be the common empirical risk minimization where it solves \u03b8\u2217 = arg min\u03b8 \u2211 l(f(xi; \u03b8), yi), where l is the loss, \u03b8 parametrizes a family of models and f( ; \u03b8\u2217) is the predictor. For any predictor f , we also need a performance metric of V (f). We can think of V as the test performance of f on whatever metric of interest.\nThe two questions that we want to investigate are: 1) what is an equitable measure of the value of each (xi, yi) to the learning algorithm A with respect to the performance metric V ; and 2) how do we efficiently compute this data value in practical settings. For example, suppose we have data from N = 1000 patients and we train a small neural network to build a heart disease classifier. We also have some independent metric to assess the performance of the trained classifier\u2014e.g. its prediction accuracy on a test set. Then we would like to quantify the value of each patient\u2019s data to the classifier\u2019s performance on this task.\nNote that we do not define a universal value for data. Instead, the value of each datum depend on the learning algorithm, the performance metric as well as on other data in the training set. This dependency is reasonable and desirable in machine learning. Certain data points could be more important if we are training a logistic regression instead of a neural network. Similarly, if the performance metric changes\u2014e.g. regressing to the age of heart disease onset instead of heart disease incidence\u2014then the value of certain patient\u2019s data should change. Moreover the performance metric could be computed on a different population/distribution than the\ntraining data; we make no assumptions about how it is done.\nA common method to evaluate data importance is some form of leave-one-out (LOO) test: compare the difference in the predictor\u2019s performance when trained on the full dataset vs. the performance when trained on the full set minus one point (Cook, 1977). The drop in performance in one measure of the \u201cvalue\u201d of that point. LOO is often approximated by leverage or influence score, which measures how the predictor changes when the weight of one point changes slightly (Cook & Weisberg, 1982). We will show below that leave-one-out does not satisfy natural properties we expect for equitable data valuation, and it performs poorly in experiments. For a simple intuition of why leave-one-out fails, suppose our predictor is a nearest-neighbor classifier\u2014 i.e. for each test point we find its nearest neighbor in the training set and assign it that label. Moreover suppose every training point has two exact copies in the training set. Removing one point from training does not change the predictor at all, since its copy is still present. Therefore the leave-one-out approach would assign every training point zero value, regardless of how well the actual predictor performs. This simple example illustrates that leave-one-out does not capture potentially complex interactions between subsets of data. Our proposed data Shapley value provides more meaningful valuation by precisely accounting for such interactions.\nOur contributions We provide a natural formulation of the important problem of equitable data valuation in machine learning. We propose data Shapley value, leveraging powerful results from game theory, to quantify the the contribution of individual data points to a learning task. Data Shapley uniquely satisfies three natural properties of equitable valuation. Moreover, our empirical studies demonstrate that data Shapley has several additional utilities: 1) it gives more insights into the importance of each data point than the common leave-one-out score; 2) it can identify outliers and corrupted data; 3) it can inform how to acquire future data to improve the predictor."
        },
        {
            "heading": "2. Equitable Data Valuation for ML",
            "text": "Preliminaries Let D = {(xi, yi)}n1 be our fixed training set. We do not make any distributional assumptions about D and the data need not be independent. The yi\u2019s can be categorical or real for classification and regression, respectively. Let A denote the learning algorithm. We view A as a black-box that takes as input a training data set of size between 0 and\u221e, and returns a predictor. We are particularly interested in the predictor trained on subsets S \u2286 D. The performance score V is another black-box oracle that takes as input any predictor and returns a score. We write V (S,A), or just V (S) for short, to denote the performance\nscore of the predictor trained on data S. Our goal is to compute a data value \u03c6i(D,A, V ) \u2208 R, as a function of D,A and V , to quantify the value of the i-th datum. We will often write it as \u03c6i(V ) or just \u03c6i to simplify notation. For convenience, we will sometimes overload the notation for S and D so that it can also indicate the set of indices\u2014i.e. i \u2208 S if (xi, yi) is in that subset and D = {1, ..., n}.\nExample Suppose yi\u2019s are binary and A corresponds to a logistic regression learner\u2014i.e. A takes any dataset and returns a logistic regression fitted to it. The score V here could be the 0/1 accuracy on a separate test set. Then V (S) is the 0/1 test accuracy when the logistic regression is trained on a subset S \u2286 D. If S = \u2205, then V (S) is the performance of a randomly initialized classifier. In general, the test data used to compute V could be from a different distribution than that of D.\nEquitable properties of data valuation We believe that \u03c6 should satisfy the following properties in order to be equitable:\n1. If (xi, yi) does not change the performance if it\u2019s added to any subset of the training data, then it should be given zero value. More precisely, suppose for all S \u2286 D \u2212 {i}, V (S) = V (S \u222a {i}), then \u03c6i = 0.\n2. If for data i and j and any subset S \u2286 D \u2212 {i, j}, we have V (S \u222a {i}) = V (S \u222a {j}), then \u03c6i = \u03c6j . In other words, if i and j, when added to any subset of our training data, always produce exactly the same change in the predictor\u2019s score, then i and j should be given the same value by symmetry. 3. In most ML settings, V = \u2212 \u2211 k\u2208test set lk where lk\nis the loss of the predictor on the k-th test point (we took a minus so that lower loss is higher score). We can define Vk = \u2212lk to be the predictor\u2019s performance on the k-th test point. Similarly \u03c6i(Vk) quantifies the value of the i-th training point to the k-th test point. If datum i contributes values \u03c6i(V1) and \u03c6i(V2) to the predictions of test points 1 and 2, respectively, then we expect the value of i in predicting both test points\u2014 i.e. when V = V1 + V2\u2014to be \u03c6i(V1) + \u03c6i(V2). In words: when the overall prediction score is the sum of K separate predictions, the value of a datum should be the sum of its value for each prediction. Formally: \u03c6i(V +W ) = \u03c6i(V )+\u03c6i(W ) for performance scores V and W .\nWhile there are other desirable properties of data valuation worth discussing, these three properties listed above actually pin down the form of \u03c6i up to a proportionality constant.\nProposition 2.1. Any data valuation \u03c6(D,A, V ) that satisfies properties 1-3 above must have the form\n\u03c6i = C \u2211\nS\u2286D\u2212{i}\nV (S \u222a {i})\u2212 V (S)( n\u22121 |S| ) (1)\nwhere the sum is over all subsets of D not containing i and C is an arbitrary constant. We call \u03c6i the data Shapley value of point i.\nProof. The expression of \u03c6i in Eqn. 1 is the same as the Shapley value defined in game theory, up to the constant C (Shapley, 1953; Shapley et al., 1988). This motivates calling \u03c6i the data Shapley value. The proof also follows directly from the uniqueness of the game theoretic Shapley value, by reducing our problem to a cooperative game (Dubey, 1975). In cooperative game theory, there are n players and there is a score function v : 2[n] \u2192 R. Basically v(S) is the reward if the players in subset S work together. Shapley proposed a way to divide the score among the n players so that each player receives his/her fair payment, where fairness is codified by properties that are mathematically equivalent to the three properties that we listed. We can view data valuation as a cooperative game: each training datum is a player, and the training data work together through the learner A to achieve prediction score v = V . The data Shapley value is analogous to the payment that each player receives.\nThe choice of C is an arbitrary scaling and does not affect any of our experiments and analysis.\nInterpretation of data Shapley Eqn. 1 could be interpreted as a weighted sum of all possible \u201cmarginal contributions\u201d of i; where the weight is inverse the number of subsets of size |S| in D \u2212 {i}. This formulation is close to that of leave-one-out where instead of considering the last marginal contribution V (D) \u2212 V (D \u2212 {i}), we consider each point\u2019s marginal contribution assuming that instead of the whole training set, a random subset of it is given. In other words, we can assume the scenario where instead of the train data, we were given a random subset of it; Shapley formula outputs an equitable value by capturing all these possible subset scenarios."
        },
        {
            "heading": "3. Approximating Data Shapley",
            "text": "As discussed in the previous section, the Shapley formula in Eqn. 1 uniquely provides an equitable assignment of values to data points. Computing data Shapley, however, requires computing all the possible marginal contributions which is exponentially large in the train data size. In addition, for each S \u2286 D, computing V (S) involves learning a predictor on S using the learning algorithm A. As a consequence,\ncalculating the exact Shapley value is not tractable for real world data sets. In this section, we discuss approximation methods to estimate the data Shapley value."
        },
        {
            "heading": "3.1. Approximating Shapley Value",
            "text": "As mentioned, computing the Shapley value has exponential complexity in number of data points n. Here, we discuss two methods for circumventing this problem:\nMonte-Carlo method: We can rewrite Eqn. 1 into an equivalent formulation by setting C = 1/n!. Let \u03a0 be the uniform distribution over all n! permutations of data points, we have:\n\u03c6i = E\u03c0\u223c\u03a0[V (Si\u03c0 \u222a {i})\u2212 V (Si\u03c0)] (2)\nwhere Si\u03c0 is the set of data points coming before datum i in permutation \u03c0 (Si\u03c0 = \u2205 if i is the first element).\nAs described in Eqn. 2, calculating the Shapley value can be represented as an expectation calculation problem. Therefore, Monte-Carlo method have been developed and analyzed to estimate the Shapley value (Mann & Shapley, 1962; Castro et al., 2009b; Maleki et al., 2013). First, we sample a random permutations of data points. Then, we scan the permutation from the first element to the last element and calculate the marginal contribution of every new data point. Repeating the same procedure over multiple Monte Carlo permutations, the final estimation of the data Shapley is simply the average of all the calculated marginal contributions. This Monte Carlo sampling gives an unbiased estimate of the data Shapley. In practice, we generate Monte Carlo estimates until the average has empirically converged. Previous work has analyzed error bounds of Monte-carlo approximation of Shapley value (Maleki et al., 2013). Fig 6 in Appendix A depicts examples of convergence of data Shapley. In practice, convergence is reached with number of samples on the order n; usually 3n Monte Carlo samples is sufficient for convergence.\nTruncation: In the machine learning setting, V (S) for S \u2286 N is usually the predictive performance of the model learned using S on a separate test set. Because the test set is finite, V (S) is itself an approximation to the true performance of the trained model on the test distribution, which we do not know. In practice, it is sufficient to estimate the data Shapley value up to the intrinsic noise in V (S), which can be quantified by measuring variation in the performance of the same predictor across bootstrap samples of the test set (Friedman et al., 2001). On the other hand, as the size of S increases, the change in performance by adding only one more training point becomes smaller and smaller (Mahajan et al., 2018; Beleites et al., 2013). Combining these two observations lead to a natural truncation approach.\nAlgorithm 1 Truncated Monte Carlo Shapley Input: Train data D = {1, . . . , n}, learning algorithm A, performance score V Output: Shapley value of training points: \u03c61, . . . , \u03c6n Initialize \u03c6i = 0 for i = 1, . . . , n and t = 0 while Convergence criteria not met do t\u2190 t+ 1 \u03c0t: Random permutation of train data points vt0 \u2190 V (\u2205,A) for j \u2208 {1, . . . , n} do\nif |V (D)\u2212 vtj\u22121| < Performance Tolerance then vtj = v t j\u22121 else vtj \u2190 V ({\u03c0t[1], . . . , \u03c0t[j]},A) end if \u03c6\u03c0t[j] \u2190 t\u22121t \u03c6\u03c0t\u22121[j] + 1 t (v t j \u2212 vtj\u22121)\nend for end for\nWe can define a \u201cperformance tolerance\u201d based on the bootstrap variation in V . As we scan through a sampled permutation and calculate marginal contributions, we truncate the calculation of marginal contributions in a sampled permutation whenever V (S) is within the performance tolerance of V (D) and set the marginal contribution to be zero for the rest of data points in this permutation. Appendix B shows that truncation leads to substantial computational savings without introducing significant estimation bias. In the rest of the paper, we refer to the combination of truncation with Monte-Carlo as the \u201cTrunctated Monte Carlo Shapley\u201d(TMC-Shapley); described with more details in Algorithm 1."
        },
        {
            "heading": "3.2. Approximating Performance Metric V",
            "text": "For every S \u2286 D, calculating V (S) requires A to learn a new model. For a small D and a fast A\u2014e.g. logistic regression, LASSO\u2014it is possible to use the TMC-Shapley method as stated. However, in settings where the number of data points is large or the predictive model requires high computational power (e.g. deep neural networks), applying TMC-Shapley can be quite expensive. We propose two strategies to further reduce the computational cost of data Shapley for large data settings.\nGradient Shapley For a wide family of predictive models, A involves a variation of stochastic gradient descent where randomly selected batches of D update the model parameters iteratively. One simple approximation of a completely trained model in these settings is to consider training the model with only one pass through the training data; in other words, we train the model for one \u201cepoch\u201d of D. This\nAlgorithm 2 Gradient Shapley Input: Parametrized and differentiable loss function L (.; \u03b8), train data D = {1, . . . , n} , performance score function V (\u03b8) Output: Shapley value of training points: \u03c61, . . . , \u03c6n Initialize \u03c6i = 0 for i = 1, . . . , n and t = 0 while Convergence criteria not met do t\u2190 t+ 1 \u03c0t: Random permutation of train data points \u03b8t0 \u2190 Random parameters vt0 \u2190 V (\u03b8t0) for j \u2208 {1, . . . , n} do \u03b8tj \u2190 \u03b8tj\u22121 \u2212 \u03b1\u2207\u03b8L (\u03c0t[j]; \u03b8j\u22121) vtj \u2190 V (\u03b8tj) \u03c6\u03c0t[j] \u2190 t\u22121t \u03c6\u03c0t\u22121[j] + 1 t (v t j \u2212 vtj\u22121)\nend for end for\napproximation fits nicely within the framework of Algorithm 1: for a sampled permutation of data points, update the model by performing gradient descent on one data point at a time; the marginal contribution is the change in model\u2019s performance. Details are described in Algorithm 2, which we call Gradient Shapley or G-Shapley for short. In order to have the best approximation, we perform hyper-parameter search for the learning algorithm to find the one resulting best performance for a model trained on only one pass of the data which, in our experiments, result in learning rates bigger than ones used for multi-epoch model training. Appendix D discusses numerical examples of how good of an approximation G-Shapley method yields in this work\u2019s experimental results.\nValue of groups of data points In many settings, in order to have more robust interpretations or because the training set is very large, we prefer to compute the data Shapley for groups of data points rather than for individual data. For example, in a heart disease prediction setting, we could group the patients into discrete bins based on age, gender, ethnicity and other features, and then quantify the data Shapley of each bin. In these settings, we can calculate the Shapley value of a group using the same procedure as Algorithm 1, replacing the data point i by group i. As a consequence, even for a very large data set, we can calculate the group Shapley value if the number of groups is reasonable."
        },
        {
            "heading": "4. Experiments & Applications",
            "text": "In this section, we demonstrate the estimation and applications of data Shapley across systematic experiments on real and synthetic data. We show that points with high Shapley value are critical for model\u2019s performance and vice\nversa. We then discuss the effect of acquiring new data points similar to highly valued training points compared to acquiring new data randomly. Moreover we conduct two experiments showing that data points that are noisy or have label corruption will be assigned low Shapley value. Lastly we demonstrate that Shapley values can also give informative scores for groups of individuals. Taken together, these experiments suggest that, in addition to its equitable properties, data Shapley provides meaningful values to quantify the importance of data and can inform downstream analysis. Given that leverage and influence scores seek to approximate leave-one-out score (Koh & Liang, 2017), throughout the experiments, we focus on comparing the performance of the Shapley methods to that of the leave-one-out (LOO) method. LOO is computed as the difference in the model performance V between the model trained on the full dataset with and without the point of interest.\nIn all of the following experiments, we have a train set, a separate test set used for calculating V , and a held-out set used for reporting the final results of each figure. Our convergence criteria for TMC-Shapley and G-Shapley is 1 n \u2211n i=1 |\u03c6ti\u2212\u03c6 t\u2212100 i | |\u03c6ti|\n< 0.05. For all the experiments, calculating data Shapley values took less than 24 hours on four machines running in parallel (each with 4 cpus) except for one of the experiments where the model is a Conv-Net for which 4 GPUs were utilized in parallel for 120 hours. It should be mentioned that both data Shapley algorithms are parallelizable up to the number of iterations and therefore, the computations can become faster using more machines in parallel."
        },
        {
            "heading": "4.1. Data Shapley for Disease Prediction",
            "text": "In this experiment, we use the UK Biobank data set (Sudlow et al., 2015); the task is predicting whether an individual will be diagnosed with Malignant neoplasm of breast and skin (ICD10 codes C50 and C44, binary classification) using 285 features. Balanced binary data sets for each task are created and we use 1000 individuals for the task of training. Logistic regression yields a test accuracy of 68.7% and 56.4% for breast and skin cancer prediction, respectively. Performance is computed as the accuracy of trained model on 1000 separate patients. The varying accuracies for the two tasks allow us to investigate data Shapley for classifiers that are more or less accurate. We first compute the TMCShapley, G-Shapley, and leave-one-out values. The TMCShapley converges in 4000 Monte Carlo iterations for both tasks while G-Shapley is already converged at iteration 1500. Appendix A shows examples of convergence for randomly selected data points in the train sets.\nImportance of valuable datum After calculating data values, we remove data points from the training set starting\nfrom the most valuable datum to the least valuable and train a new model each time. Fig. 1(a) shows the change in the performance as valuable data points are thrown away; points that data Shapley considers valuable are crucially important for the model performance while leave-one-out valuation is only slightly better than random valuation (i.e. removing random points). Fig. 1(b) depicts the results for the opposite setting; we remove data points starting from the least valuable. Interestingly points with low Shapley value in these training set actually harm the model\u2019s performance and removing them will improve accuracy.\nAcquiring new data Looking at which type of train data have high Shapley value and inform us how to collect new data\u2014by recruiting similar individuals\u2014in order to improve the model performance. Let\u2019s consider the following practical scenario: we want to add a number of new patients to the training data to improve our model. Adding an individual carries a cost, so we have to choose among a pool of 2000 candidates. We run two experiments: first we try to add points that are similar to high value training points and then we repeat the same experiment by adding low value points. To this end, we fit a Random Forest regression model to the calculated data Shapley values. The regression model learns to predict a data point\u2019s value given its observables. Using the trained regression model, we estimate the value of patients in the patient pool. Fig. 1(c) depicts how the model performance changes as we add patients with high estimated value to our training set; the model\u2019s performance increases more effectively than adding new patients randomly. Considering the opposite case, Fig. 1(d) shows that by choosing the wrong patients to add, we can actually hurt the current model\u2019s performance."
        },
        {
            "heading": "4.2. Synthetic Data",
            "text": "We use synthetic data to further analyze Shapley values. The data generating process is as follows. First, features are sampled from a 50-dimensional Gaussian distribution N (0, I) . Each sample i\u2019s label is then assigned a binary label yi where P (yi = 1) = f() for a function f(.). We create to sets of data sets: 20 data sets were feature-label relationship is linear (linear f(.)) , and 20 data sets where f(.) is a third order polynomial. For the first sets of data set we us a logistic regression model and for the second set we use both a logistic regression and a neural network with one hidden layer. We then start removing training points from the most valuable to the least valuable and track the change in model performance. Fig. 2 shows the results for using train data size of 100 and 1000; for all of the settings, the Shapley valuation methods do a better job than the leaveone-out in determining datum with the most positive effect on model performance. Note here that Shapley value is always dependent on the chosen model: in a dataset with\nnon-linear feature-label relationship, data points that will improve a non-linear model\u2019s performance, can be harmful to a linear model and therefore valueless."
        },
        {
            "heading": "4.3. Label Noise",
            "text": "Labeling data sets using crowd-sourcing is susceptible to mistakes (Fre\u0301nay & Verleysen, 2014) and mislabeling the data can be used as a simple data poisoning method (Steinhardt et al., 2017). In this experiment, given a train data with noisy labels, we check and correct the mislabeled examples by inspecting the data points from the least valuable to the most valuable as we expect the mislabeled examples to be among the lowest valuable points (some have negative Shapley value). Fig. 3 shows the effectiveness of this method using TMC-Shapley, Gradient-Shapley (If applicable), and leave-one-out methods compared to the random inspection benchmark. We run the experiment for three different data sets and three different predictive models. In the first experiment, we use the spam classification data set (Metsis et al., 2006). 3000 data points are used for training a Multinomial Naive Bayes model that takes the bag of words representation of emails as input. We randomly flip the label for 20% of training points. TMC-Shapley converges in 5000\niterations.In the next experiment, we use the flower image classification data set1 with 5 different classes. We pass the flower images through Inception-V3 model and train a multinomial logistic regression model on the learned network\u2019s representation of 1000 images where 10% of the images are mislabeled. Both Shapley algorithms converge in 2000 iterations. At last, we train a convolutional neural network with one convolutional and two feed-forward layers on 1000 images from the Fashion MNIST data set(Xiao et al., 2017) to classify T-Shirts and Tops against Shirts. 10% of data points have flipped labels. TMC-Shapley and G-Shapley both converge in 2000 iterations. The value is computed on separate sets of size 1000. Fig. 3 displays the results. Fig 4 shows the 5 least TMC-Shapley valued images for Flowers and Fashion MNIST data sets where all are mislabeled examples."
        },
        {
            "heading": "4.4. Data Quality and Value",
            "text": "In this experiment, we used the Dog vs. Fish data set introduced in (Koh & Liang, 2017). For each class, 1200 images are extracted from Imagenet (Russakovsky et al., 2015). We\n1adapted from https://goo.gl/Xgr1a1\nuse a state of the art Inception-v3 network(Szegedy et al., 2016) with all layers but the top layer frozen. 100 images are randomly sampled as the training set and 1000 images are used to compute the value function. We corrupt 10% of train data by adding white noise and compute the average TMC-Shapley value of clean and noisy images and repeat the same experiment with different levels of noise. As it is shown in Fig. 5(a), as the noise level increases (the data quality drops), the data Shapley value of the noisy images decreases."
        },
        {
            "heading": "4.5. Group Shapley Value",
            "text": "In this experiment, we use a balanced subset of the hospital readmission data set (Strack et al., 2014) for binary prediction of a patient\u2019s readmission. We group patients into 146 groups by intersections of demographic features of gender, race, and age. A gradient boosting classifier trained on a train set of size 60000 yields and accuracy of 58.4%. We then calculate the TMC-Shapey values of groups. Fig 5(b) shows that the most valuable groups are also the most important ones for model\u2019s performance. In addition to computational efficiency, an important advantage of group Shapley is its easy interpretations. For instance, in this data set, groups of older patients have higher value than younger ones, racial minorities get less value, and groups of females tend to be more valuable than males with respect to data Shapley, and so forth."
        },
        {
            "heading": "5. Related Works",
            "text": "Shapley value was proposed in a classic paper in game theory (Shapley, 1953) and has been widely influential in economics (Shapley et al., 1988). It has been applied to analyze and model diverse problems including voting, resource allocation and bargaining (Milnor & Shapley, 1978; Gul, 1989). To the best of our knowledge, Shapley value has not been used to quantify data value in a machine learning context like ours. Shapley value has been recently proposed as a feature importance score for the purpose of interpreting black-box predictive models (Kononenko et al., 2010; Datta et al., 2016; Lundberg & Lee, 2017; Cohen et al., 2007; Chen et al., 2018; Lundberg et al., 2018). Their goal is to quantify, for a given prediction, which features are the most influential for the model output. Our goal is very different in that we aim to quantify the value of individual data points (not features). There is also a literature in estimating Shapley value using Monte Carlo methods, network approximations, as well as analytically solving Shapley value in specialized settings (Fatima et al., 2008; Michalak et al., 2013; Castro et al., 2009a; Maleki et al., 2013; Hamers et al., 2016)\nIn linear regression, Cook\u2019s Distance measures the effect of deleting one point on the regression model (Cook, 1977). Leverage and influence are related notions that measures how perturbing each point affects the model parameters and model predictions on other data (Cook & Weisberg, 1982; Koh & Liang, 2017). These methods, however, do not\nsatisfy any equitability conditions, and also have been shown to have robustness issues (Ghorbani et al., 2017). In the broad discourse, value of data and how individuals should be compensated has been intensely discussed by economists and policy makers along with the discussion of incentivizing participants to generate useful data.(Arrieta Ibarra et al., 2017; Posner & Weyl, 2018)"
        },
        {
            "heading": "6. Discussion",
            "text": "We proposed data Shapley as an equitable framework to quantify the value of individual training datum for the learning algorithm. Data Shapley uniquely satisfies three natural properties of equitable data valuation. There are ML settings where these properties may not be desirable and perhaps other properties need to be added. It is a very important direction of future work to clearly understand these different scenarios and study the appropriate notions of data value. Drawing on the connections from economics, we believe the three properties we listed is a reasonable starting point. While our experiments demonstrate several desirable features of data Shapley, we should interpret it with care. Due to the space limit, we have skipped over many important\nconsiderations about the intrinsic value of personal data, and we focused on valuation in the very specific context of training set for supervised learning algorithms. We acknowledge that there are nuances in the value of data\u2014e.g. privacy, personal association\u2014not captured by our framework. Moreover we do not propose that people should be exactly compensated by their data Shapley value; we believe data Shapley is more useful for the quantitative insight it provides.\nIn the data Shapley framework, the value of individual datum depends on the learning algorithm, evaluation metric as well as other data points in the training set. Therefore when we discuss data with high (or low) Shapley value, all of this context is assumed to be given. A datum that is not valuable for one context could be very valuable if the context changes. Understanding how data Shapley behaves for different learning functions and metrics is an interesting direction of follow up work."
        }
    ],
    "title": "Data Shapley: Equitable Valuation of Data for Machine Learning",
    "year": 2019
}